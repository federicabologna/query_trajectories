{
    "query": "is membership inference attack",
    "user_id": "lib_user",
    "task_id": "edd3da48-ed66-4e87-9266-e3a6e5647554",
    "timestamp": "2025-06-23T23:32:26.388537",
    "n_retrieval": 256,
    "n_retrieved": 211,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.475908,
    "decomposed_query": {
        "rewritten_query": "Membership inference attack.",
        "keyword_query": "membership inference attack",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009273,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
            "venue": "European Conference on Computer Vision",
            "year": 2022,
            "reference_count": 39,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12535",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2110279247",
                    "name": "Hongbin Liu"
                },
                {
                    "authorId": "144516687",
                    "name": "N. Gong"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Semi-supervised learning (SSL) leverages both labeled and unlabeled data to train machine learning (ML) models. State-of-the-art SSL methods can achieve comparable performance to supervised learning by leveraging much fewer labeled data. However, most existing works focus on improving the performance of SSL. In this work, we take a different angle by studying the training data privacy of SSL. Specifically, we propose the first data augmentation-based membership inference attacks against ML models trained by SSL. Given a data sample and the black-box access to a model, the goal of membership inference attack is to determine whether the data sample belongs to the training dataset of the model. Our evaluation shows that the proposed attack can consistently outperform existing membership inference attacks and achieves the best performance against the model trained by SSL. Moreover, we uncover that the reason for membership leakage in SSL is different from the commonly believed one in supervised learning, i.e., overfitting (the gap between training and testing accuracy). We observe that the SSL model is well generalized to the testing data (with almost 0 overfitting) but ''memorizes'' the training data by giving a more confident prediction regardless of its correctness. We also explore early stopping as a countermeasure to prevent membership inference attacks against SSL. The results show that early stopping can mitigate the membership inference attack, but with the cost of model's utility degradation.",
            "corpus_id": 251066729,
            "sentences": [
                {
                    "corpus_id": "251066729",
                    "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
                    "text": "In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member).",
                    "score": 0.777599274206481,
                    "section_title": "Conventional Membership Inference Attacks",
                    "char_start_offset": 9690,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 529
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                }
            ],
            "relevance_judgement": 0.99951171875,
            "relevance_judgment_input_expanded": "# Title: Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning\n# Venue: European Conference on Computer Vision\n# Authors: Xinlei He, Hongbin Liu, N. Gong, Yang Zhang\n## Abstract\nSemi-supervised learning (SSL) leverages both labeled and unlabeled data to train machine learning (ML) models. State-of-the-art SSL methods can achieve comparable performance to supervised learning by leveraging much fewer labeled data. However, most existing works focus on improving the performance of SSL. In this work, we take a different angle by studying the training data privacy of SSL. Specifically, we propose the first data augmentation-based membership inference attacks against ML models trained by SSL. Given a data sample and the black-box access to a model, the goal of membership inference attack is to determine whether the data sample belongs to the training dataset of the model. Our evaluation shows that the proposed attack can consistently outperform existing membership inference attacks and achieves the best performance against the model trained by SSL. Moreover, we uncover that the reason for membership leakage in SSL is different from the commonly believed one in supervised learning, i.e., overfitting (the gap between training and testing accuracy). We observe that the SSL model is well generalized to the testing data (with almost 0 overfitting) but ''memorizes'' the training data by giving a more confident prediction regardless of its correctness. We also explore early stopping as a countermeasure to prevent membership inference attacks against SSL. The results show that early stopping can mitigate the membership inference attack, but with the cost of model's utility degradation.\n## Conventional Membership Inference Attacks\nIn membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member).",
            "reference_string": "[251066729 | He et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Towards Demystifying Membership Inference Attacks",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 49,
            "citation_count": 112,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.09173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "25121568",
                    "name": "Stacey Truex"
                },
                {
                    "authorId": "46458150",
                    "name": "Ling Liu"
                },
                {
                    "authorId": "2327300",
                    "name": "M. E. Gursoy"
                },
                {
                    "authorId": "2112532900",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "47747953",
                    "name": "Wenqi Wei"
                }
            ],
            "abstract": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
            "corpus_id": 50778569,
            "sentences": [
                {
                    "corpus_id": "50778569",
                    "title": "Towards Demystifying Membership Inference Attacks",
                    "text": "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy [4], [43], [14] or secure multiparty computation [44], [10], [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety. \n\nFigure 1 illustrates the workflow of membership inference attack development. Given a training dataset D and a classification model F t trained on D, the machine learning service provider may provide a classification service through a prediction API. This API offers users black box access to the model F t . Users may send prediction queries with their own data to the service and receive classification predictions. An adversary uses such a service to collect information about the private dataset D on which the prediction model F t was privately trained. By leveraging any public or background knowledge of the training dataset D or the target model F t , an adversary builds a membership inference attack model F a to deploy for launching membership inference attacks in real time. \n\nTo gain an in-depth understanding of the general formulation of the membership inference attack model, we first characterize the types of adversarial knowledge and datasets required to train the attack model as well as the attack cost, the attack value, and their evaluation metrics. We then present a systematic formulation of general attacks in Section 3.",
                    "score": 0.6730724577955665,
                    "section_title": "MEMBERSHIP INFERENCE ATTACKS",
                    "char_start_offset": 7016,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 778
                        },
                        {
                            "start": 781,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1567
                        },
                        {
                            "start": 1570,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 1927
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 558,
                            "end": 561,
                            "matchedPaperCorpusId": "15331398"
                        },
                        {
                            "start": 563,
                            "end": 567,
                            "matchedPaperCorpusId": "589685"
                        },
                        {
                            "start": 569,
                            "end": 573,
                            "matchedPaperCorpusId": "2887752"
                        },
                        {
                            "start": 607,
                            "end": 611,
                            "matchedPaperCorpusId": "6942138"
                        },
                        {
                            "start": 613,
                            "end": 617,
                            "matchedPaperCorpusId": "18290778"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                },
                {
                    "corpus_id": "50778569",
                    "title": "Towards Demystifying Membership Inference Attacks",
                    "text": "We have presented the first generalized framework for the development of a membership inference attack model. This general formulation enables an in-depth characterization of membership infernece attacks against different types of machine learning models. Through extensive experimentation and empirical evidence, we show when and why machine learning models may be vulnerable to membership inference attacks. By exploring a variety of machine learning model types and their correlations with respect to the three phases of the attack generation process, we present five interesting characteristics of membership inference attacks: (1) they are data-driven attacks, (2) attack models are transferable, (3) target model type is a strong indicator of model vulnerability, (4) attack data generation techniques need not explicitly mirror the target model, and (5) membership inference attacks can persist as insider attacks in federated systems. We also include a discussion on countermeasures and mitigation methods against membership inference attacks. \n\nOur research on membership inference attacks and membership privacy continues along several dimensions. First, we are engaged in the development of countermeasures and defense methods. Second, we are currently studying the scale and diversity of membership inference attacks in federated and collaborative learning systems. Third, we are investigating the complex relationships between membership inference attacks, membership privacy, and differential privacy.",
                    "score": 0.5883568965044308,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 63292,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1051
                        },
                        {
                            "start": 1054,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1515
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "50778569",
                    "title": "Towards Demystifying Membership Inference Attacks",
                    "text": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
                    "score": 0.5641068263866128,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "50778569",
                    "title": "Towards Demystifying Membership Inference Attacks",
                    "text": "At the most abstract level, membership inference attack models are binary classifiers. Given an instance x and a target model F t , the goal of a membership inference attack model is to identify whether or not x was contained within the dataset D used to train F t . \n\nLet D consist of n training instances (x 1 , y 1 ), (x 2 , y 2 ), ..., (x n , y n ) where x i consists of m features, denoted by x i = (x i,1 , x i,2 , ..., x i,m ), and y i \u2208 Z k , where k is a finite integer value \u2265 2. Let F t : R m \u2192 R k be the target model trained using this dataset D. Given a particular feature vector x \u2208 R m , F t will then output a probability vector p \u2208 R k of the form p = (p 1 , p 2 , ..., p k ), where p i \u2208 [0, 1]\u2200i, and k i=1 p i = 1. The prediction class label y for a feature vector x is the class with highest probability value in p. Therefore y = arg max i \u2208Z k F t (x). \n\nGiven the adversary's black-box access to F t via the prediction service API, an adversary is able to query F t with any number of instances to receive corresponding probability vectors. The adversary uses this probing access, along with any prior knowledge, to generate I , a representation of adversarial knowledge of D. The first building block for implementing a black-box membership inference attack is to leverage I to generate a synthetic labeled dataset D \u2032 to mirror the data in D. This synthetic, labeled dataset D \u2032 is artificially simulated and called a shadow dataset of D. Although the word \"shadow\" was borrowed from shadow copying for systems creating back up data copies [39], the shadow dataset in our context should be thought of as a synthetic version of the real training dataset D. D \u2032 is then used to generate an attack training dataset D * , which is required to train the final membership attack model, a binary classifier F a .",
                    "score": 0.5593418093721354,
                    "section_title": "GENERAL ATTACK FORMULATION",
                    "char_start_offset": 16014,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 266
                        },
                        {
                            "start": 269,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 875
                        },
                        {
                            "start": 878,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1831
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.99951171875,
            "relevance_judgment_input_expanded": "# Title: Towards Demystifying Membership Inference Attacks\n# Venue: arXiv.org\n# Authors: Stacey Truex, Ling Liu, M. E. Gursoy, Lei Yu, Wenqi Wei\n## Abstract\nMembership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.\n## MEMBERSHIP INFERENCE ATTACKS\nIn this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy [4], [43], [14] or secure multiparty computation [44], [10], [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety. \n\nFigure 1 illustrates the workflow of membership inference attack development. Given a training dataset D and a classification model F t trained on D, the machine learning service provider may provide a classification service through a prediction API. This API offers users black box access to the model F t . Users may send prediction queries with their own data to the service and receive classification predictions. An adversary uses such a service to collect information about the private dataset D on which the prediction model F t was privately trained. By leveraging any public or background knowledge of the training dataset D or the target model F t , an adversary builds a membership inference attack model F a to deploy for launching membership inference attacks in real time. \n\nTo gain an in-depth understanding of the general formulation of the membership inference attack model, we first characterize the types of adversarial knowledge and datasets required to train the attack model as well as the attack cost, the attack value, and their evaluation metrics. We then present a systematic formulation of general attacks in Section 3.\n\n## GENERAL ATTACK FORMULATION\nAt the most abstract level, membership inference attack models are binary classifiers. Given an instance x and a target model F t , the goal of a membership inference attack model is to identify whether or not x was contained within the dataset D used to train F t . \n\nLet D consist of n training instances (x 1 , y 1 ), (x 2 , y 2 ), ..., (x n , y n ) where x i consists of m features, denoted by x i = (x i,1 , x i,2 , ..., x i,m ), and y i \u2208 Z k , where k is a finite integer value \u2265 2. Let F t : R m \u2192 R k be the target model trained using this dataset D. Given a particular feature vector x \u2208 R m , F t will then output a probability vector p \u2208 R k of the form p = (p 1 , p 2 , ..., p k ), where p i \u2208 [0, 1]\u2200i, and k i=1 p i = 1. The prediction class label y for a feature vector x is the class with highest probability value in p. Therefore y = arg max i \u2208Z k F t (x). \n\nGiven the adversary's black-box access to F t via the prediction service API, an adversary is able to query F t with any number of instances to receive corresponding probability vectors. The adversary uses this probing access, along with any prior knowledge, to generate I , a representation of adversarial knowledge of D. The first building block for implementing a black-box membership inference attack is to leverage I to generate a synthetic labeled dataset D \u2032 to mirror the data in D. This synthetic, labeled dataset D \u2032 is artificially simulated and called a shadow dataset of D. Although the word \"shadow\" was borrowed from shadow copying for systems creating back up data copies [39], the shadow dataset in our context should be thought of as a synthetic version of the real training dataset D. D \u2032 is then used to generate an attack training dataset D * , which is required to train the final membership attack model, a binary classifier F a .\n\n## CONCLUSION\nWe have presented the first generalized framework for the development of a membership inference attack model. This general formulation enables an in-depth characterization of membership infernece attacks against different types of machine learning models. Through extensive experimentation and empirical evidence, we show when and why machine learning models may be vulnerable to membership inference attacks. By exploring a variety of machine learning model types and their correlations with respect to the three phases of the attack generation process, we present five interesting characteristics of membership inference attacks: (1) they are data-driven attacks, (2) attack models are transferable, (3) target model type is a strong indicator of model vulnerability, (4) attack data generation techniques need not explicitly mirror the target model, and (5) membership inference attacks can persist as insider attacks in federated systems. We also include a discussion on countermeasures and mitigation methods against membership inference attacks. \n\nOur research on membership inference attacks and membership privacy continues along several dimensions. First, we are engaged in the development of countermeasures and defense methods. Second, we are currently studying the scale and diversity of membership inference attacks in federated and collaborative learning systems. Third, we are investigating the complex relationships between membership inference attacks, membership privacy, and differential privacy.",
            "reference_string": "[50778569 | Truex et al. | 2018 | Citations: 112]"
        },
        {
            "title": "Membership Inference Attacks against Diffusion Models",
            "venue": "2023 IEEE Security and Privacy Workshops (SPW)",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 60,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.03262",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.03262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166181346",
                    "name": "Tomoya Matsumoto"
                },
                {
                    "authorId": "2204651737",
                    "name": "Takayuki Miura"
                },
                {
                    "authorId": "34800838",
                    "name": "Naoto Yanai"
                }
            ],
            "abstract": "Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.",
            "corpus_id": 256627812,
            "sentences": [
                {
                    "corpus_id": "256627812",
                    "title": "Membership Inference Attacks against Diffusion Models",
                    "text": "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model [21]- [23]. A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models [15], [16], [18]. There are two settings [14], i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model.\n\nA typical approach for membership inference attacks is to leverage the large divergence between the loss distribution over members and non-members [24]. The divergence can be embedded by an adversary. For instance, privacy leakage, including membership inference attacks, can be more effective by training a model with poisoning samples [25]- [28].\n\nMembership inference attacks can be prevented by differential privacy [29] where gradients are perturbed [30], [31]. Since differential privacy often deteriorates inference accuracy, several works evaluated differential privacy on membership inference attacks in a quantitative fashion [32], [33].",
                    "score": 0.7631695184132121,
                    "section_title": "A. Membership Inference",
                    "char_start_offset": 4305,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 156,
                            "end": 160,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 162,
                            "end": 166,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 346,
                            "end": 350,
                            "matchedPaperCorpusId": "251135031"
                        },
                        {
                            "start": 352,
                            "end": 356,
                            "matchedPaperCorpusId": "251765197"
                        },
                        {
                            "start": 358,
                            "end": 362,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 387,
                            "end": 391,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 694,
                            "end": 698,
                            "matchedPaperCorpusId": "201598829"
                        },
                        {
                            "start": 884,
                            "end": 888,
                            "matchedPaperCorpusId": "247922814"
                        },
                        {
                            "start": 890,
                            "end": 894,
                            "matchedPaperCorpusId": "231718884"
                        },
                        {
                            "start": 967,
                            "end": 971,
                            "matchedPaperCorpusId": "2565493"
                        },
                        {
                            "start": 1002,
                            "end": 1006,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 1008,
                            "end": 1012,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1183,
                            "end": 1187,
                            "matchedPaperCorpusId": "173990571"
                        },
                        {
                            "start": 1189,
                            "end": 1193,
                            "matchedPaperCorpusId": "219686822"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks against Diffusion Models\n# Venue: 2023 IEEE Security and Privacy Workshops (SPW)\n# Authors: Tomoya Matsumoto, Takayuki Miura, Naoto Yanai\n## Abstract\nDiffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., timesteps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then show that the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of timesteps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify that DDIM is vulnerable to the attack for small sample sizes instead of achieving a lower FID. Second, sampling steps in hyperparameters are important for resistance to the attack, whereas the impact of sampling variances is quite limited.\n## A. Membership Inference\nA membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model [21]- [23]. A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models [15], [16], [18]. There are two settings [14], i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model.\n\nA typical approach for membership inference attacks is to leverage the large divergence between the loss distribution over members and non-members [24]. The divergence can be embedded by an adversary. For instance, privacy leakage, including membership inference attacks, can be more effective by training a model with poisoning samples [25]- [28].\n\nMembership inference attacks can be prevented by differential privacy [29] where gradients are perturbed [30], [31]. Since differential privacy often deteriorates inference accuracy, several works evaluated differential privacy on membership inference attacks in a quantitative fashion [32], [33].",
            "reference_string": "[256627812 | Matsumoto et al. | 2023 | Citations: 60]"
        },
        {
            "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
            "venue": "IEEE Transactions on Dependable and Secure Computing",
            "year": 2020,
            "reference_count": 64,
            "citation_count": 25,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.08016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146281955",
                    "name": "Hongwei Huang"
                },
                {
                    "authorId": "144644709",
                    "name": "Weiqi Luo"
                },
                {
                    "authorId": "48035994",
                    "name": "Guoqiang Zeng"
                },
                {
                    "authorId": "145369053",
                    "name": "J. Weng"
                },
                {
                    "authorId": "2145913542",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2145970",
                    "name": "Anjia Yang"
                }
            ],
            "abstract": "Deep Learning (DL) techniques allow ones to train models from a dataset to solve tasks. DL has attracted much interest given its fancy performance and potential market value, while security issues are amongst the most colossal concerns. However, the DL models may be prone to the membership inference attack, where an attacker determines whether a given sample is from the training dataset. Efforts have been made to hinder the attack but unfortunately, they may lead to a major overhead or impaired usability. In this article, we propose and implement DAMIA, leveraging Domain Adaptation (DA) as a defense aginist membership inference attacks. Our observation is that during the training process, DA obfuscates the dataset to be protected using another relate and similar dataset, and derives a model that underlyingly extracts the features from both datasets. Seeing that the model is obfuscated, membership inference fails, while the extracted features provide supports for usability. Extensive experiments have been conducted to validates our intuition. The model trained by DAMIA has a negligible footprint to the usability and introduces slight overhead compared with other defenses. Our experiment also excludes factors that may hinder the performance of DAMIA, and comparisons with other defenses, providing a potential guideline to vendors and researchers to benefit from our solution in a timely manner.",
            "corpus_id": 218674569,
            "sentences": [
                {
                    "corpus_id": "218674569",
                    "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
                    "text": "Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19,20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice. \n\nMathematically, a victim DL model M trained on a dataset D, which handles a classification task, categorizing all its inputs into n categories. Any sample s i fed into M will result in an output o i = (p 1 , p 2 , ..., p j , ..., p n ), where p j is the confidence score, indicating the probability of being a sample of the category j. To deploy the membership inference attack, an attacker may train a binary classifier M adv to determine whether a sample s i is from the training data D. member or not = M adv (o i ) \n\nA simple but efficient way to initiate a membership inference attack is to build a linear binary classifier M lin. adv using a confidence threshold P thresh as a decision boundary, which is selected based on extensive experiments. In particular, one may enumerate all possible confidence scores to find the P thresh , which can achieve the maximum accuracy of membership inference [21]. With P thresh , if the confidence score of a sample for a specific category k (1 \u2264 k \u2264 n) is higher than P thresh , M lin. \n\nadv will determine the sample as a member of the training set. Formally, the procedure can be defined as follows: \n\nNoted that 1 stands for member and 0 stands for nonmember. \n\nCompared with other attacks such as model inversion [7,22], the impact of membership inference attack is relatively minor. However, membership inference is easier to deploy and requires less information on the victim model compared with other attacks.",
                    "score": 0.6588676017733928,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 8710,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 738
                        },
                        {
                            "start": 741,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1259
                        },
                        {
                            "start": 1262,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1492
                        },
                        {
                            "start": 1493,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1771
                        },
                        {
                            "start": 1774,
                            "end": 1836
                        },
                        {
                            "start": 1837,
                            "end": 1887
                        },
                        {
                            "start": 1890,
                            "end": 1948
                        },
                        {
                            "start": 1951,
                            "end": 2073
                        },
                        {
                            "start": 2074,
                            "end": 2202
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1643,
                            "end": 1647,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 2003,
                            "end": 2006,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 2006,
                            "end": 2009,
                            "matchedPaperCorpusId": "207941008"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks\n# Venue: IEEE Transactions on Dependable and Secure Computing\n# Authors: Hongwei Huang, Weiqi Luo, Guoqiang Zeng, J. Weng, Yue Zhang, Anjia Yang\n## Abstract\nDeep Learning (DL) techniques allow ones to train models from a dataset to solve tasks. DL has attracted much interest given its fancy performance and potential market value, while security issues are amongst the most colossal concerns. However, the DL models may be prone to the membership inference attack, where an attacker determines whether a given sample is from the training dataset. Efforts have been made to hinder the attack but unfortunately, they may lead to a major overhead or impaired usability. In this article, we propose and implement DAMIA, leveraging Domain Adaptation (DA) as a defense aginist membership inference attacks. Our observation is that during the training process, DA obfuscates the dataset to be protected using another relate and similar dataset, and derives a model that underlyingly extracts the features from both datasets. Seeing that the model is obfuscated, membership inference fails, while the extracted features provide supports for usability. Extensive experiments have been conducted to validates our intuition. The model trained by DAMIA has a negligible footprint to the usability and introduces slight overhead compared with other defenses. Our experiment also excludes factors that may hinder the performance of DAMIA, and comparisons with other defenses, providing a potential guideline to vendors and researchers to benefit from our solution in a timely manner.\n## Membership Inference Attacks\nMembership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19,20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice. \n\nMathematically, a victim DL model M trained on a dataset D, which handles a classification task, categorizing all its inputs into n categories. Any sample s i fed into M will result in an output o i = (p 1 , p 2 , ..., p j , ..., p n ), where p j is the confidence score, indicating the probability of being a sample of the category j. To deploy the membership inference attack, an attacker may train a binary classifier M adv to determine whether a sample s i is from the training data D. member or not = M adv (o i ) \n\nA simple but efficient way to initiate a membership inference attack is to build a linear binary classifier M lin. adv using a confidence threshold P thresh as a decision boundary, which is selected based on extensive experiments. In particular, one may enumerate all possible confidence scores to find the P thresh , which can achieve the maximum accuracy of membership inference [21]. With P thresh , if the confidence score of a sample for a specific category k (1 \u2264 k \u2264 n) is higher than P thresh , M lin. \n\nadv will determine the sample as a member of the training set. Formally, the procedure can be defined as follows: \n\nNoted that 1 stands for member and 0 stands for nonmember. \n\nCompared with other attacks such as model inversion [7,22], the impact of membership inference attack is relatively minor. However, membership inference is easier to deploy and requires less information on the victim model compared with other attacks.",
            "reference_string": "[218674569 | Huang et al. | 2020 | Citations: 25]"
        },
        {
            "title": "Membership Inference Attacks From First Principles",
            "venue": "IEEE Symposium on Security and Privacy",
            "year": 2021,
            "reference_count": 78,
            "citation_count": 708,
            "influential_citation_count": 182,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2112.03570",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.03570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2483738",
                    "name": "Nicholas Carlini"
                },
                {
                    "authorId": "2059189068",
                    "name": "Steve Chien"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "144206374",
                    "name": "Shuang Song"
                },
                {
                    "authorId": "1763579",
                    "name": "A. Terzis"
                },
                {
                    "authorId": "2444919",
                    "name": "Florian Tram\u00e8r"
                }
            ],
            "abstract": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
            "corpus_id": 244920593,
            "sentences": [
                {
                    "corpus_id": "244920593",
                    "title": "Membership Inference Attacks From First Principles",
                    "text": "We define membership inference via a standard security game inspired by Yeom et al. [70] and Jayaraman et al. [25].\n\nDefinition 1 (Membership inference security game). The game proceeds between a challenger C and an adversary A:\n\n1) The challenger samples a training dataset D \u2190 D and trains a model f \u03b8 \u2190 T (D) on the dataset D.\n\n2) The challenger flips a bit b, and if b = 0, samples a fresh challenge point from the distribution (x, y) \u2190 D (such that (x, y) / \u2208 D). Otherwise, the challenger selects a point from the training set (x, y) \u2190 $ D.\n\n3) The challenger sends (x, y) to the adversary. 4) The adversary gets query access to the distribution D, and to the model f \u03b8 , and outputs a bitb \u2190 A D,f (x, y). 5) Output 1 ifb = b, and 0 otherwise.\n\nFor simplicity, we will write A(x, y) to denote the adversary's prediction on the sample (x, y) when the distribution D and model f are clear from context.\n\nNote that this game assumes that the adversary is given access to the underlying training data distribution D; while some attacks do not make use of this assumption [70], many attacks require query-access to the distribution in order to train \"shadow models\" [60] (as we will describe). The above game also assumes that the adversary is given access to both a training example and its ground-truth label.\n\nInstead of outputting a \"hard prediction\", all the attacks we consider output a continuous confidence score, which is then thresholded to yield a membership prediction. That is,\n\nwhere 1 is the indicator function, \u03c4 is some tunable decision threshold, and A outputs a real-valued confidence score.\n\nA first membership inference attack. For illustrative purposes, we begin by considering a very simple membership inference attack (due to Yeom et al. [70]). This attack relies on the observation that, because machine learning models are trained to minimize the loss of their training examples (see Equation 1), examples with lower loss are on average more likely to be members of the training data. Formally, the LOSS membership inference attack defines",
                    "score": 0.6558778023177336,
                    "section_title": "A. Definitions",
                    "char_start_offset": 9702,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 110,
                            "end": 114,
                            "matchedPaperCorpusId": "218862728"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                },
                {
                    "corpus_id": "244920593",
                    "title": "Membership Inference Attacks From First Principles",
                    "text": "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy. It is thus important that they can reliably succeed at this task. This section formalizes the membership inference attack security game ( \u00a7III-A), and introduces our membership inference evaluation methodology ( \u00a7III-B).",
                    "score": 0.7432533759001914,
                    "section_title": "III. MEMBERSHIP INFERENCE ATTACKS",
                    "char_start_offset": 9202,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks From First Principles\n# Venue: IEEE Symposium on Security and Privacy\n# Authors: Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, A. Terzis, Florian Tram\u00e8r\n## Abstract\nA membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.\n## III. MEMBERSHIP INFERENCE ATTACKS\nThe objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy. It is thus important that they can reliably succeed at this task. This section formalizes the membership inference attack security game ( \u00a7III-A), and introduces our membership inference evaluation methodology ( \u00a7III-B).\n\n## A. Definitions\nWe define membership inference via a standard security game inspired by Yeom et al. [70] and Jayaraman et al. [25].\n\nDefinition 1 (Membership inference security game). The game proceeds between a challenger C and an adversary A:\n\n1) The challenger samples a training dataset D \u2190 D and trains a model f \u03b8 \u2190 T (D) on the dataset D.\n\n2) The challenger flips a bit b, and if b = 0, samples a fresh challenge point from the distribution (x, y) \u2190 D (such that (x, y) / \u2208 D). Otherwise, the challenger selects a point from the training set (x, y) \u2190 $ D.\n\n3) The challenger sends (x, y) to the adversary. 4) The adversary gets query access to the distribution D, and to the model f \u03b8 , and outputs a bitb \u2190 A D,f (x, y). 5) Output 1 ifb = b, and 0 otherwise.\n\nFor simplicity, we will write A(x, y) to denote the adversary's prediction on the sample (x, y) when the distribution D and model f are clear from context.\n\nNote that this game assumes that the adversary is given access to the underlying training data distribution D; while some attacks do not make use of this assumption [70], many attacks require query-access to the distribution in order to train \"shadow models\" [60] (as we will describe). The above game also assumes that the adversary is given access to both a training example and its ground-truth label.\n\nInstead of outputting a \"hard prediction\", all the attacks we consider output a continuous confidence score, which is then thresholded to yield a membership prediction. That is,\n\nwhere 1 is the indicator function, \u03c4 is some tunable decision threshold, and A outputs a real-valued confidence score.\n\nA first membership inference attack. For illustrative purposes, we begin by considering a very simple membership inference attack (due to Yeom et al. [70]). This attack relies on the observation that, because machine learning models are trained to minimize the loss of their training examples (see Equation 1), examples with lower loss are on average more likely to be members of the training data. Formally, the LOSS membership inference attack defines",
            "reference_string": "[244920593 | Carlini et al. | 2021 | Citations: 708]"
        },
        {
            "title": "Traces of Memorisation in Large Language Models for Code",
            "venue": "International Conference on Software Engineering",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 17,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639133",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.11658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2199249915",
                    "name": "Ali Al-Kaswan"
                },
                {
                    "authorId": "145774460",
                    "name": "M. Izadi"
                },
                {
                    "authorId": "10734708",
                    "name": "Arie van Deursen"
                }
            ],
            "abstract": "Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pretraining data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",
            "corpus_id": 266362863,
            "sentences": [
                {
                    "corpus_id": "266362863",
                    "title": "Traces of Memorisation in Large Language Models for Code",
                    "text": "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself. \n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]. \n\nWe refer to the security game defined by Carlini et al. [9] to define a membership inference attack in Definition 1. In this game, the adversary wins if they have a non-negligible advantage > 1 2 + . In simpler terms, the adversary needs to be able to distinguish between data that was included and which was not included in the training data for a given model, while only being allowed query access to the model and data distribution. \n\nMembership inference attacks are primitive for measuring the leakage of a machine learning model and are often a starting point for more extensive attacks [9,26,38]. While membership inference is a weaker privacy violation than memorisation, the National Institute of Standards and Technology (NIST) still considers membership inference to be a violation of the confidentiality of training data [26]. \n\nDefinition 1 (Membership inference security game [9]). The game proceeds between a challenger C, an adversary A, a data distribution D and a model  : \n\n(1) The challenger samples a training dataset  \u2190 D and trains a model   \u2190 T () on the dataset . \n\n(2) The challenger flips a bit , and if  = 0, samples a fresh challenge point from the distribution (, ) \u2190 D (such that (, ) / \u2208 ). Otherwise, the challenger selects a point from the training set (, ) \u2190 . \n\n(3) The challenger sends (, ) to the adversary. (4) The adversary gets query access to the distribution D, and to the model   , and outputs a bit b (5) Output 1 if b = , and 0 otherwise.",
                    "score": 0.6423338102689996,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 6911,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 769
                        },
                        {
                            "start": 772,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1207
                        },
                        {
                            "start": 1210,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1610
                        },
                        {
                            "start": 1613,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1762
                        },
                        {
                            "start": 1765,
                            "end": 1860
                        },
                        {
                            "start": 1863,
                            "end": 1994
                        },
                        {
                            "start": 1995,
                            "end": 2067
                        },
                        {
                            "start": 2070,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2256
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 512,
                            "end": 516,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 613,
                            "end": 617,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 627,
                            "end": 631,
                            "matchedPaperCorpusId": "119302127"
                        },
                        {
                            "start": 828,
                            "end": 831,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1365,
                            "end": 1368,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1368,
                            "end": 1371,
                            "matchedPaperCorpusId": "232233426"
                        },
                        {
                            "start": 1371,
                            "end": 1374,
                            "matchedPaperCorpusId": "247315260"
                        },
                        {
                            "start": 1605,
                            "end": 1609,
                            "matchedPaperCorpusId": "232233426"
                        },
                        {
                            "start": 1662,
                            "end": 1665,
                            "matchedPaperCorpusId": "244920593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Traces of Memorisation in Large Language Models for Code\n# Venue: International Conference on Software Engineering\n# Authors: Ali Al-Kaswan, M. Izadi, Arie van Deursen\n## Abstract\nLarge language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pretraining data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.\n## Membership Inference Attacks\nMembership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself. \n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]. \n\nWe refer to the security game defined by Carlini et al. [9] to define a membership inference attack in Definition 1. In this game, the adversary wins if they have a non-negligible advantage > 1 2 + . In simpler terms, the adversary needs to be able to distinguish between data that was included and which was not included in the training data for a given model, while only being allowed query access to the model and data distribution. \n\nMembership inference attacks are primitive for measuring the leakage of a machine learning model and are often a starting point for more extensive attacks [9,26,38]. While membership inference is a weaker privacy violation than memorisation, the National Institute of Standards and Technology (NIST) still considers membership inference to be a violation of the confidentiality of training data [26]. \n\nDefinition 1 (Membership inference security game [9]). The game proceeds between a challenger C, an adversary A, a data distribution D and a model  : \n\n(1) The challenger samples a training dataset  \u2190 D and trains a model   \u2190 T () on the dataset . \n\n(2) The challenger flips a bit , and if  = 0, samples a fresh challenge point from the distribution (, ) \u2190 D (such that (, ) / \u2208 ). Otherwise, the challenger selects a point from the training set (, ) \u2190 . \n\n(3) The challenger sends (, ) to the adversary. (4) The adversary gets query access to the distribution D, and to the model   , and outputs a bit b (5) Output 1 if b = , and 0 otherwise.",
            "reference_string": "[266362863 | Al-Kaswan et al. | 2023 | Citations: 17]"
        },
        {
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 61,
            "citation_count": 25,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2208.10445",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.10445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2110631311",
                    "name": "Weilin Xu"
                },
                {
                    "authorId": "35372584",
                    "name": "Cory Cornelius"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.",
            "corpus_id": 251719178,
            "sentences": [
                {
                    "corpus_id": "251719178",
                    "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
                    "text": "Membership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
                    "score": 0.5785727300903916,
                    "section_title": "Related Work",
                    "char_start_offset": 53208,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1711
                        },
                        {
                            "start": 1714,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2092
                        },
                        {
                            "start": 2093,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 266,
                            "end": 270,
                            "matchedPaperCorpusId": "3052834"
                        },
                        {
                            "start": 273,
                            "end": 276,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 276,
                            "end": 279,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 279,
                            "end": 282,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 282,
                            "end": 285,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 285,
                            "end": 288,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 288,
                            "end": 291,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 307,
                            "end": 311,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 675,
                            "end": 679,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 804,
                            "end": 808,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 829,
                            "end": 833,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1000,
                            "end": 1004,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1378,
                            "end": 1382,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1409,
                            "end": 1412,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "214743021"
                        },
                        {
                            "start": 1886,
                            "end": 1889,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 1909,
                            "end": 1912,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 1912,
                            "end": 1915,
                            "matchedPaperCorpusId": "3052834"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models\n# Venue: arXiv.org\n# Authors: Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, Yang Zhang\n## Abstract\nMachine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.\n## Related Work\nMembership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
            "reference_string": "[251719178 | He et al. | 2022 | Citations: 25]"
        },
        {
            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 98,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.05429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2054749404",
                    "name": "Rui Wen"
                },
                {
                    "authorId": "2127727861",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2117688523",
                    "name": "Yun Shen"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.",
            "corpus_id": 231861713,
            "sentences": [
                {
                    "corpus_id": "231861713",
                    "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
                    "text": "Membership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
                    "score": 0.5766003845724906,
                    "section_title": "Related Work",
                    "char_start_offset": 35696,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 218,
                            "end": 221,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 221,
                            "end": 224,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 224,
                            "end": 227,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "203593891"
                        },
                        {
                            "start": 230,
                            "end": 233,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 395,
                            "end": 399,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 786,
                            "end": 790,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 834,
                            "end": 838,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 947,
                            "end": 951,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1033,
                            "end": 1037,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1195,
                            "end": 1198,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1198,
                            "end": 1201,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1201,
                            "end": 1204,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1271,
                            "end": 1275,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1285,
                            "end": 1289,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1312,
                            "end": 1316,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1367,
                            "end": 1371,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1611,
                            "end": 1615,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 1615,
                            "end": 1618,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1618,
                            "end": 1621,
                            "matchedPaperCorpusId": "208139345"
                        },
                        {
                            "start": 1814,
                            "end": 1818,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1927,
                            "end": 1931,
                            "matchedPaperCorpusId": "207229839"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Node-Level Membership Inference Attacks Against Graph Neural Networks\n# Venue: arXiv.org\n# Authors: Xinlei He, Rui Wen, Yixin Wu, M. Backes, Yun Shen, Yang Zhang\n## Abstract\nMany real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.\n## Related Work\nMembership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
            "reference_string": "[231861713 | He et al. | 2021 | Citations: 98]"
        },
        {
            "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
            "venue": "International Journal of Intelligent Systems",
            "year": 2022,
            "reference_count": 52,
            "citation_count": 21,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1002/int.23000",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/int.23000?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/int.23000, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143551615",
                    "name": "Zhaobo Lu"
                },
                {
                    "authorId": "145844506",
                    "name": "Hai Liang"
                },
                {
                    "authorId": "2152527649",
                    "name": "Minghao Zhao"
                },
                {
                    "authorId": "2143047511",
                    "name": "Qingzhe Lv"
                },
                {
                    "authorId": "47716189",
                    "name": "Tiancai Liang"
                },
                {
                    "authorId": "3057294",
                    "name": "Yilei Wang"
                }
            ],
            "abstract": "Machine unlearning is the process through which a deployed machine learning model is enforced to forget about some of its training data items. It normally generates two machine learning models, the original model and the unlearned model, indicating training results before and after data items are deleted. However, recent studies find that machine unlearning is vulnerable to membership inference attacks\u2014as the directivity of training and nontraining data (i.e., data items in the training set have high posterior probabilities), the attackers can utilize this property to infer whether an item has been used for original model training. Nevertheless, such attacks are incapable in label\u2010only settings, in which the attackers are infeasible to get the posteriors. In this paper, we propose a new label\u2010only membership inference attack scheme targeted at machine unlearning to eliminate the dependence on posteriors. Our heuristic is that injected turbulence on candidate samples will present different behaviors for training and nontraining data. Thus, in our scheme, the attacker iteratively query on the original/unlearned models and inject turbulence to change their predicting labels; it determines whether an item is having\u2010been\u2010delated by observing the disturbance amplitude. Extensive experiments (i.e., on MNIST, CIFAR10, CIFAR100, and STL10 data sets) show that our method achieves high inference accuracy (measured by AUC) in label\u2010only settings, for example, AUC\u2009=\u20090.96 for MNIST data set. Besides, we analyze the existing countermeasures in mitigating inference attacks and find that our scheme can bypass most of them.",
            "corpus_id": 251664530,
            "sentences": [
                {
                    "corpus_id": "251664530",
                    "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
                    "text": "During the ML model training process, the model is over-parameterized due to limited available training data and superabundant training epochs, which will cause the ML model to behave differently to training data and nontraining data. For example, an ML model classifies the training data with a high probability, while classifying the nontraining data with a relatively low probability. The adversary speculates whether a given data sample is in the training data set of an ML model, which is the membership inference attack. 28 Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model \uf04d, the adversary \uf041 can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of \uf04d, otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27",
                    "score": 0.5712245735936705,
                    "section_title": "| Membership inference",
                    "char_start_offset": 6435,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 784
                        },
                        {
                            "start": 787,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1089
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 527,
                            "end": 529,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 976,
                            "end": 979,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 979,
                            "end": 982,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 982,
                            "end": 984,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1050,
                            "end": 1052,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 1087,
                            "end": 1089,
                            "matchedPaperCorpusId": "209439842"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors\n# Venue: International Journal of Intelligent Systems\n# Authors: Zhaobo Lu, Hai Liang, Minghao Zhao, Qingzhe Lv, Tiancai Liang, Yilei Wang\n## Abstract\nMachine unlearning is the process through which a deployed machine learning model is enforced to forget about some of its training data items. It normally generates two machine learning models, the original model and the unlearned model, indicating training results before and after data items are deleted. However, recent studies find that machine unlearning is vulnerable to membership inference attacks\u2014as the directivity of training and nontraining data (i.e., data items in the training set have high posterior probabilities), the attackers can utilize this property to infer whether an item has been used for original model training. Nevertheless, such attacks are incapable in label\u2010only settings, in which the attackers are infeasible to get the posteriors. In this paper, we propose a new label\u2010only membership inference attack scheme targeted at machine unlearning to eliminate the dependence on posteriors. Our heuristic is that injected turbulence on candidate samples will present different behaviors for training and nontraining data. Thus, in our scheme, the attacker iteratively query on the original/unlearned models and inject turbulence to change their predicting labels; it determines whether an item is having\u2010been\u2010delated by observing the disturbance amplitude. Extensive experiments (i.e., on MNIST, CIFAR10, CIFAR100, and STL10 data sets) show that our method achieves high inference accuracy (measured by AUC) in label\u2010only settings, for example, AUC\u2009=\u20090.96 for MNIST data set. Besides, we analyze the existing countermeasures in mitigating inference attacks and find that our scheme can bypass most of them.\n## | Membership inference\nDuring the ML model training process, the model is over-parameterized due to limited available training data and superabundant training epochs, which will cause the ML model to behave differently to training data and nontraining data. For example, an ML model classifies the training data with a high probability, while classifying the nontraining data with a relatively low probability. The adversary speculates whether a given data sample is in the training data set of an ML model, which is the membership inference attack. 28 Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model \uf04d, the adversary \uf041 can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of \uf04d, otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27",
            "reference_string": "[251664530 | Lu et al. | 2022 | Citations: 21]"
        },
        {
            "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion Models",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2023,
            "reference_count": 80,
            "citation_count": 20,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.08207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273926279",
                    "name": "Yan Pang"
                },
                {
                    "authorId": "2274197837",
                    "name": "Tianhao Wang"
                }
            ],
            "abstract": "With the rapid advancement of diffusion-based image-generative models, the quality of generated images has become increasingly photorealistic. Moreover, with the release of high-quality pre-trained image-generative models, a growing number of users are downloading these pre-trained models to fine-tune them with downstream datasets for various image-generation tasks. However, employing such powerful pre-trained models in downstream tasks presents significant privacy leakage risks. In this paper, we propose the first reconstruction-based membership inference attack framework, tailored for recent diffusion models, and in the more stringent black-box access setting. Considering four distinct attack scenarios and three types of attacks, this framework is capable of targeting any popular conditional generator model, achieving high precision, evidenced by an impressive AUC of $0.95$.",
            "corpus_id": 266191072,
            "sentences": [
                {
                    "corpus_id": "266191072",
                    "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion Models",
                    "text": "Membership inference attacks (MIAs) primarily aim to determine whether a target data point x is within the training dataset, often referred to as the member set, of a given target model. The motivation behind these attacks is twofold: to ensure that models are not trained in a manner that misappropriates data and to safeguard against potential privacy breaches. MIA's underlying principle hinges on exploiting machine learning models' overfitting and memorization properties. Discerning the model's different reactions to member and nonmember samples makes it feasible to infer the membership of the target point x. \n\nTo formalize membership inference attacks, assume there is a data sample x, a model M \u03b8 trained with dataset D m . The",
                    "score": 0.5600710299839818,
                    "section_title": "C. Membership Inference Attacks",
                    "char_start_offset": 11266,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 617
                        },
                        {
                            "start": 620,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 738
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Black-box Membership Inference Attacks against Fine-tuned Diffusion Models\n# Venue: Network and Distributed System Security Symposium\n# Authors: Yan Pang, Tianhao Wang\n## Abstract\nWith the rapid advancement of diffusion-based image-generative models, the quality of generated images has become increasingly photorealistic. Moreover, with the release of high-quality pre-trained image-generative models, a growing number of users are downloading these pre-trained models to fine-tune them with downstream datasets for various image-generation tasks. However, employing such powerful pre-trained models in downstream tasks presents significant privacy leakage risks. In this paper, we propose the first reconstruction-based membership inference attack framework, tailored for recent diffusion models, and in the more stringent black-box access setting. Considering four distinct attack scenarios and three types of attacks, this framework is capable of targeting any popular conditional generator model, achieving high precision, evidenced by an impressive AUC of $0.95$.\n## C. Membership Inference Attacks\nMembership inference attacks (MIAs) primarily aim to determine whether a target data point x is within the training dataset, often referred to as the member set, of a given target model. The motivation behind these attacks is twofold: to ensure that models are not trained in a manner that misappropriates data and to safeguard against potential privacy breaches. MIA's underlying principle hinges on exploiting machine learning models' overfitting and memorization properties. Discerning the model's different reactions to member and nonmember samples makes it feasible to infer the membership of the target point x. \n\nTo formalize membership inference attacks, assume there is a data sample x, a model M \u03b8 trained with dataset D m . The",
            "reference_string": "[266191072 | Pang et al. | 2023 | Citations: 20]"
        },
        {
            "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
            "venue": "Conference on Computer and Communications Security",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690335",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276333336",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2312376529",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2312657759",
                    "name": "Siyuan Wu"
                },
                {
                    "authorId": "2312343419",
                    "name": "Chengrui Hu"
                },
                {
                    "authorId": "2162322403",
                    "name": "Yutong Ye"
                },
                {
                    "authorId": "2312376569",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "2276275105",
                    "name": "Dengguo Feng"
                },
                {
                    "authorId": "2312358230",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Most existing membership inference attacks (MIAs) utilize metrics (e.g., loss) calculated on the model's final state, while recent advanced attacks leverage metrics computed at various stages, including both intermediate and final stages, throughout the model training. Nevertheless, these attacks often process multiple intermediate states of the metric independently, ignoring their time-dependent patterns. Consequently, they struggle to effectively distinguish between members and non-members who exhibit similar metric values, particularly resulting in a high false-positive rate. In this study, we delve deeper into the new membership signals in the black-box scenario. We identify a new, more integrated membership signal: the Pattern of Metric Sequence, derived from the various stages of model training. We contend that current signals provide only partial perspectives of this new signal: the new one encompasses both the model's multiple intermediate and final states, with a greater emphasis on temporal patterns among them. Building upon this signal, we introduce a novel attack method called Sequential-metric based Membership Inference Attack (SeqMIA). Specifically, we utilize knowledge distillation to obtain a set of distilled models representing various stages of the target model's training. We then assess multiple metrics on these distilled models in chronological order, creating distilled metric sequence. We finally integrate distilled multi-metric sequences as a sequential multiformat and employ an attention-based RNN attack model for inference. Empirical results show SeqMIA outperforms all baselines, especially can achieve an order of magnitude improvement in terms of TPR @ 0.1% FPR. Furthermore, we delve into the reasons why this signal contributes to SeqMIA's high attack performance, and assess various defense mechanisms against SeqMIA.",
            "corpus_id": 271328769,
            "sentences": [
                {
                    "corpus_id": "271328769",
                    "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
                    "text": "Membership inference attack is one of the most popular privacy attacks against ML models.The goal of the membership inference attack is to determine whether a data sample is used to train a target model.We consider data samples as members if they are used to train the target model, otherwise, non-members.Formally, considering a data sample\n\nx, a trained ML model M , and background knowledge of an adversary, denoted by I , the membership inference attack A can be defined as the following:\n\nHere, 0 means the data sample x is not a member of M 's training dataset, and 1 otherwise.The attack model A is essentially a binary classifier.",
                    "score": 0.6997414369906545,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 9107,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 89,
                            "end": 203
                        },
                        {
                            "start": 203,
                            "end": 306
                        },
                        {
                            "start": 306,
                            "end": 341
                        },
                        {
                            "start": 343,
                            "end": 492
                        },
                        {
                            "start": 494,
                            "end": 584
                        },
                        {
                            "start": 584,
                            "end": 638
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: SeqMIA: Sequential-Metric Based Membership Inference Attack\n# Venue: Conference on Computer and Communications Security\n# Authors: Hao Li, Zheng Li, Siyuan Wu, Chengrui Hu, Yutong Ye, Min Zhang, Dengguo Feng, Yang Zhang\n## Abstract\nMost existing membership inference attacks (MIAs) utilize metrics (e.g., loss) calculated on the model's final state, while recent advanced attacks leverage metrics computed at various stages, including both intermediate and final stages, throughout the model training. Nevertheless, these attacks often process multiple intermediate states of the metric independently, ignoring their time-dependent patterns. Consequently, they struggle to effectively distinguish between members and non-members who exhibit similar metric values, particularly resulting in a high false-positive rate. In this study, we delve deeper into the new membership signals in the black-box scenario. We identify a new, more integrated membership signal: the Pattern of Metric Sequence, derived from the various stages of model training. We contend that current signals provide only partial perspectives of this new signal: the new one encompasses both the model's multiple intermediate and final states, with a greater emphasis on temporal patterns among them. Building upon this signal, we introduce a novel attack method called Sequential-metric based Membership Inference Attack (SeqMIA). Specifically, we utilize knowledge distillation to obtain a set of distilled models representing various stages of the target model's training. We then assess multiple metrics on these distilled models in chronological order, creating distilled metric sequence. We finally integrate distilled multi-metric sequences as a sequential multiformat and employ an attention-based RNN attack model for inference. Empirical results show SeqMIA outperforms all baselines, especially can achieve an order of magnitude improvement in terms of TPR @ 0.1% FPR. Furthermore, we delve into the reasons why this signal contributes to SeqMIA's high attack performance, and assess various defense mechanisms against SeqMIA.\n## Membership Inference Attack\nMembership inference attack is one of the most popular privacy attacks against ML models.The goal of the membership inference attack is to determine whether a data sample is used to train a target model.We consider data samples as members if they are used to train the target model, otherwise, non-members.Formally, considering a data sample\n\nx, a trained ML model M , and background knowledge of an adversary, denoted by I , the membership inference attack A can be defined as the following:\n\nHere, 0 means the data sample x is not a member of M 's training dataset, and 1 otherwise.The attack model A is essentially a binary classifier.",
            "reference_string": "[271328769 | Li et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
            "venue": "International Conference on Information Systems Security and Privacy",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 19,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39868788",
                    "name": "Maya Anderson"
                },
                {
                    "authorId": "2291066051",
                    "name": "Guy Amit"
                },
                {
                    "authorId": "2652502",
                    "name": "Abigail Goldsteen"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the RAG system, an attack known as a Membership Inference Attack (MIA). Despite the significance of this threat, MIAs against RAG systems have yet remained under-explored. This study addresses this gap by introducing an efficient and easy-to-use method for conducting MIA against RAG systems. We demonstrate the effectiveness of our attack using two benchmark datasets and multiple generative models, showing that the membership of a document in the retrieval database can be efficiently determined through the creation of an appropriate prompt in both black-box and gray-box settings. Moreover, we introduce an initial defense strategy based on adding instructions to the RAG template, which shows high effectiveness for some datasets and models. Our findings highlight the importance of implementing security countermeasures in deployed RAG systems and developing more advanced defenses to protect the privacy and security of retrieval databases.",
            "corpus_id": 270199828,
            "sentences": [
                {
                    "corpus_id": "270199828",
                    "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
                    "text": "Membership inference attacks (Shokri et al., 2017;Hu et al., 2022) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2022). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set. \n\nAdditionally, an attacker may also aim to determine the membership of a certain user, i.e., to check if a user's data is part of the training set, which is known as user-level membership inference (Shejwalkar et al., 2021). Throughout this paper we will address the membership inference challenge from a sample-level perspective. \n\nIn the context of RAG, membership inference can be attributed to either the membership of a sample in the training dataset of the models E or G (described in the previous subsection 2.1), or a document's membership in the retrieval dataset D. This paper focuses on the latter. Formally, the goal of the attack is to infer the membership of a target document d in the retrieval database D, i.e., to check if d \u2208 D, using only the final output of the RAG system, namely the output of the generative model G conditioned on the fetched context from the retrieval database D. \n\nTo the best of our knowledge, this is the first paper to propose such a membership inference attack tailored to RAG systems.",
                    "score": 0.6525814450784221,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 6610,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 400
                        },
                        {
                            "start": 403,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1065
                        },
                        {
                            "start": 1068,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1397
                        },
                        {
                            "start": 1400,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2097
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 29,
                            "end": 50,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 50,
                            "end": 66,
                            "matchedPaperCorpusId": "232233426"
                        },
                        {
                            "start": 826,
                            "end": 848,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1265,
                            "end": 1290,
                            "matchedPaperCorpusId": "245222525"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation\n# Venue: International Conference on Information Systems Security and Privacy\n# Authors: Maya Anderson, Guy Amit, Abigail Goldsteen\n## Abstract\nRetrieval Augmented Generation (RAG) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the RAG system, an attack known as a Membership Inference Attack (MIA). Despite the significance of this threat, MIAs against RAG systems have yet remained under-explored. This study addresses this gap by introducing an efficient and easy-to-use method for conducting MIA against RAG systems. We demonstrate the effectiveness of our attack using two benchmark datasets and multiple generative models, showing that the membership of a document in the retrieval database can be efficiently determined through the creation of an appropriate prompt in both black-box and gray-box settings. Moreover, we introduce an initial defense strategy based on adding instructions to the RAG template, which shows high effectiveness for some datasets and models. Our findings highlight the importance of implementing security countermeasures in deployed RAG systems and developing more advanced defenses to protect the privacy and security of retrieval databases.\n## Membership Inference Attacks\nMembership inference attacks (Shokri et al., 2017;Hu et al., 2022) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2022). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set. \n\nAdditionally, an attacker may also aim to determine the membership of a certain user, i.e., to check if a user's data is part of the training set, which is known as user-level membership inference (Shejwalkar et al., 2021). Throughout this paper we will address the membership inference challenge from a sample-level perspective. \n\nIn the context of RAG, membership inference can be attributed to either the membership of a sample in the training dataset of the models E or G (described in the previous subsection 2.1), or a document's membership in the retrieval dataset D. This paper focuses on the latter. Formally, the goal of the attack is to infer the membership of a target document d in the retrieval database D, i.e., to check if d \u2208 D, using only the final output of the RAG system, namely the output of the generative model G conditioned on the fetched context from the retrieval database D. \n\nTo the best of our knowledge, this is the first paper to propose such a membership inference attack tailored to RAG systems.",
            "reference_string": "[270199828 | Anderson et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Scalable Membership Inference Attacks via Quantile Regression",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 50,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.03694",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "37335063",
                    "name": "Mart\u00edn Bertr\u00e1n"
                },
                {
                    "authorId": "46321498",
                    "name": "Shuai Tang"
                },
                {
                    "authorId": "81338045",
                    "name": "Michael Kearns"
                },
                {
                    "authorId": "144848816",
                    "name": "Jamie Morgenstern"
                },
                {
                    "authorId": "1682008",
                    "name": "Aaron Roth"
                },
                {
                    "authorId": "1768074",
                    "name": "Zhiwei Steven Wu"
                }
            ],
            "abstract": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
            "corpus_id": 259375769,
            "sentences": [
                {
                    "corpus_id": "259375769",
                    "title": "Scalable Membership Inference Attacks via Quantile Regression",
                    "text": "The basic goal of privacy-preserving machine learning is to find models that are predictive on some underlying data distribution, without being disclosive of the particular data points on which they were trained. The simplest kind of attack that can be launched on a trained model-falsifying privacy guarantees-is a membership inference attack. A membership inference attack, informally, is a statistical test that is able to reliably determine whether a particular data point was included in the training set used to train the model or not. \n\nAlmost all membership inference attacks are based on the observation that models tend to overfit their training sets in different ways. In particular, they tend to systematically predict higher confidence in the true labels of data points from their training set, compared to points drawn from the same distribution not in their training set. The confidence that a model places on the true label of a data-point is thus a natural test statistic to build a membership-inference hypothesis test around. A variety of recent methods [Shokri et al., 2017, Long et al., 2020, Sablayrolles et al., 2019, Song and Mittal, 2021, Carlini et al., 2022] are based around this idea, and aim to estimate the distribution of the test statistic (the confidence assigned to the true label of a datapoint) over the distribution of datapoints that were not used in training (and sometimes, also over the distribution of datapoints that were used in training) for the purpose of designing tests that can reject the null hypothesis-that a data point under attack was not used in training-with the desired level of confidence. \n\nFigure 1: Comparing the true positive rate vs. false positive rate of our membership inference attack with the marginal baseline proposed in Yeom et al. [2018] and the state-of-the-art LiRA proposed in Carlini et al. [2022] evaluated at 2, 4, 6, and 8 shadow models. We also provide a visual readout of their 64 shadow model results, as reported in their paper (we did not have the compute necessary to reproduce this). We faithfully replicated LIRA's attack setup and produced better results than their reported values.",
                    "score": 0.6242102186967534,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 541
                        },
                        {
                            "start": 544,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1648
                        },
                        {
                            "start": 1651,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2171
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1073,
                            "end": 1093,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1093,
                            "end": 1112,
                            "matchedPaperCorpusId": "226266600"
                        },
                        {
                            "start": 1112,
                            "end": 1139,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 1139,
                            "end": 1162,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1162,
                            "end": 1185,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1792,
                            "end": 1810,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1853,
                            "end": 1874,
                            "matchedPaperCorpusId": "244920593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Scalable Membership Inference Attacks via Quantile Regression\n# Venue: Neural Information Processing Systems\n# Authors: Mart\u00edn Bertr\u00e1n, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu\n## Abstract\nMembership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.\n## Introduction\nThe basic goal of privacy-preserving machine learning is to find models that are predictive on some underlying data distribution, without being disclosive of the particular data points on which they were trained. The simplest kind of attack that can be launched on a trained model-falsifying privacy guarantees-is a membership inference attack. A membership inference attack, informally, is a statistical test that is able to reliably determine whether a particular data point was included in the training set used to train the model or not. \n\nAlmost all membership inference attacks are based on the observation that models tend to overfit their training sets in different ways. In particular, they tend to systematically predict higher confidence in the true labels of data points from their training set, compared to points drawn from the same distribution not in their training set. The confidence that a model places on the true label of a data-point is thus a natural test statistic to build a membership-inference hypothesis test around. A variety of recent methods [Shokri et al., 2017, Long et al., 2020, Sablayrolles et al., 2019, Song and Mittal, 2021, Carlini et al., 2022] are based around this idea, and aim to estimate the distribution of the test statistic (the confidence assigned to the true label of a datapoint) over the distribution of datapoints that were not used in training (and sometimes, also over the distribution of datapoints that were used in training) for the purpose of designing tests that can reject the null hypothesis-that a data point under attack was not used in training-with the desired level of confidence. \n\nFigure 1: Comparing the true positive rate vs. false positive rate of our membership inference attack with the marginal baseline proposed in Yeom et al. [2018] and the state-of-the-art LiRA proposed in Carlini et al. [2022] evaluated at 2, 4, 6, and 8 shadow models. We also provide a visual readout of their 64 shadow model results, as reported in their paper (we did not have the compute necessary to reproduce this). We faithfully replicated LIRA's attack setup and produced better results than their reported values.",
            "reference_string": "[259375769 | Bertran et al. | 2023 | Citations: 50]"
        },
        {
            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
            "venue": "Conference on Computer and Communications Security",
            "year": 2019,
            "reference_count": 73,
            "citation_count": 244,
            "influential_citation_count": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3319535.3354211",
                "status": "HYBRID",
                "license": "public-domain",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.10291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144173853",
                    "name": "Liwei Song"
                },
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "143615345",
                    "name": "Prateek Mittal"
                }
            ],
            "abstract": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
            "corpus_id": 165163934,
            "sentences": [
                {
                    "corpus_id": "165163934",
                    "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                    "text": "Membership inference advantage compared to random guessing.\n\nIn this part, we describe the membership inference attack and its performance formally, with notations listed in Table 1. For a neural network model F (we skip its parameter \u03b8 for simplicity) that is robustly trained with the adversarial constraint B \u03f5 , the membership inference attack aims to determine whether a given input example z = (x, y) is in its training set D train or not. We denote the inference strategy adopted by the adversary as I(F , B \u03f5 , z), which codes members as 1, and non-members as 0.\n\nWe use the fraction of correct membership predictions, as the metric to evaluate membership inference accuracy. We use a test set D test which does not overlap with the training set, to represent non-members. We sample a random data point (x, y) from either D train or D test with an equal 50% probability, to test the membership inference attack. We measure the membership inference accuracy as follows.\n\nwhere | \u00b7 | measures the size of a dataset.\n\nThe membership inference accuracy evaluates the probability that the adversary can guess correctly whether an input is from training set or test set. Note that a random guessing strategy will lead to a 50% inference accuracy. To further measure the effectiveness of our membership inference strategy, we also use the notion of membership inference advantage proposed by Yeom et al. [64], which is defined as the increase in inference accuracy over random guessing (multiplied by 2).",
                    "score": 0.6042032630623956,
                    "section_title": "ADV T inf",
                    "char_start_offset": 21531,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1405,
                            "end": 1409,
                            "matchedPaperCorpusId": "2656445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "165163934",
                    "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                    "text": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.",
                    "score": 0.5874886353450977,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 15158,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 149,
                            "end": 153,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 156,
                            "end": 159,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 162,
                            "end": 165,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 165,
                            "end": 168,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 330,
                            "end": 334,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1192,
                            "end": 1196,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1950,
                            "end": 1954,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Privacy Risks of Securing Machine Learning Models against Adversarial Examples\n# Venue: Conference on Computer and Communications Security\n# Authors: Liwei Song, R. Shokri, Prateek Mittal\n## Abstract\nThe arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.\n## Membership Inference Attacks\nFor a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.\n\n## ADV T inf\nMembership inference advantage compared to random guessing.\n\nIn this part, we describe the membership inference attack and its performance formally, with notations listed in Table 1. For a neural network model F (we skip its parameter \u03b8 for simplicity) that is robustly trained with the adversarial constraint B \u03f5 , the membership inference attack aims to determine whether a given input example z = (x, y) is in its training set D train or not. We denote the inference strategy adopted by the adversary as I(F , B \u03f5 , z), which codes members as 1, and non-members as 0.\n\nWe use the fraction of correct membership predictions, as the metric to evaluate membership inference accuracy. We use a test set D test which does not overlap with the training set, to represent non-members. We sample a random data point (x, y) from either D train or D test with an equal 50% probability, to test the membership inference attack. We measure the membership inference accuracy as follows.\n\nwhere | \u00b7 | measures the size of a dataset.\n\nThe membership inference accuracy evaluates the probability that the adversary can guess correctly whether an input is from training set or test set. Note that a random guessing strategy will lead to a 50% inference accuracy. To further measure the effectiveness of our membership inference strategy, we also use the notion of membership inference advantage proposed by Yeom et al. [64], which is defined as the increase in inference accuracy over random guessing (multiplied by 2).",
            "reference_string": "[165163934 | Song et al. | 2019 | Citations: 244]"
        },
        {
            "title": "When Machine Learning Meets Privacy",
            "venue": "ACM Computing Surveys",
            "year": 2020,
            "reference_count": 194,
            "citation_count": 290,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://opus.lib.uts.edu.au/bitstream/10453/146941/2/Privacy_and_machine_learning_survey_accepted%20version.pdf",
                "status": "GREEN",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.11819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145306564",
                    "name": "B. Liu"
                },
                {
                    "authorId": "145633124",
                    "name": "Ming Ding"
                },
                {
                    "authorId": "40221713",
                    "name": "Sina Shaham"
                },
                {
                    "authorId": "2352525282",
                    "name": "Wenny Rahayu"
                },
                {
                    "authorId": "1803792",
                    "name": "F. Farokhi"
                },
                {
                    "authorId": "1740858",
                    "name": "Zihuai Lin"
                }
            ],
            "abstract": "The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",
            "corpus_id": 227152059,
            "sentences": [
                {
                    "corpus_id": "227152059",
                    "title": "When Machine Learning Meets Privacy",
                    "text": "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not [98,141]. An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. [141] introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. [173] investigated \"the relationship between overfitting and privacy leakage\". Salem et al. [134] proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\". \n\nMembership inference attacks are also studied in Generative Adversarial Networks (GANs). For example, Liu et al. [84] trained an attacker network to launch membership attacks against Variational Autoencoders (VAEs) and GANs. Hayes et al. [52] focused on \"generative models in ML-as-a-service applications and train GANs to recognize training inputs\". \n\nMelis et al. [98] studied membership inference in collaborative learning. The attack is achieved by \"analyzing periodic updates to the shared model during training\". The reason that this attack is effective is that the gradients in neural networks are based on features, \"thus observations of the participants' gradient updates can be used to infer the feature values, which are in turn based on these participants' private training data\". Wang et al. [163] considered membership inference attack \"against the user-level privacy on the federated learning framework by the attack from a malicious server. The proposed attack framework exploits GAN with a multi-task discriminator, which simultaneously discriminates category, reality and client identity of input samples, and doing so recovers user-specific private data\". \n\n3.1.4 Model Memorization Attack.",
                    "score": 0.6027278858018752,
                    "section_title": "Membership Inference Attack.",
                    "char_start_offset": 21788,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 230
                        },
                        {
                            "start": 233,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 1005
                        },
                        {
                            "start": 1008,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1358
                        },
                        {
                            "start": 1361,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1800
                        },
                        {
                            "start": 1801,
                            "end": 1964
                        },
                        {
                            "start": 1965,
                            "end": 2182
                        },
                        {
                            "start": 2185,
                            "end": 2190
                        },
                        {
                            "start": 2191,
                            "end": 2217
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 160,
                            "end": 164,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 164,
                            "end": 168,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 247,
                            "end": 252,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 708,
                            "end": 713,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 800,
                            "end": 805,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1246,
                            "end": 1250,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1374,
                            "end": 1378,
                            "matchedPaperCorpusId": "53099247"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: When Machine Learning Meets Privacy\n# Venue: ACM Computing Surveys\n# Authors: B. Liu, Ming Ding, Sina Shaham, Wenny Rahayu, F. Farokhi, Zihuai Lin\n## Abstract\nThe newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.\n## Membership Inference Attack.\nMembership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not [98,141]. An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. [141] introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. [173] investigated \"the relationship between overfitting and privacy leakage\". Salem et al. [134] proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\". \n\nMembership inference attacks are also studied in Generative Adversarial Networks (GANs). For example, Liu et al. [84] trained an attacker network to launch membership attacks against Variational Autoencoders (VAEs) and GANs. Hayes et al. [52] focused on \"generative models in ML-as-a-service applications and train GANs to recognize training inputs\". \n\nMelis et al. [98] studied membership inference in collaborative learning. The attack is achieved by \"analyzing periodic updates to the shared model during training\". The reason that this attack is effective is that the gradients in neural networks are based on features, \"thus observations of the participants' gradient updates can be used to infer the feature values, which are in turn based on these participants' private training data\". Wang et al. [163] considered membership inference attack \"against the user-level privacy on the federated learning framework by the attack from a malicious server. The proposed attack framework exploits GAN with a multi-task discriminator, which simultaneously discriminates category, reality and client identity of input samples, and doing so recovers user-specific private data\". \n\n3.1.4 Model Memorization Attack.",
            "reference_string": "[227152059 | Liu et al. | 2020 | Citations: 290]"
        },
        {
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2018,
            "reference_count": 59,
            "citation_count": 950,
            "influential_citation_count": 160,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14722/ndss.2019.23119",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.01246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66697271",
                    "name": "A. Salem"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "144887171",
                    "name": "Mathias Humbert"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                }
            ],
            "abstract": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
            "corpus_id": 46933970,
            "sentences": [
                {
                    "corpus_id": "46933970",
                    "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                    "text": "Membership Inference. Membership inference attack has been successfully performed in many different data domains, ranging form biomedical data [18], [2] to mobility traces [35]. \n\nHomer et al. [18] propose the first membership inference attack on genomic data. This attack relies on the L 1 distance between the allele frequencies and the victim's genomic data. Backes et al. [2] generalize this attack to other types of biomedical data. More recently, Pyrgelis et al. [35] have shown that people's aggregate mobility traces are also prone to membership inference attack. They first formalize membership inference as a distinguishability game. Then, they implement the attack with machine learning classifiers. Large-scale evaluation on two real-world datasets has demonstrated their attack's effectiveness. Moreover, the authors show their framework can easily incorporate different defense mechanisms, such as differential privacy, to allow a comprehensive evaluation of membership inference risks. \n\nMembership Inference Against Machine Learning. Shokri et al. [38] present the first membership inference attack against machine learning models. The key contribution of this work is the proposal of shadow model training, which aims at mimicking the target model's behavior to generate training data for the attack model. \n\nThe first adversary in the current paper follows a very similar setting. We have shown that one shadow model and one attack model are sufficient to achieve an effective attack compared to the proposal of multiple shadow models and attack models by Shokri et al. [38]. Moreover, we show that data transferring attack can bypass the expensive synthetic data generation scheme and achieve a very similar performance. Another major contribution of our paper is the two effective defense mechanisms, such as dropout and model stacking. Many recent works have studied membership inference against machine learning as well from different angles [25], [17], [48], [26]. \n\nAttacks Against Machine Learning. Besides membership inference, there exist multiple other types of attacks against ML models. Fredrikson et al. [13] present the model inversion attack in biomedical data setting. In this scenario, an attacker aims to infer the missing attributes of her victim, relying on the output of a trained ML model. Later, model inversion attack is generalized to a broader scenario [12].",
                    "score": 0.5968296703995154,
                    "section_title": "VII. RELATED WORK",
                    "char_start_offset": 46931,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 177
                        },
                        {
                            "start": 180,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 1000
                        },
                        {
                            "start": 1003,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1323
                        },
                        {
                            "start": 1326,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 1987
                        },
                        {
                            "start": 1990,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2202
                        },
                        {
                            "start": 2203,
                            "end": 2329
                        },
                        {
                            "start": 2330,
                            "end": 2402
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 143,
                            "end": 147,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 149,
                            "end": 152,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 172,
                            "end": 176,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 193,
                            "end": 197,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 376,
                            "end": 379,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 469,
                            "end": 473,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 1064,
                            "end": 1068,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1588,
                            "end": 1592,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1976,
                            "end": 1980,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 2135,
                            "end": 2139,
                            "matchedPaperCorpusId": "2148083"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "46933970",
                    "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                    "text": "Machine learning (ML) has become a core component of many real-world applications, ranging from image classification to speech recognition. The success of ML has recently driven leading Internet companies, such as Google and Amazon, to deploy machine learning as a service (MLaaS). Under such services, a user uploads her own dataset to a server and the server returns a trained ML model to the user, typically as a black-box API. \n\nDespite being popular, ML models are vulnerable to various security and privacy attacks, such as model inversion [12], adversarial examples [15], and model extraction [43], [30], [45]. In this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims \u00a7 Corresponding author to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status. Previously, membership inference has been successfully conducted in many other domains, such as biomedical data [2] and mobility data [35]. \n\nShokri et al. [38] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple machine learning models (one for each prediction class), referred to as attack models, to make membership inference over the target model's output, i.e., posterior probabilities. Given that the target model is a black-box API, Shokri et al. propose to construct multiple shadow models to mimic the target model's behavior and derive the data necessary, i.e., the posteriors and the ground truth membership, to train attack models. \n\nThere are two main assumptions made by Shokri et al. [38]. First, the attacker needs to establish multiple shadow models with each one sharing the same structure as the target model. This is achieved by using the same MLaaS that trains the target model to build the shadow models.",
                    "score": 0.6063908091981324,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 430
                        },
                        {
                            "start": 433,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1327
                        },
                        {
                            "start": 1330,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1913
                        },
                        {
                            "start": 1916,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 546,
                            "end": 550,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 573,
                            "end": 577,
                            "matchedPaperCorpusId": "6706414"
                        },
                        {
                            "start": 600,
                            "end": 604,
                            "matchedPaperCorpusId": "2984526"
                        },
                        {
                            "start": 606,
                            "end": 610,
                            "matchedPaperCorpusId": "3278569"
                        },
                        {
                            "start": 612,
                            "end": 616,
                            "matchedPaperCorpusId": "3423242"
                        },
                        {
                            "start": 1300,
                            "end": 1303,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 1322,
                            "end": 1326,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 1344,
                            "end": 1348,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1969,
                            "end": 1973,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models\n# Venue: Network and Distributed System Security Symposium\n# Authors: A. Salem, Yang Zhang, Mathias Humbert, Mario Fritz, M. Backes\n## Abstract\nMachine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.\n## I. INTRODUCTION\nMachine learning (ML) has become a core component of many real-world applications, ranging from image classification to speech recognition. The success of ML has recently driven leading Internet companies, such as Google and Amazon, to deploy machine learning as a service (MLaaS). Under such services, a user uploads her own dataset to a server and the server returns a trained ML model to the user, typically as a black-box API. \n\nDespite being popular, ML models are vulnerable to various security and privacy attacks, such as model inversion [12], adversarial examples [15], and model extraction [43], [30], [45]. In this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims \u00a7 Corresponding author to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status. Previously, membership inference has been successfully conducted in many other domains, such as biomedical data [2] and mobility data [35]. \n\nShokri et al. [38] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple machine learning models (one for each prediction class), referred to as attack models, to make membership inference over the target model's output, i.e., posterior probabilities. Given that the target model is a black-box API, Shokri et al. propose to construct multiple shadow models to mimic the target model's behavior and derive the data necessary, i.e., the posteriors and the ground truth membership, to train attack models. \n\nThere are two main assumptions made by Shokri et al. [38]. First, the attacker needs to establish multiple shadow models with each one sharing the same structure as the target model. This is achieved by using the same MLaaS that trains the target model to build the shadow models.\n\n## VII. RELATED WORK\nMembership Inference. Membership inference attack has been successfully performed in many different data domains, ranging form biomedical data [18], [2] to mobility traces [35]. \n\nHomer et al. [18] propose the first membership inference attack on genomic data. This attack relies on the L 1 distance between the allele frequencies and the victim's genomic data. Backes et al. [2] generalize this attack to other types of biomedical data. More recently, Pyrgelis et al. [35] have shown that people's aggregate mobility traces are also prone to membership inference attack. They first formalize membership inference as a distinguishability game. Then, they implement the attack with machine learning classifiers. Large-scale evaluation on two real-world datasets has demonstrated their attack's effectiveness. Moreover, the authors show their framework can easily incorporate different defense mechanisms, such as differential privacy, to allow a comprehensive evaluation of membership inference risks. \n\nMembership Inference Against Machine Learning. Shokri et al. [38] present the first membership inference attack against machine learning models. The key contribution of this work is the proposal of shadow model training, which aims at mimicking the target model's behavior to generate training data for the attack model. \n\nThe first adversary in the current paper follows a very similar setting. We have shown that one shadow model and one attack model are sufficient to achieve an effective attack compared to the proposal of multiple shadow models and attack models by Shokri et al. [38]. Moreover, we show that data transferring attack can bypass the expensive synthetic data generation scheme and achieve a very similar performance. Another major contribution of our paper is the two effective defense mechanisms, such as dropout and model stacking. Many recent works have studied membership inference against machine learning as well from different angles [25], [17], [48], [26]. \n\nAttacks Against Machine Learning. Besides membership inference, there exist multiple other types of attacks against ML models. Fredrikson et al. [13] present the model inversion attack in biomedical data setting. In this scenario, an attacker aims to infer the missing attributes of her victim, relying on the output of a trained ML model. Later, model inversion attack is generalized to a broader scenario [12].",
            "reference_string": "[46933970 | Salem et al. | 2018 | Citations: 950]"
        },
        {
            "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
            "venue": "Conference on Computer and Communications Security",
            "year": 2021,
            "reference_count": 69,
            "citation_count": 52,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2102.04140",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.04140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well. In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.",
            "corpus_id": 231846491,
            "sentences": [
                {
                    "corpus_id": "231846491",
                    "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
                    "text": "Membership inference attack is one of the most popular privacy attacks against ML models [7,8,10,19,28,31,33,49,52,57]. The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}. \n\nHere, the target model is the contrastive model introduced in Section 2. A successful membership inference attack can cause severe privacy risks. For instance, if a model is trained on data samples collected from people with certain sensitive information, then successfully inferring a sample from a person being a member of the model can directly reveal the person's sensitive information. \n\nFollowing previous work [10,33,49,52,57], we assume that an adversary only has black-box access to the target model T , i.e., they can only query T with their data samples and obtain the outputs. In addition, the adversary also has a shadow dataset D shadow , which comes from the same distribution as the target model's training dataset. The shadow dataset D shadow is used to train a shadow model S, the goal of which is to obtain the necessary information to perform the attack. We further assume that the shadow model shares the same architecture as the target model [52]. This is realistic as the adversary can use the same machine learning service as the target model owner to train their shadow model. Alternatively, the adversary can also learn the target model's architecture first by applying model extraction attacks [40,41,60,63].",
                    "score": 0.566025250521321,
                    "section_title": "Attack Definition and Threat Model",
                    "char_start_offset": 11960,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 341
                        },
                        {
                            "start": 344,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 734
                        },
                        {
                            "start": 737,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1445
                        },
                        {
                            "start": 1446,
                            "end": 1579
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 89,
                            "end": 92,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 100,
                            "end": 103,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 103,
                            "end": 106,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 106,
                            "end": 109,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 109,
                            "end": 112,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 112,
                            "end": 115,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 115,
                            "end": 118,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 765,
                            "end": 768,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 768,
                            "end": 771,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 771,
                            "end": 774,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 774,
                            "end": 777,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1308,
                            "end": 1312,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1565,
                            "end": 1569,
                            "matchedPaperCorpusId": "3278569"
                        },
                        {
                            "start": 1569,
                            "end": 1572,
                            "matchedPaperCorpusId": "54457412"
                        },
                        {
                            "start": 1572,
                            "end": 1575,
                            "matchedPaperCorpusId": "2984526"
                        },
                        {
                            "start": 1575,
                            "end": 1578,
                            "matchedPaperCorpusId": "3423242"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Quantifying and Mitigating Privacy Risks of Contrastive Learning\n# Venue: Conference on Computer and Communications Security\n# Authors: Xinlei He, Yang Zhang\n## Abstract\nData is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well. In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.\n## Attack Definition and Threat Model\nMembership inference attack is one of the most popular privacy attacks against ML models [7,8,10,19,28,31,33,49,52,57]. The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}. \n\nHere, the target model is the contrastive model introduced in Section 2. A successful membership inference attack can cause severe privacy risks. For instance, if a model is trained on data samples collected from people with certain sensitive information, then successfully inferring a sample from a person being a member of the model can directly reveal the person's sensitive information. \n\nFollowing previous work [10,33,49,52,57], we assume that an adversary only has black-box access to the target model T , i.e., they can only query T with their data samples and obtain the outputs. In addition, the adversary also has a shadow dataset D shadow , which comes from the same distribution as the target model's training dataset. The shadow dataset D shadow is used to train a shadow model S, the goal of which is to obtain the necessary information to perform the attack. We further assume that the shadow model shares the same architecture as the target model [52]. This is realistic as the adversary can use the same machine learning service as the target model owner to train their shadow model. Alternatively, the adversary can also learn the target model's architecture first by applying model extraction attacks [40,41,60,63].",
            "reference_string": "[231846491 | He et al. | 2021 | Citations: 52]"
        },
        {
            "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
            "venue": "USENIX Security Symposium",
            "year": 2021,
            "reference_count": 70,
            "citation_count": 132,
            "influential_citation_count": 19,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.02551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108101945",
                    "name": "Yugeng Liu"
                },
                {
                    "authorId": "2054749404",
                    "name": "Rui Wen"
                },
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "66697271",
                    "name": "A. Salem"
                },
                {
                    "authorId": "48806102",
                    "name": "Zhikun Zhang"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "1728207",
                    "name": "Emiliano De Cristofaro"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
            "corpus_id": 231802143,
            "sentences": [
                {
                    "corpus_id": "231802143",
                    "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
                    "text": "Membership Inference (MemInf) [53] against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature [6,7,23,29,31,37,49,51,53]. Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]. \n\nIn the following, we illustrate how to implement membership inference (MemInf) under different threat models. \n\naux [51]. We start with the most common and difficult setting for the attack [51,53], whereby the adversary has black-box access (M B ) to the target model and a shadow auxiliary dataset (D S aux ). The adversary first splits the shadow dataset into two parts and uses one to train a shadow model on the same task. Next, the adversary uses the entire shadow dataset to query the shadow model. For each querying sample, the shadow model returns its posteriors and the predicted label: if the sample is part of the shadow model's training set, the adversary labels it as a member and as a non-member otherwise. With this labeled dataset, the adversary trains an attack model, which is a binary membership classifier. Finally, to determine whether a data sample is a member of the target model's training dataset, the sample is fed to the target model, and the posteriors and the predicted label (transformed to a binary indicator on whether the prediction is correct) are fed to the attack model. \n\nBlack-Box/Partial MemInf,M B ,D P aux [51].",
                    "score": 0.5645970257482036,
                    "section_title": "Membership Inference",
                    "char_start_offset": 8150,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 337
                        },
                        {
                            "start": 340,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 488
                        },
                        {
                            "start": 489,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 877
                        },
                        {
                            "start": 880,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1306
                        },
                        {
                            "start": 1307,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2032
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 30,
                            "end": 34,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 461,
                            "end": 464,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 464,
                            "end": 466,
                            "matchedPaperCorpusId": "218502126"
                        },
                        {
                            "start": 466,
                            "end": 469,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 469,
                            "end": 472,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 472,
                            "end": 475,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 475,
                            "end": 478,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 478,
                            "end": 481,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 481,
                            "end": 484,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 484,
                            "end": 487,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 996,
                            "end": 1000,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1069,
                            "end": 1073,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1073,
                            "end": 1076,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models\n# Venue: USENIX Security Symposium\n# Authors: Yugeng Liu, Rui Wen, Xinlei He, A. Salem, Zhikun Zhang, M. Backes, Emiliano De Cristofaro, Mario Fritz, Yang Zhang\n## Abstract\nInference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.\n## Membership Inference\nMembership Inference (MemInf) [53] against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature [6,7,23,29,31,37,49,51,53]. Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]. \n\nIn the following, we illustrate how to implement membership inference (MemInf) under different threat models. \n\naux [51]. We start with the most common and difficult setting for the attack [51,53], whereby the adversary has black-box access (M B ) to the target model and a shadow auxiliary dataset (D S aux ). The adversary first splits the shadow dataset into two parts and uses one to train a shadow model on the same task. Next, the adversary uses the entire shadow dataset to query the shadow model. For each querying sample, the shadow model returns its posteriors and the predicted label: if the sample is part of the shadow model's training set, the adversary labels it as a member and as a non-member otherwise. With this labeled dataset, the adversary trains an attack model, which is a binary membership classifier. Finally, to determine whether a data sample is a member of the target model's training dataset, the sample is fed to the target model, and the posteriors and the predicted label (transformed to a binary indicator on whether the prediction is correct) are fed to the attack model. \n\nBlack-Box/Partial MemInf,M B ,D P aux [51].",
            "reference_string": "[231802143 | Liu et al. | 2021 | Citations: 132]"
        },
        {
            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2054749404",
                    "name": "Rui Wen"
                },
                {
                    "authorId": "2257034706",
                    "name": "Michael Backes"
                },
                {
                    "authorId": "2257291195",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
            "corpus_id": 272423578,
            "sentences": [
                {
                    "corpus_id": "272423578",
                    "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
                    "text": "Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability. \n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7]). \n\nThe game proceeds between a challenger C and an adversary A:   The adversary A is provided with auxiliary information about the data distribution D. This allows the adversary to sample a shadow dataset from the same or a similar distribution, which is a common assumption in the existing literature. \n\nThe attack accuracy for the adversary is defined as follows: \n\nTo assess the privacy leakage caused by membership inference attacks (MIAs), we employ two metrics commonly used in prior research, focusing on both worst-case and averagecase performance: \n\n1. (Log-scale) ROC Analysis [7], which focuses on the true-positive rate at low false-positive rates, effectively capturing the worst-case privacy vulnerabilities of machine learning models. \n\n2. Membership Advantage [59, 71], defined as \n\nThis metric represents the advantage over random guessing, multiplied by 2, providing an average-case measure to gain an overview of the attack's efficacy. \n\nIn this work, we investigate four specific membership inference attacks. For the CIFAR10 and CelebA tasks, a training set of 50,000 samples is employed, while for the TinyIm-ageNet task, we utilize a training set of 100,000 samples to construct the target model. \n\nTo assess the membership status of samples, we first adopt a methodology based on previous research [12,31] that considers the distance to the decision boundary as a reflection of membership status. Specifically, they claim that samples located near the decision boundary are more likely to be nonmembers, whereas samples positioned in the central region of the decision area are more likely to be members.",
                    "score": 0.5580699948986352,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 13561,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 662
                        },
                        {
                            "start": 665,
                            "end": 964
                        },
                        {
                            "start": 967,
                            "end": 1027
                        },
                        {
                            "start": 1030,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1411
                        },
                        {
                            "start": 1414,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1616
                        },
                        {
                            "start": 1619,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1881
                        },
                        {
                            "start": 1884,
                            "end": 2082
                        },
                        {
                            "start": 2083,
                            "end": 2290
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 34,
                            "end": 38,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 38,
                            "end": 41,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 41,
                            "end": 44,
                            "matchedPaperCorpusId": "251953448"
                        },
                        {
                            "start": 44,
                            "end": 47,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 47,
                            "end": 50,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 50,
                            "end": 53,
                            "matchedPaperCorpusId": "244345608"
                        },
                        {
                            "start": 657,
                            "end": 660,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1249,
                            "end": 1252,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1984,
                            "end": 1988,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1988,
                            "end": 1991,
                            "matchedPaperCorpusId": "237563320"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?\n# Venue: Network and Distributed System Security Symposium\n# Authors: Rui Wen, Michael Backes, Yang Zhang\n## Abstract\nMachine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.\n## Membership Inference Attack\nMembership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability. \n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7]). \n\nThe game proceeds between a challenger C and an adversary A:   The adversary A is provided with auxiliary information about the data distribution D. This allows the adversary to sample a shadow dataset from the same or a similar distribution, which is a common assumption in the existing literature. \n\nThe attack accuracy for the adversary is defined as follows: \n\nTo assess the privacy leakage caused by membership inference attacks (MIAs), we employ two metrics commonly used in prior research, focusing on both worst-case and averagecase performance: \n\n1. (Log-scale) ROC Analysis [7], which focuses on the true-positive rate at low false-positive rates, effectively capturing the worst-case privacy vulnerabilities of machine learning models. \n\n2. Membership Advantage [59, 71], defined as \n\nThis metric represents the advantage over random guessing, multiplied by 2, providing an average-case measure to gain an overview of the attack's efficacy. \n\nIn this work, we investigate four specific membership inference attacks. For the CIFAR10 and CelebA tasks, a training set of 50,000 samples is employed, while for the TinyIm-ageNet task, we utilize a training set of 100,000 samples to construct the target model. \n\nTo assess the membership status of samples, we first adopt a methodology based on previous research [12,31] that considers the distance to the decision boundary as a reflection of membership status. Specifically, they claim that samples located near the decision boundary are more likely to be nonmembers, whereas samples positioned in the central region of the decision area are more likely to be members.",
            "reference_string": "[272423578 | Wen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Label-Only Membership Inference Attacks",
            "venue": "International Conference on Machine Learning",
            "year": 2020,
            "reference_count": 70,
            "citation_count": 516,
            "influential_citation_count": 63,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.14321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1415982317",
                    "name": "Christopher A. Choquette-Choo"
                },
                {
                    "authorId": "2444919",
                    "name": "Florian Tram\u00e8r"
                },
                {
                    "authorId": "2483738",
                    "name": "Nicholas Carlini"
                },
                {
                    "authorId": "1967156",
                    "name": "Nicolas Papernot"
                }
            ],
            "abstract": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
            "corpus_id": 220831381,
            "sentences": [
                {
                    "corpus_id": "220831381",
                    "title": "Label-Only Membership Inference Attacks",
                    "text": "Membership inference attacks (Shokri et al., 2016) are a form of privacy leakage that identify if a given data sample was in a machine learning model's training dataset. Given a sample x and access to a trained model h, the adversary uses a classifier or decision rule f h to compute a membership prediction f (x; h) \u2208 {0, 1}, with the goal that f (x; h) = 1 whenever x is a training point. The main challenge in mounting a membership inference attack is creating the classifier f , under various assumptions about the adversary's knowledge of h and its training data distribution.\n\nPrior work assumes that an adversary has only black-box access to the trained model h, via a query interface that on input x returns part or all of the confidence vector h(x).",
                    "score": 0.6399168139024071,
                    "section_title": "A.2. Membership Inference",
                    "char_start_offset": 37003,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Label-Only Membership Inference Attacks\n# Venue: International Conference on Machine Learning\n# Authors: Christopher A. Choquette-Choo, Florian Tram\u00e8r, Nicholas Carlini, Nicolas Papernot\n## Abstract\nMembership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.\n## A.2. Membership Inference\nMembership inference attacks (Shokri et al., 2016) are a form of privacy leakage that identify if a given data sample was in a machine learning model's training dataset. Given a sample x and access to a trained model h, the adversary uses a classifier or decision rule f h to compute a membership prediction f (x; h) \u2208 {0, 1}, with the goal that f (x; h) = 1 whenever x is a training point. The main challenge in mounting a membership inference attack is creating the classifier f , under various assumptions about the adversary's knowledge of h and its training data distribution.\n\nPrior work assumes that an adversary has only black-box access to the trained model h, via a query interface that on input x returns part or all of the confidence vector h(x).",
            "reference_string": "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]"
        },
        {
            "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 265,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.11723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281289264",
                    "name": "Chao Liu"
                },
                {
                    "authorId": "2280335814",
                    "name": "Boxi Chen"
                },
                {
                    "authorId": "2280146329",
                    "name": "Wei Shao"
                },
                {
                    "authorId": "2280200066",
                    "name": "Chris Zhang"
                },
                {
                    "authorId": "2264107865",
                    "name": "Kelvin Wong"
                },
                {
                    "authorId": "2280349425",
                    "name": "Yi Zhang"
                }
            ],
            "abstract": "The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.",
            "corpus_id": 267069190,
            "sentences": [
                {
                    "corpus_id": "267069190",
                    "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
                    "text": "Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches. \n\nOne of the seminal works in this domain was presented by Shokri et al. [51]. They provided a concrete framework to understand membership inference attacks, introducing an attack model f attack . The input x attack is a prediction confidence vector consisting of the correct label class and a target model (the model being attacked). The output of the attack model is a prediction class \"in\" (member) or \"out\" (non-member). \n\nFor further insights, we show the experimental results in TA-BLE II and briefly introduce seminal and contemporary works on membership inference attacks. Key references include [51]- [80]. (For some unintroduced papers, we also display them in   [52] identify individual records with high accuracy in real-world datasets using blackbox queries on ML models. They argue that combining generalization and perturbation with careful training set selection can be used to defend against such attacks. Nasr et al. [82] assess the privacy vulnerability of the stochastic gradient descent algorithm and design a deep learning attack model to calculate the membership probability of target data points. They also evaluate the privacy of pre-trained state-of-the-art models on the CIFAR-100 dataset to validate the effectiveness of whitebox inference attacks. Liwei Song et al. [55] propose two new membership inference attacks against adversarial robust models, leveraging predictions from adversarial examples and validated worst-case predictions.",
                    "score": 0.6158949023921279,
                    "section_title": "A. Membership Inference Attacks",
                    "char_start_offset": 26516,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 866
                        },
                        {
                            "start": 869,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1063
                        },
                        {
                            "start": 1064,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1291
                        },
                        {
                            "start": 1294,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1482
                        },
                        {
                            "start": 1483,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2333
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 940,
                            "end": 944,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1471,
                            "end": 1475,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1802,
                            "end": 1806,
                            "matchedPaperCorpusId": "54444175"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them\n# Venue: arXiv.org\n# Authors: Chao Liu, Boxi Chen, Wei Shao, Chris Zhang, Kelvin Wong, Yi Zhang\n## Abstract\nThe advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as adversary models, attack targets, and key security attributes (confidentiality, availability, and integrity). We delve into the underlying techniques of ML attacks in IoT environment, providing a critical evaluation of their mechanisms and impacts. Furthermore, our research thoroughly assesses 65 libraries, both author-contributed and third-party, evaluating their role in safeguarding model and data privacy. We emphasize the availability and usability of these libraries, aiming to arm the community with the necessary tools to bolster their defenses against the evolving threat landscape. Through our comprehensive review and analysis, this paper seeks to contribute to the ongoing discourse on ML-based IoT security, offering valuable insights and practical solutions to secure ML models and data in the rapidly expanding field of artificial intelligence in IoT.\n## A. Membership Inference Attacks\nMembership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches. \n\nOne of the seminal works in this domain was presented by Shokri et al. [51]. They provided a concrete framework to understand membership inference attacks, introducing an attack model f attack . The input x attack is a prediction confidence vector consisting of the correct label class and a target model (the model being attacked). The output of the attack model is a prediction class \"in\" (member) or \"out\" (non-member). \n\nFor further insights, we show the experimental results in TA-BLE II and briefly introduce seminal and contemporary works on membership inference attacks. Key references include [51]- [80]. (For some unintroduced papers, we also display them in   [52] identify individual records with high accuracy in real-world datasets using blackbox queries on ML models. They argue that combining generalization and perturbation with careful training set selection can be used to defend against such attacks. Nasr et al. [82] assess the privacy vulnerability of the stochastic gradient descent algorithm and design a deep learning attack model to calculate the membership probability of target data points. They also evaluate the privacy of pre-trained state-of-the-art models on the CIFAR-100 dataset to validate the effectiveness of whitebox inference attacks. Liwei Song et al. [55] propose two new membership inference attacks against adversarial robust models, leveraging predictions from adversarial examples and validated worst-case predictions.",
            "reference_string": "[267069190 | Liu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
            "venue": "USENIX Security Symposium",
            "year": 2019,
            "reference_count": 59,
            "citation_count": 257,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66697271",
                    "name": "A. Salem"
                },
                {
                    "authorId": "3407762",
                    "name": "Apratim Bhattacharyya"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. \nIn this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model (CBM-GAN) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.",
            "corpus_id": 91184074,
            "sentences": [
                {
                    "corpus_id": "91184074",
                    "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
                    "text": "Membership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
                    "score": 0.6028345797635967,
                    "section_title": "Related Works",
                    "char_start_offset": 46212,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 780
                        },
                        {
                            "start": 783,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1376
                        },
                        {
                            "start": 1379,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2052
                        },
                        {
                            "start": 2053,
                            "end": 2159
                        },
                        {
                            "start": 2160,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 235,
                            "end": 239,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 764,
                            "end": 767,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 767,
                            "end": 770,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 770,
                            "end": 773,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 776,
                            "end": 779,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1414,
                            "end": 1418,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1739,
                            "end": 1743,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 1743,
                            "end": 1746,
                            "matchedPaperCorpusId": "5051282"
                        },
                        {
                            "start": 1778,
                            "end": 1782,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1782,
                            "end": 1785,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 2067,
                            "end": 2071,
                            "matchedPaperCorpusId": "2984526"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning\n# Venue: USENIX Security Symposium\n# Authors: A. Salem, Apratim Bhattacharyya, M. Backes, Mario Fritz, Yang Zhang\n## Abstract\nMachine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. \nIn this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model (CBM-GAN) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.\n## Related Works\nMembership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
            "reference_string": "[91184074 | Salem et al. | 2019 | Citations: 257]"
        },
        {
            "title": "Membership Leakage in Label-Only Exposures",
            "venue": "Conference on Computer and Communications Security",
            "year": 2020,
            "reference_count": 58,
            "citation_count": 246,
            "influential_citation_count": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.15528",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.15528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
            "corpus_id": 237563320,
            "sentences": [
                {
                    "corpus_id": "237563320",
                    "title": "Membership Leakage in Label-Only Exposures",
                    "text": "Various research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
                    "score": 0.5916638537064477,
                    "section_title": "Related Works",
                    "char_start_offset": 38139,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 179
                        },
                        {
                            "start": 182,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1879
                        },
                        {
                            "start": 1882,
                            "end": 1920
                        },
                        {
                            "start": 1921,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2180
                        },
                        {
                            "start": 2181,
                            "end": 2292
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 318,
                            "end": 321,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 321,
                            "end": 324,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 324,
                            "end": 327,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 347,
                            "end": 351,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 367,
                            "end": 371,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 738,
                            "end": 742,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 832,
                            "end": 836,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 987,
                            "end": 991,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 991,
                            "end": 994,
                            "matchedPaperCorpusId": "211532510"
                        },
                        {
                            "start": 997,
                            "end": 1000,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1000,
                            "end": 1003,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1056,
                            "end": 1060,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 2073,
                            "end": 2077,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Membership Leakage in Label-Only Exposures\n# Venue: Conference on Computer and Communications Security\n# Authors: Zheng Li, Yang Zhang\n## Abstract\nMachine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.\n## Related Works\nVarious research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
            "reference_string": "[237563320 | Li et al. | 2020 | Citations: 246]"
        },
        {
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Symposium on Security and Privacy",
            "year": 2016,
            "reference_count": 41,
            "citation_count": 4160,
            "influential_citation_count": 619,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1610.05820",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.05820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "34828439",
                    "name": "M. Stronati"
                },
                {
                    "authorId": "3469125",
                    "name": "Congzheng Song"
                },
                {
                    "authorId": "1723945",
                    "name": "Vitaly Shmatikov"
                }
            ],
            "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
            "corpus_id": 10488675,
            "sentences": [],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks Against Machine Learning Models\n# Venue: IEEE Symposium on Security and Privacy\n# Authors: R. Shokri, M. Stronati, Congzheng Song, Vitaly Shmatikov\n## Abstract\nWe quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.\n",
            "reference_string": "[10488675 | Shokri et al. | 2016 | Citations: 4160]"
        },
        {
            "title": "Leveraging Multiple Adversarial Perturbation Distances for Enhanced Membership Inference Attack in Federated Learning",
            "venue": "Symmetry",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/sym16121677",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/sym16121677?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/sym16121677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336171446",
                    "name": "Fan Xia"
                },
                {
                    "authorId": "2336189090",
                    "name": "Yuhao Liu"
                },
                {
                    "authorId": "2336172334",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "2297431838",
                    "name": "Zheng Yu"
                },
                {
                    "authorId": "2336264478",
                    "name": "Xingwei Cai"
                },
                {
                    "authorId": "2336144088",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2318042338",
                    "name": "Zhiyong Zha"
                },
                {
                    "authorId": "2336109193",
                    "name": "Dai Hou"
                },
                {
                    "authorId": "2323202813",
                    "name": "Kai Peng"
                }
            ],
            "abstract": "In recent years, federated learning (FL) has gained significant attention for its ability to protect data privacy during distributed training. However, it also introduces new privacy leakage risks. Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning. Existing research on membership inference attacks in federated learning has primarily focused on leveraging intrinsic model parameters or manipulating the training process. However, the widespread adoption of privacy-preserving frameworks in federated learning has significantly diminished the effectiveness of traditional attack methods. To overcome this limitation, this paper aims to explore an efficient Membership Inference Attack algorithm tailored for encrypted federated learning scenarios, providing new perspectives for optimizing privacy-preserving technologies. Specifically, this paper proposes a novel Membership Inference Attack algorithm based on multiple adversarial perturbation distances (MAPD_MIA) by leveraging the asymmetry in adversarial perturbation distributions near decision boundaries between member and non-member samples. By analyzing these asymmetric perturbation characteristics, the algorithm achieves accurate membership identification. Experimental results demonstrate that the proposed algorithm achieves accuracy rates of 63.0%, 68.7%, and 59.5%, and precision rates of 59.0%, 65.9%, and 55.8% on CIFAR10, CIFAR100, and MNIST datasets, respectively, outperforming three mainstream Membership Inference Attack methods. Furthermore, the algorithm exhibits robust attack performance against two common defense mechanisms, MemGuard and DP-SGD. This study provides new benchmarks and methodologies for evaluating membership privacy leakage risks in federated learning scenarios.",
            "corpus_id": 274869679,
            "sentences": [],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Leveraging Multiple Adversarial Perturbation Distances for Enhanced Membership Inference Attack in Federated Learning\n# Venue: Symmetry\n# Authors: Fan Xia, Yuhao Liu, Bo Jin, Zheng Yu, Xingwei Cai, Hao Li, Zhiyong Zha, Dai Hou, Kai Peng\n## Abstract\nIn recent years, federated learning (FL) has gained significant attention for its ability to protect data privacy during distributed training. However, it also introduces new privacy leakage risks. Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning. Existing research on membership inference attacks in federated learning has primarily focused on leveraging intrinsic model parameters or manipulating the training process. However, the widespread adoption of privacy-preserving frameworks in federated learning has significantly diminished the effectiveness of traditional attack methods. To overcome this limitation, this paper aims to explore an efficient Membership Inference Attack algorithm tailored for encrypted federated learning scenarios, providing new perspectives for optimizing privacy-preserving technologies. Specifically, this paper proposes a novel Membership Inference Attack algorithm based on multiple adversarial perturbation distances (MAPD_MIA) by leveraging the asymmetry in adversarial perturbation distributions near decision boundaries between member and non-member samples. By analyzing these asymmetric perturbation characteristics, the algorithm achieves accurate membership identification. Experimental results demonstrate that the proposed algorithm achieves accuracy rates of 63.0%, 68.7%, and 59.5%, and precision rates of 59.0%, 65.9%, and 55.8% on CIFAR10, CIFAR100, and MNIST datasets, respectively, outperforming three mainstream Membership Inference Attack methods. Furthermore, the algorithm exhibits robust attack performance against two common defense mechanisms, MemGuard and DP-SGD. This study provides new benchmarks and methodologies for evaluating membership privacy leakage risks in federated learning scenarios.\n",
            "reference_string": "[274869679 | Xia et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
            "venue": "Italian National Conference on Sensors",
            "year": 2021,
            "reference_count": 37,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1424-8220/21/23/7806/pdf?version=1637744633",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8659496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111246337",
                    "name": "Jinmyeong Shin"
                },
                {
                    "authorId": "2100675",
                    "name": "Seok-Hwan Choi"
                },
                {
                    "authorId": "2111227849",
                    "name": "Yoon-Ho Choi"
                }
            ],
            "abstract": "As the amount of data collected and analyzed by machine learning technology increases, data that can identify individuals is also being collected in large quantities. In particular, as deep learning technology\u2014which requires a large amount of analysis data\u2014is activated in various service fields, the possibility of exposing sensitive information of users increases, and the user privacy problem is growing more than ever. As a solution to this user\u2019s data privacy problem, homomorphic encryption technology, which is an encryption technology that supports arithmetic operations using encrypted data, has been applied to various field including finance and health care in recent years. If so, is it possible to use the deep learning service while preserving the data privacy of users by using the data to which homomorphic encryption is applied? In this paper, we propose three attack methods to infringe user\u2019s data privacy by exploiting possible security vulnerabilities in the process of using homomorphic encryption-based deep learning services for the first time. To specify and verify the feasibility of exploiting possible security vulnerabilities, we propose three attacks: (1) an adversarial attack exploiting communication link between client and trusted party; (2) a reconstruction attack using the paired input and output data; and (3) a membership inference attack by malicious insider. In addition, we describe real-world exploit scenarios for financial and medical services. From the experimental evaluation results, we show that the adversarial example and reconstruction attacks are a practical threat to homomorphic encryption-based deep learning models. The adversarial attack decreased average classification accuracy from 0.927 to 0.043, and the reconstruction attack showed average reclassification accuracy of 0.888, respectively.",
            "corpus_id": 244809678,
            "sentences": [
                {
                    "corpus_id": "244809678",
                    "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
                    "text": "Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack [6], the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties. \n\nThe membership inference attack which extracts the membership information only is not a critical attack in itself. However, since each party knows additional information about the others, membership information can be linked with this background knowledge and used to identify someone's identity and sensitive information.",
                    "score": 0.7345342611326589,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 17872,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 625
                        },
                        {
                            "start": 626,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1107
                        },
                        {
                            "start": 1108,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1456
                        },
                        {
                            "start": 1457,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1715
                        },
                        {
                            "start": 1718,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 2040
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1203,
                            "end": 1206,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Is Homomorphic Encryption-Based Deep Learning Secure Enough?\n# Venue: Italian National Conference on Sensors\n# Authors: Jinmyeong Shin, Seok-Hwan Choi, Yoon-Ho Choi\n## Abstract\nAs the amount of data collected and analyzed by machine learning technology increases, data that can identify individuals is also being collected in large quantities. In particular, as deep learning technology\u2014which requires a large amount of analysis data\u2014is activated in various service fields, the possibility of exposing sensitive information of users increases, and the user privacy problem is growing more than ever. As a solution to this user\u2019s data privacy problem, homomorphic encryption technology, which is an encryption technology that supports arithmetic operations using encrypted data, has been applied to various field including finance and health care in recent years. If so, is it possible to use the deep learning service while preserving the data privacy of users by using the data to which homomorphic encryption is applied? In this paper, we propose three attack methods to infringe user\u2019s data privacy by exploiting possible security vulnerabilities in the process of using homomorphic encryption-based deep learning services for the first time. To specify and verify the feasibility of exploiting possible security vulnerabilities, we propose three attacks: (1) an adversarial attack exploiting communication link between client and trusted party; (2) a reconstruction attack using the paired input and output data; and (3) a membership inference attack by malicious insider. In addition, we describe real-world exploit scenarios for financial and medical services. From the experimental evaluation results, we show that the adversarial example and reconstruction attacks are a practical threat to homomorphic encryption-based deep learning models. The adversarial attack decreased average classification accuracy from 0.927 to 0.043, and the reconstruction attack showed average reclassification accuracy of 0.888, respectively.\n## Membership Inference Attack\nSimilar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack [6], the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties. \n\nThe membership inference attack which extracts the membership information only is not a critical attack in itself. However, since each party knows additional information about the others, membership information can be linked with this background knowledge and used to identify someone's identity and sensitive information.",
            "reference_string": "[244809678 | Shin et al. | 2021 | Citations: 4]"
        },
        {
            "title": "Privacy Threats in Stable Diffusion Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 68,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900852898",
                    "name": "Thomas Cilloni"
                },
                {
                    "authorId": "2061097936",
                    "name": "Charles Fleming"
                },
                {
                    "authorId": "2057153656",
                    "name": "Charles Walter"
                }
            ],
            "abstract": "This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.",
            "corpus_id": 265220927,
            "sentences": [
                {
                    "corpus_id": "265220927",
                    "title": "Privacy Threats in Stable Diffusion Models",
                    "text": "The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset. This section formalizes MI attacks as a security game and briefly describes common evaluation metrics. \n\n1) Membership Inference Attacks Formalized: The definition of a membership inference attack as a security game follows from [12] and [48]. The game has an attacker who is the carrier of the membership inference attack and a defender who is the owner of the victim ML model f and has knowledge of its training data distribution D. \n\n1) The defender samples a training dataset D train \u2190 D, and optionally additional testing/validation datasets D * \u2190 D, and trains a model f \u03b8 \u2190 T (D train ). \n\n2) The defender flips a coin c D , and if the result is heads, gives the adversary a sample (x, y) \u2208 D train ; otherwise, it supplies a sample (x, y) \u2208 D /D train . \n\n3) The adversary, with oracle access to the model f \u03b8 and the distribution D, tries to guess whether (x, y) is originated from D train or from D /D train . Their guess is produced using a predictive model/system M that can use both D and f \u03b8 , and is registered as a coin face c A \u2190 M (x, y, D, f \u03b8 ). 4) If the adversary guesses the coin face correctly (c A = c D ), the membership of the sample is correctly inferred. Over multiple iterations of the game, one can measure the attacker's power. In this formulation, the attacker is given both samples from the training data distribution and their associated labels. The attacker also has query access to the training data distribution, which allows them to train a virtually unlimited number of shadow models to mimic the behavior of the victim model [24], thereby enabling theoretically more powerful attacks. These two assumptions are what MI attacks commonly require, and this threat model is most widely used in the literature; however, an attacker does not necessarily need to use all of this information to carry out MI attacks.",
                    "score": 0.5984619294180568,
                    "section_title": "B. Membership Inference Attacks",
                    "char_start_offset": 9907,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 373
                        },
                        {
                            "start": 376,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 705
                        },
                        {
                            "start": 708,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1032
                        },
                        {
                            "start": 1035,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2120
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 500,
                            "end": 504,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 509,
                            "end": 513,
                            "matchedPaperCorpusId": "218862728"
                        },
                        {
                            "start": 1837,
                            "end": 1841,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Privacy Threats in Stable Diffusion Models\n# Venue: arXiv.org\n# Authors: Thomas Cilloni, Charles Fleming, Charles Walter\n## Abstract\nThis paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.\n## B. Membership Inference Attacks\nThe goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset. This section formalizes MI attacks as a security game and briefly describes common evaluation metrics. \n\n1) Membership Inference Attacks Formalized: The definition of a membership inference attack as a security game follows from [12] and [48]. The game has an attacker who is the carrier of the membership inference attack and a defender who is the owner of the victim ML model f and has knowledge of its training data distribution D. \n\n1) The defender samples a training dataset D train \u2190 D, and optionally additional testing/validation datasets D * \u2190 D, and trains a model f \u03b8 \u2190 T (D train ). \n\n2) The defender flips a coin c D , and if the result is heads, gives the adversary a sample (x, y) \u2208 D train ; otherwise, it supplies a sample (x, y) \u2208 D /D train . \n\n3) The adversary, with oracle access to the model f \u03b8 and the distribution D, tries to guess whether (x, y) is originated from D train or from D /D train . Their guess is produced using a predictive model/system M that can use both D and f \u03b8 , and is registered as a coin face c A \u2190 M (x, y, D, f \u03b8 ). 4) If the adversary guesses the coin face correctly (c A = c D ), the membership of the sample is correctly inferred. Over multiple iterations of the game, one can measure the attacker's power. In this formulation, the attacker is given both samples from the training data distribution and their associated labels. The attacker also has query access to the training data distribution, which allows them to train a virtually unlimited number of shadow models to mimic the behavior of the victim model [24], thereby enabling theoretically more powerful attacks. These two assumptions are what MI attacks commonly require, and this threat model is most widely used in the literature; however, an attacker does not necessarily need to use all of this information to carry out MI attacks.",
            "reference_string": "[265220927 | Cilloni et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Understanding Membership Inferences on Well-Generalized Learning Models",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 46,
            "citation_count": 224,
            "influential_citation_count": 14,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.04889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3147214",
                    "name": "Yunhui Long"
                },
                {
                    "authorId": "3094927",
                    "name": "Vincent Bindschaedler"
                },
                {
                    "authorId": "2152507640",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "3203018",
                    "name": "Diyue Bu"
                },
                {
                    "authorId": "50141047",
                    "name": "Xiaofeng Wang"
                },
                {
                    "authorId": "2112389071",
                    "name": "Haixu Tang"
                },
                {
                    "authorId": "1785347",
                    "name": "Carl A. Gunter"
                },
                {
                    "authorId": "145126969",
                    "name": "Kai Chen"
                }
            ],
            "abstract": "Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood. In this paper, we report a study that discovers overfitting to be a sufficient but not a necessary condition for an MIA to succeed. More specifically, we demonstrate that even a well-generalized model contains vulnerable instances subject to a new generalized MIA (GMIA). In GMIA, we use novel techniques for selecting vulnerable instances and detecting their subtle influences ignored by overfitting metrics. Specifically, we successfully identify individual records with high precision in real-world datasets by querying black-box machine learning models. Further we show that a vulnerable record can even be indirectly attacked by querying other related records and existing generalization techniques are found to be less effective in protecting the vulnerable instances. Our findings sharpen the understanding of the fundamental cause of the problem: the unique influences the training instance may have on the model.",
            "corpus_id": 3619789,
            "sentences": [
                {
                    "corpus_id": "3619789",
                    "title": "Understanding Membership Inferences on Well-Generalized Learning Models",
                    "text": "In a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model. \n\nOne of the first prominent examples of membership inference attacks occur in the context of Genome-Wide Association Studies (GWAS). The seminal work of Homer et al. [18] show that p-values, a type of aggregated statistics routinely published when reporting the results of studies, could be used to successfully infer membership status. Although this attack requires that the adversary know the genome of the target individual, it teaches an important lesson: seemly harmless aggregate statistics may contain sufficient information for successful membership inferences. As a consequence of this attack, NIH has removed all aggregate data of GWAS from public websites [40]. \n\nMore recently, it was shown that membership inference attacks can occur in the context of machine learning. Shokri et al. [32] demonstrated that an adversary with only black-box access to a classifier could successfully infer membership status. However, their attack only works when the classifier is highly overfitted to its training dataset.",
                    "score": 0.5975704590798082,
                    "section_title": "Background 2.1 Membership Inference Attacks",
                    "char_start_offset": 10863,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 770
                        },
                        {
                            "start": 771,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1106
                        },
                        {
                            "start": 1109,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1452
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 600,
                            "end": 604,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 1101,
                            "end": 1105,
                            "matchedPaperCorpusId": "30161502"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Understanding Membership Inferences on Well-Generalized Learning Models\n# Venue: arXiv.org\n# Authors: Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, Kai Chen\n## Abstract\nMembership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood. In this paper, we report a study that discovers overfitting to be a sufficient but not a necessary condition for an MIA to succeed. More specifically, we demonstrate that even a well-generalized model contains vulnerable instances subject to a new generalized MIA (GMIA). In GMIA, we use novel techniques for selecting vulnerable instances and detecting their subtle influences ignored by overfitting metrics. Specifically, we successfully identify individual records with high precision in real-world datasets by querying black-box machine learning models. Further we show that a vulnerable record can even be indirectly attacked by querying other related records and existing generalization techniques are found to be less effective in protecting the vulnerable instances. Our findings sharpen the understanding of the fundamental cause of the problem: the unique influences the training instance may have on the model.\n## Background 2.1 Membership Inference Attacks\nIn a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model. \n\nOne of the first prominent examples of membership inference attacks occur in the context of Genome-Wide Association Studies (GWAS). The seminal work of Homer et al. [18] show that p-values, a type of aggregated statistics routinely published when reporting the results of studies, could be used to successfully infer membership status. Although this attack requires that the adversary know the genome of the target individual, it teaches an important lesson: seemly harmless aggregate statistics may contain sufficient information for successful membership inferences. As a consequence of this attack, NIH has removed all aggregate data of GWAS from public websites [40]. \n\nMore recently, it was shown that membership inference attacks can occur in the context of machine learning. Shokri et al. [32] demonstrated that an adversary with only black-box access to a classifier could successfully infer membership status. However, their attack only works when the classifier is highly overfitted to its training dataset.",
            "reference_string": "[3619789 | Long et al. | 2018 | Citations: 224]"
        },
        {
            "title": "On the Privacy Risks of Algorithmic Recourse",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "year": 2022,
            "reference_count": 52,
            "citation_count": 31,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2211.05427",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.05427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89583148",
                    "name": "Martin Pawelczyk"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "5880154",
                    "name": "Seth Neel"
                }
            ],
            "abstract": "As predictive models are increasingly being employed to make consequential decisions, there is a growing emphasis on developing techniques that can provide algorithmic recourse to affected individuals. While such recourses can be immensely beneficial to affected individuals, potential adversaries could also exploit these recourses to compromise privacy. In this work, we make the first attempt at investigating if and how an adversary can leverage recourses to infer private information about the underlying model's training data. To this end, we propose a series of novel membership inference attacks which leverage algorithmic recourse. More specifically, we extend the prior literature on membership inference attacks to the recourse setting by leveraging the distances between data instances and their corresponding counterfactuals output by state-of-the-art recourse methods. Extensive experimentation with real world and synthetic datasets demonstrates significant privacy leakage through recourses. Our work establishes unintended privacy leakage as an important risk in the widespread adoption of recourse methods.",
            "corpus_id": 253446930,
            "sentences": [
                {
                    "corpus_id": "253446930",
                    "title": "On the Privacy Risks of Algorithmic Recourse",
                    "text": "However, neither of these works explore if and how adversaries may leverage recourses to infer sensitive information about the underlying model's training data. \n\nIn this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not. In formulating this new class of attacks, we exploit the intuition that the distance between an instance and its corresponding recourse may capture information about whether that instance was used to train the model. We instantiate the aforementioned class of attacks to propose two novel membership inference attacks. Our first attack infers membership by thresholding on the distance between a given instance and its corresponding algorithmic recourse. Our second attack draws inspiration from state-of-the-art loss-based membership inference attacks [5,47] and proposes a likelihood ratio test (LRT) that accounts for algorithmic recourse. Our attacks operate under the assumption that the adversary can only query the recourse algorithm once. This assumption is a lot more practical than those considered in related works [2], and is inline with real-world settings where an end user would typically be provided with a single recourse and will not be able to query the underlying model or recourse algorithm multiple times [41,46]. To the best of our knowledge, our work is the first to introduce membership inference attacks which leverage algorithmic recourse. \n\nWe experiment with multiple real world datasets spanning diverse domains such as lending, healthcare, and law to evaluate the effectiveness of the proposed attacks. Our experimental results clearly demonstrate the efficacy of the proposed attacks, and highlight significant privacy leakage through recourses generated by a wide range of recourse algorithms. In addition, the proposed attacks also outperform the state-of-the-art loss-based membership inference attacks (which do not leverage recourses) on data with sufficiently high dimensionality, thus highlighting the promise of our recourse-based attacks as generic membership inference attacks.",
                    "score": 0.591234134425322,
                    "section_title": "Introduction",
                    "char_start_offset": 2063,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 163,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1785
                        },
                        {
                            "start": 1788,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2438
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1646,
                            "end": 1650,
                            "matchedPaperCorpusId": "51734899"
                        },
                        {
                            "start": 1650,
                            "end": 1653,
                            "matchedPaperCorpusId": "3995299"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: On the Privacy Risks of Algorithmic Recourse\n# Venue: International Conference on Artificial Intelligence and Statistics\n# Authors: Martin Pawelczyk, Himabindu Lakkaraju, Seth Neel\n## Abstract\nAs predictive models are increasingly being employed to make consequential decisions, there is a growing emphasis on developing techniques that can provide algorithmic recourse to affected individuals. While such recourses can be immensely beneficial to affected individuals, potential adversaries could also exploit these recourses to compromise privacy. In this work, we make the first attempt at investigating if and how an adversary can leverage recourses to infer private information about the underlying model's training data. To this end, we propose a series of novel membership inference attacks which leverage algorithmic recourse. More specifically, we extend the prior literature on membership inference attacks to the recourse setting by leveraging the distances between data instances and their corresponding counterfactuals output by state-of-the-art recourse methods. Extensive experimentation with real world and synthetic datasets demonstrates significant privacy leakage through recourses. Our work establishes unintended privacy leakage as an important risk in the widespread adoption of recourse methods.\n## Introduction\nHowever, neither of these works explore if and how adversaries may leverage recourses to infer sensitive information about the underlying model's training data. \n\nIn this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not. In formulating this new class of attacks, we exploit the intuition that the distance between an instance and its corresponding recourse may capture information about whether that instance was used to train the model. We instantiate the aforementioned class of attacks to propose two novel membership inference attacks. Our first attack infers membership by thresholding on the distance between a given instance and its corresponding algorithmic recourse. Our second attack draws inspiration from state-of-the-art loss-based membership inference attacks [5,47] and proposes a likelihood ratio test (LRT) that accounts for algorithmic recourse. Our attacks operate under the assumption that the adversary can only query the recourse algorithm once. This assumption is a lot more practical than those considered in related works [2], and is inline with real-world settings where an end user would typically be provided with a single recourse and will not be able to query the underlying model or recourse algorithm multiple times [41,46]. To the best of our knowledge, our work is the first to introduce membership inference attacks which leverage algorithmic recourse. \n\nWe experiment with multiple real world datasets spanning diverse domains such as lending, healthcare, and law to evaluate the effectiveness of the proposed attacks. Our experimental results clearly demonstrate the efficacy of the proposed attacks, and highlight significant privacy leakage through recourses generated by a wide range of recourse algorithms. In addition, the proposed attacks also outperform the state-of-the-art loss-based membership inference attacks (which do not leverage recourses) on data with sufficiently high dimensionality, thus highlighting the promise of our recourse-based attacks as generic membership inference attacks.",
            "reference_string": "[253446930 | Pawelczyk et al. | 2022 | Citations: 31]"
        },
        {
            "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 14,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.12401",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.12401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2173325427",
                    "name": "Na Huang"
                },
                {
                    "authorId": "143770118",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "2889630",
                    "name": "Hengliang Luo"
                },
                {
                    "authorId": "1696030",
                    "name": "M. de Rijke"
                },
                {
                    "authorId": "2780667",
                    "name": "Z. Ren"
                }
            ],
            "abstract": "Learned recommender systems may inadvertently leak information about their training data, leading to privacy violations. We investigate privacy threats faced by recommender systems through the lens of membership inference. In such attacks, an adversary aims to infer whether a user's data is used to train the target recommender. To achieve this, previous work has used a shadow recommender to derive training data for the attack model, and then predicts the membership by calculating difference vectors between users' historical interactions and recommended items. State-of-the-art methods face two challenging problems: (i) training data for the attack model is biased due to the gap between shadow and target recommenders, and (ii) hidden states in recommenders are not observational, resulting in inaccurate estimations of difference vectors. To address the above limitations, we propose a Debiasing Learning for Membership Inference Attacks against recommender systems (DL-MIA) framework that has four main components: (i) a difference vector generator, (ii) a disentangled encoder, (iii) a weight estimator, and (iv) an attack model. To mitigate the gap between recommenders, a variational auto-encoder (VAE) based disentangled encoder is devised to identify recommender invariant and specific features. To reduce the estimation bias, we design a weight estimator, assigning a truth-level score for each difference vector to indicate estimation accuracy. We evaluate DL-MIA against both general recommenders and sequential recommenders on three real-world datasets. Experimental results show that DL-MIA effectively alleviates training and estimation biases simultaneously, and \u00cdachieves state-of-the-art attack performance.",
            "corpus_id": 250048543,
            "sentences": [
                {
                    "corpus_id": "250048543",
                    "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
                    "text": "Recently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
                    "score": 0.5759545309807956,
                    "section_title": "Membership inference attacks",
                    "char_start_offset": 5381,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1862
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 129,
                            "end": 132,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 132,
                            "end": 135,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 156,
                            "end": 160,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 295,
                            "end": 299,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 667,
                            "end": 671,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 715,
                            "end": 719,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 819,
                            "end": 823,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 949,
                            "end": 953,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1097,
                            "end": 1100,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1100,
                            "end": 1103,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1272,
                            "end": 1276,
                            "matchedPaperCorpusId": "237532289"
                        },
                        {
                            "start": 1493,
                            "end": 1497,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1507,
                            "end": 1511,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1534,
                            "end": 1538,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1561,
                            "end": 1564,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1564,
                            "end": 1567,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1584,
                            "end": 1587,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1587,
                            "end": 1590,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1647,
                            "end": 1651,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1740,
                            "end": 1744,
                            "matchedPaperCorpusId": "237532289"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Debiasing Learning for Membership Inference Attacks Against Recommender Systems\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Zihan Wang, Na Huang, Fei Sun, Pengjie Ren, Zhumin Chen, Hengliang Luo, M. de Rijke, Z. Ren\n## Abstract\nLearned recommender systems may inadvertently leak information about their training data, leading to privacy violations. We investigate privacy threats faced by recommender systems through the lens of membership inference. In such attacks, an adversary aims to infer whether a user's data is used to train the target recommender. To achieve this, previous work has used a shadow recommender to derive training data for the attack model, and then predicts the membership by calculating difference vectors between users' historical interactions and recommended items. State-of-the-art methods face two challenging problems: (i) training data for the attack model is biased due to the gap between shadow and target recommenders, and (ii) hidden states in recommenders are not observational, resulting in inaccurate estimations of difference vectors. To address the above limitations, we propose a Debiasing Learning for Membership Inference Attacks against recommender systems (DL-MIA) framework that has four main components: (i) a difference vector generator, (ii) a disentangled encoder, (iii) a weight estimator, and (iv) an attack model. To mitigate the gap between recommenders, a variational auto-encoder (VAE) based disentangled encoder is devised to identify recommender invariant and specific features. To reduce the estimation bias, we design a weight estimator, assigning a truth-level score for each difference vector to indicate estimation accuracy. We evaluate DL-MIA against both general recommenders and sequential recommenders on three real-world datasets. Experimental results show that DL-MIA effectively alleviates training and estimation biases simultaneously, and \u00cdachieves state-of-the-art attack performance.\n## Membership inference attacks\nRecently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
            "reference_string": "[250048543 | Wang et al. | 2022 | Citations: 14]"
        },
        {
            "title": "PPA: Preference Profiling Attack Against Federated Learning",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2022,
            "reference_count": 78,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14722/ndss.2023.23171",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.04856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1845880105",
                    "name": "Chunyi Zhou"
                },
                {
                    "authorId": "39922366",
                    "name": "Yansong Gao"
                },
                {
                    "authorId": "2068511826",
                    "name": "Anmin Fu"
                },
                {
                    "authorId": "2157740727",
                    "name": "Kai Chen"
                },
                {
                    "authorId": "151498397",
                    "name": "Zhiyang Dai"
                },
                {
                    "authorId": "2116763991",
                    "name": "Zhi Zhang"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2155342827",
                    "name": "Yuqing Zhang"
                }
            ],
            "abstract": "Federated learning (FL) trains a global model across a number of decentralized users, each with a local dataset. Compared to traditional centralized learning, FL does not require direct access to local datasets and thus aims to mitigate data privacy concerns. However, data privacy leakage in FL still exists due to inference attacks, including membership inference, property inference, and data inversion. In this work, we propose a new type of privacy inference attack, coined Preference Profiling Attack (PPA), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies. In general, PPA can profile top-k (i.e., k = 1, 2, 3 and k = 1 in particular) preferences contingent on the local client (user)'s characteristics. Our key insight is that the gradient variation of a local user's model has a distinguishable sensitivity to the sample proportion of a given class, especially the majority (minority) class. By observing a user model's gradient sensitivity to a class, PPA can profile the sample proportion of the class in the user's local dataset, and thus the user's preference of the class is exposed. The inherent statistical heterogeneity of FL further facilitates PPA. We have extensively evaluated the PPA's effectiveness using four datasets (MNIST, CIFAR10, RAF-DB and Products-10K). Our results show that PPA achieves 90% and 98% top-1 attack accuracy to the MNIST and CIFAR10, respectively. More importantly, in real-world commercial scenarios of shopping (i.e., Products-10K) and social network (i.e., RAF-DB), PPA gains a top-1 attack accuracy of 78% in the former case to infer the most ordered items (i.e., as a commercial competitor), and 88% in the latter case to infer a victim user's most often facial expressions, e.g., disgusted.",
            "corpus_id": 246706163,
            "sentences": [
                {
                    "corpus_id": "246706163",
                    "title": "PPA: Preference Profiling Attack Against Federated Learning",
                    "text": "The membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
                    "score": 0.5720511221436828,
                    "section_title": "B. Membership Inference Attack",
                    "char_start_offset": 9055,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1332
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 375,
                            "end": 379,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 624,
                            "end": 627,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 828,
                            "end": 832,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "244077745"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1259,
                            "end": 1263,
                            "matchedPaperCorpusId": "199546273"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: PPA: Preference Profiling Attack Against Federated Learning\n# Venue: Network and Distributed System Security Symposium\n# Authors: Chunyi Zhou, Yansong Gao, Anmin Fu, Kai Chen, Zhiyang Dai, Zhi Zhang, Minhui Xue, Yuqing Zhang\n## Abstract\nFederated learning (FL) trains a global model across a number of decentralized users, each with a local dataset. Compared to traditional centralized learning, FL does not require direct access to local datasets and thus aims to mitigate data privacy concerns. However, data privacy leakage in FL still exists due to inference attacks, including membership inference, property inference, and data inversion. In this work, we propose a new type of privacy inference attack, coined Preference Profiling Attack (PPA), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies. In general, PPA can profile top-k (i.e., k = 1, 2, 3 and k = 1 in particular) preferences contingent on the local client (user)'s characteristics. Our key insight is that the gradient variation of a local user's model has a distinguishable sensitivity to the sample proportion of a given class, especially the majority (minority) class. By observing a user model's gradient sensitivity to a class, PPA can profile the sample proportion of the class in the user's local dataset, and thus the user's preference of the class is exposed. The inherent statistical heterogeneity of FL further facilitates PPA. We have extensively evaluated the PPA's effectiveness using four datasets (MNIST, CIFAR10, RAF-DB and Products-10K). Our results show that PPA achieves 90% and 98% top-1 attack accuracy to the MNIST and CIFAR10, respectively. More importantly, in real-world commercial scenarios of shopping (i.e., Products-10K) and social network (i.e., RAF-DB), PPA gains a top-1 attack accuracy of 78% in the former case to infer the most ordered items (i.e., as a commercial competitor), and 88% in the latter case to infer a victim user's most often facial expressions, e.g., disgusted.\n## B. Membership Inference Attack\nThe membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
            "reference_string": "[246706163 | Zhou et al. | 2022 | Citations: 23]"
        },
        {
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "venue": "USENIX Security Symposium",
            "year": 2020,
            "reference_count": 60,
            "citation_count": 375,
            "influential_citation_count": 66,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.10595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144173853",
                    "name": "Liwei Song"
                },
                {
                    "authorId": "143615345",
                    "name": "Prateek Mittal"
                }
            ],
            "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
            "corpus_id": 214623088,
            "sentences": [
                {
                    "corpus_id": "214623088",
                    "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
                    "text": "A recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
                    "score": 0.5711772083768655,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2175
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 157,
                            "end": 159,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "52218951"
                        },
                        {
                            "start": 162,
                            "end": 165,
                            "matchedPaperCorpusId": "5051282"
                        },
                        {
                            "start": 165,
                            "end": 168,
                            "matchedPaperCorpusId": "91184074"
                        },
                        {
                            "start": 168,
                            "end": 171,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 171,
                            "end": 174,
                            "matchedPaperCorpusId": "2904063"
                        },
                        {
                            "start": 353,
                            "end": 357,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 357,
                            "end": 360,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 463,
                            "end": 466,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 616,
                            "end": 620,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1063,
                            "end": 1066,
                            "matchedPaperCorpusId": "202539472"
                        },
                        {
                            "start": 1066,
                            "end": 1069,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1069,
                            "end": 1072,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1072,
                            "end": 1075,
                            "matchedPaperCorpusId": "201126909"
                        },
                        {
                            "start": 1110,
                            "end": 1114,
                            "matchedPaperCorpusId": "173990571"
                        },
                        {
                            "start": 1114,
                            "end": 1117,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 1223,
                            "end": 1227,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1227,
                            "end": 1230,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1345,
                            "end": 1349,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1471,
                            "end": 1475,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1753,
                            "end": 1757,
                            "matchedPaperCorpusId": "202734167"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Systematic Evaluation of Privacy Risks of Machine Learning Models\n# Venue: USENIX Security Symposium\n# Authors: Liwei Song, Prateek Mittal\n## Abstract\nMachine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.\n## Introduction\nA recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
            "reference_string": "[214623088 | Song et al. | 2020 | Citations: 375]"
        },
        {
            "title": "Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 78,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.06686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275287112",
                    "name": "Sanghyun Hong"
                },
                {
                    "authorId": "2327330670",
                    "name": "Fan Wu"
                },
                {
                    "authorId": "2303406541",
                    "name": "Anthony Gruber"
                },
                {
                    "authorId": "2265425696",
                    "name": "Kookjin Lee"
                }
            ],
            "abstract": "Neural ordinary differential equations (NODEs) are an emerging paradigm in scientific computing for modeling dynamical systems. By accurately learning underlying dynamics in data in the form of differential equations, NODEs have been widely adopted in various domains, such as healthcare, finance, computer vision, and language modeling. However, there remains a limited understanding of the privacy implications of these fundamentally different models, particularly with regard to their membership inference risks. In this work, we study the membership inference risks associated with NODEs. We first comprehensively evaluate NODEs against membership inference attacks. We show that NODEs are twice as resistant to these privacy attacks compared to conventional feedforward models such as ResNets. By analyzing the variance in membership risks across different NODE models, we identify the factors that contribute to their lower risks. We then demonstrate, both theoretically and empirically, that membership inference risks can be further mitigated by utilizing a stochastic variant of NODEs: Neural stochastic differential equations (NSDEs). We show that NSDEs are differentially-private (DP) learners that provide the same provable privacy guarantees as DP-SGD, the de-facto mechanism for training private models. NSDEs are also effective in mitigating existing membership inference attacks, demonstrating risks comparable to private models trained with DP-SGD while offering an improved privacy-utility trade-off. Moreover, we propose a drop-in-replacement strategy that efficiently integrates NSDEs into conventional feedforward models to enhance their privacy.",
            "corpus_id": 275471364,
            "sentences": [
                {
                    "corpus_id": "275471364",
                    "title": "Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations",
                    "text": "Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy [19] as a worst-case certification. Existing attacks. Prior work has developed various attacks to exploit differences and identify membership. In our work, we evaluate NODEs against each of the representative attacks described below: \u2022 Yeom et al. [79] formulate an attack which focuses on differences in loss values: for a specific example  = (, ), a model will be more accurate in its predictions when it has seen  during training. The attack predicts  as a member if and only if, for a threshold , the model's loss on  is below ; otherwise, it is classified as a non-member. \u2022 Shokri et al. [66] attack based on shadow models which better approximate the differences in a model's responses to members versus non-members. When constructing shadow models, the attacker artificially generates datasets for training and testing these models so that the members and non-members are known in advance. The attacker collects responses from shadow models for both members and non-members and trains a classifier to predict membership of  based on the target model's response. \u2022 Song and Mittal [67] leverage the prediction correctness to compute the threshold  for identifying membership. Instead of training classifiers to perform attacks,  is designed such that correct predictions with high confidence yield the lowest values while confident but incorrect predictions achieve the highest. \u2022 Watson et al. [73] proposed per-example difficulty calibration in which an attacker leverages shadow models trained without a particular example to compute the average confidence level of a model. The average is then subtracted from the example's confidence obtained from the target model to calibrate the difficulty of the sample. \u2022 Carlini et al. [5] introduced the Likelihood Ratio Attack (LiRA), which carefully controls how the adversary splits shadow training data.",
                    "score": 0.5642738165308872,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 12035,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1905
                        },
                        {
                            "start": 1906,
                            "end": 2104
                        },
                        {
                            "start": 2105,
                            "end": 2239
                        },
                        {
                            "start": 2240,
                            "end": 2379
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 520,
                            "end": 524,
                            "matchedPaperCorpusId": "2565493"
                        },
                        {
                            "start": 768,
                            "end": 772,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1114,
                            "end": 1118,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1608,
                            "end": 1612,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1922,
                            "end": 1926,
                            "matchedPaperCorpusId": "244130249"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations\n# Venue: arXiv.org\n# Authors: Sanghyun Hong, Fan Wu, Anthony Gruber, Kookjin Lee\n## Abstract\nNeural ordinary differential equations (NODEs) are an emerging paradigm in scientific computing for modeling dynamical systems. By accurately learning underlying dynamics in data in the form of differential equations, NODEs have been widely adopted in various domains, such as healthcare, finance, computer vision, and language modeling. However, there remains a limited understanding of the privacy implications of these fundamentally different models, particularly with regard to their membership inference risks. In this work, we study the membership inference risks associated with NODEs. We first comprehensively evaluate NODEs against membership inference attacks. We show that NODEs are twice as resistant to these privacy attacks compared to conventional feedforward models such as ResNets. By analyzing the variance in membership risks across different NODE models, we identify the factors that contribute to their lower risks. We then demonstrate, both theoretically and empirically, that membership inference risks can be further mitigated by utilizing a stochastic variant of NODEs: Neural stochastic differential equations (NSDEs). We show that NSDEs are differentially-private (DP) learners that provide the same provable privacy guarantees as DP-SGD, the de-facto mechanism for training private models. NSDEs are also effective in mitigating existing membership inference attacks, demonstrating risks comparable to private models trained with DP-SGD while offering an improved privacy-utility trade-off. Moreover, we propose a drop-in-replacement strategy that efficiently integrates NSDEs into conventional feedforward models to enhance their privacy.\n## Membership Inference Attacks\nMembership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy [19] as a worst-case certification. Existing attacks. Prior work has developed various attacks to exploit differences and identify membership. In our work, we evaluate NODEs against each of the representative attacks described below: \u2022 Yeom et al. [79] formulate an attack which focuses on differences in loss values: for a specific example  = (, ), a model will be more accurate in its predictions when it has seen  during training. The attack predicts  as a member if and only if, for a threshold , the model's loss on  is below ; otherwise, it is classified as a non-member. \u2022 Shokri et al. [66] attack based on shadow models which better approximate the differences in a model's responses to members versus non-members. When constructing shadow models, the attacker artificially generates datasets for training and testing these models so that the members and non-members are known in advance. The attacker collects responses from shadow models for both members and non-members and trains a classifier to predict membership of  based on the target model's response. \u2022 Song and Mittal [67] leverage the prediction correctness to compute the threshold  for identifying membership. Instead of training classifiers to perform attacks,  is designed such that correct predictions with high confidence yield the lowest values while confident but incorrect predictions achieve the highest. \u2022 Watson et al. [73] proposed per-example difficulty calibration in which an attacker leverages shadow models trained without a particular example to compute the average confidence level of a model. The average is then subtracted from the example's confidence obtained from the target model to calibrate the difficulty of the sample. \u2022 Carlini et al. [5] introduced the Likelihood Ratio Attack (LiRA), which carefully controls how the adversary splits shadow training data.",
            "reference_string": "[275471364 | Hong et al. | 2025 | Citations: 0]"
        },
        {
            "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape",
            "venue": "ACM Computing Surveys",
            "year": 2024,
            "reference_count": 137,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.03636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2133363467",
                    "name": "Joshua C. Zhao"
                },
                {
                    "authorId": "2268859358",
                    "name": "Saurabh Bagchi"
                },
                {
                    "authorId": "121011351",
                    "name": "S. Avestimehr"
                },
                {
                    "authorId": "2300143852",
                    "name": "Kevin S. Chan"
                },
                {
                    "authorId": "2228303",
                    "name": "S. Chaterji"
                },
                {
                    "authorId": "2300092534",
                    "name": "Dimitris Dimitriadis"
                },
                {
                    "authorId": "2300130544",
                    "name": "Jiacheng Li"
                },
                {
                    "authorId": "2265278308",
                    "name": "Ninghui Li"
                },
                {
                    "authorId": "2300093540",
                    "name": "Arash Nourian"
                },
                {
                    "authorId": "2302008236",
                    "name": "Holger Roth"
                }
            ],
            "abstract": "Deep learning has shown incredible potential across a wide array of tasks, and accompanied by this growth has been an insatiable appetite for data. However, a large amount of data needed for enabling deep learning is stored on personal devices, and recent concerns on privacy have further highlighted challenges for accessing such data. As a result, federated learning (FL) has emerged as an important privacy-preserving technology that enables collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server. However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \u201dreverse engineered\u201d to infer information about the private training data. It has been shown under a wide variety of settings that this privacy premise does not hold. In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL. We identify the current limitations of these attacks and highlight the settings in which the privacy of ann FL client can be broken. We further dissect some of the successful industry applications of FL and draw lessons for future successful adoption. We survey the emerging landscape of privacy regulation for FL and conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants.",
            "corpus_id": 269605305,
            "sentences": [
                {
                    "corpus_id": "269605305",
                    "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape",
                    "text": "The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer. Figure 4 summarizes the process of membership inference attacks. \n\nManuscript submitted to ACM CSUR Fig. 4: Membership inference attack workflow from [142]. The attacker first collects information (e.g. parameter updates) from clients, then uses auxiliary dataset to train shadow models which can mimic the whole federated learning procedure, so that one attack model can be trained using the information collected from shadow models. Lastly, the attacker feeds the information collected from real clients into the attack model to perform membership inference attacks.",
                    "score": 0.6349141309137748,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 61847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 603
                        },
                        {
                            "start": 606,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1107
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 689,
                            "end": 694,
                            "matchedPaperCorpusId": "240354058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape\n# Venue: ACM Computing Surveys\n# Authors: Joshua C. Zhao, Saurabh Bagchi, S. Avestimehr, Kevin S. Chan, S. Chaterji, Dimitris Dimitriadis, Jiacheng Li, Ninghui Li, Arash Nourian, Holger Roth\n## Abstract\nDeep learning has shown incredible potential across a wide array of tasks, and accompanied by this growth has been an insatiable appetite for data. However, a large amount of data needed for enabling deep learning is stored on personal devices, and recent concerns on privacy have further highlighted challenges for accessing such data. As a result, federated learning (FL) has emerged as an important privacy-preserving technology that enables collaborative training of machine learning models without the need to send the raw, potentially sensitive, data to a central server. However, the fundamental premise that sending model updates to a server is privacy-preserving only holds if the updates cannot be \u201dreverse engineered\u201d to infer information about the private training data. It has been shown under a wide variety of settings that this privacy premise does not hold. In this survey paper, we provide a comprehensive literature review of the different privacy attacks and defense methods in FL. We identify the current limitations of these attacks and highlight the settings in which the privacy of ann FL client can be broken. We further dissect some of the successful industry applications of FL and draw lessons for future successful adoption. We survey the emerging landscape of privacy regulation for FL and conclude with future directions for taking FL toward the cherished goal of generating accurate models while preserving the privacy of the data from its participants.\n## Membership Inference Attacks\nThe goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer. Figure 4 summarizes the process of membership inference attacks. \n\nManuscript submitted to ACM CSUR Fig. 4: Membership inference attack workflow from [142]. The attacker first collects information (e.g. parameter updates) from clients, then uses auxiliary dataset to train shadow models which can mimic the whole federated learning procedure, so that one attack model can be trained using the information collected from shadow models. Lastly, the attacker feeds the information collected from real clients into the attack model to perform membership inference attacks.",
            "reference_string": "[269605305 | Zhao et al. | 2024 | Citations: 3]"
        },
        {
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07562, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301151068",
                    "name": "Andrey V. Galichin"
                },
                {
                    "authorId": "134451470",
                    "name": "Mikhail Aleksandrovich Pautov"
                },
                {
                    "authorId": "2301152807",
                    "name": "Alexey Zhavoronkin"
                },
                {
                    "authorId": "2279751953",
                    "name": "Oleg Y. Rogov"
                },
                {
                    "authorId": "2257279038",
                    "name": "Ivan Oseledets"
                }
            ],
            "abstract": "While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.",
            "corpus_id": 269756904,
            "sentences": [
                {
                    "corpus_id": "269756904",
                    "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
                    "text": "The objective of the membership inference attack (MIA) [4] is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data.\n\nFormally, given a data sample (x, y), the target model f \u03b8 trained on (possibly fully unknown) dataset D and additional information about f \u03b8 denoted by I, membership inference attack A is defined as the function\n\nA detailed explanation of the proposed membership inference attack will be given in the next sections.",
                    "score": 0.6288191197830052,
                    "section_title": "B. Membership Inference Attacks",
                    "char_start_offset": 5180,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 167,
                            "end": 340
                        },
                        {
                            "start": 340,
                            "end": 517
                        },
                        {
                            "start": 519,
                            "end": 731
                        },
                        {
                            "start": 733,
                            "end": 835
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 55,
                            "end": 58,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Andrey V. Galichin, Mikhail Aleksandrovich Pautov, Alexey Zhavoronkin, Oleg Y. Rogov, Ivan Oseledets\n## Abstract\nWhile Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.\n## B. Membership Inference Attacks\nThe objective of the membership inference attack (MIA) [4] is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data.\n\nFormally, given a data sample (x, y), the target model f \u03b8 trained on (possibly fully unknown) dataset D and additional information about f \u03b8 denoted by I, membership inference attack A is defined as the function\n\nA detailed explanation of the proposed membership inference attack will be given in the next sections.",
            "reference_string": "[269756904 | Galichin et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning",
            "venue": "Expert systems with applications",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 26,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.10199",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.10199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1527113700",
                    "name": "Yuyuan Li"
                },
                {
                    "authorId": "1694815",
                    "name": "Chaochao Chen"
                },
                {
                    "authorId": "1687974",
                    "name": "Xiaolin Zheng"
                },
                {
                    "authorId": "2214803049",
                    "name": "Yizhao Zhang"
                },
                {
                    "authorId": "2211782518",
                    "name": "Biao Gong"
                },
                {
                    "authorId": "2152811080",
                    "name": "Jun Wang"
                }
            ],
            "abstract": "Recent regulations on the Right to be Forgotten have greatly influenced the way of running a recommender system, because users now have the right to withdraw their private data. Besides simply deleting the target data in the database, unlearning the associated data lineage e.g., the learned personal features and preferences in the model, is also necessary for data withdrawal. Existing unlearning methods are mainly devised for generalized machine learning models in classification tasks. In this paper, we first identify two main disadvantages of directly applying existing unlearning methods in the context of recommendation, i.e., (i) unsatisfactory efficiency for large-scale recommendation models and (ii) destruction of collaboration across users and items. To tackle the above issues, we propose an extra-efficient recommendation unlearning method based on Selective and Collaborative Influence Function (SCIF). Our proposed method can (i) avoid any kind of retraining which is computationally prohibitive for large-scale systems, (ii) further enhance efficiency by selectively updating user embedding and (iii) preserve the collaboration across the remaining users and items. Furthermore, in order to evaluate the unlearning completeness, we define a Membership Inference Oracle (MIO), which can justify whether the unlearned data points were in the training set of the model, i.e., whether a data point was completely unlearned. Extensive experiments on two benchmark datasets demonstrate that our proposed method can not only greatly enhance unlearning efficiency, but also achieve adequate unlearning completeness. More importantly, our proposed method outperforms the state-of-the-art unlearning method regarding comprehensive recommendation metrics.",
            "corpus_id": 258236265,
            "sentences": [
                {
                    "corpus_id": "258236265",
                    "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning",
                    "text": "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by Shokri et al. (2017). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model. To improve the performance of machine learning classifiers, i.e., attacker, Shokri et al. (2017) uses shadow models, which simulate the behavior of target model, to generate sufficient training data for the attacker. The following work has investigated various settings of shadow model training and presented several defence techniques (Salem et al., 2018;Wu et al., 2020;Yu et al., 2021). However, current research has primarily concentrated on exploiting shallow classification models, leaving deep learning and regression models such as collaborative filtering largely unexplored.",
                    "score": 0.608808590219945,
                    "section_title": "Membership Inference",
                    "char_start_offset": 11153,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 383,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 638,
                            "end": 658,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 918,
                            "end": 934,
                            "matchedPaperCorpusId": "203836240"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Selective and Collaborative Influence Function for Efficient Recommendation Unlearning\n# Venue: Expert systems with applications\n# Authors: Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang\n## Abstract\nRecent regulations on the Right to be Forgotten have greatly influenced the way of running a recommender system, because users now have the right to withdraw their private data. Besides simply deleting the target data in the database, unlearning the associated data lineage e.g., the learned personal features and preferences in the model, is also necessary for data withdrawal. Existing unlearning methods are mainly devised for generalized machine learning models in classification tasks. In this paper, we first identify two main disadvantages of directly applying existing unlearning methods in the context of recommendation, i.e., (i) unsatisfactory efficiency for large-scale recommendation models and (ii) destruction of collaboration across users and items. To tackle the above issues, we propose an extra-efficient recommendation unlearning method based on Selective and Collaborative Influence Function (SCIF). Our proposed method can (i) avoid any kind of retraining which is computationally prohibitive for large-scale systems, (ii) further enhance efficiency by selectively updating user embedding and (iii) preserve the collaboration across the remaining users and items. Furthermore, in order to evaluate the unlearning completeness, we define a Membership Inference Oracle (MIO), which can justify whether the unlearned data points were in the training set of the model, i.e., whether a data point was completely unlearned. Extensive experiments on two benchmark datasets demonstrate that our proposed method can not only greatly enhance unlearning efficiency, but also achieve adequate unlearning completeness. More importantly, our proposed method outperforms the state-of-the-art unlearning method regarding comprehensive recommendation metrics.\n## Membership Inference\nMembership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by Shokri et al. (2017). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model. To improve the performance of machine learning classifiers, i.e., attacker, Shokri et al. (2017) uses shadow models, which simulate the behavior of target model, to generate sufficient training data for the attacker. The following work has investigated various settings of shadow model training and presented several defence techniques (Salem et al., 2018;Wu et al., 2020;Yu et al., 2021). However, current research has primarily concentrated on exploiting shallow classification models, leaving deep learning and regression models such as collaborative filtering largely unexplored.",
            "reference_string": "[258236265 | Li et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
            "venue": "IEEE Access",
            "year": 2020,
            "reference_count": 127,
            "citation_count": 124,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09064510.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2987435?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2987435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46301056",
                    "name": "Mingfu Xue"
                },
                {
                    "authorId": "1984508",
                    "name": "Chengxiang Yuan"
                },
                {
                    "authorId": "1421758751",
                    "name": "Heyi Wu"
                },
                {
                    "authorId": "2108051942",
                    "name": "Yushu Zhang"
                },
                {
                    "authorId": "2109335717",
                    "name": "Weiqiang Liu"
                }
            ],
            "abstract": "Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",
            "corpus_id": 218473513,
            "sentences": [
                {
                    "corpus_id": "218473513",
                    "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
                    "text": "Liu et al. [73] illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. [70] propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model [70]. According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in [70] is generic, but the success of member inference attacks depends on the overfitting of the model [13], [70]. If it is a well-generalized model, the success rate of the membership inference attack is low.",
                    "score": 0.5966020149953974,
                    "section_title": "2) MEMBERSHIP INFERENCE ATTACK",
                    "char_start_offset": 44055,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 1015
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 11,
                            "end": 15,
                            "matchedPaperCorpusId": "14847504"
                        },
                        {
                            "start": 252,
                            "end": 256,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 531,
                            "end": 535,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 808,
                            "end": 812,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 909,
                            "end": 913,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 915,
                            "end": 919,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Machine Learning Security: Threats, Countermeasures, and Evaluations\n# Venue: IEEE Access\n# Authors: Mingfu Xue, Chengxiang Yuan, Heyi Wu, Yushu Zhang, Weiqiang Liu\n## Abstract\nMachine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning; backdoors in the training set; adversarial example attacks; model theft; recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.\n## 2) MEMBERSHIP INFERENCE ATTACK\nLiu et al. [73] illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. [70] propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model [70]. According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in [70] is generic, but the success of member inference attacks depends on the overfitting of the model [13], [70]. If it is a well-generalized model, the success rate of the membership inference attack is low.",
            "reference_string": "[218473513 | Xue et al. | 2020 | Citations: 124]"
        },
        {
            "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 40,
            "citation_count": 24,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.10648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1803792",
                    "name": "F. Farokhi"
                },
                {
                    "authorId": "1708760",
                    "name": "M. K\u00e2afar"
                }
            ],
            "abstract": "Machine learning models have been shown to be vulnerable to membership inference attacks, i.e., inferring whether individuals' data have been used for training models. The lack of understanding about factors contributing success of these attacks motivates the need for modelling membership information leakage using information theory and for investigating properties of machine learning models and training algorithms that can reduce membership information leakage. We use conditional mutual information leakage to measure the amount of information leakage from the trained machine learning model about the presence of an individual in the training dataset. We devise an upper bound for this measure of information leakage using Kullback--Leibler divergence that is more amenable to numerical computation. We prove a direct relationship between the Kullback--Leibler membership information leakage and the probability of success for a hypothesis-testing adversary examining whether a particular data record belongs to the training dataset of a machine learning model. We show that the mutual information leakage is a decreasing function of the training dataset size and the regularization weight. We also prove that, if the sensitivity of the machine learning model (defined in terms of the derivatives of the fitness with respect to model parameters) is high, more membership information is potentially leaked. This illustrates that complex models, such as deep neural networks, are more susceptible to membership inference attacks in comparison to simpler models with fewer degrees of freedom. We show that the amount of the membership information leakage is reduced by $\\mathcal{O}(\\log^{1/2}(\\delta^{-1})\\epsilon^{-1})$ when using Gaussian $(\\epsilon,\\delta)$-differentially-private additive noises.",
            "corpus_id": 210942951,
            "sentences": [
                {
                    "corpus_id": "210942951",
                    "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
                    "text": "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention [1,5,6,7]. These attacks have been deployed on various machine learning models; see, e.g., [1,8,9,10,11,12]. The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting. \n\nMany defence mechanisms have been proposed against membership inference attacks. A game-theoretic approach is proposed in [13], where a regularization term using the accuracy of membership inference attacks is incorporated when training machine learning models. Others have introduced indistinguishability for membership inference attacks as an estimate of the discrimination of the model on training and test datasets [14]. Alternatively, it has been suggested that we can counter membership inference attacks by reducing over-fitting [15]. Membership inference attacks are shown to work better on certain subgroups of the population, e.g., underrepresented minorities, resulting in disparate vulnerability [14]. Furthermore, success of membership inference attack may not predict success of attribute inference attacks with only access to partial view of data records [16]. Another approach is to use differentially-private machine learning at the cost of significantly reducing the utility [17,18]. However, none of these capture the possibly many factors contributing to the success of membership inference attacks. \n\nThis motivates taking a deeper look at the factors behind the success of membership inference attacks using information-theoretic membership information leakage metrics. This is the topic of this paper. \n\nFinally, we would like to point out recent results exploring differential privacy and mutual information, e.g., see [19,20]. Although these results provide important insights into information-theoretic guarantees of differential privacy, they are far from the context of this paper and do not consider membership inference attacks.",
                    "score": 0.5959223320998446,
                    "section_title": "Related Work",
                    "char_start_offset": 3591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 544
                        },
                        {
                            "start": 547,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1666
                        },
                        {
                            "start": 1669,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 1871
                        },
                        {
                            "start": 1874,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2205
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 186,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 186,
                            "end": 188,
                            "matchedPaperCorpusId": "50778569"
                        },
                        {
                            "start": 188,
                            "end": 190,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 190,
                            "end": 192,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 274,
                            "end": 277,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 277,
                            "end": 279,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 279,
                            "end": 281,
                            "matchedPaperCorpusId": "202539472"
                        },
                        {
                            "start": 281,
                            "end": 284,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 284,
                            "end": 287,
                            "matchedPaperCorpusId": "195477472"
                        },
                        {
                            "start": 287,
                            "end": 290,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 669,
                            "end": 673,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1083,
                            "end": 1087,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1417,
                            "end": 1421,
                            "matchedPaperCorpusId": "201660180"
                        },
                        {
                            "start": 1540,
                            "end": 1544,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 1990,
                            "end": 1994,
                            "matchedPaperCorpusId": "9204999"
                        },
                        {
                            "start": 1994,
                            "end": 1997,
                            "matchedPaperCorpusId": "1484107"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Modelling and Quantifying Membership Information Leakage in Machine Learning\n# Venue: arXiv.org\n# Authors: F. Farokhi, M. K\u00e2afar\n## Abstract\nMachine learning models have been shown to be vulnerable to membership inference attacks, i.e., inferring whether individuals' data have been used for training models. The lack of understanding about factors contributing success of these attacks motivates the need for modelling membership information leakage using information theory and for investigating properties of machine learning models and training algorithms that can reduce membership information leakage. We use conditional mutual information leakage to measure the amount of information leakage from the trained machine learning model about the presence of an individual in the training dataset. We devise an upper bound for this measure of information leakage using Kullback--Leibler divergence that is more amenable to numerical computation. We prove a direct relationship between the Kullback--Leibler membership information leakage and the probability of success for a hypothesis-testing adversary examining whether a particular data record belongs to the training dataset of a machine learning model. We show that the mutual information leakage is a decreasing function of the training dataset size and the regularization weight. We also prove that, if the sensitivity of the machine learning model (defined in terms of the derivatives of the fitness with respect to model parameters) is high, more membership information is potentially leaked. This illustrates that complex models, such as deep neural networks, are more susceptible to membership inference attacks in comparison to simpler models with fewer degrees of freedom. We show that the amount of the membership information leakage is reduced by $\\mathcal{O}(\\log^{1/2}(\\delta^{-1})\\epsilon^{-1})$ when using Gaussian $(\\epsilon,\\delta)$-differentially-private additive noises.\n## Related Work\nMembership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention [1,5,6,7]. These attacks have been deployed on various machine learning models; see, e.g., [1,8,9,10,11,12]. The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting. \n\nMany defence mechanisms have been proposed against membership inference attacks. A game-theoretic approach is proposed in [13], where a regularization term using the accuracy of membership inference attacks is incorporated when training machine learning models. Others have introduced indistinguishability for membership inference attacks as an estimate of the discrimination of the model on training and test datasets [14]. Alternatively, it has been suggested that we can counter membership inference attacks by reducing over-fitting [15]. Membership inference attacks are shown to work better on certain subgroups of the population, e.g., underrepresented minorities, resulting in disparate vulnerability [14]. Furthermore, success of membership inference attack may not predict success of attribute inference attacks with only access to partial view of data records [16]. Another approach is to use differentially-private machine learning at the cost of significantly reducing the utility [17,18]. However, none of these capture the possibly many factors contributing to the success of membership inference attacks. \n\nThis motivates taking a deeper look at the factors behind the success of membership inference attacks using information-theoretic membership information leakage metrics. This is the topic of this paper. \n\nFinally, we would like to point out recent results exploring differential privacy and mutual information, e.g., see [19,20]. Although these results provide important insights into information-theoretic guarantees of differential privacy, they are far from the context of this paper and do not consider membership inference attacks.",
            "reference_string": "[210942951 | Farokhi et al. | 2020 | Citations: 24]"
        },
        {
            "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 81,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283439172",
                    "name": "Zhenlong Liu"
                },
                {
                    "authorId": "2283510083",
                    "name": "Wenyu Jiang"
                },
                {
                    "authorId": "2350314174",
                    "name": "Feng Zhou"
                },
                {
                    "authorId": "2283524907",
                    "name": "Hongxin Wei"
                }
            ],
            "abstract": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncertainty, resulting in bias in low-density regions. In this work, we propose a novel approach - Bayesian Membership Inference Attack (BMIA), which performs conditional attack through Bayesian inference. In particular, we transform a trained reference model into Bayesian neural networks by Laplace approximation, enabling the direct estimation of the conditional score distribution by probabilistic model parameters. Our method addresses both epistemic and aleatoric uncertainty with only a reference model, enabling efficient and powerful MIA. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of BMIA.",
            "corpus_id": 276928094,
            "sentences": [
                {
                    "corpus_id": "276928094",
                    "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
                    "text": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncertainty, resulting in bias in low-density regions. In this work, we propose a novel approach - Bayesian Membership Inference Attack (BMIA), which performs conditional attack through Bayesian inference. In particular, we transform a trained reference model into Bayesian neural networks by Laplace approximation, enabling the direct estimation of the conditional score distribution by probabilistic model parameters. Our method addresses both epistemic and aleatoric uncertainty with only a reference model, enabling efficient and powerful MIA. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of BMIA.",
                    "score": 0.5748407169060119,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Efficient Membership Inference Attacks by Bayesian Neural Network\n# Venue: arXiv.org\n# Authors: Zhenlong Liu, Wenyu Jiang, Feng Zhou, Hongxin Wei\n## Abstract\nMembership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncertainty, resulting in bias in low-density regions. In this work, we propose a novel approach - Bayesian Membership Inference Attack (BMIA), which performs conditional attack through Bayesian inference. In particular, we transform a trained reference model into Bayesian neural networks by Laplace approximation, enabling the direct estimation of the conditional score distribution by probabilistic model parameters. Our method addresses both epistemic and aleatoric uncertainty with only a reference model, enabling efficient and powerful MIA. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of BMIA.\n",
            "reference_string": "[276928094 | Liu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "venue": "arXiv.org",
            "year": 2017,
            "reference_count": 56,
            "citation_count": 40,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.01604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26378728",
                    "name": "Samuel Yeom"
                },
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ],
            "abstract": "Machine learning algorithms that are applied to sensitive data pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through their structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. \nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about training data from machine learning models, either through training set membership inference or model inversion attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference, and when certain conditions on the influence of certain features are present, model inversion attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks, and begins to shed light on what other factors may be in play. Finally, we explore the connection between two types of attack, membership inference and model inversion, and show that there are deep connections between the two that lead to effective new attacks.",
            "corpus_id": 195346528,
            "sentences": [
                {
                    "corpus_id": "195346528",
                    "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
                    "text": "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access. \n\nExperiment 1 below formalizes membership inference attacks. The experiment first samples a fresh dataset from D, and then flips a coin b to decide whether to draw the adversary's challenge point z from the training set or the original distribution. A is then given the challenge, along with the additional information described above, and must guess the value of b. \n\nExperiment 1 (Membership experiment Exp M (A, A, n, D)). Let A be an adversary, A be a learning algorithm, n be a positive integer, and D be a distribution over data points (x, y). The membership experiment proceeds as follows: \n\n1. Sample S \u223c D n , and let A S = A(S). \n\n2. Choose b \u2190 {0, 1} uniformly at random.",
                    "score": 0.5700012577042748,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 12901,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 613
                        },
                        {
                            "start": 614,
                            "end": 715
                        },
                        {
                            "start": 718,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1083
                        },
                        {
                            "start": 1086,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1313
                        },
                        {
                            "start": 1316,
                            "end": 1355
                        },
                        {
                            "start": 1358,
                            "end": 1399
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: The Unintended Consequences of Overfitting: Training Data Inference Attacks\n# Venue: arXiv.org\n# Authors: Samuel Yeom, Matt Fredrikson, S. Jha\n## Abstract\nMachine learning algorithms that are applied to sensitive data pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through their structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. \nThis paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about training data from machine learning models, either through training set membership inference or model inversion attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference, and when certain conditions on the influence of certain features are present, model inversion attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks, and begins to shed light on what other factors may be in play. Finally, we explore the connection between two types of attack, membership inference and model inversion, and show that there are deep connections between the two that lead to effective new attacks.\n## Membership Inference Attacks\nIn a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access. \n\nExperiment 1 below formalizes membership inference attacks. The experiment first samples a fresh dataset from D, and then flips a coin b to decide whether to draw the adversary's challenge point z from the training set or the original distribution. A is then given the challenge, along with the additional information described above, and must guess the value of b. \n\nExperiment 1 (Membership experiment Exp M (A, A, n, D)). Let A be an adversary, A be a learning algorithm, n be a positive integer, and D be a distribution over data points (x, y). The membership experiment proceeds as follows: \n\n1. Sample S \u223c D n , and let A S = A(S). \n\n2. Choose b \u2190 {0, 1} uniformly at random.",
            "reference_string": "[195346528 | Yeom et al. | 2017 | Citations: 40]"
        },
        {
            "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey",
            "venue": "IEEE Access",
            "year": 2020,
            "reference_count": 125,
            "citation_count": 82,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09194237.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3023084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3023084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3385931",
                    "name": "Harry Chandra Tanuwidjaja"
                },
                {
                    "authorId": "36105968",
                    "name": "Rakyong Choi"
                },
                {
                    "authorId": "30708702",
                    "name": "Seunggeun Baek"
                },
                {
                    "authorId": "1741995",
                    "name": "Kwangjo Kim"
                }
            ],
            "abstract": "The exponential growth of big data and deep learning has increased the data exchange traffic in society. Machine Learning as a Service, (MLaaS) which leverages deep learning techniques for predictive analytics to enhance decision-making, has become a hot commodity. However, the adoption of MLaaS introduces data privacy challenges for data owners and security challenges for deep learning model owners. Data owners are concerned about the safety and privacy of their data on MLaaS platforms, while MLaaS platform owners worry that their models could be stolen by adversaries who pose as clients. Consequently, Privacy-Preserving Deep Learning (PPDL) arises as a possible solution to this problem. Recently, several papers about PPDL for MLaaS have been published. However, to the best of our knowledge, no previous paper has summarized the existing literature on PPDL and its specific applicability to the MLaaS environment. In this paper, we present a comprehensive survey of privacy-preserving techniques, starting from classical privacy-preserving techniques to well-known deep learning techniques. Additionally, we present a detailed description of PPDL and address the issue of using PPDL for MLaaS. Furthermore, we undertake detailed comparisons between state-of-the-art PPDL methods. Subsequently, we classify an adversarial model on PPDL by highlighting possible PPDL attacks and their potential solutions. Ultimately, our paper serves as a single point of reference for detailed knowledge on PPDL and its applicability to MLaaS environments for both new and experienced researchers.",
            "corpus_id": 221725546,
            "sentences": [
                {
                    "corpus_id": "221725546",
                    "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey",
                    "text": "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. [96] provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data. Some defensive mechanisms dedicated to membership inference have been proposed including dropout [114] and adversarial regularization [115]. \n\nIn cryptography-based PPDL models, the security against the membership inference attack can be reduced to the security of the underlying cryptosystems. In such models, the adversarial server cannot obtain model parameters in plaintext. Only if the model is public can the adversary have black-box access of the model, just like any outsider attacker. For HW-based models, the adversarial server owns white-box models, allowing the use of white-box membership inference attacks. \n\nFor DP-based models, the trade-off between the model accuracy and the performance of membership inference attacks according to the selection of the privacy parameter has been studied [116]. Appropriate choices of the privacy parameter result in moderate utility with low membership inference accuracy. However, further experiments are required for the extensibility of their analysis toward other types of tasks outside image classification.",
                    "score": 0.6334864169157041,
                    "section_title": "B. MEMBERSHIP INFERENCE ATTACK",
                    "char_start_offset": 65858,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 499
                        },
                        {
                            "start": 502,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1116
                        },
                        {
                            "start": 1119,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1469
                        },
                        {
                            "start": 1470,
                            "end": 1596
                        },
                        {
                            "start": 1599,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1900
                        },
                        {
                            "start": 1901,
                            "end": 2040
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 424,
                            "end": 428,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1110,
                            "end": 1115,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1782,
                            "end": 1787,
                            "matchedPaperCorpusId": "13699042"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey\n# Venue: IEEE Access\n# Authors: Harry Chandra Tanuwidjaja, Rakyong Choi, Seunggeun Baek, Kwangjo Kim\n## Abstract\nThe exponential growth of big data and deep learning has increased the data exchange traffic in society. Machine Learning as a Service, (MLaaS) which leverages deep learning techniques for predictive analytics to enhance decision-making, has become a hot commodity. However, the adoption of MLaaS introduces data privacy challenges for data owners and security challenges for deep learning model owners. Data owners are concerned about the safety and privacy of their data on MLaaS platforms, while MLaaS platform owners worry that their models could be stolen by adversaries who pose as clients. Consequently, Privacy-Preserving Deep Learning (PPDL) arises as a possible solution to this problem. Recently, several papers about PPDL for MLaaS have been published. However, to the best of our knowledge, no previous paper has summarized the existing literature on PPDL and its specific applicability to the MLaaS environment. In this paper, we present a comprehensive survey of privacy-preserving techniques, starting from classical privacy-preserving techniques to well-known deep learning techniques. Additionally, we present a detailed description of PPDL and address the issue of using PPDL for MLaaS. Furthermore, we undertake detailed comparisons between state-of-the-art PPDL methods. Subsequently, we classify an adversarial model on PPDL by highlighting possible PPDL attacks and their potential solutions. Ultimately, our paper serves as a single point of reference for detailed knowledge on PPDL and its applicability to MLaaS environments for both new and experienced researchers.\n## B. MEMBERSHIP INFERENCE ATTACK\nGenerally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. [96] provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data. Some defensive mechanisms dedicated to membership inference have been proposed including dropout [114] and adversarial regularization [115]. \n\nIn cryptography-based PPDL models, the security against the membership inference attack can be reduced to the security of the underlying cryptosystems. In such models, the adversarial server cannot obtain model parameters in plaintext. Only if the model is public can the adversary have black-box access of the model, just like any outsider attacker. For HW-based models, the adversarial server owns white-box models, allowing the use of white-box membership inference attacks. \n\nFor DP-based models, the trade-off between the model accuracy and the performance of membership inference attacks according to the selection of the privacy parameter has been studied [116]. Appropriate choices of the privacy parameter result in moderate utility with low membership inference accuracy. However, further experiments are required for the extensibility of their analysis toward other types of tasks outside image classification.",
            "reference_string": "[221725546 | Tanuwidjaja et al. | 2020 | Citations: 82]"
        },
        {
            "title": "Amplifying Membership Exposure via Data Poisoning",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 32,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.00463",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1711982",
                    "name": "Yufei Chen"
                },
                {
                    "authorId": "2088079836",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2117688523",
                    "name": "Yun Shen"
                },
                {
                    "authorId": "2116638601",
                    "name": "Cong Wang"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks. Such attacks typically lead to test-time accuracy degradation or controlled misprediction. In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples. To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class. We first propose a generic dirty-label attack for supervised classification algorithms. We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look\"natural\"to evade human moderation. We extensively evaluate our attacks on computer vision benchmarks. Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation. To mitigate the potential negative impacts of our attacks, we also investigate feasible countermeasures.",
            "corpus_id": 253244424,
            "sentences": [
                {
                    "corpus_id": "253244424",
                    "title": "Amplifying Membership Exposure via Data Poisoning",
                    "text": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
                    "score": 0.6312298380822267,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 6204,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 876
                        },
                        {
                            "start": 879,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1585
                        },
                        {
                            "start": 1588,
                            "end": 1621
                        },
                        {
                            "start": 1622,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2044
                        },
                        {
                            "start": 2047,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2243
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 154,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 157,
                            "end": 160,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 634,
                            "end": 637,
                            "matchedPaperCorpusId": "17295025"
                        },
                        {
                            "start": 1003,
                            "end": 1007,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 1007,
                            "end": 1010,
                            "matchedPaperCorpusId": "218514464"
                        },
                        {
                            "start": 1010,
                            "end": 1013,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1148,
                            "end": 1152,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1407,
                            "end": 1411,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1411,
                            "end": 1414,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1754,
                            "end": 1758,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 1758,
                            "end": 1761,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1761,
                            "end": 1764,
                            "matchedPaperCorpusId": "174799799"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Amplifying Membership Exposure via Data Poisoning\n# Venue: Neural Information Processing Systems\n# Authors: Yufei Chen, Chao Shen, Yun Shen, Cong Wang, Yang Zhang\n## Abstract\nAs in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks. Such attacks typically lead to test-time accuracy degradation or controlled misprediction. In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples. To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class. We first propose a generic dirty-label attack for supervised classification algorithms. We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look\"natural\"to evade human moderation. We extensively evaluate our attacks on computer vision benchmarks. Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation. To mitigate the potential negative impacts of our attacks, we also investigate feasible countermeasures.\n## Membership Inference Attack\nIn a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
            "reference_string": "[253244424 | Chen et al. | 2022 | Citations: 32]"
        },
        {
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "venue": "USENIX Security Symposium",
            "year": 2019,
            "reference_count": 50,
            "citation_count": 272,
            "influential_citation_count": 21,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.11798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35802340",
                    "name": "Klas Leino"
                },
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                }
            ],
            "abstract": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
            "corpus_id": 195699554,
            "sentences": [
                {
                    "corpus_id": "195699554",
                    "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                    "text": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "score": 0.6265915468054878,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference\n# Venue: USENIX Security Symposium\n# Authors: Klas Leino, Matt Fredrikson\n## Abstract\nMembership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.\n",
            "reference_string": "[195699554 | Leino et al. | 2019 | Citations: 272]"
        },
        {
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 36,
            "citation_count": 32,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.04872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2113958751",
                    "name": "Yang Zou"
                },
                {
                    "authorId": "48806102",
                    "name": "Zhikun Zhang"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "While being deployed in many critical applications as core components, machine learning (ML) models are vulnerable to various security and privacy attacks. One major privacy attack in this domain is membership inference, where an adversary aims to determine whether a target data sample is part of the training set of a target ML model. So far, most of the current membership inference attacks are evaluated against ML models trained from scratch. However, real-world ML models are typically trained following the transfer learning paradigm, where a model owner takes a pretrained model learned from a different dataset, namely teacher model, and trains her own student model by fine-tuning the teacher model with her own data. \nIn this paper, we perform the first systematic evaluation of membership inference attacks against transfer learning models. We adopt the strategy of shadow model training to derive the data for training our membership inference classifier. Extensive experiments on four real-world image datasets show that membership inference can achieve effective performance. For instance, on the CIFAR100 classifier transferred from ResNet20 (pretrained with Caltech101), our membership inference achieves $95\\%$ attack AUC. Moreover, we show that membership inference is still effective when the architecture of target model is unknown. Our results shed light on the severity of membership risks stemming from machine learning models in practice.",
            "corpus_id": 221586480,
            "sentences": [
                {
                    "corpus_id": "221586480",
                    "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
                    "text": "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier.",
                    "score": 0.6265153158908842,
                    "section_title": "Membership Inference against ML Models",
                    "char_start_offset": 9379,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 590
                        },
                        {
                            "start": 593,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 867
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning\n# Venue: arXiv.org\n# Authors: Yang Zou, Zhikun Zhang, M. Backes, Yang Zhang\n## Abstract\nWhile being deployed in many critical applications as core components, machine learning (ML) models are vulnerable to various security and privacy attacks. One major privacy attack in this domain is membership inference, where an adversary aims to determine whether a target data sample is part of the training set of a target ML model. So far, most of the current membership inference attacks are evaluated against ML models trained from scratch. However, real-world ML models are typically trained following the transfer learning paradigm, where a model owner takes a pretrained model learned from a different dataset, namely teacher model, and trains her own student model by fine-tuning the teacher model with her own data. \nIn this paper, we perform the first systematic evaluation of membership inference attacks against transfer learning models. We adopt the strategy of shadow model training to derive the data for training our membership inference classifier. Extensive experiments on four real-world image datasets show that membership inference can achieve effective performance. For instance, on the CIFAR100 classifier transferred from ResNet20 (pretrained with Caltech101), our membership inference achieves $95\\%$ attack AUC. Moreover, we show that membership inference is still effective when the architecture of target model is unknown. Our results shed light on the severity of membership risks stemming from machine learning models in practice.\n## Membership Inference against ML Models\nIn machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier.",
            "reference_string": "[221586480 | Zou et al. | 2020 | Citations: 32]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273850257",
            "title": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
            "text": "Membership inference attacks are a typical method that steals private information of training data, aiming to infer whether a sample is a member of a target model's training set. \n\nDefinition 2 (Membership Inference Attacks [28]): Given a machine learning model M o , a training set D train and a target sample x i . Let I(\u2022) be an indicator function for membership inference attacks, used to determine the membership information of x i .",
            "score": 0.9524825717129961,
            "section_title": "C. Membership inference attacks",
            "char_start_offset": 12065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 438
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "264075924",
            "title": "Privacy and Security in Distributed Learning: A Review of Challenges, Solutions, and Open Research Issues",
            "text": "They refer to a class of attacks that aim to determine whether a specific individual's data was used to train a ML model [38]. In other words, the goal of a membership inference attack is to determine if a particular individual is a member of the dataset that was used to train the model. Membership inference attacks can be carried out by analyzing the output of the ML model, and attempting to infer whether a specific data point was used in the model's training. This can be done by comparing the model's predictions on the target data point to its predictions on a separate set of data points that were not used in training. If the model's predictions on the target data point are significantly more accurate than its predictions on the other data points, it can be inferred that the target data point was likely used to train the model.",
            "score": 0.7911882840639852,
            "section_title": "3) MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 24697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 841
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "86836429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "251066729",
            "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
            "text": "In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member).",
            "score": 0.777599274206481,
            "section_title": "Conventional Membership Inference Attacks",
            "char_start_offset": 9690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "In this section, we first discuss nine representative membership inference attacks. Then, we summarize six defenses to mitigate membership inference attacks.",
            "score": 0.775002419619403,
            "section_title": "Attacks and Defenses",
            "char_start_offset": 11088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "We first re-evaluate the effectiveness of two membership inference defenses [19,29], and then re-evaluate the white-box membership inference attacks proposed by Nasr et al. [30]. Following prior work [39,42], we sample the input (x, y) from either the target model's training set or test set with an equal 50% probability. Thus, the random guessing strategy results in a 50% membership inference attack accuracy.",
            "score": 0.7744337723556262,
            "section_title": "Experiment results",
            "char_start_offset": 24088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 412
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "165163934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "233296028",
            "title": "Membership Inference Attack Susceptibility of Clinical Language Models",
            "text": "Membership inference attacks are accomplished by exploiting the difference in a model's responses to training and OOT data samples. We use the advantage of membership inference attacks to estimate the empirical privacy leakage. The experimental framework for obtaining privacy leakage using a membership inference adversary is defined by (Yeom et al., 2018) as follows:",
            "score": 0.7667396334194305,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 8583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 369
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "256627812",
            "title": "Membership Inference Attacks against Diffusion Models",
            "text": "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model [21]- [23]. A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models [15], [16], [18]. There are two settings [14], i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model.\n\nA typical approach for membership inference attacks is to leverage the large divergence between the loss distribution over members and non-members [24]. The divergence can be embedded by an adversary. For instance, privacy leakage, including membership inference attacks, can be more effective by training a model with poisoning samples [25]- [28].\n\nMembership inference attacks can be prevented by differential privacy [29] where gradients are perturbed [30], [31]. Since differential privacy often deteriorates inference accuracy, several works evaluated differential privacy on membership inference attacks in a quantitative fashion [32], [33].",
            "score": 0.7631695184132121,
            "section_title": "A. Membership Inference",
            "char_start_offset": 4305,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "251135031"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "251765197"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "201598829"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "231718884"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "2565493"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "219686822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "247996732",
            "title": "User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition in Federated Learning",
            "text": "We use an attack framework similar to membership inference attack [13]. Below is a summary of the attack framework, and a more detailed description can be found in [6].",
            "score": 0.760043814256854,
            "section_title": "Attack Framework",
            "char_start_offset": 4193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 168
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "226300898",
            "title": "Securing Machine Learning in the Cloud: A Systematic Review of Cloud Machine Learning Security",
            "text": "In a typical membership inference attack, for given input data and black box access to the ML model, an attacker attempts to figure out if the given input sample was the part of the training set or not. To realize a membership inference attack against a target model, a classification model is trained for distinguishing between the predictions of the target model against the inputs on which it was trained and that those on which it was not trained (Shokri et al., 2017).",
            "score": 0.7548871347408543,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 23190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 473
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 472,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "250311494",
            "title": "Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms",
            "text": "Membership inference aims at disclosing the membership of a particular sample to the training dataset or a certain class. Furthermore, this form of attack can work even if the objective is unrelated to the basic characteristics of the class [72].",
            "score": 0.750951529389939,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 14784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 246
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "244920593",
            "title": "Membership Inference Attacks From First Principles",
            "text": "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy. It is thus important that they can reliably succeed at this task. This section formalizes the membership inference attack security game ( \u00a7III-A), and introduces our membership inference evaluation methodology ( \u00a7III-B).",
            "score": 0.7432533759001914,
            "section_title": "III. MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 9202,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "208247985",
            "title": "Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability",
            "text": "Membership inference attacks seek to identify whether or not an individual is a member of the dataset used to train a particular target ML model. We discuss the definition and generation of membership inference attacks which will serve as the basic reference model of membership inference.",
            "score": 0.7382565755394928,
            "section_title": "II. MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 6124,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "244809678",
            "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
            "text": "Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack [6], the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties. \n\nThe membership inference attack which extracts the membership information only is not a critical attack in itself. However, since each party knows additional information about the others, membership information can be linked with this background knowledge and used to identify someone's identity and sensitive information.",
            "score": 0.7345342611326589,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 17872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 1203,
                    "end": 1206,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Note that our preliminary goal is to establish a benchmark for membership inference, thus we perform similar experiments as previous works [24,30,42] (e.g., RQ2 and RQ4) to validate and confirm their results. In addition, we investigate membership leakages from various new angles, such as collecting a real-world dataset (from the Internet) as the shadow dataset to conduct membership inference, and first demonstrate that JS distance correlates with attack performance above the overfitting level, which can be used to directly estimate the membership risks. Also, we discover that data augmentations can serve as a more effective mechanism to improve the model's utility while reducing membership leakage. MEMBERSHIP-DOCTOR. We implement our code as a modular framework named MEMBERSHIP-DOCTOR to evaluate state-of-the-art membership inference attacks and defenses. MEMBERSHIP-DOCTOR (shown in Figure 1) can easily integrate additional attacks and defenses with new datasets and models by adding the functional code to the attack/defense module. Our code will be publicly available which will facilitate researchers to benchmark new attacks and defenses in the field of membership inferences.",
            "score": 0.7237898786750004,
            "section_title": "Our Contributions",
            "char_start_offset": 7250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1195
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "235826383"
                },
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "46933970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "272423548",
            "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
            "text": "Consider a model M trained on dataset D. The objective of a membership inference attack is to ascertain whether a data point x belongs to D (i.e., x \u2208 D) or not (i.e., x / \u2208 D). Formally, we aim to develop a scoring function s(x, M) \u2192 R, where the membership prediction is determined by a threshold \u03c4 : \n\nx",
            "score": 0.7138961069413368,
            "section_title": "Problem Formulation",
            "char_start_offset": 6653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 306
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "244809678",
            "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
            "text": "Since our attack model on membership inference attack assumes the best scenario for Shokri et al.'s membership inference attack method [6], we did not modify their method. So, we just implemented Shokri et al.'s membership inference attack method to suit our environment.",
            "score": 0.7103710032832844,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 22930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 271
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "220831381",
            "title": "Label-Only Membership Inference Attacks",
            "text": "The goal of a membership inference attack is to determine whether or not a candidate data point was used to train a given model. In Table 3, we summarize different sets of assumptions made in prior work about the adversary's knowledge and query access to the model.",
            "score": 0.7076135160498355,
            "section_title": "C. Threat Model",
            "char_start_offset": 47674,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "271328769",
            "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
            "text": "Membership inference attack is one of the most popular privacy attacks against ML models.The goal of the membership inference attack is to determine whether a data sample is used to train a target model.We consider data samples as members if they are used to train the target model, otherwise, non-members.Formally, considering a data sample\n\nx, a trained ML model M , and background knowledge of an adversary, denoted by I , the membership inference attack A can be defined as the following:\n\nHere, 0 means the data sample x is not a member of M 's training dataset, and 1 otherwise.The attack model A is essentially a binary classifier.",
            "score": 0.6997414369906545,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 9107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 341
                },
                {
                    "start": 343,
                    "end": 492
                },
                {
                    "start": 494,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "218674569",
            "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
            "text": "We propose that DA is feasible to be leveraged against membership inference attacks. To our best known, we are the first to adopt DA in the domain of defending membership inference attacks.",
            "score": 0.6983457237367776,
            "section_title": "\u2022",
            "char_start_offset": 7108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "224705937",
            "title": "From distributed machine learning to federated learning: In the view of data privacy and security",
            "text": "A membership inference attack is a common tracing attack to determine whether a specific individual is a member of a given dataset or not ? . In machine learning, deep learning or federated learning, membership inference attacks aim to determine whether a given data point is present in the training data ? . The accuracy of the attack corresponds to the proportion of correct membership predictions made by the adversary, while precision is calculated as the proportion of examples inferred to be members that are indeed members of the target model's training set. These attacks take advantage of the fact that the behavior of a training model between the training set and the test set may be very different (i.e., the model may be overfit). As such, an adversary can train a machine learning model to recognize the differences in its own behavior versus the target model to determine whether or not an input record is involved in the training process ? ? ? .",
            "score": 0.6938837330389125,
            "section_title": "Preliminary of Membership Inference Attack",
            "char_start_offset": 41878,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "270703358",
            "title": "Noisy Neighbors: Efficient membership inference attacks against LLMs",
            "text": "This work set out to elaborate a strategy for membership inference attacks.Differently from prior research focusing on improving the strength of the attacker, we develop a technique trying to achieve a similar efficacy, while reducing the computational burden for an auditor trying to assess the privacy",
            "score": 0.6921037448558431,
            "section_title": "Conclusion",
            "char_start_offset": 12527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 75,
                    "end": 303
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "257757067",
            "title": "A Survey on Secure and Private Federated Learning Using Blockchain: Theory and Application in Resource-Constrained Computing",
            "text": "(i) Membership inference attack -Attackers determine if an individual was present in the training dataset. The authors in [26] constructed a membership inference attack that exploits the observation that machine learning models often behave differently on the data they were trained on versus the data they see for the first time [26].",
            "score": 0.6920616645035806,
            "section_title": "B. Brief Introduction to Blockchain",
            "char_start_offset": 10452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 335
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "We have thus far provided a general formulation of membership inference attacks. In this section, we characterize such attacks through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Although the target model is a dominating factor in determining vulnerability, attack data generation techniques need not explicitly mirror the target model. Finally, we show that membership inference attacks can persist as insider attacks in federated systems.",
            "score": 0.6796343794209683,
            "section_title": "GENERAL ATTACK CHARACTERIZATION",
            "char_start_offset": 29890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy [4], [43], [14] or secure multiparty computation [44], [10], [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety. \n\nFigure 1 illustrates the workflow of membership inference attack development. Given a training dataset D and a classification model F t trained on D, the machine learning service provider may provide a classification service through a prediction API. This API offers users black box access to the model F t . Users may send prediction queries with their own data to the service and receive classification predictions. An adversary uses such a service to collect information about the private dataset D on which the prediction model F t was privately trained. By leveraging any public or background knowledge of the training dataset D or the target model F t , an adversary builds a membership inference attack model F a to deploy for launching membership inference attacks in real time. \n\nTo gain an in-depth understanding of the general formulation of the membership inference attack model, we first characterize the types of adversarial knowledge and datasets required to train the attack model as well as the attack cost, the attack value, and their evaluation metrics. We then present a systematic formulation of general attacks in Section 3.",
            "score": 0.6730724577955665,
            "section_title": "MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 7016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "15331398"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "589685"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "2887752"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "6942138"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "18290778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "Our work is mainly inspired by [41], the first exploratory work in membership inference, which shows membership inference risks for a deep learning trained target model and attack model. In this paper we extend the work done in [41] to a more general setting towards demystifying the adverse effect of membership inference across different types of models with both general and empirical characterization of why membership inference attacks are more effective in some scenarios than in others.",
            "score": 0.6673757500125608,
            "section_title": "RELATED WORK",
            "char_start_offset": 62784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 493
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "52211986",
            "title": "LOGAN: Membership Inference Attacks Against Generative Models",
            "text": "Overall, membership inference attacks are often a gateway to further attacks. That is, the adversary first infers whether data of a victim is part of the information she has access to (a trained model in our case), and then mount other attacks (e.g., profiling [49], property inference [4,41], etc.), which might leak additional information about the victim.",
            "score": 0.6650438996353752,
            "section_title": "Motivation",
            "char_start_offset": 4129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 358
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "7524824"
                },
                {
                    "start": 286,
                    "end": 289,
                    "matchedPaperCorpusId": "14757739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "257663933",
            "title": "Do Backdoors Assist Membership Inference Attacks?",
            "text": "We describe a backdoor-assisted membership inference attack as the problem setting of this paper below. We first define an attack formally and its metrics. We then describe the key questions in detail.",
            "score": 0.6649286961048371,
            "section_title": "III. BACKDOOR-ASSISTED MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 7315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "268819372",
            "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
            "text": "Membership inference attacks (Shokri et al., 2017;Yeom et al., 2018;Bentley et al., 2020;Choquette-Choo et al., 2021;Wen et al., 2023) predict whether or not a specific data point was part of the training set of a model.Most membership inference attacks are completely \"black-box\" (Sablayrolles et al., 2019): they rely only on the model's loss (computed via the logits output).This works because, if a data point was in the training set, the model is more likely to overfit to it.Recent attacks (Carlini et al., 2022) work by training shadow models (Shokri et al., 2017) on subsets of the underlying dataset, which allow an adversary to estimate how likely any given sample should be if it was-or wasn'tin the training dataset.Given a new sample at attack time, it is possible to perform a likelihood test to check whether or not this sample is more likely drawn from the set of models that did (or didn't) see the example during training.\n\nMembership inference attacks have also been extended to generative models, including large language models (Carlini et al., 2021) and diffusion models (Duan et al., 2023).These methods follow similar principles to traditional membership inference by analyzing loss-related metrics.On the other hand, Carlini et al. (2023) achieves membership inference by examining sampling density.More recently, Debenedetti et al. (2023) have identified several privacy side channels.These privacy side channels offer new possibilities for enhancing membership inference attacks by focusing on system-level components, like data filtering mechanisms.\n\nClosely related to our topic, Tram\u00e8r et al. (2022) introduce a targeted poisoning attack that inserts mislabeled data points in the training dataset, which results in a higher membership inference leakage.However, the attack assumption here is strong: it assumes that the adversary has control over the sensitive training data the victim will train on.",
            "score": 0.6646734088924827,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 4099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 940
                },
                {
                    "start": 942,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1411
                },
                {
                    "start": 1411,
                    "end": 1577
                },
                {
                    "start": 1579,
                    "end": 1784
                },
                {
                    "start": 1784,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 50,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 50,
                    "end": 68,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 89,
                    "end": 117,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 117,
                    "end": 134,
                    "matchedPaperCorpusId": "252992499"
                },
                {
                    "start": 281,
                    "end": 308,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 496,
                    "end": 518,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 550,
                    "end": 571,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1049,
                    "end": 1071,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "256503774"
                },
                {
                    "start": 1242,
                    "end": 1263,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "218674569",
            "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
            "text": "Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19,20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice. \n\nMathematically, a victim DL model M trained on a dataset D, which handles a classification task, categorizing all its inputs into n categories. Any sample s i fed into M will result in an output o i = (p 1 , p 2 , ..., p j , ..., p n ), where p j is the confidence score, indicating the probability of being a sample of the category j. To deploy the membership inference attack, an attacker may train a binary classifier M adv to determine whether a sample s i is from the training data D. member or not = M adv (o i ) \n\nA simple but efficient way to initiate a membership inference attack is to build a linear binary classifier M lin. adv using a confidence threshold P thresh as a decision boundary, which is selected based on extensive experiments. In particular, one may enumerate all possible confidence scores to find the P thresh , which can achieve the maximum accuracy of membership inference [21]. With P thresh , if the confidence score of a sample for a specific category k (1 \u2264 k \u2264 n) is higher than P thresh , M lin. \n\nadv will determine the sample as a member of the training set. Formally, the procedure can be defined as follows: \n\nNoted that 1 stands for member and 0 stands for nonmember. \n\nCompared with other attacks such as model inversion [7,22], the impact of membership inference attack is relatively minor. However, membership inference is easier to deploy and requires less information on the victim model compared with other attacks.",
            "score": 0.6588676017733928,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 8710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 1948
                },
                {
                    "start": 1951,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 1643,
                    "end": 1647,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 2003,
                    "end": 2006,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 2006,
                    "end": 2009,
                    "matchedPaperCorpusId": "207941008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "275336117",
            "title": "AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation",
            "text": "A membership inference attack aims to determine if a data point was used to train a specific machine learning model, threatening individual privacy. Due to growing concerns, many attack techniques have emerged. This paper categorizes these attacks, which are also key to the defense strategies discussed.",
            "score": 0.6561527081606819,
            "section_title": "Existing Membership Inference Attacks",
            "char_start_offset": 2510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "244920593",
            "title": "Membership Inference Attacks From First Principles",
            "text": "We define membership inference via a standard security game inspired by Yeom et al. [70] and Jayaraman et al. [25].\n\nDefinition 1 (Membership inference security game). The game proceeds between a challenger C and an adversary A:\n\n1) The challenger samples a training dataset D \u2190 D and trains a model f \u03b8 \u2190 T (D) on the dataset D.\n\n2) The challenger flips a bit b, and if b = 0, samples a fresh challenge point from the distribution (x, y) \u2190 D (such that (x, y) / \u2208 D). Otherwise, the challenger selects a point from the training set (x, y) \u2190 $ D.\n\n3) The challenger sends (x, y) to the adversary. 4) The adversary gets query access to the distribution D, and to the model f \u03b8 , and outputs a bitb \u2190 A D,f (x, y). 5) Output 1 ifb = b, and 0 otherwise.\n\nFor simplicity, we will write A(x, y) to denote the adversary's prediction on the sample (x, y) when the distribution D and model f are clear from context.\n\nNote that this game assumes that the adversary is given access to the underlying training data distribution D; while some attacks do not make use of this assumption [70], many attacks require query-access to the distribution in order to train \"shadow models\" [60] (as we will describe). The above game also assumes that the adversary is given access to both a training example and its ground-truth label.\n\nInstead of outputting a \"hard prediction\", all the attacks we consider output a continuous confidence score, which is then thresholded to yield a membership prediction. That is,\n\nwhere 1 is the indicator function, \u03c4 is some tunable decision threshold, and A outputs a real-valued confidence score.\n\nA first membership inference attack. For illustrative purposes, we begin by considering a very simple membership inference attack (due to Yeom et al. [70]). This attack relies on the observation that, because machine learning models are trained to minimize the loss of their training examples (see Equation 1), examples with lower loss are on average more likely to be members of the training data. Formally, the LOSS membership inference attack defines",
            "score": 0.6558778023177336,
            "section_title": "A. Definitions",
            "char_start_offset": 9702,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "218862728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "269981985",
            "title": "Federated Learning in Healthcare: Model Misconducts, Security, Challenges, Applications, and Future Research Directions - A Systematic Review",
            "text": "It's a technique where an attacker tries to reconstruct private information, such as an individual's medical records, by exploiting the output of an ML model.By repeatedly testing the model with carefully chosen inputs, the attacker can glean insights into the training data used to construct the model.Membership inference, as an attack method, aims to ascertain whether an individual's data was included in the training set of an ML model.Through analysis of the model's responses to different inputs, the attacker can deduce the presence or absence of specific data, posing risks to privacy and anonymity.",
            "score": 0.6546852988530485,
            "section_title": "Model Inversion",
            "char_start_offset": 39571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "270199828",
            "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
            "text": "Membership inference attacks (Shokri et al., 2017;Hu et al., 2022) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2022). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set. \n\nAdditionally, an attacker may also aim to determine the membership of a certain user, i.e., to check if a user's data is part of the training set, which is known as user-level membership inference (Shejwalkar et al., 2021). Throughout this paper we will address the membership inference challenge from a sample-level perspective. \n\nIn the context of RAG, membership inference can be attributed to either the membership of a sample in the training dataset of the models E or G (described in the previous subsection 2.1), or a document's membership in the retrieval dataset D. This paper focuses on the latter. Formally, the goal of the attack is to infer the membership of a target document d in the retrieval database D, i.e., to check if d \u2208 D, using only the final output of the RAG system, namely the output of the generative model G conditioned on the fetched context from the retrieval database D. \n\nTo the best of our knowledge, this is the first paper to propose such a membership inference attack tailored to RAG systems.",
            "score": 0.6525814450784221,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 6610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 50,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 50,
                    "end": 66,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 826,
                    "end": 848,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1265,
                    "end": 1290,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "269981985",
            "title": "Federated Learning in Healthcare: Model Misconducts, Security, Challenges, Applications, and Future Research Directions - A Systematic Review",
            "text": "Membership inference attacks seek to ascertain whether a particular individual's data was part of the training dataset for an ML model [134].To carry out these attacks, one needs to train a separate ML model that can identify if a specific data point contributed to the target model's training process [135].Such attacks exploit the use of personal data in the model's training phase to deduce private information about individuals [25].",
            "score": 0.648862265975755,
            "section_title": "Membership inference attacks",
            "char_start_offset": 40498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 437
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 140,
                    "matchedPaperCorpusId": "225220194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "247922814",
            "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
            "text": "Membership inference (MI) captures one of the most generic notions of privacy leakage in machine learning. Indeed, any form of data leakage from a model's training set (e.g., attribute inference or data extraction) implies the ability to infer membership of some training examples. As a result, membership inference is a natural target for evaluating the impact of poisoning attacks on data privacy.\n\nIn this section, we introduce and analyze data poisoning attacks that improve membership inference by one to two orders of magnitude. Section 4.2 describes a targeted attack that increases leakage of a specific sample ( , ), and Section 4.3 contains an analysis of this attack's success. Section 4.4 explores untargeted attacks that increase privacy leakage on all training points simultaneously.",
            "score": 0.6477537713465168,
            "section_title": "MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 23042,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "271909916",
            "title": "Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders",
            "text": "A membership inference attack refers to a privacy attack where an adversary attempts to determine whether a particular sample was part of the training set used to train a target machine learning model [11,4,41]. In this context, all training data are considered as \"members\" while any data not included in the training set (i.e., the unseen data) are regarded as \"non-members\". We formalize the attacker as a binary classification model A, which receives a query sample x and the corresponding output from a target model M(x). In this work, the attacker's goal is to infer whether the given sample is inside the pre-training set of the target model, i.e., A(x, M(x)) = 1 x \u2208 Dpre with 1 denoting the indicator function and Dpre representing the pre-training dataset.",
            "score": 0.6452123966830774,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 7283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 766
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "266149825",
            "title": "Diffence: Fencing Membership Privacy With Diffusion Models",
            "text": "A membership inference attack [10] aims to determine if a specific sample was in a model's training data, posing risks of sensitive individual information leakage. This section overviews various such attacks and defenses, highlighting their diversity across scenarios.",
            "score": 0.6441468840796963,
            "section_title": "VI. RELATED WORKS",
            "char_start_offset": 50279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 268
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "266362863",
            "title": "Traces of Memorisation in Large Language Models for Code",
            "text": "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself. \n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]. \n\nWe refer to the security game defined by Carlini et al. [9] to define a membership inference attack in Definition 1. In this game, the adversary wins if they have a non-negligible advantage > 1 2 + . In simpler terms, the adversary needs to be able to distinguish between data that was included and which was not included in the training data for a given model, while only being allowed query access to the model and data distribution. \n\nMembership inference attacks are primitive for measuring the leakage of a machine learning model and are often a starting point for more extensive attacks [9,26,38]. While membership inference is a weaker privacy violation than memorisation, the National Institute of Standards and Technology (NIST) still considers membership inference to be a violation of the confidentiality of training data [26]. \n\nDefinition 1 (Membership inference security game [9]). The game proceeds between a challenger C, an adversary A, a data distribution D and a model  : \n\n(1) The challenger samples a training dataset  \u2190 D and trains a model   \u2190 T () on the dataset . \n\n(2) The challenger flips a bit , and if  = 0, samples a fresh challenge point from the distribution (, ) \u2190 D (such that (, ) / \u2208 ). Otherwise, the challenger selects a point from the training set (, ) \u2190 . \n\n(3) The challenger sends (, ) to the adversary. (4) The adversary gets query access to the distribution D, and to the model   , and outputs a bit b (5) Output 1 if b = , and 0 otherwise.",
            "score": 0.6423338102689996,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 6911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2067
                },
                {
                    "start": 2070,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "119302127"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1365,
                    "end": 1368,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1368,
                    "end": 1371,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1371,
                    "end": 1374,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 1605,
                    "end": 1609,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "220831381",
            "title": "Label-Only Membership Inference Attacks",
            "text": "Membership inference attacks (Shokri et al., 2016) are a form of privacy leakage that identify if a given data sample was in a machine learning model's training dataset. Given a sample x and access to a trained model h, the adversary uses a classifier or decision rule f h to compute a membership prediction f (x; h) \u2208 {0, 1}, with the goal that f (x; h) = 1 whenever x is a training point. The main challenge in mounting a membership inference attack is creating the classifier f , under various assumptions about the adversary's knowledge of h and its training data distribution.\n\nPrior work assumes that an adversary has only black-box access to the trained model h, via a query interface that on input x returns part or all of the confidence vector h(x).",
            "score": 0.6399168139024071,
            "section_title": "A.2. Membership Inference",
            "char_start_offset": 37003,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "269605305",
            "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape",
            "text": "The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer. Figure 4 summarizes the process of membership inference attacks. \n\nManuscript submitted to ACM CSUR Fig. 4: Membership inference attack workflow from [142]. The attacker first collects information (e.g. parameter updates) from clients, then uses auxiliary dataset to train shadow models which can mimic the whole federated learning procedure, so that one attack model can be trained using the information collected from shadow models. Lastly, the attacker feeds the information collected from real clients into the attack model to perform membership inference attacks.",
            "score": 0.6349141309137748,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 61847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1107
                }
            ],
            "ref_mentions": [
                {
                    "start": 689,
                    "end": 694,
                    "matchedPaperCorpusId": "240354058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "271162091",
            "title": "Provable Privacy Advantages of Decentralized Federated Learning via Distributed Optimization",
            "text": "2) Membership inference attack: The goal of membership inference attacks is to determine whether a specific data sample is part of the training set of a particular model. We deployed the gradient-based membership inference attacks proposed in [56] which are tailored for FL and have demonstrated superior performance compared to prior approaches such as the so-called loss-based [57] and modified entropy-based approaches [58]. The gradient-based approach uses cosine similarity between model updates and instance gradients, expressed as follows: \n\nThe latter one uses the indicator shown as \n\nThe corresponding results are presented in Section VIII-C and VIII-D.",
            "score": 0.6340484507797508,
            "section_title": "B. Empirical evaluation via privacy attacks",
            "char_start_offset": 19365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 663
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "259298807"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "214623088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "221725546",
            "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey",
            "text": "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. [96] provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data. Some defensive mechanisms dedicated to membership inference have been proposed including dropout [114] and adversarial regularization [115]. \n\nIn cryptography-based PPDL models, the security against the membership inference attack can be reduced to the security of the underlying cryptosystems. In such models, the adversarial server cannot obtain model parameters in plaintext. Only if the model is public can the adversary have black-box access of the model, just like any outsider attacker. For HW-based models, the adversarial server owns white-box models, allowing the use of white-box membership inference attacks. \n\nFor DP-based models, the trade-off between the model accuracy and the performance of membership inference attacks according to the selection of the privacy parameter has been studied [116]. Appropriate choices of the privacy parameter result in moderate utility with low membership inference accuracy. However, further experiments are required for the extensibility of their analysis toward other types of tasks outside image classification.",
            "score": 0.6334864169157041,
            "section_title": "B. MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 65858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1116
                },
                {
                    "start": 1119,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1110,
                    "end": 1115,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "13699042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "In this section, we describe our first adversary considered for membership inference attack. For this adversary, we mainly relax the assumption on her shadow model design. In consequence, membership inference attack can be performed in a much more efficient and less costly way. \n\nWe start by defining the threat model. Then, we describe our first simplification, i.e., using one shadow model instead of multiple. In the end, we propose our second simplification which frees the adversary from knowing the target model's structure.",
            "score": 0.6332949093527289,
            "section_title": "III. TOWARDS MODEL INDEPENDENT MEMBERSHIP INFERENCE ATTACKS (ADVERSARY 1)",
            "char_start_offset": 14341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 531
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "271843339",
            "title": "Range Membership Inference Attacks",
            "text": "Membership inference attacks (MIAs) have traditionally been formulated as an inference game [16]- [18], [23], [26] between a challenger and an adversary. In this section, we first review the standard MIA framework and its evaluation protocol, then discuss its intrinsic limitations as a privacy auditing tool. These limitations motivate our proposed extension, range membership inference attacks (RaMIAs), which broaden the notion of membership to include points that leak sensitive information while not exactly in the training set, .",
            "score": 0.6325641948636287,
            "section_title": "III. FROM MIA TO RAMIA",
            "char_start_offset": 11437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 535
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "218862728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "253244424",
            "title": "Amplifying Membership Exposure via Data Poisoning",
            "text": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
            "score": 0.6312298380822267,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 6204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 634,
                    "end": 637,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 1003,
                    "end": 1007,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "218514464"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "174799799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "265654401",
            "title": "Preserving data privacy in machine learning systems",
            "text": "Points of interaction between the data owners and the other threat actors and potential threats against privacy and confidentiality from their perspective. Herein a \u2713 indicates the presence of a threat, an \u2717 the absence. When there is no interaction between the owner of the data and the threat actor, it is marked as not applicable (N/A). \n\nThe first membership inference attack was designed by Shokri et al. (2017) and implemented based on the concept of shadow models, which are models trained on some attacker dataset that is similar to the target model training set. The attack is modelled as a binary classification task trained on the prediction of shadow models in the adversary data set. Multiple works have then followed extending the attack to various settings such as in federated learning (Melis et al., 2018), transfer learning (Zou et al., 2020), generative models (Hayes et al., 2017), language models (Carlini et al., 2019;Song and Shmatikov, 2019a) and speech recognition models (Shah et al., 2021). \n\nThe membership inference attack can be combined with other privacy attacks such as model stealing or model reconstruction attacks (Wu et al., 2020a;Tram\u00e8r et al., 2016;Milli et al., 2019), where the adversary attempts to design a model h () using black-box access to a target model  with f being an approximation or even a perfect match to  . \n\nThe reconstructed model then is a perfect candidate for a performant shadow model that mimics the targeted model, and hence results in a more efficient membership inference attack. \n\nThis type of attacks relies on the fact that the models behave differently when seeing new data points compared to the training data. Models tend to be more confident about the training data, hence the prediction loss is significantly lower than the prediction loss of an unseen data point. \n\nThe poor generalisation of models is one of the main factors that improves the accuracy of membership inference attacks. An over-fitted model tends to hard memorise the training data points rather than learning the underlying distribution. Yeom et al. (2017); Song and Shmatikov (2019b) proved that overfitting is a sufficient condition to perform a membership inference attack, but not a necessary one.",
            "score": 0.6297383030238904,
            "section_title": "Table 1",
            "char_start_offset": 30390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 416,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 940,
                    "end": 966,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 997,
                    "end": 1016,
                    "matchedPaperCorpusId": "236880971"
                },
                {
                    "start": 1168,
                    "end": 1188,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1188,
                    "end": 1207,
                    "matchedPaperCorpusId": "49741763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "269430642",
            "title": "Membership Information Leakage in Federated Contrastive Learning",
            "text": "For active membership inference attacks, the approach involves observing the rate of increase in member and nonmember data loss and utilizes a threshold for inference.An appropriate loss threshold is applied to infer member and non-member data, where loss refers to the data loss after the increase minus the data loss before the increase.During the training process of the active membership inference attack, inference is made by comparing with the loss of member data.Specifically, a batch of data to be inferred undergoes gradient ascent, and the resulting loss is subtracted from the member data loss, then the difference is observed.A small difference indicates member data, whereas a large difference It is crucial to note that while the aforementioned experimental setup and evaluations are structured, it is imperative to consider the ethical implications and privacy concerns associated with membership inference attacks when conducting such experiments.These methods should be leveraged to assess system robustness and security rather than exploiting potential vulnerabilities for unethical purposes.",
            "score": 0.6290846821494759,
            "section_title": "V. EXPERIMENTS",
            "char_start_offset": 28089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "269756904",
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "text": "The objective of the membership inference attack (MIA) [4] is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data.\n\nFormally, given a data sample (x, y), the target model f \u03b8 trained on (possibly fully unknown) dataset D and additional information about f \u03b8 denoted by I, membership inference attack A is defined as the function\n\nA detailed explanation of the proposed membership inference attack will be given in the next sections.",
            "score": 0.6288191197830052,
            "section_title": "B. Membership Inference Attacks",
            "char_start_offset": 5180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 517
                },
                {
                    "start": 519,
                    "end": 731
                },
                {
                    "start": 733,
                    "end": 835
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
            "score": 0.6265915468054878,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "221586480",
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "text": "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier.",
            "score": 0.6265153158908842,
            "section_title": "Membership Inference against ML Models",
            "char_start_offset": 9379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "221586480",
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "text": "Membership inference attacks have been extensively studied in many different data domains, ranging from biomedical data [1,9] to mobility traces [24]. Shokri et al. [27] presented the first membership inference attack against ML models. The general idea is to use shadow models to mimic the target model's behavior to generate training data for the attack model. Salem et al. [25] gradually removed the assumptions of [27] by proposing three different attacks. Since then, membership inference has been extensively investigated in various ML models, such as federated learning [18], generative adversarial networks [4,10], natural language processing [28], and computer vision segmentation [12]. To mitigate the threat of membership inference, a number of defense mechanisms have been proposed, which can be classified into three categories: reducing overfitting [15,25,27], perturbing posteriors [13], and adversarial training [19]. However, all of the previous studies focus on the target models training from scratch. This is the first work studying membership inference in the practical transfer learning setting.",
            "score": 0.6262492159557009,
            "section_title": "Membership Inference",
            "char_start_offset": 4598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1117
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "202539472"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "209439842"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "257378365",
            "title": "Can Membership Inferencing be Refuted?",
            "text": "The goal of a membership inference (MI) attack [51], [11], [12], [28], [63], [54], [59] is to predict whether a data point was used in training a given model. MI attacks are currently the most widely deployed attack for auditing privacy of machine learning models. We consider a model owner who has trained a ML model M \u03b8 on a dataset D. Both the model M \u03b8 and the dataset D is proprietary to the model owner. \n\nPrior work [12], [28], [63] has made attempts at formalizing the MI attack along the lines of a security game which is inspired by cryptography. The attack takes the model M \u03b8 , a data point drawn from the input distribution x * \u223c D, possibly some auxiliary knowledge \u03c8 (for instance, query access to the distribution D) and outputs its prediction bit b \u2190 A(x * , M \u03b8 , \u03c8) indicating its belief about x * 's membership. The security games are played between two parties -the challenger C and the adversary A. For the MI attack, the model owner acts as the challenger and the security game, G M I (\u2022), is defined as follows: \n\nThe membership inference attack is defined as follows: \n\n1) The challenger C samples a dataset D \u223c D via a fixed random seed, S D , and trains a model M \u03b8 \u2190 T (D) on it by using a fixed random seed, S T , in the training algorithm T (\u2022). \n\n2) The challenger C flips a bit b, and if b = 0, samples a fresh challenge point from the distribution x * \u223c D (such that x * \u2208 D). Otherwise, the challenger selects a data point from the training set x * \u2190 D. The challenger uses a fresh random seed S x * for both the cases. \n\n3) The challenger C sends x * to the adversary A. \n\n4) Let \u03c8 denote the adversary's auxiliary knowledge (for instance, query access to the distribution D). The adversary outputs a bit b \u2190 A(x * , M \u03b8 , \u03c8).",
            "score": 0.6245929769210986,
            "section_title": "B. Membership Inference Attacks",
            "char_start_offset": 6875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 65,
                    "end": 69,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "259375769",
            "title": "Scalable Membership Inference Attacks via Quantile Regression",
            "text": "The basic goal of privacy-preserving machine learning is to find models that are predictive on some underlying data distribution, without being disclosive of the particular data points on which they were trained. The simplest kind of attack that can be launched on a trained model-falsifying privacy guarantees-is a membership inference attack. A membership inference attack, informally, is a statistical test that is able to reliably determine whether a particular data point was included in the training set used to train the model or not. \n\nAlmost all membership inference attacks are based on the observation that models tend to overfit their training sets in different ways. In particular, they tend to systematically predict higher confidence in the true labels of data points from their training set, compared to points drawn from the same distribution not in their training set. The confidence that a model places on the true label of a data-point is thus a natural test statistic to build a membership-inference hypothesis test around. A variety of recent methods [Shokri et al., 2017, Long et al., 2020, Sablayrolles et al., 2019, Song and Mittal, 2021, Carlini et al., 2022] are based around this idea, and aim to estimate the distribution of the test statistic (the confidence assigned to the true label of a datapoint) over the distribution of datapoints that were not used in training (and sometimes, also over the distribution of datapoints that were used in training) for the purpose of designing tests that can reject the null hypothesis-that a data point under attack was not used in training-with the desired level of confidence. \n\nFigure 1: Comparing the true positive rate vs. false positive rate of our membership inference attack with the marginal baseline proposed in Yeom et al. [2018] and the state-of-the-art LiRA proposed in Carlini et al. [2022] evaluated at 2, 4, 6, and 8 shadow models. We also provide a visual readout of their 64 shadow model results, as reported in their paper (we did not have the compute necessary to reproduce this). We faithfully replicated LIRA's attack setup and produced better results than their reported values.",
            "score": 0.6242102186967534,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 1073,
                    "end": 1093,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "226266600"
                },
                {
                    "start": 1112,
                    "end": 1139,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1139,
                    "end": 1162,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1162,
                    "end": 1185,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1792,
                    "end": 1810,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1853,
                    "end": 1874,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "253018888",
            "title": "How Does a Deep Learning Model Architecture Impact Its Privacy?",
            "text": "Our study focuses on two types of membership inference attacks: network-based and likelihood-based attacks. We chose these two types of attacks because the network-based attack is commonly used as a baseline in many research papers, making it a conventional attack to consider. Additionally, the likelihood-based attack is a more recent state-of-the-art attack that has demonstrated high effectiveness, making it an important attack to evaluate as well. By considering these two types of attacks, we can effectively represent the performance of membership inference attacks against various victim models and gain insights into potential privacy risks associated with different machine learning models.",
            "score": 0.6226328581541378,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 14367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "261770610",
            "title": "<inline-formula><tex-math notation=\"LaTeX\">${\\sf VeriDIP}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">VeriDIP</mml:mi></mml:math><inline-graphic xlink:href=\"hu-ieq1-3313577.gif\"/></alternatives></inline-formula>: Verifying Ownership of Deep Neural Networks Through Privacy Lea",
            "text": "Membership inference (MI) attacks [29] aim to predict whether a particular example is part of a training dataset. Recently, some researchers [41], [42] have proposed utilizing MI attacks as a means to measure privacy leakage. Other works [30], [38] have theoretically established that the privacy leakage measured by MI attacks serves as a lower bound for \u03f5. In this work, we leverage the concept of membership advantage [38] and utilize it as a fingerprint for our model. We provide a review of the related definition below. \n\nBefore getting into membership advantage, we first define the MI attack following [29], [38]. \n\nLet A be a membership inference attack algorithm, f S is a machine learning model trained on S \u223c D n . The procedure of the membership inference experiment is as follows: 1) Toss a coin at random b \u2190 {0, 1}; 2) If b = 1, then the sample z draws from S, denoted as z \u223c S. \n\nIf b = 0, then the sample z comes from D, denoted as z \u223c D; \n\nreturns 1 to represent the attacker correctly guessing the answer of b, denoted as A (z, f S , D) = b and vice versa. \n\nIn Definition 1, the attack algorithm A (z, f S , D) inputs arbitrary sample z, model f S , public data distribution D, and outputs the judgment about whether the sample z is used to train model f S . \n\nMembership advantage [38] represents the advantage of an MI attacker's ability to guess the decision boundary of training samples and other samples over random guess.",
            "score": 0.6201781013382595,
            "section_title": "Membership Inference",
            "char_start_offset": 20034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "220647336"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "260125925",
            "title": "Security and Privacy Issues of Federated Learning",
            "text": "Membership Inference attacks [13]  Training Data Guessing Attacks: Membership inference attacks are akin to \"guessing\" the training data used to train a machine learning model. By leveraging the global model, attackers attempt to deduce whether particular data points were present in the training set. Employing various predictive modeling techniques, they try to infer details about the original training data through educated guesses.",
            "score": 0.6198037537498517,
            "section_title": "C. Membership Inference Attacks",
            "char_start_offset": 13267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Takeaways. We show that membership inference attacks can be effective even under relaxed assumptions on the shadow dataset and shadow model's architecture/hyperparameters. By collecting images from the Internet with the same classes provided by the target model, our evaluation shows that attacks remain effective in this more realistic case, indicating that the threat of membership inference is underrated. Such observation reveals that there might be some more intrinsic factors that lead to the success of membership inference attacks regardless of datasets and models. We further investigate this in Section 4.4.",
            "score": 0.6167364138256621,
            "section_title": "Implication of Different Threat Models",
            "char_start_offset": 36840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 617
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "267069190",
            "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
            "text": "Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches. \n\nOne of the seminal works in this domain was presented by Shokri et al. [51]. They provided a concrete framework to understand membership inference attacks, introducing an attack model f attack . The input x attack is a prediction confidence vector consisting of the correct label class and a target model (the model being attacked). The output of the attack model is a prediction class \"in\" (member) or \"out\" (non-member). \n\nFor further insights, we show the experimental results in TA-BLE II and briefly introduce seminal and contemporary works on membership inference attacks. Key references include [51]- [80]. (For some unintroduced papers, we also display them in   [52] identify individual records with high accuracy in real-world datasets using blackbox queries on ML models. They argue that combining generalization and perturbation with careful training set selection can be used to defend against such attacks. Nasr et al. [82] assess the privacy vulnerability of the stochastic gradient descent algorithm and design a deep learning attack model to calculate the membership probability of target data points. They also evaluate the privacy of pre-trained state-of-the-art models on the CIFAR-100 dataset to validate the effectiveness of whitebox inference attacks. Liwei Song et al. [55] propose two new membership inference attacks against adversarial robust models, leveraging predictions from adversarial examples and validated worst-case predictions.",
            "score": 0.6158949023921279,
            "section_title": "A. Membership Inference Attacks",
            "char_start_offset": 26516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1802,
                    "end": 1806,
                    "matchedPaperCorpusId": "54444175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "270560767",
            "title": "Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives",
            "text": "For example, Hitaj et al. [60] outline a novel class of active attacks that involve launching a model inversion attack on some GANs to reconstruct facial images of the users through white-box mechanisms.Here, the adversary uses deception to extract accurate yet sensitive information about the victim.\n\nModel extract attacks.In a model extraction attacks, the adversary has no prior knowledge of the target model's parameters or training data and so attempts to obtain the model's parameters by extracting the target model.As a result, the adversary actually increases the success rate of subsequent attacks on the model's training data.Ateniese and colleagues [6] were the first to address the notion of extracting unexpected but useful information from a trained model.They designed a meta-classifier and trained it to hack other classifiers so as to infer sensitive information about the training set.In [121] Membership Inference Attacks.In a membership inference attack, the goal is to determine whether a sample record has been used to train a model, i.e., to infer whether that record is a member of the training set.The consequences of such attacks can be very serious for the individuals involved.For instance, if a data record is known to have been used in a model trained to classify types of cancer, a membership inference attack could potentially leak information about the health of that individual [112].Several recent studies have demonstrated that machine learning models, no matter the learning schema, are vulnerable to membership inference attacks [61,93,112,125], even in black-box settings [125].For example, an adversary might discern whether a data record forms part of the model's training set through an API.In this category of attack, Nasr [93] devised a white-box membership inference strike that can retrieve private data from the model but fails to obtain data from a model but cannot obtain data from other models with the same distribution.\n\nProperty Inference Attacks.Property inference involves identifying properties that hold for specific subsets of the training data but are not universally applicable to all class members.In these attacks, adversaries access trained models to extract global statistics about the training data [22].",
            "score": 0.614969845084998,
            "section_title": "Privacy attacks",
            "char_start_offset": 60905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 301
                },
                {
                    "start": 303,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1972
                },
                {
                    "start": 1974,
                    "end": 2001
                },
                {
                    "start": 2001,
                    "end": 2160
                },
                {
                    "start": 2160,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 907,
                    "end": 912,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1413,
                    "end": 1418,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1572,
                    "end": 1575,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1612,
                    "end": 1617,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "273662363",
            "title": "Auditing f-Differential Privacy in One Run",
            "text": "We can also use any attack algorithm A. \n\nWe note that membership inference attacks have received a lot of attention recently (Homer et al., 2008;Shokri et al., 2017;Leino and Fredrikson, 2020;Bertran et al., 2024;Hu et al., 2022;Matthew et al., 2023;Duan et al., 2024;Zarifzadeh et al., 2023). These attack had a key difference from our attack setup and that is the fact that there is only a single example that the adversary is trying to make the inference for. Starting from the work of (Shokri et al., 2017), researchers have tried to improve attacks in various settings (Ye et al., 2022;Zarifzadeh et al., 2023). For example, using calibration techniques has been an effective way to improve membership inference attacks (Watson et al., 2021;Carlini et al., 2022). Researchers have also changed their focus from average case performance of the attack to the tails of the distribution and measured the precision at low recall values (Ye et al., 2022;Nasr et al., 2021).",
            "score": 0.6149643330810477,
            "section_title": "Guessing games",
            "char_start_offset": 17209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 42,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 973
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 146,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 146,
                    "end": 166,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 166,
                    "end": 193,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 193,
                    "end": 214,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 214,
                    "end": 230,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 747,
                    "end": 768,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "We adpot a formulation of Membership Inference attacks similar to that of Yeom et al. [47]. First a value, b, is chosen uniformly at random from {0, 1}. If b = 1, the attacker, A, is then given an instance z = (x, y) from the general population; otherwise, if b = 0, z is sampled uniformly at random from the elements of the training set, S, used to generate target model, \u011d. The attacker then attempts to predict b given z and some additional knowledge, aux( \u011d), about \u011d determined by the threat model (see Section 2.3 below).",
            "score": 0.6127186758959186,
            "section_title": "Membership Inference",
            "char_start_offset": 13065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 527
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "248870291",
            "title": "Black-box based limited query membership inference attack",
            "text": "Since Shokri et al. [6] proposed an inference attack against MLaaS, the research on member inference has received extensive attention and has been successfully implemented in many fields. It uses the output information of the model to infer whether a sample exists in the training data set of the model, which seriously threatens the privacy and security of the machine learning model. Membership inference attacks are divided into white-box attacks [11][12][13][14] and blackbox attacks [5,6,15] according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability. In addition, membership inference attacks can also be divided into attacks against independent models [16][17][18][19][20][21] and attacks against Federated Learning [11,22]. \n\nAt present, the research on membership inference attack mainly focuses on the black-box attack of independent models. The attacker uses the model's predicted behavior difference between member data and non-member data to train a binary attack model that recognizes this difference. In order to obtain the training data of the attacking model in the blackbox mode, Shokri [6]  Shokri's method [6] is based on the assumption of extensive query and knowledge of the target model structure. They obtained synthetic data by querying the target model and used this data to train multiple shadow models with the same structure as the target model. In practical applications, a large number of accesses to the target model require high query costs, which largely limits the application scope of membership inference attacks. Therefore, how to construct a shadow model similar to the target model under a small number of queries is the key to solving the hypothesis of membership inference attack.",
            "score": 0.61222829172969,
            "section_title": "A. MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 4381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 488,
                    "end": 491,
                    "matchedPaperCorpusId": "186206717"
                },
                {
                    "start": 491,
                    "end": 493,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "233864706"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "210888275"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1462,
                    "end": 1465,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "235210365",
            "title": "Generative Adversarial Networks",
            "text": "Membership inference attacks can be launched towards a machine learning and/or deep learning model to determine if a specific data point is in the given model's training dataset or not [114]. \n\nTypically, after a model is trained, an attacker feeds data into the model and gets the corresponding prediction results that can be used as additional knowledge to perform black-box membership inference attacks. Such an attack will cause privacy leakage and even other severe consequences. \n\nFor instance, with a patient's medical records and a predictive model trained for a disease, an attacker can know whether the patient has a certain disease by implementing membership inference attacks. To defend against such attacks, the techniques of GAN, anonymization, and obfuscation have been exploited to design countermeasures [4,29].",
            "score": 0.610806563368865,
            "section_title": "Membership Privacy",
            "char_start_offset": 34124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 828
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 190,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "13754095"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "2468323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "224704932",
            "title": "Unexpected Information Leakage of Differential Privacy Due to the Linear Property of Queries",
            "text": "The claim is proved in next subsection. \n\nThe whole idea of this paper is showed in Fig 1 , including the method to obtain multiple answers for the target query and the membership inference attacks method which is presented in detail at next section.",
            "score": 0.6107619439629018,
            "section_title": "Proposed Method",
            "char_start_offset": 19240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 42,
                    "end": 250
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "247940198",
            "title": "A Differentially Private Framework for Deep Learning With Convexified Loss Functions",
            "text": "The goal of Membership Inference (MI) Attacks is to infer the membership of a given data sample by the prediction output of the data sample. In this paper we specify MI attacks as black-box MI attacks implemented via the shadow model technique by Shokri et al. [2]. That is, the MI adversaries only have access to the distribution of the target/private dataset and the prediction output of a given data sample. \n\nFor simplicity, a black-box MI attack via shadow models is a function A : D kn+1 \u2192 {0, 1}. The adversary performs a shadow models-based MI attack in three steps. Firstly, the adversary trains k shadow models with shadow datasets \n\nmimicking the behaviour of the target model trained with the target/private dataset X. Then the adversary uses the k shadow models to make predictions for training data (members) and test data (non-members). These prediction vectors will then be the training data to train an attack model (a binary classifier). Finally, for a given data sample x \u223c D, the adversary queries the prediction probability vector p of x from the target model, then feeds p to the trained attack model to predict the membership ({0, 1}) of x. \n\nTable 2 summarises the notations used in this paper.",
            "score": 0.6107044092652947,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 17244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1218
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "In this section, we first define membership inference attack in the machine learning setting. Then, we introduce the datasets used for our evaluation.",
            "score": 0.6091451761848051,
            "section_title": "II. PRELIMINARIES",
            "char_start_offset": 10548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "258236265",
            "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning",
            "text": "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by Shokri et al. (2017). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model. To improve the performance of machine learning classifiers, i.e., attacker, Shokri et al. (2017) uses shadow models, which simulate the behavior of target model, to generate sufficient training data for the attacker. The following work has investigated various settings of shadow model training and presented several defence techniques (Salem et al., 2018;Wu et al., 2020;Yu et al., 2021). However, current research has primarily concentrated on exploiting shallow classification models, leaving deep learning and regression models such as collaborative filtering largely unexplored.",
            "score": 0.608808590219945,
            "section_title": "Membership Inference",
            "char_start_offset": 11153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1145
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 638,
                    "end": 658,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 918,
                    "end": 934,
                    "matchedPaperCorpusId": "203836240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "265330154",
            "title": "Comparative Analysis of Membership Inference Attacks in Federated and Centralized Learning",
            "text": "We conducted a comprehensive study of the effectiveness of the membership inference attack in the FL and CL environments considering different attack techniques, optimizers, datasets, and countermeasures. Existing related work focuses on the CL environment and the effectiveness of one single countermeasure. In this paper, we investigated the FL environment, compared it with the CL environment, and studied the effectiveness of combining two mitigation techniques together.",
            "score": 0.6083252517701564,
            "section_title": "\u2022",
            "char_start_offset": 3024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "221586480",
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "text": "Table 3 summarizes the attack performance on three student models. We observe that the membership inference performs well on three student models, meaning the membership inference is still effective in student models. \n\nSummaries. Through extensive experiments, we draw the following conclusions: 1) having access to the teacher model can severely leak the membership information of teacher dataset; 2) the student model would not leak membership information of teacher dataset; 3) the student model could inevitably leak membership information of student dataset.",
            "score": 0.6082432619430922,
            "section_title": "Q1: Effectiveness of Different Membership Inference Attacks",
            "char_start_offset": 18271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "269756904",
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "text": "In this work, we propose GLiRA, a novel framework that applies knowledge distillation to perform membership inference attacks.We demonstrate that explicit distillation of the target model yields sufficient information to determine the membership status of the input data points.Our approach operates in a black-box setting without requiring any information about the target model.The method can be used to increase the effectiveness of other membership inference attack methods requiring training shadow models.We evaluate our approach on multiple datasets and neural network architectures, comparing it against concurrent methods, and show that our approach outperforms state-of-theart membership inference attacks in most of the considered experimental setups.Future work includes exploring more fine-grained techniques to transfer target model behaviour and studying ways to reduce computational costs by decreasing the number of trained shadow models.",
            "score": 0.6080412206499118,
            "section_title": "VIII. CONCLUSION",
            "char_start_offset": 33648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "244117091",
            "title": "Property Inference Attacks Against GANs",
            "text": "Membership Inference Attacks. \n\nMembership inference attack tries to infer whether a sample belongs to a specific dataset. Previous studies have demonstrated successful attacks against various targets, such as biomedical data [5,21,29] and location data [50]. \n\nShokri et al. [56] introduce the first membership inference attack against machine learning models. The key idea is to leverage a set of shadow models to mimic the target model's behavior, and then train an attack model to discriminate member from non-member samples on model outputs. Salem et al. [55] show that the membership inference attack can attain high accuracy even relaxing the three assumptions in [56]. In recent years, membership inference attacks have been investigated on various other scenarios, e.g., white-box models [34,44], federated learning [43], generative models [10,22], machine unlearning [11], graph neural networks [25,47], recommender systems [67], selfsupervised models [26,37], label-only cases [13,36], etc. \n\nDespite current research efforts on the membership inference threat for generative models, a wide range of privacy issues of generative models still remain largely unexplored. To fill this gap, we present the first study to specify the property inference attack against GANs. Our results show that even with limited knowledge and access to the target, it is still possible to infer sensitive properties of the training dataset accurately. \n\nProperty Inference Attacks. Property inference attacks aim to infer properties of the target model or the training dataset which are unintended to share by the producer. In fact, sensitive properties cover a wide range of information, which would violate intellectual property if exposed. They can be model-related, such as the model structure and activation functions; as well as data-related, such as where the data are produced or the distribution of the training data. And our work lies in the data-related property inference attacks against GANs. \n\nGanju et al. [17] propose the first property inference attack against discriminative models, which focuses on fully connected neural networks (FCNN), while ours focuses on GANs.",
            "score": 0.6069852247238389,
            "section_title": "Related Work",
            "char_start_offset": 57069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 32,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 849,
                    "end": 853,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 853,
                    "end": 856,
                    "matchedPaperCorpusId": "3052834"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "231846491"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "237290083"
                },
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 992,
                    "end": 995,
                    "matchedPaperCorpusId": "237563320"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "Machine learning (ML) has become a core component of many real-world applications, ranging from image classification to speech recognition. The success of ML has recently driven leading Internet companies, such as Google and Amazon, to deploy machine learning as a service (MLaaS). Under such services, a user uploads her own dataset to a server and the server returns a trained ML model to the user, typically as a black-box API. \n\nDespite being popular, ML models are vulnerable to various security and privacy attacks, such as model inversion [12], adversarial examples [15], and model extraction [43], [30], [45]. In this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims \u00a7 Corresponding author to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status. Previously, membership inference has been successfully conducted in many other domains, such as biomedical data [2] and mobility data [35]. \n\nShokri et al. [38] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple machine learning models (one for each prediction class), referred to as attack models, to make membership inference over the target model's output, i.e., posterior probabilities. Given that the target model is a black-box API, Shokri et al. propose to construct multiple shadow models to mimic the target model's behavior and derive the data necessary, i.e., the posteriors and the ground truth membership, to train attack models. \n\nThere are two main assumptions made by Shokri et al. [38]. First, the attacker needs to establish multiple shadow models with each one sharing the same structure as the target model. This is achieved by using the same MLaaS that trains the target model to build the shadow models.",
            "score": 0.6063908091981324,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "6706414"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1969,
                    "end": 1973,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "257050689",
            "title": "A Survey of Trustworthy Federated Learning with Perspectives on Security, Robustness and Privacy",
            "text": "Membership inference is a technique used to determine whether a given data sample was part of the training dataset [120,130]. For instance, it can be used to identify whether a particular patient's records were used to train a classifier for a specific disease. \n\nTraing Phase Membership Leakage. The work by Shokri et al. [186] focused on the membership inference attack against privacy leakage in machine learning, where an inference model is trained to determine whether a given data sample belongs to the training dataset. \n\nTruex et al. [202] extended this approach to a broader context, demonstrating its data-driven nature and high transferability. \n\nRecent studies have identified the gradients and embedding layers as the two areas facing privacy leakage in membership inference attacks [139]. It has been shown that the embedding of a deep learning network can expose the locations of words in the training data through non-zero gradients, allowing an adversary to conduct a membership inference attack. To address this issue, Hitaj et al. [92] evaluated membership inference attacks against generative models, revealing that many models based on boundary equilibrium GANs or deep GANs are vulnerable to privacy leaks. \n\nInference Phase Membership Leakage During the inference phase, Fredrikson et al. [68] developed an inversion method for retrieving private information, revealing that it can expose user-level information. \n\nIn a similar vein, Melis et al. [140] investigated membership privacy leakage during the inference phase, demonstrating that deep learning models can disclose the placements of words in a batch. In this case, inference attacks are primarily responsible for privacy leaks when attackers can only access model query outputs, such as those returned by a machine learning-as-a-service API [202]. \n\nFurthermore, Shokri et al. [186] explored privacy leakage during the inference phase by examining the inference membership attack against model query results. In this approach, an inference model is trained to distinguish between training and non-training data samples.",
            "score": 0.6058353078745289,
            "section_title": "Membership Leakage",
            "char_start_offset": 72450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 328,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 542,
                    "end": 547,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1050,
                    "end": 1054,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1470,
                    "end": 1475,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1823,
                    "end": 1828,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1859,
                    "end": 1864,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "The above results fully demonstrate the effectiveness of our decision-based attacks. Here, we delve more deeply into the reasons for the success of membership inference. Our boundary attack utilizes the magnitude of the perturbation to determine whether the sample is a member or not, and the key to stop searching perturbations is the final decision change of the model. Here, the status of decision change actually contains information about the decision boundary, i.e., the perturbed sample crosses the decision boundary. This suggests a new perspective on the relationship between member samples and non-member samples, and we intend to analyze membership leakage from this perspective. Since previous experiments have verified our key intuition that the perturbation required to change the predicted label of a member sample is larger than that of a non-member, we argue that the distance between the member sample and its decision boundary is typically larger than that of the non-member sample. Next, we will verify it both quantitatively and qualitatively.",
            "score": 0.6058310374919781,
            "section_title": "Membership Leakage Analysis",
            "char_start_offset": 26313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "236570958",
            "title": "Defense against Membership Inference Attack Applying Domain Adaptation with Addictive Noise",
            "text": "We utilize the accuracy of membership inference attacks on the victim models to reflect the effectiveness. Specifically, the accuracy of membership inference attack is denoted as Acc adv . The closer the Acc adv is to 50%, the better effect the defense has. Since if the Acc adv is to 50%, that means the attacker infers the membership in a random guess manner, which indicates that the attack has no other information to leverage for membership inference. \n\nNote that in our evaluation, the simplified membership inference attack proposed in [11] is used to calculate Acc adv , since this attack is easier to perform and requires less information, which is more practical in the real-world setting, such as MLaaS. \n\nConcretely, the attack model is trained as follow: \n\n1) The attacker train only one local shadow model, whose behavior is similar to the victim model; \n\n2) The attacker collects the output from the shadow model by feeding the training data and non-training and labeled as member and non-member respectively. Noted that these two datasets are of the same size; \n\n3) The attacker uses the collected output to train a binary classifier as the attack model.",
            "score": 0.6043883407137842,
            "section_title": "Effectiveness",
            "char_start_offset": 20394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1170
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "165163934",
            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
            "text": "Membership inference advantage compared to random guessing.\n\nIn this part, we describe the membership inference attack and its performance formally, with notations listed in Table 1. For a neural network model F (we skip its parameter \u03b8 for simplicity) that is robustly trained with the adversarial constraint B \u03f5 , the membership inference attack aims to determine whether a given input example z = (x, y) is in its training set D train or not. We denote the inference strategy adopted by the adversary as I(F , B \u03f5 , z), which codes members as 1, and non-members as 0.\n\nWe use the fraction of correct membership predictions, as the metric to evaluate membership inference accuracy. We use a test set D test which does not overlap with the training set, to represent non-members. We sample a random data point (x, y) from either D train or D test with an equal 50% probability, to test the membership inference attack. We measure the membership inference accuracy as follows.\n\nwhere | \u00b7 | measures the size of a dataset.\n\nThe membership inference accuracy evaluates the probability that the adversary can guess correctly whether an input is from training set or test set. Note that a random guessing strategy will lead to a 50% inference accuracy. To further measure the effectiveness of our membership inference strategy, we also use the notion of membership inference advantage proposed by Yeom et al. [64], which is defined as the increase in inference accuracy over random guessing (multiplied by 2).",
            "score": 0.6042032630623956,
            "section_title": "ADV T inf",
            "char_start_offset": 21531,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "244117091",
            "title": "Property Inference Attacks Against GANs",
            "text": "Moreover, the enhanced membership inference's AUC changes slightly when our property inference attack delivers a proportion less than the underlying property, but the AUC decreases significantly to the baseline when the inferred proportion comes closer to 50%. This further illustrates the applicability of our membership inference attack enhancement algorithm based on the property inference attack.",
            "score": 0.6033403471560921,
            "section_title": "A(x)",
            "char_start_offset": 56652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 400
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "249605407",
            "title": "Membership Inference via Backdooring",
            "text": "This essential challenge has unfortunately not been well recognized and investigated as a first-class citizen in existing machine unlearning literature, and our work herein will bridge this gap. \n\nMembership inference, a recently-emerging technique that aims to identify whether a data sample was used to train a target model or not, can be a promising solution to this challenge. However, existing membership inference techniques are mainly developed in the setting of membership privacy attacks [Shokri et al., 2017;Yeom et al., 2018;Salem et al., 2019]. A typical example is that an attacker can construct an attack model to identify whether a clinical record has been used to train a model associated with a certain disease, breaching the record owner's privacy. Being mainly explored from the attacker's perspective, most existing membership inference methods assume that the attacker has rich information for inference, e.g., the knowledge of training data distribution and the architectures of target models. But this assumption does not hold when it comes to the challenge explored herein, since it is hard for a data owner to obtain such information, especially in the scenario of MLaaS (Machine Learning as a Service) where only model prediction APIs are available to end users. Moreover, existing membership inference attack models fail to achieve sufficiently high attack accuracy when the target modes are well-generalized, and the attack model training is often computation-intensive [Hu et al., 2021]. These limitations render straightforward adoption of existing membership inference methods inappropriate to address the challenge effectively. \n\nIn this paper, we propose a novel membership inference approach called Membership Inference via Backdooring (MIB), inspired by the backdoor technology in ML [Gu et al., 2019;Chen et al., 2017;Li et al., 2020]. The intuition of MIB is that a data owner proactively adds markers to her data samples when releasing them online, so that at a later stage she can conduct membership inference to determine whether a model in question (i.e., target model) has exploited her released data for model training. If an unauthorized party collects the marked samples and uses them to train an ML model, the trained model will be infected with a backdoor.",
            "score": 0.6028574195649627,
            "section_title": "Introduction",
            "char_start_offset": 2045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 536,
                    "matchedPaperCorpusId": "67846878"
                },
                {
                    "start": 536,
                    "end": 555,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1498,
                    "end": 1515,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1854,
                    "end": 1870,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "91184074",
            "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
            "text": "Membership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
            "score": 0.6028345797635967,
            "section_title": "Related Works",
            "char_start_offset": 46212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 764,
                    "end": 767,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 776,
                    "end": 779,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1739,
                    "end": 1743,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1778,
                    "end": 1782,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1782,
                    "end": 1785,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2067,
                    "end": 2071,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "227152059",
            "title": "When Machine Learning Meets Privacy",
            "text": "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not [98,141]. An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. [141] introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. [173] investigated \"the relationship between overfitting and privacy leakage\". Salem et al. [134] proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\". \n\nMembership inference attacks are also studied in Generative Adversarial Networks (GANs). For example, Liu et al. [84] trained an attacker network to launch membership attacks against Variational Autoencoders (VAEs) and GANs. Hayes et al. [52] focused on \"generative models in ML-as-a-service applications and train GANs to recognize training inputs\". \n\nMelis et al. [98] studied membership inference in collaborative learning. The attack is achieved by \"analyzing periodic updates to the shared model during training\". The reason that this attack is effective is that the gradients in neural networks are based on features, \"thus observations of the participants' gradient updates can be used to infer the feature values, which are in turn based on these participants' private training data\". Wang et al. [163] considered membership inference attack \"against the user-level privacy on the federated learning framework by the attack from a malicious server. The proposed attack framework exploits GAN with a multi-task discriminator, which simultaneously discriminates category, reality and client identity of input samples, and doing so recovers user-specific private data\". \n\n3.1.4 Model Memorization Attack.",
            "score": 0.6027278858018752,
            "section_title": "Membership Inference Attack.",
            "char_start_offset": 21788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 247,
                    "end": 252,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 708,
                    "end": 713,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 800,
                    "end": 805,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "231662446",
            "title": "Fidelity and Privacy of Synthetic Medical Data",
            "text": "We simulated a membership inference attack, the results of which are shown in figure 15a. We observe that for Hamming distance threshold of 15, the success of membership inference attack has the highest variation, with precision values ranging from 0.65 to slightly less than 0.5 (worse than random). At a recall level above 50% (hamming distance threshold of 20) we get low precision equivalent to membership inference disclosure risk score of 0.28% representing very low risk of disclosure. Table 8a shows our results of the attribute inference attack simulation, using age, sex and race as quasi-identifiers, to infer the values of four sensitive attributes. In all scenarios the overall disclosure risk as well as the disclosure risk for each individual sensitive variable is below the 5% threshold, representing very low disclosure risk.",
            "score": 0.6026363688234274,
            "section_title": "DIG dataset results analysis",
            "char_start_offset": 61054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 842
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "53172224",
            "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
            "text": "Membership inference attacks involve an adversary who observes the output of some computations over a hidden dataset D-for example, calculations of aggregate statistics or training of a machine learning model on D-and aims to determine whether a specific data record is a member of D. \n\nSuccessful membership inference attacks against aggregate statistics have been demonstrated in the context of genomic studies [3], [16], location time-series [28], and noisy statistics in general [9]. \n\nMembership inference attacks against ML models are surveyed in more detail in Section VII. They can be performed in a white-box or black-box setting. In the black-box setting, the adversary queries the model with a specific record and attempts to infer from the model's outputs (e.g., probabilities assigned to different classes by a classification model) whether the record was among those used to train the model or not. For example, Shokri et al. [31] demonstrated a method for learning the statistical difference between the outputs of a classification model on members and non-members. The key technique-which inspires our approach in this paper-is to train a membership discriminator using the probability vector output by the model as the feature. \n\nAuditing text-generation models involves an application of membership inference, but there are several critical differences that require the development of new membership inference techniques specifically adapted to the auditing scenarios and the idiosyncrasies of memorization in text-generation models. We discussed these differences in Section I and briefly summarize them here. In realistic auditing scenarios, (1) the auditor's goal is user-level membership inference, as opposed to recordlevel membership inference which was the focus of prior work. \n\n(2) Deployed models that a user may wish to audit can be well-generalized and not overfitted to the training data in the conventional sense. Furthermore, (3) deployed models output only ranked lists of words and not the underlying numeric probabilities, removing the main feature that was used by previous membership inference attacks. Also, (4) the models' outputs may be limited to only a few dozen or hundreds of words, as opposed to the entire vocabulary, and the auditor may be limited to a small number of queries.",
            "score": 0.6014299337843487,
            "section_title": "C. Membership inference",
            "char_start_offset": 11758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "254366520",
            "title": "Memorization of Named Entities in Fine-Tuned BERT Models",
            "text": "The goal of a membership inference attack is to determine whether or not an individual data instance is part of the training dataset for a given model. This attack typically assumes a black-box query access to the model. The common approach to this type of attack is to use a shadow training technique to imitate the behavior of a specific target model. In shadow training, a model (shadow model) is trained on a dataset that has a disjoint but identically formatted training data as the target model. The trained inference model is then used to recognize differences on the target model predictions between inputs used for training and inputs not present in the training data [48].",
            "score": 0.6002192398125374,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 11433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 682
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "221655472",
            "title": "Quantifying Membership Inference Vulnerability via Generalization Gap and Other Model Metrics",
            "text": "to fit target model M 's parameters. More details are provided in the Attack Setting section. We note that the attack outlined in this paper is still effective in the more general context of determining membership within an arbitrary set. It could be employed for an arbitrary set as long as the attacker has similar auxiliary information for the set of interest. \n\nFor membership inference, this paper provides a simple and theoretically optimal attack strategy (under certain conditions). Although our attack requires auxiliary information regarding the target model's performance, the information needed by the attacker could be obtained either through knowledge of how the model is created or by 'modeling the model' through the creation of shadow models, first introduced in (Shokri et al. 2017). Thanks to the optimality of our method, we obtain a bound on the effectiveness of any attack, provided the attacker does not possess additional information beyond what we assume. We then compare our strategy to state-of-the-art membership inference attacks that have access to potentially much more information than our attack. Our results help to shed light on which information is likely being used in these membership inference attacks, and thus reveal which techniques, if any, might be useful in shielding against these attacks.",
            "score": 0.5992284484534653,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1335
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 800,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "252280738",
            "title": "M^4I: Multi-modal Models Membership Inference",
            "text": "Membership inference is a binary classification task. It aims to infer whether a data sample belongs to the training set of the target machine learning model. By querying the target model with a data sample, we can obtain the corresponding output. Then, the attack model takes it as the input and gives a binary output, indicating if this sample has been seen in the training set.",
            "score": 0.5989258843546577,
            "section_title": "Membership Inference",
            "char_start_offset": 5258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "249431594",
            "title": "Subject Membership Inference Attacks in Federated Learning",
            "text": "Membership inference attacks on Machine Learning (ML) models aim to infer the presence of a given data record in the data used to train the models [1]. Most often, this corresponds to testing for membership of certain train samples, like health records, in the training data of ML models, given access to just the fully trained models. This threat model has received a lot of attention in the ML privacy community, with numerous works on formalizing and theorizing this inference risk [2], [3], proposing attacks [1], [4], and mitigation strategies deployed in practical scenarios [5], [6]. However, the membership inference threat model hinges on one critical assumption: the adversary has access to exact records potentially part of training data. Studying membership inference is useful for auditing and to test model memorization, but remains disconnected from goals of actual adversaries that care about targeting individual persons and their data, and not necessarily particular data records. In realistic settings, the adversary often does not even have access to the exact data records used for model training. Consider an adversary that wants to probe a ML model to test membership of an individual's data in the model's training data. An adversary is more likely to have access to some representative face images of the target individual (also referred to as subject or data subject), but not necessarily the ones used for training the model [7]. A membership inference attack is thus ill-defined in such a setting, since access to exact records is unavailable. This threat model, where the attacker is interested in identifying existence of a subject in the training data without necessarily using actual training data records, can be termed as subject membership inference, and serves as a counterpart to subject-level privacy [8]. \n\nSubject membership inference can be thought of as a subset of distribution inference [9]: the adversary seeks to infer whether the training data was sampled from a distribution with the target subject's data (D 1 ) or not (D 0 ). Subject-level privacy is crucial because ultimately, we are interested in preserving the privacy of an individual, not just that of a data item. Although existing membership-inference attacks may yield non-trivial advantage in this scenario, there is a need for attacks designed specifically to infer subject membership.",
            "score": 0.5985608060151759,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2396
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "2565493"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 1452,
                    "end": 1455,
                    "matchedPaperCorpusId": "257984935"
                },
                {
                    "start": 1931,
                    "end": 1934,
                    "matchedPaperCorpusId": "237491948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "265220927",
            "title": "Privacy Threats in Stable Diffusion Models",
            "text": "The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset. This section formalizes MI attacks as a security game and briefly describes common evaluation metrics. \n\n1) Membership Inference Attacks Formalized: The definition of a membership inference attack as a security game follows from [12] and [48]. The game has an attacker who is the carrier of the membership inference attack and a defender who is the owner of the victim ML model f and has knowledge of its training data distribution D. \n\n1) The defender samples a training dataset D train \u2190 D, and optionally additional testing/validation datasets D * \u2190 D, and trains a model f \u03b8 \u2190 T (D train ). \n\n2) The defender flips a coin c D , and if the result is heads, gives the adversary a sample (x, y) \u2208 D train ; otherwise, it supplies a sample (x, y) \u2208 D /D train . \n\n3) The adversary, with oracle access to the model f \u03b8 and the distribution D, tries to guess whether (x, y) is originated from D train or from D /D train . Their guess is produced using a predictive model/system M that can use both D and f \u03b8 , and is registered as a coin face c A \u2190 M (x, y, D, f \u03b8 ). 4) If the adversary guesses the coin face correctly (c A = c D ), the membership of the sample is correctly inferred. Over multiple iterations of the game, one can measure the attacker's power. In this formulation, the attacker is given both samples from the training data distribution and their associated labels. The attacker also has query access to the training data distribution, which allows them to train a virtually unlimited number of shadow models to mimic the behavior of the victim model [24], thereby enabling theoretically more powerful attacks. These two assumptions are what MI attacks commonly require, and this threat model is most widely used in the literature; however, an attacker does not necessarily need to use all of this information to carry out MI attacks.",
            "score": 0.5984619294180568,
            "section_title": "B. Membership Inference Attacks",
            "char_start_offset": 9907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 1837,
                    "end": 1841,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "To evaluate the effectiveness of defenses against membership inference attacks, we make the following two recommendations, besides using our metric-based benchmark attacks.",
            "score": 0.5980499362206975,
            "section_title": "Rigorously evaluating membership inference defenses",
            "char_start_offset": 21687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "3619789",
            "title": "Understanding Membership Inferences on Well-Generalized Learning Models",
            "text": "In a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model. \n\nOne of the first prominent examples of membership inference attacks occur in the context of Genome-Wide Association Studies (GWAS). The seminal work of Homer et al. [18] show that p-values, a type of aggregated statistics routinely published when reporting the results of studies, could be used to successfully infer membership status. Although this attack requires that the adversary know the genome of the target individual, it teaches an important lesson: seemly harmless aggregate statistics may contain sufficient information for successful membership inferences. As a consequence of this attack, NIH has removed all aggregate data of GWAS from public websites [40]. \n\nMore recently, it was shown that membership inference attacks can occur in the context of machine learning. Shokri et al. [32] demonstrated that an adversary with only black-box access to a classifier could successfully infer membership status. However, their attack only works when the classifier is highly overfitted to its training dataset.",
            "score": 0.5975704590798082,
            "section_title": "Background 2.1 Membership Inference Attacks",
            "char_start_offset": 10863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1452
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "30161502"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "Membership Inference. Membership inference attack has been successfully performed in many different data domains, ranging form biomedical data [18], [2] to mobility traces [35]. \n\nHomer et al. [18] propose the first membership inference attack on genomic data. This attack relies on the L 1 distance between the allele frequencies and the victim's genomic data. Backes et al. [2] generalize this attack to other types of biomedical data. More recently, Pyrgelis et al. [35] have shown that people's aggregate mobility traces are also prone to membership inference attack. They first formalize membership inference as a distinguishability game. Then, they implement the attack with machine learning classifiers. Large-scale evaluation on two real-world datasets has demonstrated their attack's effectiveness. Moreover, the authors show their framework can easily incorporate different defense mechanisms, such as differential privacy, to allow a comprehensive evaluation of membership inference risks. \n\nMembership Inference Against Machine Learning. Shokri et al. [38] present the first membership inference attack against machine learning models. The key contribution of this work is the proposal of shadow model training, which aims at mimicking the target model's behavior to generate training data for the attack model. \n\nThe first adversary in the current paper follows a very similar setting. We have shown that one shadow model and one attack model are sufficient to achieve an effective attack compared to the proposal of multiple shadow models and attack models by Shokri et al. [38]. Moreover, we show that data transferring attack can bypass the expensive synthetic data generation scheme and achieve a very similar performance. Another major contribution of our paper is the two effective defense mechanisms, such as dropout and model stacking. Many recent works have studied membership inference against machine learning as well from different angles [25], [17], [48], [26]. \n\nAttacks Against Machine Learning. Besides membership inference, there exist multiple other types of attacks against ML models. Fredrikson et al. [13] present the model inversion attack in biomedical data setting. In this scenario, an attacker aims to infer the missing attributes of her victim, relying on the output of a trained ML model. Later, model inversion attack is generalized to a broader scenario [12].",
            "score": 0.5968296703995154,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 46931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1323
                },
                {
                    "start": 1326,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2329
                },
                {
                    "start": 2330,
                    "end": 2402
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 376,
                    "end": 379,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1588,
                    "end": 1592,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1976,
                    "end": 1980,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 2135,
                    "end": 2139,
                    "matchedPaperCorpusId": "2148083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "270063114",
            "title": "A Systematic Review of Federated Generative Models",
            "text": "Prior work shows that FL may be vulnerable to attacks during and after the learning phase, targeting robustness and privacy.Especially for sensitive datasets such as Personal Information, Medical Records, Financial Reports,etc [Bouacida and Mohapatra 2021;Mamun et al. 2023].\n\nWe can split the type of attacks into two main groups: (i) privacy (e.g., Reconstruction, Inference Attacks, Model Inversion) and (ii) integrity (e.g., Backdoor, Model Poisoning) attacks.Here, we list the common attacks that happen in These updates might accidentally give away more information than intended.We have different inference attacks, such as Property Inference, Attribute Inference, and Membership Inference [Nasr et al. 2019;Gu et al. 2022].\n\nMembership Inference Attacks.Membership Inference attacks try to discover whether a data sample belongs to the training data [Nasr et al. 2019].For instance, a membership inference attack could reveal whether data from a specific patient was used in training the model for predicting Alzheimer's disease.If an attacker can find that a particular patient's data was included in the model's training dataset, it could use that the patient is either at risk of or is already diagnosed with Alzheimer's.This information could lead to privacy violations against the patient.\n\nAttribute Inference Attack.An attribute inference attack seeks to uncover specific attributes or confidential information about individuals in a dataset.For example, consider a dataset with anonymized user data featuring details such as age, location, and browsing habits.An attacker might analyze observable patterns in this data to infer private attributes like political preferences or health conditions.The primary objective of such an attack is to predict sensitive information about individuals using the data that is available [Gong and Liu 2018].\n\nProperty Inference Attacks.Gradient exchange leakage can infer when a property appears and disappears in the dataset during training (e.g., identifying when a person first appears in the photos used to train a face recognition classifier).Property inference attacks assume that the adversary has auxiliary training data correctly labeled with the property he wants to infer [Nasr et al. 2019;Gu et al. 2022].\n\nThere is a slight difference between Attribute and Membership inference.",
            "score": 0.5967587800543607,
            "section_title": "ATTACKS AGAINST FEDERATED LEARNING",
            "char_start_offset": 19257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 275
                },
                {
                    "start": 277,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 731
                },
                {
                    "start": 733,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1302
                },
                {
                    "start": 1304,
                    "end": 1331
                },
                {
                    "start": 1331,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1858
                },
                {
                    "start": 1860,
                    "end": 1887
                },
                {
                    "start": 1887,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2268
                },
                {
                    "start": 2270,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 256,
                    "matchedPaperCorpusId": "233465558"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 858,
                    "end": 876,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1848,
                    "end": 1857,
                    "matchedPaperCorpusId": "4940117"
                },
                {
                    "start": 2234,
                    "end": 2252,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "218473513",
            "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
            "text": "Liu et al. [73] illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. [70] propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model [70]. According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in [70] is generic, but the success of member inference attacks depends on the overfitting of the model [13], [70]. If it is a well-generalized model, the success rate of the membership inference attack is low.",
            "score": 0.5966020149953974,
            "section_title": "2) MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 44055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1015
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "14847504"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "246652540",
            "title": "Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning",
            "text": "Here we comment on the limitations of membership inference as a baseline attack, as membership inference is not tuned to distinguishing between two points (one of which is guaranteed to be in the training set). Indeed, membership inference attackers only get only one instance as input, while our formalization of deletion inference gets two inputs. However, please note that we compare our deletion inference attackers to reductions to membership inference adversaries. The reduction is allowed to call the MI adversary multiple times. Indeed our reduction of the previous subsection calls the MI adversary twice, and this change makes the reduction to MI (which is a DI adversary  The success probabilities are also compared with two baseline attacks that are obtained by reductions to the membership inference attack of [SSSS17]. \n\nitself) powerful enough to be able to win the DI inference game with probability close to 1, so long as its (regular) MI oracle wins its own game with probability close to 1.",
            "score": 0.5963919296173346,
            "section_title": "Remark 3.6 (About using reduction to MI as baseline).",
            "char_start_offset": 37422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1009
                }
            ],
            "ref_mentions": [
                {
                    "start": 823,
                    "end": 831,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "210942951",
            "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
            "text": "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention [1,5,6,7]. These attacks have been deployed on various machine learning models; see, e.g., [1,8,9,10,11,12]. The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting. \n\nMany defence mechanisms have been proposed against membership inference attacks. A game-theoretic approach is proposed in [13], where a regularization term using the accuracy of membership inference attacks is incorporated when training machine learning models. Others have introduced indistinguishability for membership inference attacks as an estimate of the discrimination of the model on training and test datasets [14]. Alternatively, it has been suggested that we can counter membership inference attacks by reducing over-fitting [15]. Membership inference attacks are shown to work better on certain subgroups of the population, e.g., underrepresented minorities, resulting in disparate vulnerability [14]. Furthermore, success of membership inference attack may not predict success of attribute inference attacks with only access to partial view of data records [16]. Another approach is to use differentially-private machine learning at the cost of significantly reducing the utility [17,18]. However, none of these capture the possibly many factors contributing to the success of membership inference attacks. \n\nThis motivates taking a deeper look at the factors behind the success of membership inference attacks using information-theoretic membership information leakage metrics. This is the topic of this paper. \n\nFinally, we would like to point out recent results exploring differential privacy and mutual information, e.g., see [19,20]. Although these results provide important insights into information-theoretic guarantees of differential privacy, they are far from the context of this paper and do not consider membership inference attacks.",
            "score": 0.5959223320998446,
            "section_title": "Related Work",
            "char_start_offset": 3591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 186,
                    "end": 188,
                    "matchedPaperCorpusId": "50778569"
                },
                {
                    "start": 188,
                    "end": 190,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 190,
                    "end": 192,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 277,
                    "end": 279,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 279,
                    "end": 281,
                    "matchedPaperCorpusId": "202539472"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "195477472"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "201660180"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1990,
                    "end": 1994,
                    "matchedPaperCorpusId": "9204999"
                },
                {
                    "start": 1994,
                    "end": 1997,
                    "matchedPaperCorpusId": "1484107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "265654401",
            "title": "Preserving data privacy in machine learning systems",
            "text": "In a membership inference (Shokri et al., 2017;Bernau et al., 2019;Jia et al., 2019a;Li et al., 2020) attack (see Fig. 2) the adversary tries to identify whether a given data point (  ,   ) or a sample of data points were part of the training data set   used to train a given model \u210e  . Revealing that a certain record was used to train a specific machine learning model is a strong indication of private information leakage about the individual data points in the training set. e.g., knowing that a medical record was used to train a machine learning model deployed for diabetes detection can reveal that the person concerned has diabetes. \n\nThese attacks exist in the black-box and the white-box modes. In the black box mode, the attacker has only a query access to the model without any inner information about it, hence only the query results are used to infer the membership of data points within the original training set. In the white-box setting, the attacker has either access to the inner details (description) of the model or can download it locally.",
            "score": 0.5953991736483737,
            "section_title": "Membership inference attacks",
            "char_start_offset": 29317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1061
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 47,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "We formulate the membership inference attack as a binary classification task where the attacker aims to classify whether a sample has been used to train the victim machine learning model. Formally, we define \n\nwhere the attack model A output 1 if the attacker infers that the sample x is included in the training set, and 0 otherwise. \u03b8 denotes the target model parameters while M represents the general model publishing mechanism, i.e., type of access available to the attacker. For example, M is an identity function for the white-box access case and can be the inference function for the black-box case.",
            "score": 0.5950183514652329,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 12317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "244743394",
            "title": "A Systematic Review of Federated Learning in the Healthcare Area: From the Perspective of Data Properties and Applications",
            "text": "Almadhoun et al. [62] demonstrated the first membership inference attack in the medical area that infers the personal information of the participants in a genomic dataset. Truex et al. [63] showed the threats of membership inference attack when the attacker is a member in the FL environment. The member could be the aggregation service or one of the client nodes. Their FL configuration is different from the one discussed above. Instead of pooling the weights to construct a new global model, each node trains their local model and contributes just the prediction probability when inferring a new instance. The process of membership inference attack consists of three steps [61]. Firstly, the attackers aim to develop a shadow dataset \u2032 that mimics the target model training dataset . Secondly, the attacker create a shadow model using the shadow dataset \u2032 which mimics the target model behavior. In this step, the attacker observed the shadow model behavior in response to instances known to have been provided during training against those that were not. This behavior is utilized to create an attack dataset that captures the different instances in the training data and data that have not been seen previously. Finally, this attack dataset is used to construct a binary classifier that predicts whether an instance was previously used in the target model output.",
            "score": 0.5950079983852604,
            "section_title": "Data Privacy Attacks on Federated Learning",
            "char_start_offset": 43538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "220499299"
                },
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98583984375
        },
        {
            "corpus_id": "243938683",
            "title": "Disclosure control of machine learning models from trusted research environments (TRE): New challenges and opportunities",
            "text": "In a membership inference attack, an attacker has access to some particular data instances and attempts to determine whether they were part of to the data used to train the model.This attack model was first introduced by, [42].\n\nMembership inference attacks leverage the observation that models will often make more confident predictions on data that they were exposed to in training than unseen data.Therefore, high predictive confidence can help to infer the likelihood of a data point belonging to a training set.Overconfidence on training examples is associated with model overfitting, [43] which can be the result of poor model architecture, inappropriate training or too few training examples.This has been demonstrated by [44],\n\nwho have also experimented on membership inference attacks on federated learning systems where a model is trained on data from various locations simultaneously.To perform membership inference attacks, an attacker will often train an attack model with the predictive probabilities that the model they are attacking (the target model) produces for data that was and was not used for training.As they will not have the data that was actually used for training, they typically train their own versions of the target model (known as shadow models).Although it has been shown that attacks are still possible based upon shadow models which share neither architecture nor training data with the target model [45], the more information the attacker has about the original data and the target model, the more likely they are to be able to succeed.\n\nMembership inference attacks can also be applied to NLP applications such as in [46], where Carlini et al were able to identify training examples from large language models such as GPT-2.They demonstrated that even without overfitting, large language models such as GPT-2 can still memorize sensitive data.\n\nSimilarly, Vakili et al [47] investigated the privacy preservation of language models such as BERT in clinical data.",
            "score": 0.5947056354988953,
            "section_title": "Membership inference attacks",
            "char_start_offset": 15601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 227
                },
                {
                    "start": 229,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 734
                },
                {
                    "start": 736,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1573
                },
                {
                    "start": 1575,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1907,
                    "end": 1911,
                    "matchedPaperCorpusId": "246061169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "221507916",
            "title": "A Comprehensive Analysis of Information Leakage in Deep Transfer Learning",
            "text": "The basic idea of the inference attack is to exploit the leakages when a model is being trained or released to reveal some unintended information from the training data. In this section, we briefly present two types of inference attacks, i.e., membership and property attacks, for machine learning models. Membership Inference Attack. Membership inference is a typical attack that aims to determine whether a sample is used as part of the training dataset. Membership inference may reveal sensitive information that leads to privacy breach. For example, if we can infer a patient's existence in the training dataset for a medical study of a certain disease, we can probably claim that this patient has such a disease. Recent works have demonstrated the membership attack attempts for machine learning models under the black-box setting [19,30,32], white-box setting [22,26] or both [9]. Shadow training is a widely adopted technique for membership inference, where multiple shadow models are trained to mimic the behavior of the target model [19,30,32]. This technique assumes the attacker to have some prior knowledge about the population from the targeted model training dataset was drawn. Recent works by [22,26] explicitly exploit the vulnerabilities in gradient updates to perform attacks with white-box access. Property Inference Attack. Another common type of attack is property inference that aims to reveal certain unintended or sensitive properties (e.g., the fraction of the data belongs to a certain minority group) of the participating training datasets that the model producer does not intend to share when the model is released. A property is usually uncorrelated or loosely correlated with the main training task. Pioneer works of [2,6,7] conducted the property attacks that characterize the entire training dataset. While, [22] aimed to infer properties for a subset of the training inputs, i.e., in terms of single batches which they termed as single-batch properties. In this regard, membership attack can be viewed as a special case of property attack when scope for property attack is on a sample basis. \n\nThe above-mentioned two types of attacks are closely related. Most of existing works perform those attacks against general machine learning models, while a few focus on the federated learning and collaborative learning scenarios [22,26].",
            "score": 0.5939624261979269,
            "section_title": "Inference Attacks for DNN Models",
            "char_start_offset": 7033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1208,
                    "end": 1212,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1747,
                    "end": 1750,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1750,
                    "end": 1752,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1752,
                    "end": 1754,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "221397750",
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "text": "By evaluating membership inference attacks over a large scope of different dataset, we highlight issues with the rapidly developing research thread of membership inference. Often attack performance is assessed with different type of models, while such performance cannot be seen in isolation of data and training procedure. We urgently need more transparency in reporting membership attack performance in order to be really in a position to compare and measure progress in this area. While a large fraction of the work is focused on attacks, we also provide defense evaluation. We investigate \"standard\" DP-SGD, but also look at effective \"post-hoc\" defenses, that might be a lot more practical (e.g. faced with \"legacy\" models) as they can operate on already trained models and provide strong protection. \n\nMost notably, we investigate the argmax defense, which previously was thought to prevent membership inference attacks. While for previous attacks this is true, we show a new sampling attack that attracts by repeated query of the model surrogate information of the model that is able to recover a large fraction of the attack performance. In turn, we also present a modification of the randomized response defense, that is in part capable of mitigating the new attack vector. \n\nAt time of publication, we will make all our attacks and defenses publicly available -which constitute the largest assembly of techniques in the scope of membership inference attacks that we know of. We believe that this is a contribution that the community will also benefit from. We will also publish our victim models and encourage to re-use these as a point of reference, but advocate transparency and advise to publish more details on training procedures in future work as otherwise comparable results and ultimately the scientific progress in this area is compromised.",
            "score": 0.5936400368261748,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 35808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "218674569",
            "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
            "text": "However, membership inference is easier to deploy and requires less information on the victim model compared with other attacks. Therefore, when an attacker attempts to compromise a model so as to derive the sensitive information, the membership inference attack is served as a \"metric\", probing whether the model is potentially vulnerable. If the membership inference attack is possible, other attacks with higher severity can then be launched.",
            "score": 0.5934955129526838,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 10784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 445
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "258967264",
            "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
            "text": "Membership inference attacks can be used by malicious actors to compromise the privacy of individuals whose data has been used to train models. However, studying and expanding our knowledge of such attacks is crucial in order to build a better understanding for threat models and to build better defense mechanisms that take into account the tools available to malicious actors. Due to the importance of this aspect, we have extensively highlighted existing work studying how to defend against MIAs in Section 6. As we are aware of the potential risks that arise from membership inference attacks, we will not freely publicize our code, but instead give access for research projects upon request. \n\nWith regards to the data we used, we do not see any issues as all datasets are publicly available and have been used for a long time in NLP research or data science competitons.",
            "score": 0.5933144711455436,
            "section_title": "Ethical Considerations",
            "char_start_offset": 29432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 876
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "266933337",
            "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
            "text": "Membership Inference Attacks [139] Distinguishing between member data and non-member data. \n\nFeasible Property Inference Attacks [140] Using visible attribute data to infer hidden attribute data. Data Reconstruction Attacks [141] Retrieving the training data by exploiting model parameters. Model Inversion Attacks [142] Reconstructing input data by reverse-engineering an output. \n\nPoisoning Attacks Data Poisoning Attacks [143] Manipulating training data to cause model inference failure. Scenario Dependent \u25cb Backdoor Attacks [144] Implanting specific triggers into models through poisoning.",
            "score": 0.5930742978764306,
            "section_title": "Inference Attacks",
            "char_start_offset": 36702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 594
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 34,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 129,
                    "end": 134,
                    "matchedPaperCorpusId": "244117091"
                },
                {
                    "start": 224,
                    "end": 229,
                    "matchedPaperCorpusId": "255267953"
                },
                {
                    "start": 315,
                    "end": 320,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 424,
                    "end": 429,
                    "matchedPaperCorpusId": "256218299"
                },
                {
                    "start": 529,
                    "end": 534,
                    "matchedPaperCorpusId": "255595966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "221397750",
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "text": "In this section we present the experimental results of membership inference attacks and the effectiveness of the suggested defenses against them. \n\nFor the first two parts we mostly focus on the interplay of adversaries and defense mechanisms from a practical point of view. To this end we choose a collection of 8 different datasets that were used in the most influential membership inference attack studies (e.g. (Shokri et al., 2017;Salem et al., 2018;Jia et al., 2019;Nasr et al., 2018)) to be able to compare the results with one unified measure. \n\nWe choose two differentially-private defenses (see Section. 3.1), one that perturbs the parameters of the model and one that perturbs the outputs, and apply them on the adversaries. \n\nAt last, with an insight into how adversaries work and what datasets are worth further investigating, we present the results on our novel sampling attack technique.",
            "score": 0.5923232331764994,
            "section_title": "Experiments",
            "char_start_offset": 20119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 902
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 436,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 455,
                    "end": 472,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 472,
                    "end": 490,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Various research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
            "score": 0.5916638537064477,
            "section_title": "Related Works",
            "char_start_offset": 38139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "211532510"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 2073,
                    "end": 2077,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "253446930",
            "title": "On the Privacy Risks of Algorithmic Recourse",
            "text": "However, neither of these works explore if and how adversaries may leverage recourses to infer sensitive information about the underlying model's training data. \n\nIn this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not. In formulating this new class of attacks, we exploit the intuition that the distance between an instance and its corresponding recourse may capture information about whether that instance was used to train the model. We instantiate the aforementioned class of attacks to propose two novel membership inference attacks. Our first attack infers membership by thresholding on the distance between a given instance and its corresponding algorithmic recourse. Our second attack draws inspiration from state-of-the-art loss-based membership inference attacks [5,47] and proposes a likelihood ratio test (LRT) that accounts for algorithmic recourse. Our attacks operate under the assumption that the adversary can only query the recourse algorithm once. This assumption is a lot more practical than those considered in related works [2], and is inline with real-world settings where an end user would typically be provided with a single recourse and will not be able to query the underlying model or recourse algorithm multiple times [41,46]. To the best of our knowledge, our work is the first to introduce membership inference attacks which leverage algorithmic recourse. \n\nWe experiment with multiple real world datasets spanning diverse domains such as lending, healthcare, and law to evaluate the effectiveness of the proposed attacks. Our experimental results clearly demonstrate the efficacy of the proposed attacks, and highlight significant privacy leakage through recourses generated by a wide range of recourse algorithms. In addition, the proposed attacks also outperform the state-of-the-art loss-based membership inference attacks (which do not leverage recourses) on data with sufficiently high dimensionality, thus highlighting the promise of our recourse-based attacks as generic membership inference attacks.",
            "score": 0.591234134425322,
            "section_title": "Introduction",
            "char_start_offset": 2063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "51734899"
                },
                {
                    "start": 1650,
                    "end": 1653,
                    "matchedPaperCorpusId": "3995299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "Similar to the taxonomies of attacks, we also give readers a general picture of membership inference defenses to help readers find the most relevant papers easily. The taxonomy of membership inference defenses in illustrated in Fig. 9. In this taxonomy, we categorize all released papers of membership",
            "score": 0.5912017613569516,
            "section_title": "Taxonomies of Membership Inference Defenses",
            "char_start_offset": 68428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "In this section, we discuss several main challenges and potential research opportunities on membership inference attacks and defenses in order to inspire interested readers to explore this field more.",
            "score": 0.5894584331952215,
            "section_title": "FUTURE DIRECTIONS",
            "char_start_offset": 71867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Membership leakage in ML models emerges when an adversary aims to determine whether a candidate data sample is used to train a certain ML model. More formally, given a candidate data sample x, a trained ML model M , and external knowledge of an adversary, denoted by \u2126, a membership inference attack A can be defined as the following function. \n\nA : x, M , \u2126 \u2192 {0, 1}. \n\nHere, 0 means x is not a member of M 's training set and 1 otherwise. The attack model A is essentially a binary classifier. Depending on the assumptions, it can be constructed in different ways, which will be presented in later sections.",
            "score": 0.5891371587491592,
            "section_title": "Membership Leakage in Machine Learning Models",
            "char_start_offset": 7008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 609
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "270559979",
            "title": "Do Parameters Reveal More than Loss for Membership Inference?",
            "text": "In this section, we examine previous assertions concerning optimal membership inference (Section 3.1) and show, for models trained with SGD, that optimal membership inference requires parameter access (Section 3.2). Our theory directly implies an attack (Section 3.3).",
            "score": 0.5889147653952922,
            "section_title": "Black-Box Access is not Sufficient",
            "char_start_offset": 11269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "Inference Attacks. The inference attacks against machine learning can be divided into model inference and data inference attacks. In model inference attacks, an attacker could infer the parameters [54], hyper-parameters [55], architecture [37] and functionality [40] of a target model. In data inference attacks, the attacker aims at inferring information about the data that the target model operates on. These attacks include membership inference attack [49], model inversion attack (input inference) [14,60], attribute inference [15,58], statistics inference [2] and side-channel attack [57]. \n\nIn this paper, we concentrate on data inference attacks, notably membership inference attack and model inversion attack. Existing defense methods mainly focused on membership inference attacks [24,28,35,49]. Little has been studied about the model inversion attack. Xiao et al. [59] studied the adversarial reconstruction problem where they aim to prevent the latent representations from being decoded into the original input data. To this end, they regularized the encoder with an adversarial loss from a decoder. They studied the face attribute prediction model which outputs 40 binary facial attributes. Our paper, on the contrary, studies black-box classification models whose output is constrained by a probability distribution wherein the values sum up to 1. Moreover, they did not consider the adversarial scenario where the attacker has no access to the same data distribution as the original training data. \n\nGeneral Membership Inference Attack. Membership inference attack is performed to determine whether a given data sample is part of a target dataset. It is not limited to machine learning models. Homer et al. [21] proposed one of the first membership inference attacks in the biomedical setting on genomic data. Some studies also performed membership inference attacks on other biomedical data such as MicroRNA [3] and DNA methylation [17]. Pyrgelis et al. [45,46] further showed that it is possible to perform membership inference attack on location datasets as well. Shokri et al. [49] performed membership inference attack in the machine learning setting which is the same setting in this paper. \n\nSecure & Privacy-Preserving Machine Learning.",
            "score": 0.588636954689906,
            "section_title": "RELATED WORK",
            "char_start_offset": 55003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2212
                },
                {
                    "start": 2215,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "54457412"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "3920864"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 798,
                    "end": 801,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1723,
                    "end": 1727,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 1971,
                    "end": 1975,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 2097,
                    "end": 2101,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "257378138",
            "title": "Client-specific Property Inference against Secure Aggregation in Federated Learning",
            "text": "We disaggregate the linear features used by the membership inference attack that yields increased attack accuracy compared to related work [36]. This negative result shows that accurate private information leakage is still possible with secure aggregation, even without client-specific background knowledge, and that membership inference attacks remain a significant risk, even with a passive adversary. \u2022 We detect clients that exhibit malicious behavior by launching (untargeted) poisoning attacks. To the best of our knowledge, prior works have only addressed poisoning detection without secure aggregation [13,24,48,52]. This positive result shows that secure aggregation is not enough to hide poisoning attacks, which decreases the incentive and therefore the risk of such attacks. The operation of PROLIN is illustrated in Figure 1.",
            "score": 0.5885421995594059,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 838
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "235417297"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "253018943"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "221542915"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "263886687"
                },
                {
                    "start": 620,
                    "end": 623,
                    "matchedPaperCorpusId": "245650333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "We have presented the first generalized framework for the development of a membership inference attack model. This general formulation enables an in-depth characterization of membership infernece attacks against different types of machine learning models. Through extensive experimentation and empirical evidence, we show when and why machine learning models may be vulnerable to membership inference attacks. By exploring a variety of machine learning model types and their correlations with respect to the three phases of the attack generation process, we present five interesting characteristics of membership inference attacks: (1) they are data-driven attacks, (2) attack models are transferable, (3) target model type is a strong indicator of model vulnerability, (4) attack data generation techniques need not explicitly mirror the target model, and (5) membership inference attacks can persist as insider attacks in federated systems. We also include a discussion on countermeasures and mitigation methods against membership inference attacks. \n\nOur research on membership inference attacks and membership privacy continues along several dimensions. First, we are engaged in the development of countermeasures and defense methods. Second, we are currently studying the scale and diversity of membership inference attacks in federated and collaborative learning systems. Third, we are investigating the complex relationships between membership inference attacks, membership privacy, and differential privacy.",
            "score": 0.5883568965044308,
            "section_title": "CONCLUSION",
            "char_start_offset": 63292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "248266789",
            "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models",
            "text": "The question of privacy attacks other than re-identification of personal attributes remains unaddressed by our work. One immediate extension we envisage is to also produce results for membership inference attacks: a form of attack where an adversary uses model outputs to infer whether an individual's data was used in the training set for a model (Shokri et al. 2017;Leino and Fredrikson 2020).",
            "score": 0.5883499676356915,
            "section_title": "\u2022",
            "char_start_offset": 72054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 395
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 368,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 368,
                    "end": 393,
                    "matchedPaperCorpusId": "195699554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Paper Organization. The rest of this paper is organized as follows. Section 2 presents the definitions of membership inference for the ML models, threat models, datasets, and model architectures used in this paper. Section 3 and Section 4 introduce our two attack methods and evaluation methods. In Section 5, we provide an in-depth analysis of the success of membership inference. Section 6 provides multiple defenses against decision-based attacks. Section 7 presents related work, and Section 8 concludes the paper.",
            "score": 0.5882954762395667,
            "section_title": "Cat",
            "char_start_offset": 6425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "267212057",
            "title": "Inference Attacks Against Face Recognition Model without Classification Layers",
            "text": "Member inference attacks aim to speculate whether a record has been involved in the training of the target model. In recent years, there have been significant efforts devoted to membership inference attacks and defenses. Specifically, [26] first proposed the membership inference attack, where the attacker achieves membership inference through multiple attack models. [25] relaxed the assumptions in membership inference attacks; other works [4,20] proposed membership inference attacks for scenarios where only labels are available. In addition, [2] analyzed the membership inference attack from a causal perspective. Meanwhile, some works focused on defenses against membership inference attacks, and the available algorithms can be broadly categorized into two types: overfitting reduction [25,26,30] and perturbation model prediction [15,23,33]. On the contrary, the attack method presented in this paper does not require the use of labels and model predictions. It not only achieves good attack performance, and more importantly, it renders the existing defense methods that are based on the presence of the classification layer vulnerable again. \n\nRecently, attack techniques [8,19] with no demand of the classification layers attracted great attention, which were developed mainly for pedestrian re-identification (Re-ID). FR is a more challenging task compared with pedestrian re-identification in the sense that the face data set contains only face-related information but has more identities and samples than the Re-ID data set. Moreover, [19] requires acquiring multiple samples of the same class, while [8] needs to obtain partial training data. These conditions are difficult to fulfill in realistic scenarios. The developed method in this paper outperforms the existing membership inference attack algorithms in terms of improved success rate under the same amount of available information.",
            "score": 0.588129343649524,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 5715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 798,
                    "end": 801,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "165163934",
            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
            "text": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.",
            "score": 0.5874886353450977,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 15158,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1950,
                    "end": 1954,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "268357756",
            "title": "Towards more accurate and useful data anonymity vulnerability measures",
            "text": "Finally, note that neither of these attacks have better precision than simply predicting that every individual is a member (recall of 1.0).Though out of scope for this paper, it would interesting to explore whether these membership inference attacks exceed an allowed statistical baseline for other recall values.\n\nWe recommend that membership inference attack papers report attack effectiveness as a series of precision-recall curves using a common set of observational skews, for instance 1:2, 1:5, 1:10, 1:50 etc. Doing so would facilitate risk assessments while still allowing researchers to make apples-to-apples comparisons of attack effectiveness.",
            "score": 0.5872634468179408,
            "section_title": "Base rate errors",
            "char_start_offset": 51648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7294921875
        },
        {
            "corpus_id": "253223856",
            "title": "On the Vulnerability of Data Points under Multiple Membership Inference Attacks and Target Models",
            "text": "We use the definition of Membership Inference Attack Game presented by Yeom et al. [10] and followed in the works of Carlini et al. [16], and Jayaraman et al. [17]. Definition 1 (Membership Inference Attack Game). The game between a challenger C and an adversary A:\n\n1) The challenger samples a training dataset D from the underlying distribution D and trains a model f \u03b8 based on D.\n\n2) The challenger flips a bit b. If b = 0, the challenger samples a data point (x, y) from the distribution D. If b = 1, the challenger randomly selects a data point (x, y) from the training dataset D.\n\n3) The challenger sends (x, y) to the adversary.",
            "score": 0.585967282092144,
            "section_title": "Membership Inference Attack Game",
            "char_start_offset": 8072,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "218862728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "221397750",
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "text": "In the previous section we defined the membership inference attacks and described two generic models to carry out these attacks in practice. Now, we will explore methods to defend against them. \n\nTo defend against these attacks, we need to understand what factors make these attacks possible and how we can limit and paralyze the adversary. Most of the previous work on defenses against membership inference attacks can be summarized into two categories: \n\nGeneralization-based techniques (Shokri et al., 2017) was the first to define the membership inference attacks in a machine learning setting. They also identify the overfitting of the victim model as one of the main culprits for vulnerability to membership inference attacks. They hypothesize that the victim model memorizes its training set such that the posteriors show a statistical difference between the seen and hold-out data. A more comprehensive study about the correlation of overfitting to membership inference attacks can be found in (Yeom et al., 2018). \n\nThese findings prompt a line of defense that relies on enforcing generalization on the victim model. (Shokri et al., 2017) suggest using L2 regularization of the parameters and restricting the number of training epochs. (Salem et al., 2018) use dropout and ensemble learning to train the victim model to help it generalize better. In a slightly different approach, (Nasr et al., 2018) utilizes adversarial training of the victim model in the form of a min-max game to help the model generate indistinguishable predictions on its training set and an unseen dataset. \n\nNoising-based techniques Adding randomness to different parameters of the victim model at different stages is one of the most natural ways to confuse any adversary. In fact, the first defenses against membership inference attacks on the genome data (Wang et al., 2009) proposes adding carefully-crafted noise to the published dataset. \n\nJia et al (Jia et al., 2019) suggest adding noise to the output of the victim model. They generate specially-composed noise vectors for the victim model's posteriors such that they act as adversarial examples for the attacker.",
            "score": 0.5858717879117572,
            "section_title": "Defenses for Membership Inference Attacks",
            "char_start_offset": 8510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 489,
                    "end": 510,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1002,
                    "end": 1021,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1245,
                    "end": 1265,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1390,
                    "end": 1409,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1841,
                    "end": 1860,
                    "matchedPaperCorpusId": "8091014"
                },
                {
                    "start": 1939,
                    "end": 1957,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "259137851",
            "title": "Gaussian Membership Inference Privacy",
            "text": "There is a long line of prior work developing [5,7,15,23,29,30,31] or analyzing [35,36,37] privacy attacks on machine learning models. A common class of attacks called membership inference attacks focus on determining if a given instance is present in the training data of a particular model [5,8,9,29,30,31,32,40,41]. Most of these attacks typically exploit the differences in the distribution of model confidence on the true label (or the loss) between the instances that are in the training set and those that are not [5,30,31,40]. For example, Shokri et al. [31] proposed a loss-based membership inference attack which determines if an instance is in the training set by testing if the loss of the model for that instance is less than a specific threshold. Other membership inference attacks are also predominantly loss-based attacks where the calibration of the threshold varies from one proposed attack to the other [5,30,40]. Some Info Loss [41] CFD [29] Loss LRT [5] CFD LRT [29] Gradient LRT Query access to 2: Summarizing the assumptions underlying the different MI attacks. The recourse based attacks do not require access to the true labels nor do they need to know the correct loss functions, but they additionally require access to a recourse generating API R. To the best our knowledge, our gradient attack is the only one for which analytical results exist. \n\nworks leverage different information that goes beyond the loss functions to do membership inference attacks. For instance, Shokri et al. [32] and Pawelczyk et al. [29] leverage model explanations to orchestrate membership inference attacks. \n\nComparison to existing attacks. In Table 2, we summarize the assumptions underlying different membership inference attacks. Note that our attack does not require the training of multiple shadow models on data from the data distribution D N . Instead, we derive the distributions of the LRT test statistic under the null and alternative hypotheses in closed form (see Appendix E), which drops the requirement of training (appropriately parameterized) shadow models to approximate these two distributions.",
            "score": 0.5856498834526661,
            "section_title": "Extended comparison to privacy attacks.",
            "char_start_offset": 29444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "249712435"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 60,
                    "end": 63,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "246485733"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "258170547"
                },
                {
                    "start": 295,
                    "end": 297,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 297,
                    "end": 299,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 527,
                    "end": 530,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "253446930"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "255569927",
            "title": "White-box Inference Attacks against Centralized Machine Learning and Federated Learning",
            "text": "The degree of privacy information leakage of a certain model can be defined as: the attacking party can get one or some private data through this mode. The former shows an increased utility, while the latter reflects a privacy loss. This paper uses white-box member inference attacks to quantify this privacy leakage. Generally speaking, the purpose of an algorithm for member inference attacks is to reason about the identity of a particular data (a member instance or a non-member instance) in the target training set.In practice, attackers with different training premises use member inference attacks to infer the membership of a given data to the target model dataset.Part of the data belonging to the target model data set is observed and used by the attackers to infer more relevant information of the target data set. Therefore, under the attack mode of member reasoning, the important private information of the training data is likely to be displayed through the leakage degree of the target model. This paper uses the way of member reasoning attack to obtain more visual and valuable machine learning model mechanism and vulnerabilities, the results can reflect the information leakage degree and privacy security of the machine model in the learning process.",
            "score": 0.5839236149093442,
            "section_title": "Member Reasoning Attack",
            "char_start_offset": 8885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "254854601",
            "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
            "text": "We first explore whether the model is more vulnerable to membership inference attacks [47] or not after fine-tuning. Membership inference attacks aim to infer whether a given sample is in the training set of a target model or not. A successful membership inference attack can cause severe privacy leakage. Normally, there are three different ways to conduct membership inference attacks: neural networkbased attacks [38,47], metric-based attacks [27,49,50,59], and query-based attacks [11,32]. In this work, we use the neural network-based attack due to its popularity. \n\nThreat Model. We first assume that the adversary only has black-box access to the target model, which means they can only query the model and obtain the output. Then, following previous works [36,39], we further assume that the adversary has part of the target model's training data (treated as members) and testing data (non-members). The adversary can use them for training an attack model and inferring the membership status for other data samples. Note that we adopt the strongest attacker assumption defined in [36] to estimate the worst-case scenario for membership leakage. \n\nMethodology. Our method can be described in two steps: \n\n1. The adversary first queries the target model with both the target model's (partial) training and testing samples, and they label the corresponding outputs as members and non-members. \n\n2. Second, the adversary uses the outputs and the corresponding labels to train their attack model, which is a three-layer neural network model. \n\nThe evaluations are conducted on both the backdoored model and the super-fine-tuned model to see whether finetuning will increase or decrease membership inference risks. \n\nExperimental Settings. We evaluate the membership inference attack in the standalone scenario, which means that the fine-tuning dataset is the same as the pre-training dataset. For each dataset, we randomly sample half of its testing samples and the same number of training samples as the attack training dataset. Then, we select the other half of its testing samples (serving as non-members) and the same number of training samples (serving as members with no overlap on the attack training dataset) to evaluate the attack performance.",
            "score": 0.5835875699722732,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 39552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 453,
                    "end": 456,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 764,
                    "end": 768,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "231802143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "268531455",
            "title": "Large language models in 6G security: challenges and opportunities",
            "text": "The attack aims to deduce sensitive information or insights about the model and its training data by making specific queries or observations, waiting for unintended information leakage from the responses to exploit.Several subset attacks can be viewed, such as attribute inference attacks [41,76,77] and membership inferences [75,13,15,25].\n\nIn both types, the term \"inference\" refers to the process of deriving information from the model's outputs or behavior, albeit with distinct kinds of information being extracted in each.Given their names, the attribute inference attack allows attackers to infer attributes such as demographics, preferences, or characteristics present in the data while membership inference attacks aim to determine whether a specific data record was a part of the training dataset for an LLM.The attackers seek to identify if a specific piece of data (e.g. an individual's record) was used during the model's training process.This attack is concerning because it can reveal sensitive information about individuals, breach privacy, and compromise data confidentiality.If successful, attackers could infer membership in sensitive groups or datasets, leading to privacy violations, identity theft, or unauthorized access to confidential information.",
            "score": 0.5825232387246271,
            "section_title": "Inference attacks:",
            "char_start_offset": 6509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 340
                },
                {
                    "start": 342,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1272
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "272367776",
            "title": "Membership Inference Attacks Against In-Context Learning",
            "text": "Our proposed attack demonstrates effective performance in inferring the membership status of target samples, revealing significant privacy threats. However, as of our current knowledge, there is a lack of a comprehensive defense framework to safeguard In-Context Learning (ICL) from membership inference attacks. \n\nIn this section, we explore three potential defenses aiming to minimize the information leakage from the language model regarding its prompt.",
            "score": 0.5808971913121086,
            "section_title": "Potential Defenses",
            "char_start_offset": 44280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "218862728",
            "title": "Revisiting Membership Inference Under Realistic Assumptions",
            "text": "The membership inference attacks we consider need to output a Boolean result for each test, converting a real number measure from a test into a Boolean that indicates whether or not a given input is considered a member. The effectiveness of an attack depends critically on the value of this decision threshold. \n\nWe propose a simple procedure to select the decision threshold for any threshold-based attack, which allows the adversary to achieve as much privacy leakage as possible while limited to an expected maximum false positive rate:",
            "score": 0.5793573457096136,
            "section_title": "Setting the Decision Threshold",
            "char_start_offset": 20404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Membership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
            "score": 0.5785727300903916,
            "section_title": "Related Work",
            "char_start_offset": 53208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "3052834"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 1886,
                    "end": 1889,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1909,
                    "end": 1912,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "3052834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "248157274",
            "title": "Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms",
            "text": "Membership Inference is the task of predicting whether a given sample was in the training set of a given model. Homer et al. [2008] showed the first proof of concept, and Shokri et al. [2017] showed that a wide variety of machine learning models are vulnerable to such attacks. Shokri et al. [2017] train neural networks to attack machine learning models, and measure the success of the attack by the percentage of correctly predicted (train/test) samples, or equivalently the advantage [Yeom et al., 2018]. While Shokri et al. [2017] trained neural networks to attack machine learning models, it was shown later that simple heuristics such as the loss [Yeom et al., 2018, Sablayrolles et al., 2019] is a more accurate and robust measure of membership inference. \n\nRecent works [Watson et al., 2022, Carlini et al., 2021, Rezaei and Liu, 2021] have proposed to evaluate membership inference by the precision/recall trade-off [Watson et al., 2022] or the precision at low levels of recall [Carlini et al., 2021]. In particular, such works show that some setups which were thought to be private because the membership accuracy is significantly less than 100% can actually reveal membership of a small group of samples with very high precision. \n\nThere is also a line of work developed to design algorithms to specifically defend against membership inference attacks . Although differential privacy would bound the membership inference, these empirical defenses are potentially able to achieve better utility while withstanding against existing membership inference attacks.",
            "score": 0.5778914585654166,
            "section_title": "Membership inference attacks",
            "char_start_offset": 9393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1571
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 131,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 171,
                    "end": 191,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 514,
                    "end": 534,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 653,
                    "end": 671,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 671,
                    "end": 699,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 778,
                    "end": 798,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 820,
                    "end": 843,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 925,
                    "end": 946,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "53172224",
            "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
            "text": "Membership inference. As discussed in Section II-C, release of aggregate statistics about private data is generically vulnerable to membership inference attacks [9]. Membership inference against ML models was studied in [13], [22], [29], [31]. \n\nShokri et al. [31] developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting. \n\nRahman et al. [29] extend the membership inference attack to differentially private ML models and show that the model may need to sacrifice its test accuracy to achieve membership privacy. Hayes et al. [13] studied membership inference attacks against generative models. Long et al. [22] showed that wellgeneralized models can leak membership information, but their attack requires the adversary to first identify a handful of vulnerable records in the training dataset. Yeom et al. [40] formalized membership inference and theoretically showed that ovefitting is sufficient but not necessary. \n\nWe believe that our auditing method is the first positive application of membership inference. Furthermore, it's the first example of user-level membership inference against ML models and the first that targets text-generation models, which are different and more complex than the models previously considered in the membership inference literature. Other innovations vs. prior methods were summarized in Sections I and II-C, such as the fact that our methods work with wellgeneralized methods that do not output numeric probabilities. \n\nMemorization in ML models. Zhang et al. [41] showed that deep learning models can achieve perfect accuracy even on randomly labeled training data. Song et al. [32] presented malicious training algorithms that intentionally encode the training data in the parameters of the model without affecting its accuracy on the main task. By contrast, we demonstrate that popular text-generation models unintentionally memorize training data in their hidden activation units and outputs.",
            "score": 0.5777397737444468,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 46177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "2904063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "Membership inference (MI) attacks aim to determine whether a given data point was present in the dataset used to train a given target model. In this section, we begin by introducing the necessary background needed to formally define membership inference, as well as explicitly defining the threat model used in our analysis.",
            "score": 0.5775745262108711,
            "section_title": "Background",
            "char_start_offset": 10803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 324
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "278130013",
            "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics",
            "text": "We here modify the attribute inference attack privacy game to the membership inference case, randomizing the membership of the target users in the protected dataset instead of the sensitive attribute values. The game is as follows (Figure 9). First, the defender samples a secret bit b uniformly at random from {0, 1} to determine the membership of the target user u * . \n\nSecond, the defender removes the target user from the private dataset D, if b = 0, or removes another target user \n\nThird, the defender obtains the fixed aggregate statistics Q(D) by evaluating the aggregate statistics Q on the protected dataset D. The defender sends the values Q(D) and the size of the protected dataset |D| to the attacker. \n\nFourth, the attacker aims to infer the membership of the target user. The attacker runs an attack A and obtains a membership prediction b \u2032 .",
            "score": 0.5771908012780798,
            "section_title": "Framework for Membership Inference Attack as a Privacy Game",
            "char_start_offset": 29241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 859
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "235358658",
            "title": "Formalizing Distribution Inference Risks",
            "text": "Property inference attacks reveal statistical properties about a training set but are difficult to distinguish from the primary purposes of statistical machine learning, which is to produce models that capture statistical properties about a distribution. Motivated by Yeom et al.'s membership inference framework, we propose a formal and generic definition of property inference attacks. The proposed notion describes attacks that can distinguish between possible training distributions, extending beyond previous property inference attacks that infer the ratio of a particular type of data in the training data set. In this paper, we show how our definition captures previous property inference attacks as well as a new attack that reveals the average degree of nodes of a training graph and report on experiments giving insight into the potential risks of property inference attacks.",
            "score": 0.576910036422089,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "231861713",
            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
            "text": "Membership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
            "score": 0.5766003845724906,
            "section_title": "Related Work",
            "char_start_offset": 35696,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "203593891"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1198,
                    "end": 1201,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1618,
                    "end": 1621,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 1814,
                    "end": 1818,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "250048543",
            "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
            "text": "Recently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
            "score": 0.5759545309807956,
            "section_title": "Membership inference attacks",
            "char_start_offset": 5381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1097,
                    "end": 1100,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1100,
                    "end": 1103,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1561,
                    "end": 1564,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1564,
                    "end": 1567,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1587,
                    "end": 1590,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "237532289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "276928094",
            "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
            "text": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncertainty, resulting in bias in low-density regions. In this work, we propose a novel approach - Bayesian Membership Inference Attack (BMIA), which performs conditional attack through Bayesian inference. In particular, we transform a trained reference model into Bayesian neural networks by Laplace approximation, enabling the direct estimation of the conditional score distribution by probabilistic model parameters. Our method addresses both epistemic and aleatoric uncertainty with only a reference model, enabling efficient and powerful MIA. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of BMIA.",
            "score": 0.5748407169060119,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "238814662",
            "title": "Noiseless Privacy: Definition, Guarantees, and Applications",
            "text": "For membership inference, we use the membership experiment from [73]. We train a model u based on the training dataset. We select b with equal probability from f0; 1g. Then, select a single record from the training dataset if b \u00bc 1 or from outside the training dataset if b \u00bc 0. We transmit Fig. 3. The advantage of the adversary Adv when using the correlation-based policy (left), the mean square error policy (center), and the peak-based policy (right).    the selected record to the adversary. The adversary estimates the realization of random variable b, denoted byb 2 f0; 1g, based on the selected record and trained model following the threshold-based adversary in [73]. The adversary's advantage (in comparison to randomly selecting an estimate) is given by Adv :\u00bc 2jPfb \u00bc bg \u00c0 1=2j: Fig. 8 shows the advantage of the adversary in membership inference attacks versus the privacy budget h for various training dataset sizes. As expected, the attacks become more successful as h increases.",
            "score": 0.5741027838135176,
            "section_title": "http",
            "char_start_offset": 44363,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 68,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "208247985",
            "title": "Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability",
            "text": "Membership inference attacks seek to infer the membership of individual training instances of a privately trained model. This paper presents a membership privacy analysis and evaluation system, MPLens, with three unique contributions. First, through MPLens, we demonstrate how membership inference attack methods can be leveraged in adversarial ML. Second, we highlight with MPLens how the vulnerability of pre-trained models under membership inference attack is not uniform across all classes, particularly when the training data is skewed. We show that risk from membership inference attacks is routinely increased when models use skewed training data. Finally, we investigate the effectiveness of differential privacy as a mitigation technique against membership inference attacks. We discuss the trade-offs of implementing such a mitigation strategy with respect to the model complexity, the learning task complexity, the dataset complexity and the privacy parameter settings.",
            "score": 0.5734755621115843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "277741124",
            "title": "An Early Experience with Confidential Computing Architecture for On-Device Model Protection",
            "text": "To showcase the security gain of the framework, we implement a membership inference attack on the models, showing that running models within a realm, on average, provides an 8.3% decrease in the success rate of membership inference attacks against the training dataset.",
            "score": 0.5733256817336778,
            "section_title": "\u2022",
            "char_start_offset": 4730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "254220843",
            "title": "Membership Inference Attacks Against Semantic Segmentation Models",
            "text": "The success of membership inference attacks was evaluated in model-dependent and model-agnostic settings. In the former setting, the shadow model architecture was identical to the architecture of the victim model, whereas the latter setting allows a mismatch between the victim and the shadow architectures. Regardless of the setting, shadow models were trained with the same hyperparameters and training set sizes as the corresponding victim models.",
            "score": 0.5727864054820168,
            "section_title": "Model Dependency.",
            "char_start_offset": 16595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 450
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "246706163",
            "title": "PPA: Preference Profiling Attack Against Federated Learning",
            "text": "The membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
            "score": 0.5720511221436828,
            "section_title": "B. Membership Inference Attack",
            "char_start_offset": 9055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1332
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "244077745"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1259,
                    "end": 1263,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "276902979",
            "title": "Privacy Auditing of Large Language Models",
            "text": "Membership inference attacks are also used in auditing the privacy of differentially private models. The effectiveness of privacy auditing hinges on the selection of optimal \"canaries\". We introduce new methods for generating easy-to-memorize input space canaries, and use these to improve the performance of existing privacy auditing methods and obtain tighter empirical bounds on privacy leakage. We provide the first privacy audit for the black-box setting for LLMs. Our audit achieves a non-trivial lower bound of \u03b5 \u2248 1 for a model trained to an upper bound of \u03b5 = 4. \n\nPrivacy Attacks in Machine Learning. Membership Inference (Shokri et al., 2017;Choquette-Choo et al., 2021;Carlini et al., 2022a;Jagielski et al., 2023a), attribute inference (Yeom et al., 2018;Fredrikson et al., 2015), and data extraction (Carlini et al., 2019;2023a;b;Biderman et al., 2023a;Tirumala et al., 2022;Mireshghallah et al., 2022;Huang et al., 2022;Lukas et al., 2023;Jagielski et al., 2023b;Ippolito et al., 2023;Anil et al., 2023;Kudugunta et al., 2023;Panda et al., 2024a) are the three main attacks on privacy in machine learning. Our attacks are based on membership inference, and require the logprobs of the model to compute the loss. Morris et al. (2024); Carlini et al. (2024) show that it is still possible for the attacker to access the logprobs when the logprobs are not directly available. Although we do not consider data extraction in this work, membership inference can lead to data extraction by using knowledge of the \"outlier\" token to iteratively guide decoding. We believe that using our method to improve existing data extraction attacks is an interesting future direction. \n\nMembership Inference Attacks on LLMs.",
            "score": 0.5717493498599021,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1720
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 653,
                    "end": 681,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 681,
                    "end": 703,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 703,
                    "end": 727,
                    "matchedPaperCorpusId": "257378537"
                },
                {
                    "start": 768,
                    "end": 792,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 844,
                    "end": 867,
                    "matchedPaperCorpusId": "258291763"
                },
                {
                    "start": 867,
                    "end": 889,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 889,
                    "end": 916,
                    "matchedPaperCorpusId": "256461422"
                },
                {
                    "start": 916,
                    "end": 935,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "256459554"
                },
                {
                    "start": 954,
                    "end": 978,
                    "matchedPaperCorpusId": "250243645"
                },
                {
                    "start": 978,
                    "end": 1000,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 1018,
                    "end": 1041,
                    "matchedPaperCorpusId": "261682406"
                },
                {
                    "start": 1041,
                    "end": 1061,
                    "matchedPaperCorpusId": "268231023"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "265445033"
                },
                {
                    "start": 1249,
                    "end": 1270,
                    "matchedPaperCorpusId": "268357903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "195346528",
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "text": "Shokri et al. [7] develop a novel and intriguing membership inference attack, and apply it to popular machine learning as-a-service APIs. Their attacks are based on so-called \"shadow models\" that approximate the behavior of the model under attack. The shadow models are used to build another machine learning model called the \"attack model\", which is trained to distinguish points in training data from otherwise based on the output they induce on the original model (under attack). As we discussed in Section 6.2, our simple threshold adversary comes surprisingly close to the accuracy of their attack, given the differences in complexity and requisite adversarial assumptions between the attacks. \n\nBecause the attack proposed by Shokri et al. itself relies on machine learning to find a function that separates training and non-training points, it is not immediately clear why the attack works, but the authors hypothesize that it is related to overfitting and the \"diversity\" of the training data. They graph generalization error against the precision of their attack and find some evidence of a relationship, although they also find that the relationship is not perfect, and conclude that model structure must also be relevant. The results presented in this paper make the connection to overfitting precise in many settings, and the colluding trainer we give in Section 6.4 demonstrates exactly how model structure can be exploited to create membership inference vulnerability. \n\nLi et al. [6] explore membership inference, distinguishing between \"positive\" and \"negative\" membership privacy. They show how this framework defines a family of related privacy definitions that are parameterized on distributions of the adversary's prior knowledge, and show that a number of previous definitions can be instantiated in this way.",
            "score": 0.5714601224961876,
            "section_title": "Privacy and machine learning",
            "char_start_offset": 70802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 17,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1495,
                    "end": 1498,
                    "matchedPaperCorpusId": "17295025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "251664530",
            "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
            "text": "During the ML model training process, the model is over-parameterized due to limited available training data and superabundant training epochs, which will cause the ML model to behave differently to training data and nontraining data. For example, an ML model classifies the training data with a high probability, while classifying the nontraining data with a relatively low probability. The adversary speculates whether a given data sample is in the training data set of an ML model, which is the membership inference attack. 28 Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model \uf04d, the adversary \uf041 can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of \uf04d, otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27",
            "score": 0.5712245735936705,
            "section_title": "| Membership inference",
            "char_start_offset": 6435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 529,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 982,
                    "end": 984,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1050,
                    "end": 1052,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1087,
                    "end": 1089,
                    "matchedPaperCorpusId": "209439842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "A recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
            "score": 0.5711772083768655,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 157,
                    "end": 159,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "91184074"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "2904063"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1063,
                    "end": 1066,
                    "matchedPaperCorpusId": "202539472"
                },
                {
                    "start": 1066,
                    "end": 1069,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1069,
                    "end": 1072,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1072,
                    "end": 1075,
                    "matchedPaperCorpusId": "201126909"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1114,
                    "end": 1117,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "268264144",
            "title": "A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges",
            "text": "Membership inference attack can cause information leakage of the subject [160]. Attackers aim to reveal whether or not a sample is included in the training dataset. The target sample can be either a node [68], an edge [69], a subgraph [86], or an entire graph [70]. For example, MIAGraph [86]",
            "score": 0.5709838940001597,
            "section_title": "Membership Inference Attack.",
            "char_start_offset": 51193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 292
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 78,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "218502486"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "231632628"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "239016646"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "231632628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "226975926",
            "title": "Bootstrap Aggregation for Point-based Generalized Membership Inference Attacks",
            "text": "An efficient scheme is introduced that extends the generalized membership inference attack to every point in a model's training data set. Our approach leverages data partitioning to create variable sized training sets for the reference models. We then train an attack model for every single training example for a reference model configuration based upon output for each individual point. This allows us to quantify the membership inference attack vulnerability of each training data point. Using this approach, we discovered that smaller amounts of reference model training data led to a stronger attack. Furthermore, the reference models do not need to be of the same architecture as the target model, providing additional attack efficiencies. The attack may also be performed by an adversary even when they do not have the complete original data set.",
            "score": 0.5709773000015564,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "269430991",
            "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
            "text": "Machine learning models can memorize parts of their training data, enabling various privacy attacks. Membership inference-which we focus on in this work-corresponds to the most general form of data leakage: inferring whether a particular data point was part of a model's training set [51]. Stronger attacks such as attribute inference [21] or data extraction [7,8] aim to recover partial or full training samples by interacting with a model.",
            "score": 0.570380781661848,
            "section_title": "PRELIMINARIES AND RELATED WORK 2.1 Privacy Attacks",
            "char_start_offset": 4012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 441
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 335,
                    "end": 339,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 362,
                    "end": 364,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "195346528",
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "text": "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access. \n\nExperiment 1 below formalizes membership inference attacks. The experiment first samples a fresh dataset from D, and then flips a coin b to decide whether to draw the adversary's challenge point z from the training set or the original distribution. A is then given the challenge, along with the additional information described above, and must guess the value of b. \n\nExperiment 1 (Membership experiment Exp M (A, A, n, D)). Let A be an adversary, A be a learning algorithm, n be a positive integer, and D be a distribution over data points (x, y). The membership experiment proceeds as follows: \n\n1. Sample S \u223c D n , and let A S = A(S). \n\n2. Choose b \u2190 {0, 1} uniformly at random.",
            "score": 0.5700012577042748,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 12901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "254719563",
            "title": "An Understanding of the Vulnerability of Datasets to Disparate Membership Inference Attacks",
            "text": "However, instead of tainted training data, evasion attacks utilize model inputs that seem normal to general inspection but prey on model weaknesses for the disruption of input classification or prediction. Finally, model inversion attacks seek to gather information on the training data used for the development of the attacked model. This attack is divided into property inference attacks, introduced by Ateniese et al., and membership inference attacks, introduced by Fredrickson et al. [7,16]. \n\nProperty inference attacks seek an understanding of a training dataset's statistical information. An example of issues caused by this attack include the use of this information to understand competitor training datasets and, thus, build better classifiers and potentially violate intellectual property rights. Membership inference attacks seek to determine the inclusion of specific records within the training dataset of a model and can result in privacy-infringement issues, such as the discovery of personally identifiable information (PII) and personal health information (PHI) as well as identification of proprietary or confidential information. \n\nThis effort focuses on membership inference attacks and, in particular, explores disparate membership inference attacks. Disparate membership inference attacks differ from general membership inference attacks in that they focus on attacking individual classes instead of the entire dataset as a whole. As discussed in more detail in Introduction: Previous Work, recent efforts have shown increased attack success when targeting more vulnerable subgroups instead of the entire dataset. These studies found minority subsets of data to be more vulnerable to attack, even after models were trained with fairness constraints and differential privacy, unless these were applied to an extent that sacrificed the accuracy of the model. This increased vulnerability to attack of minority subsets of datasets can prove troublesome for both privacy and competition. Typically, smaller subsets of data within a dataset are less represented because they are harder to obtain. In the case of health classification algorithms, these could be observations of patients with rare diseases. In the case of commercial competition, these could be examples of rare findings within a manufacturing or marketing dataset of key competitive advantages. In either of these cases, the discovery of that information by an adversary can prove detrimental to the organizations and individuals involved, whether through loss of privacy, profit, or competitive advantage.",
            "score": 0.5692317425642122,
            "section_title": "Introduction",
            "char_start_offset": 2168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2379
                },
                {
                    "start": 2380,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "14757739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "248798734",
            "title": "How to Combine Membership-Inference Attacks on Multiple Updated Models",
            "text": "We formalize the problem of membership inference with model updates by adapting the membership inference experiment of [40] to the model update setting. We present the experiment in full generality, and introduce speci c contexts for which we subsequently develop speci c attack algorithms. \n\nThe membership-inference experiment. Let  up be a model update algorithm,  0 an initial training set,  0 an initial model,  up an update set size, and each D  with  \u2208 [] a distribution over samples (, ). We de ne the membership-inference experiment in Algorithm 2. The experiment allows the adversary access to all updated models, and requires it to distinguish between update and test data. \n\nAlgorithm 2: The membership-inference experiment with model updates We instantiate this experiment in three settings, for attack algorithm development (Section 4) and evaluation (Section 5): single update, multiple updates, and single update distribution shift. \n\nSingle model update. In the single update setting, we consider  = 1, D 1 = D, so the initial training distribution does not change to sample updates. This setting lets us understand the di erence between having access to the models before and after an update compared to only the nal model. To evaluate the performance of the attacks, we can measure accuracy, E[ 0 ], and precision, E[ 0 | 0 = 1]. An attack maximizing precision may di er from one maximizing accuracy. We also measure recall, E[ 0 = 1| = 1], but, as noted in [23] and [19], a membership inference attack achieving high precision is likely to be more harmful than one achieving high recall. 2 An attack which classi es every sample as appearing in the training set is not harmful, but obtains a high recall; meanwhile, an attack correctly identifying a single sample as appearing in the training set achieves tiny recall, but is harmful to that sample. \n\nMultiple model updates. The multiple update setting considers  > 1 and D  = D for all , so the training distribution remains constant. This lets us understand the di erence between access to multiple models and only the last model. There is a richer set of metrics that can be used here to evaluate attack performance.",
            "score": 0.5676067501142655,
            "section_title": "Formalization of MI under Model Updates",
            "char_start_offset": 10865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "218862728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "239998048",
            "title": "Local and Central Differential Privacy for Robustness and Privacy in Federated Learning",
            "text": "In ML, a membership inference attack aims to determine if a specific data point was used to train a model [40,100]. There are new challenges to mount such attacks in FL; for instance, does the data point belong to a particular participant or any participant in that setting? Moreover, it might be more difficult for the adversary to infer membership through overfitting. \n\nNasr et al. [77]'s attack. The main intuition is that each training data point affects the gradients of the loss function in a recognizable way, i.e., the adversary can use the Stochastic Gradient Descent algorithm (SGD) to extract information from other participants' data. More specifically, she can perform gradient ascent on a target data point before updating local parameters. If the point is part of a participant's set, SGD reacts by abruptly reducing the gradient, and this can be recognized to infer membership successfully. Note that the attacker can be either one of the participants or the server. An adversarial participant can observe the aggregated model updates and, by injecting adversarial model updates, can extract information about the union of the training dataset of all other participants. Whereas, the server can control the view of each target participant on the aggregated model updates and extract information from its dataset. \n\nWhen the adversary is the server, Nasr et al. [77] use the term global attacker, whereas, if she is one of the participants,  local attacker. Moreover, the attack can be either active or passive; in the former, the attacker influences the target model by crafting malicious model updates, while, in the latter, she only makes observations without affecting the learning process. For the active attack, they implement three different types of attacks involving the global attacker: 1) gradient ascent, 2) isolating, and 3) isolating gradient ascent. The first attack consists in applying the gradient ascent algorithm on a member instance, which triggers the target model to minimize loss by descending in the direction of its local model's gradient for that instance; whereas, for non-members, the model does not change its gradient since they do not affect the training loss function. The second one is performed by the server by isolating a target participant to create a local view of the training process for it.",
            "score": 0.5668459386056109,
            "section_title": "Membership Inference",
            "char_start_offset": 38851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "54444175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "232222684",
            "title": "On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models",
            "text": "With an increase in low-cost machine learning APIs, advanced machine learning models may be trained on private datasets and monetized by providing them as a service. However, privacy researchers have demonstrated that these models may leak information about records in the training dataset via membership inference attacks. In this paper, we take a closer look at another inference attack reported in literature, called attribute inference, whereby an attacker tries to infer missing attributes of a partially known record used in the training dataset by accessing the machine learning model as an API. We show that even if a classification model succumbs to membership inference attacks, it is unlikely to be susceptible to attribute inference attacks. We demonstrate that this is because membership inference attacks fail to distinguish a member from a nearby non-member. We call the ability of an attacker to distinguish the two (similar) vectors as strong membership inference. We show that membership inference attacks cannot infer membership in this strong setting, and hence inferring attributes is infeasible. However, under a relaxed notion of attribute inference, called approximate attribute inference, we show that it is possible to infer attributes close to the true attributes. We verify our results on three publicly available datasets, five membership, and three attribute inference attacks reported in literature.",
            "score": 0.5667660430088464,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "278145266",
            "title": "Metaverse meets distributed machine learning: A contemporary review on the development with privacy-preserving concerns",
            "text": "Objective. An inference attack focuses on revealing secrets by supplying varying input data and monitoring the results. There are multiple types of inference attacks, each with fundamental differences. Unlike the attribute inference attack, which assumes the attacker already has some knowledge of a training record before trying to guess the missing attribute, membership inferences rebuild the records by running possible options through a machine learning model and monitoring the output to understand if it was present in the original model. Through the obtaining of a local client's preferences, an attacker can go on to use the information for malicious purposes, such as social engineering attacks or supporting targeted identity theft. \n\nA defense for this, which is currently available, is through leveraging cryptographic-based federated learning techniques.",
            "score": 0.5666232760378294,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 49928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "257663933",
            "title": "Do Backdoors Assist Membership Inference Attacks?",
            "text": "We discussed backdoor-assisted membership inference attacks, which do not deteriorate the accuracy. We first evaluated whether backdoor-assisted membership inference attacks with the original backdoors [7] and the imperceptible backdoors [10], [13], [15] are successful in comparison with the existing poisoning-assisted membership inference attack [5]. We then showed that backdoor-assisted membership inference attacks are unsuccessful in contrast to the existing poisoningassisted membership inference attack by Tramer et al. [5] We also analyzed their resultant models with respect to loss distributions and neuron activations to deeply understand the reason for the unsuccessful results. Then, we confirmed that triggers cannot affect the distribution of clean samples; namely, any clean sample becomes inliers while the existing attack makes it an outlier. Thus, we believe that backdoors cannot assist membership inference attacks.",
            "score": 0.5661942738927879,
            "section_title": "VI. CONCLUSION",
            "char_start_offset": 15320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 938
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 205,
                    "matchedPaperCorpusId": "131777414"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "244397177"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "199405468"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "248562973"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "247922814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "231846491",
            "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
            "text": "Membership inference attack is one of the most popular privacy attacks against ML models [7,8,10,19,28,31,33,49,52,57]. The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}. \n\nHere, the target model is the contrastive model introduced in Section 2. A successful membership inference attack can cause severe privacy risks. For instance, if a model is trained on data samples collected from people with certain sensitive information, then successfully inferring a sample from a person being a member of the model can directly reveal the person's sensitive information. \n\nFollowing previous work [10,33,49,52,57], we assume that an adversary only has black-box access to the target model T , i.e., they can only query T with their data samples and obtain the outputs. In addition, the adversary also has a shadow dataset D shadow , which comes from the same distribution as the target model's training dataset. The shadow dataset D shadow is used to train a shadow model S, the goal of which is to obtain the necessary information to perform the attack. We further assume that the shadow model shares the same architecture as the target model [52]. This is realistic as the adversary can use the same machine learning service as the target model owner to train their shadow model. Alternatively, the adversary can also learn the target model's architecture first by applying model extraction attacks [40,41,60,63].",
            "score": 0.566025250521321,
            "section_title": "Attack Definition and Threat Model",
            "char_start_offset": 11960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1579
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 100,
                    "end": 103,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 771,
                    "end": 774,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 774,
                    "end": 777,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 1569,
                    "end": 1572,
                    "matchedPaperCorpusId": "54457412"
                },
                {
                    "start": 1572,
                    "end": 1575,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1575,
                    "end": 1578,
                    "matchedPaperCorpusId": "3423242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "266149496",
            "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
            "text": "We here present the objective of a Membership Inference (MI) attack along with required side information, and briefly introduce diffusion models for context.",
            "score": 0.5654245766475812,
            "section_title": "Background and Preliminaries",
            "char_start_offset": 5572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "269756904",
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "text": "In this setting, the teacher model and student model(s) have identical architectures.In [11], the authors note that most of the existing membership inference attack methods leverage only the information from the output of the given target model, relating these methods to the black-box ones.They propose to additionally exploit the information about the target model's training process during the membership inference attack.The authors do not deviate from the blackbox setup of the attack: to integrate the information about the model' training and evaluate the data point's membership status based on the distilled models' behaviour at different distillation stages.The authors of [15] integrate the knowledge distillation into the process of the training of the shadow models.This work is the closest to ours, however it differs from ours in several important aspects.First of all, in our work, we additionally consider the true black-box setup, where an attacker is unaware of the architecture of the target model and, hence, can not adapt the knowledge distillation procedure accordingly.Secondly, we modify the loss function for the knowledge distillation term and experimentally show that it leads to the higher precision of the membership inference attack.Additionally, we evaluate the impact of the weight of the divergence term in the corresponding loss function on the success of the attack.",
            "score": 0.5652567905658241,
            "section_title": "III. RELATED WORK",
            "char_start_offset": 10647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1402
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 92,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "244345608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "236154858",
            "title": "Generative Models for Security: Attacks, Defenses, and Opportunities",
            "text": "A prominent type of exploratory attack on machine learning is membership inference attacks. In a membership inference attack, the attacker aims to determine whether a specific individual's data was part of the target model's training data [209]. These attacks exploit the fact that machine learning models such as classifiers often behave differently when asked to make a prediction for a data point from their training dataset compared to for a data point not part of the training dataset (but from the same distribution as the training dataset). This behavior is related to overfitting, although recent work suggests that models may be vulnerable even in the absence of overfitting [136,146,148]. \n\nMembership inference attacks on generative models seek to exploit the same effect. In particular, some membership inference attacks have been developed against GANs [94] and generative models [98]. Additionally, VAEs are vulnerable to reconstruction attacks [98].",
            "score": 0.5648510719403206,
            "section_title": "Exploratory Attacks",
            "char_start_offset": 44248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 964
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 244,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 684,
                    "end": 689,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "226266600"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "249940711",
            "title": "Data Privacy and Trustworthy Machine Learning",
            "text": "the black-box setting, the attacker can only interact with the model's predictions. Shokri et al. 12 designed membership inference attacks, which are algorithms that simulate the game and measure the information leakage of models about their training data. Moreover, reports by the National Institute of Standards and Technology 15 and the U.K.'s Information Commissioner's Office 10 specifically mention membership inference as a confidentiality violation and potential threat to the training data in artificial intelligence.\n\nIn the black-box setting, membership inference attacks attempt to exploit the signals contained in a model's predictions. The major examples of such a signal are prediction error and uncertainty. Predictions tend to be more accurate and certain for members of the training set compared to unseen points. Powerful membership inference attacks have been demonstrated for many different scenarios in machine learning. 9, 16 We can use membership inference attacks to quantitatively measure the privacy risks of machine learning algorithms in many diverse scenarios. Throughout this article, we use membership inference attacks as a tool to measure information leakage of private data. This way, we can study privacy implications of the other aspects of trustworthy machine learning.\n\nGiven a set of points X R n 3 with membership : m X {0,1}\n\n\" to a training set, we define the average privacy risk of a trained model as (see Ye et al. 16 for a more comprehensive discussion on how to measure privacy risk in machine learning.)\n\nis the signal the attacker can observe for each point after a model is trained. The signal can be the model's prediction, model's loss, an explanation, or",
            "score": 0.5648245581143615,
            "section_title": "Threats to Data Privacy",
            "char_start_offset": 3892,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 100,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "231802143",
            "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
            "text": "Membership Inference (MemInf) [53] against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature [6,7,23,29,31,37,49,51,53]. Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]. \n\nIn the following, we illustrate how to implement membership inference (MemInf) under different threat models. \n\naux [51]. We start with the most common and difficult setting for the attack [51,53], whereby the adversary has black-box access (M B ) to the target model and a shadow auxiliary dataset (D S aux ). The adversary first splits the shadow dataset into two parts and uses one to train a shadow model on the same task. Next, the adversary uses the entire shadow dataset to query the shadow model. For each querying sample, the shadow model returns its posteriors and the predicted label: if the sample is part of the shadow model's training set, the adversary labels it as a member and as a non-member otherwise. With this labeled dataset, the adversary trains an attack model, which is a binary membership classifier. Finally, to determine whether a data sample is a member of the target model's training dataset, the sample is fed to the target model, and the posteriors and the predicted label (transformed to a binary indicator on whether the prediction is correct) are fed to the attack model. \n\nBlack-Box/Partial MemInf,M B ,D P aux [51].",
            "score": 0.5645970257482036,
            "section_title": "Membership Inference",
            "char_start_offset": 8150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 464,
                    "end": 466,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 484,
                    "end": 487,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "275471364",
            "title": "Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations",
            "text": "Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy [19] as a worst-case certification. Existing attacks. Prior work has developed various attacks to exploit differences and identify membership. In our work, we evaluate NODEs against each of the representative attacks described below: \u2022 Yeom et al. [79] formulate an attack which focuses on differences in loss values: for a specific example  = (, ), a model will be more accurate in its predictions when it has seen  during training. The attack predicts  as a member if and only if, for a threshold , the model's loss on  is below ; otherwise, it is classified as a non-member. \u2022 Shokri et al. [66] attack based on shadow models which better approximate the differences in a model's responses to members versus non-members. When constructing shadow models, the attacker artificially generates datasets for training and testing these models so that the members and non-members are known in advance. The attacker collects responses from shadow models for both members and non-members and trains a classifier to predict membership of  based on the target model's response. \u2022 Song and Mittal [67] leverage the prediction correctness to compute the threshold  for identifying membership. Instead of training classifiers to perform attacks,  is designed such that correct predictions with high confidence yield the lowest values while confident but incorrect predictions achieve the highest. \u2022 Watson et al. [73] proposed per-example difficulty calibration in which an attacker leverages shadow models trained without a particular example to compute the average confidence level of a model. The average is then subtracted from the example's confidence obtained from the target model to calibrate the difficulty of the sample. \u2022 Carlini et al. [5] introduced the Likelihood Ratio Attack (LiRA), which carefully controls how the adversary splits shadow training data.",
            "score": 0.5642738165308872,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 12035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2379
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "2565493"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1922,
                    "end": 1926,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "The first paper about the membership inference attack was published by Shokri et al. [31] under the assumption of the black-box model. The shadow model was used to train a machine learning model similar to the target model, and the shadow model was used to learn differences of confidence vector about members and non-members. Later, Nasr et al. [27] first proposed the membership inference attack under the white-box model and federated learning model. They classified between members and non-members by distinguishing model data such as the gradient changes of each layer or the output of each layer. Song et al. [33] mainly discussed the practical threat of machine learning in the article. The defensive measures against adversarial samples were to generate a robust machine learning model. But, this robust machine learning model was more sensitive to membership inference attacks. The article [32] set different thresholds, which was used to determine whether the prediction confidence belongs to the member, for a different class label to improve the accuracy of the attack. At the same time, the redefined prediction entropy was used to strengthen the membership inference attack. The paper [40] designed new membership inference algorithms against machine learning models and achieved significantly higher inference accuracy when the augmented data was also used in training but the augmented mechanism was known to the adversary. However, the malicious attacker could only utilize the original image or randomly chosen transformations which yielded a significantly lower inference success rate. There are also some membership inference attacks where the adversary's advantages are limited. Hui et al. [17] proposed an membership inference attack when the adversary couldn't collect enough sample with output probabilities and labels as either members or non-members and Choquette et al. [8] introduced label-only membership attacks with the adversary only got access to models' predicted labels. In addition, there was an new membership inference attack [42] to recommander systems where the adversary could only observe the ordered recommended items.",
            "score": 0.5641501997037297,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 39513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1897,
                    "end": 1900,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
            "score": 0.5641068263866128,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "234785882",
            "title": "Protect privacy of deep classification networks by exploiting their generative power",
            "text": "One main category of privacy attacks consists of inference attacks, which contains membership inference attacks and attribute inference attacks. Membership inference attacks aim to infer whether an example was in the target model's training dataset, e.g., inferring whether a patient's record was used in medical research. Shokri et al. (2017) designed a black-box membership inference attack against machine learning models. Subsequently, researchers introduced several variants of the attack, such as attacks on GANs (Hayes et al., 2019), VAEs (Hilprecht & H\u00e4rterich, 2019), model explanations (Shokri et al., 2019), and collaborative learning models (Nasr et al., 2019). We focus on mitigating membership inference attacks on DNN classifiers in this paper. \n\nA well-known membership inference attack is the shadow-model attack (Shokri et al., 2017). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others.",
            "score": 0.5638545748225047,
            "section_title": "Membership inference attacks",
            "char_start_offset": 6268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 343,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 546,
                    "end": 575,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 830,
                    "end": 851,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "246431210",
            "title": "Bounding Training Data Reconstruction in Private (Deep) Learning",
            "text": "P n z x L 6 k c l u 1 w 6 v i 4 X K u f j O H J g F + y B A 2 C D E 1 A B V 6 A K a g C D R / A K 3 s G H 8 W S 8 G Z / G 1 2 h 0 z h h 7 d s A f G N 8 / Y m C t S g = = < / l a t e x i t > (a) Membership inference attack game",
            "score": 0.5636291903534535,
            "section_title": "Private Learner",
            "char_start_offset": 8665,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.182861328125
        },
        {
            "corpus_id": "221507667",
            "title": "Vulnerability of Face Recognition Systems Against Composite Face Reconstruction Attack",
            "text": "Given the name or identification of a person, the goal of membership attack is to reveal whether or not the information of this person has been used as training data. For example, inference attacks against a cancer diagnosis system can be exploited by the adversary to identify if a specific person is a cancer diagnosed case. One of the first researches tried to show the eminence of inference attacks was proposed by Homer et al. [18]. Backes et al. [19] studied the viability of membership inference to challenge the privacy of individuals contributing their microRNA expressions to collect training datasets. Homer et al. [20] focused on attacks on genomic research studies, where an attacker tries to infer the membership of a specific person's data within an aggregate genomic dataset, or aggregate locations [21]. Shokri et al. [22] took advantage of multiple shadow models that approximate the behavior of the target model by training the attack model on the labeled inputs and outputs of the shadow models. Hayes et al. [23] studied to what extent the membership inference attacks can be successful against generative models. Truex et al. [24] studied the model vulnerability through proposing a generalized formulation of a black-box membership inference attacks using different model combinations and multiple datasets. Melis et al. [25] studied the possibility of successful membership inference attack in distributed learning systems.",
            "score": 0.5636089852659614,
            "section_title": "B. Membership Inference Attack",
            "char_start_offset": 6728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1447
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "52211986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "To help researchers conduct empirical studies on membership inference attacks and defenses, we summarize most, if not all, the datasets and metrics that have been used in previous work. This aims to pave the way for the community to build a good benchmark in this area for future empirical analysis and in-depth technical understanding. (5) Online Updating Resource. We create an open-source repository1 that includes most, if not all, the relevant work. This repository provides all paper links and released code links to help researchers interested in this area. As a small number of surveyed papers are only available in pre-print, authors are welcome to update us when the full publication information becomes available. We will keep updating the repository with new work in this domain in the future. We hope this open-source repository can shed light on future research about membership inference analysis on ML models. \n\nThe rest of the paper is organized as follows. Section 2 introduces ML preliminaries. In Section 3, we introduce the existing attack approaches and provide taxonomies to categorize the released papers. In Section 4, we discuss why MIAs can work on ML models. Section 5 provides taxonomies for membership inference defenses. Section 6 summarizes datasets, metrics and open-source implementation of popular approaches. We discuss the challenges and propose the future directions in Section 7. We conclude this paper in Section 8.",
            "score": 0.5629855235851506,
            "section_title": "INTRODUCTION",
            "char_start_offset": 8499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1455
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "237353275",
            "title": "Selective Differential Privacy for Language Modeling",
            "text": "We perform two types of attacks: 1) canary insertion and 2) membership inference. Canary insertion is proposed by Carlini et al. (2019). It first inserts random sequences called canaries into the training dataset, then trains the model, and finally calculates the following exposure for the inserted canaries to measure if the model will unintentionally memorize these canaries. Canaries are of a specific format, e.g., s=\"The number is \", where are filled with random values from a randomness space R such as a space of digits from 0 to 9. To obtain the exposure, we enumerate all possible sequences in the specified form, and calculate the negative log-rank with an additional constant, as shown below. Lower exposure indicates the model is more private. In our setting, we insert the secret \"My ID is 341752\" into the training data for 10 times to make the differences between models more salient.\n\nDefinition 5. Given a canary s[r], a model with parameters \u03b8, and the randomness space R, the exposure of s[r] is\n\nMembership Inference is a widely used attack method that identifies if a given sample is a member of the training dataset. Lower inference accuracy means that it is harder to infer a member from the model and thus the model is safer. Carlini et al. (2020) proposed an advance membership inference attack for language models. The basic idea is to calculate the given samples' perplexities under the model, rank them and choose the ones with the lowest perplexities (highest likelihood by the model).\n\nIn our experiments, we randomly select 500 protected secrets from the training set, and randomly generate 500 samples of similar format, to form a dataset for the membership inference attack, so a random guess would give us an accuracy of 50%. For WikiText-2, the secrets are digit sequences; for CUSTOMERSIM, customer names are the secrets.",
            "score": 0.5629298399635428,
            "section_title": "Attack Details",
            "char_start_offset": 18472,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 135,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "270199828",
            "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
            "text": "In this section we present our membership inference attack (denoted as RAG-MIA), which is shown in Figure 2. The input to the generative model consists of the RAG template into which the retrieved database entries and the user prompt are inserted, as depicted in Figure 1. Since the attacker's interaction with the system is limited to the user prompt part of the RAG template, the attack must be based on a prompt that achieves both of the following goals: \n\n\u2022 Cause the system to fetch the desired sample from the retrieval database; \n\n\u2022 Cause the system to generate an output that either contains the membership status of the sample, or from which it can be deduced. \n\nThe first goal can be easily fulfilled by creating a prompt that has high similarity with the desired sample in the retrieval database. Essentially, any prompt that contains the target sample without too much additional text should be sufficient. However, the prompt must also cause the generation process to expose the membership status of the sample. Prompting the system with the target sample alone will not necessarily achieve this goal. \n\nTo handle this, we designed the following attack prompt format (along with a few additional variations presented in Table 1): \"Does this: \"{Target Sam-ple}\" appear in the context? Answer with Yes or No.\". In Figure 3 we present an example of this attack prompt format with a specific target sample. Using this prompt we are able to achieve both goals: cause the right sample to be retrieved from the retrieval database, and make the generation phase produce a binary response regarding the sample's presence in the context. In the black-box setting, we use the model's answer (Yes/No) alone to deduce the membership status of samples. \n\nAs an enhancement to our attack, in cases where the adversary has access to the log-probabilities of the selected tokens (Zhang et al., 2024;Duan et al., 2024), we additionally employ an attack model (see Section 2.2) to determine membership.",
            "score": 0.5625483100345013,
            "section_title": "METHODOLOGY",
            "char_start_offset": 12088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1996
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Machine learning (ML) has witnessed tremendous progress over the past decade and has been applied across a wide range of privacy-critical applications, such as face recognition [28,61] and medical image analysis [9,29,51]. Such developments rely on not only novel training algorithms and architectures, but also access to sensitive and private data, such as health data. Various recent research [23,25,31,35,36,45,46,48,49,54,57,60] has shown that ML models are vulnerable to privacy attacks. One major attack in this domain is membership inference: An adversary aims to determine whether or not a data sample is used to train a target ML model. \n\nExisting membership inference attacks [25, 31, 35, 46, 48,   1 Our code is available at https://github.com/zhenglisec/Decisionbased-MIA. 49, 57] rely on the confidence scores (e.g. class probabilities or logits) returned by a target ML model as their inputs. The success of membership inference is due to the inherent overfitting property of ML models, i.e., an ML model is more confident facing a data sample it was trained on, and this confidence is reflected in the model's output scores. See Figure 1 for an illustration of accessible components of an ML model for such score-based threat model. A major drawback for these score-based attacks is that they can be trivially mitigated if the model only exposes the predicted label, i.e., the final model decision, instead of confidence scores. The fact that score-based attacks can be easily averted makes it more difficult to evaluate whether a model is truly vulnerable to membership inference or not, which may lead to premature claims of privacy for ML models.",
            "score": 0.5615225475645301,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1664
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "7811489"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "4941890"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "15315839"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "14434529"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "211532510"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "208139345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "224817947",
            "title": "Amnesiac Machine Learning",
            "text": "Membership inference attacks were performed with 16 shadow models, each trained on CIFAR100 for 10 epochs. The target model was likewise trained for 10 epochs. We evaluate the effectiveness of membership inference attacks using the recall metric, a metric that give us great insight into how effective these attacks are at leaking data. This metric is more helpful than accuracy in an environment where we care a lot about correctly recognizing positive instances. This gives us information about how effective our method is at preventing membership inference data leaks. All membership inference attacks were performed targeting a set of individual examples, and then the data removal techniques were performed to attempt to remove learned data from this set of individual examples.\n\nMembership inference attacks were performed on a  1 show that the naive retraining method does not prevent the membership inference attacks for more than 2 full epochs of retraining. This highlights the insufficiency of this method to protect against data leaking attacks, and emphasizes the need for other methods. By con- Figure 4: Model Inversion Attack results after 1, 5, and 10 epochs of naive retraining trast, the unlearning and amnesiac unlearning methods both protect against membership inference attacks with less than an epoch of retraining (and in the case of amnesiac unlearning, with no retraining whatsoever).",
            "score": 0.5610648011555022,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 27563,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "235731926",
            "title": "Evaluating the Cybersecurity Risk of Real-world, Machine Learning Production Systems",
            "text": "In membership inference attacks the adversary exploits the ML model These attacks exploit the fact that ML models often behave differently on the data that they were trained on than they behave on the test data [73]. Specifically, as introduced in [73], these attacks utilize shadow models that only require access to the prediction vector of the target ML system. However, these attacks assume the adversary has partial knowledge about the target system's training data by exploiting either partial access to raw",
            "score": 0.5604490052677146,
            "section_title": "Membership Inference",
            "char_start_offset": 32711,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "251104755",
            "title": "Membership Inference Attacks via Adversarial Examples",
            "text": "We describe a novel framework for membership inference attacks (MIA) using adversarial examples with general functionals in the output space; (ii) We present a particular method based on the total variation of the score function; (iii) We propose a defense mechanism against MIA leveraging adversarial examples deferred to the supplementary material. \n\nOutline. Section 2 introduces the relevant background for the analysis. Section 3 provides our novel MIA framework using counterexamples in the output space. Section 4 gathers numerical experiments. Finally, Section 5 concludes our work with a discussion of further avenue. The proofs and relevant material are given in the Appendix.",
            "score": 0.5603871872267586,
            "section_title": "Contributions. (i)",
            "char_start_offset": 2732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 686
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "251765197",
            "title": "Auditing Membership Leakages of Multi-Exit Networks",
            "text": "In this section, we describe our first adversary considered for leveraging exit information to mount membership inference attacks. For this adversary, we mainly make a strong assumption about the adversary's knowledge. In consequence, the research question of whether extra exit information will leak more membership information can be investigated in a more effective and lower-cost way. In the following, we start by defining the threat model, then describe the adversary's attack methodology. In the end, we present a comprehensive experimental evaluation. \n\nThreat Model. We assume that the adversary has direct access to exit information, i.e., exit depth. More concretely, given a data sample and a 6-exit model, the model outputs not only predictions (score or label) but also exit information, e.g., predictions from the first exit point (exit 0) or the sixth exit point (exit 5). Note that, here we directly consider the exit index as the exit depth. exit 0 means the shortest path from the entry point to the first exit, while exit 5 means the longest path from the entry point to the final exit. \n\nIn addition, we make the same assumptions for other settings, such as data knowledge, training knowledge, model knowledge, and output knowledge. For example, in the gradient-based attack, we keep the assumption unchanged that the adversary has access to the intermediate computations of the target model. \n\nMethodology. The methodology is organized into two stages: hyperparameter stealing and enhanced membership inference. \n\nHyperparameter Stealing. The adversary first queries the target model using a large number of data samples, which can come from the shadow dataset or random data samples collected from the Internet. They then count all exit indexes and sort them from smallest to largest. Thus, the largest index implies the number of exits attached to the backbone model. \n\nEnhanced Membership Inference. According to the two different types of attack models used in existing attacks, we propose different methods for each attack model to exploit the exit information. \n\n\u2022 MLP Attack Model. In gradient-based and scorebased attacks using MLP as the attack model, given the exit information (number and depth), the adversary first converts it to a one-hot encoding, which is the same as the one-hot encoding of the true label used in the gradient-based attack.",
            "score": 0.5602694922815571,
            "section_title": "Adversary 1",
            "char_start_offset": 30396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "266191072",
            "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion Models",
            "text": "Membership inference attacks (MIAs) primarily aim to determine whether a target data point x is within the training dataset, often referred to as the member set, of a given target model. The motivation behind these attacks is twofold: to ensure that models are not trained in a manner that misappropriates data and to safeguard against potential privacy breaches. MIA's underlying principle hinges on exploiting machine learning models' overfitting and memorization properties. Discerning the model's different reactions to member and nonmember samples makes it feasible to infer the membership of the target point x. \n\nTo formalize membership inference attacks, assume there is a data sample x, a model M \u03b8 trained with dataset D m . The",
            "score": 0.5600710299839818,
            "section_title": "C. Membership Inference Attacks",
            "char_start_offset": 11266,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "246485733",
            "title": "Parameters or Privacy: A Provable Tradeoff Between Overparameterization and Membership Inference",
            "text": "It is the hope of the authors that by more clearly exposing the link between membership inference vulnerability and generalization performance, researchers can make informed decisions about how to achieve the best tradeoff they can for their application. That said, by studying the performance of optimal membership inference attacks, it is possible that this work will call attention to vulnerabilities in existing model architectures which may be exploited. Furthermore, in settings where privacy is absolutely crucial, such as in medical applications, additional care should be taken to guard privacy beyond the guarantees of this work.",
            "score": 0.5594238010241253,
            "section_title": "A.2 Ethical Considerations",
            "char_start_offset": 29152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "At the most abstract level, membership inference attack models are binary classifiers. Given an instance x and a target model F t , the goal of a membership inference attack model is to identify whether or not x was contained within the dataset D used to train F t . \n\nLet D consist of n training instances (x 1 , y 1 ), (x 2 , y 2 ), ..., (x n , y n ) where x i consists of m features, denoted by x i = (x i,1 , x i,2 , ..., x i,m ), and y i \u2208 Z k , where k is a finite integer value \u2265 2. Let F t : R m \u2192 R k be the target model trained using this dataset D. Given a particular feature vector x \u2208 R m , F t will then output a probability vector p \u2208 R k of the form p = (p 1 , p 2 , ..., p k ), where p i \u2208 [0, 1]\u2200i, and k i=1 p i = 1. The prediction class label y for a feature vector x is the class with highest probability value in p. Therefore y = arg max i \u2208Z k F t (x). \n\nGiven the adversary's black-box access to F t via the prediction service API, an adversary is able to query F t with any number of instances to receive corresponding probability vectors. The adversary uses this probing access, along with any prior knowledge, to generate I , a representation of adversarial knowledge of D. The first building block for implementing a black-box membership inference attack is to leverage I to generate a synthetic labeled dataset D \u2032 to mirror the data in D. This synthetic, labeled dataset D \u2032 is artificially simulated and called a shadow dataset of D. Although the word \"shadow\" was borrowed from shadow copying for systems creating back up data copies [39], the shadow dataset in our context should be thought of as a synthetic version of the real training dataset D. D \u2032 is then used to generate an attack training dataset D * , which is required to train the final membership attack model, a binary classifier F a .",
            "score": 0.5593418093721354,
            "section_title": "GENERAL ATTACK FORMULATION",
            "char_start_offset": 16014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1831
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "258967264",
            "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
            "text": "As the performance of membership inference attacks depend on the training procedure of the attacked model as well as its degree of overfitting, it is not possible to simply compare attack performance metrics from other papers to ours. Instead, we had to reimplement existing attacks to compare them to our approach. While we followed the authors' descriptions in their papers as closely as possible, we cannot guarantee that their attacks were perfectly implemented and the comparison to our method is therefore 100% fair.",
            "score": 0.5582111626829054,
            "section_title": "Implementation of baseline attacks",
            "char_start_offset": 28883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "231632628",
            "title": "Membership Inference Attack on Graph Neural Networks",
            "text": "We model the problem of membership inference as a binary classification task where the goal is to determine if a given node v \u2208 V t . We denote our attack model by A. \n\nWe organize the adversary's methodology (also shown in Figure 1) into three phases, shadow model training, attack model training, and membership inference.",
            "score": 0.5581026815909996,
            "section_title": "B. Attack Methodology",
            "char_start_offset": 10772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 324
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "272423578",
            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
            "text": "Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability. \n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7]). \n\nThe game proceeds between a challenger C and an adversary A:   The adversary A is provided with auxiliary information about the data distribution D. This allows the adversary to sample a shadow dataset from the same or a similar distribution, which is a common assumption in the existing literature. \n\nThe attack accuracy for the adversary is defined as follows: \n\nTo assess the privacy leakage caused by membership inference attacks (MIAs), we employ two metrics commonly used in prior research, focusing on both worst-case and averagecase performance: \n\n1. (Log-scale) ROC Analysis [7], which focuses on the true-positive rate at low false-positive rates, effectively capturing the worst-case privacy vulnerabilities of machine learning models. \n\n2. Membership Advantage [59, 71], defined as \n\nThis metric represents the advantage over random guessing, multiplied by 2, providing an average-case measure to gain an overview of the attack's efficacy. \n\nIn this work, we investigate four specific membership inference attacks. For the CIFAR10 and CelebA tasks, a training set of 50,000 samples is employed, while for the TinyIm-ageNet task, we utilize a training set of 100,000 samples to construct the target model. \n\nTo assess the membership status of samples, we first adopt a methodology based on previous research [12,31] that considers the distance to the decision boundary as a reflection of membership status. Specifically, they claim that samples located near the decision boundary are more likely to be nonmembers, whereas samples positioned in the central region of the decision area are more likely to be members.",
            "score": 0.5580699948986352,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 13561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1984,
                    "end": 1988,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1988,
                    "end": 1991,
                    "matchedPaperCorpusId": "237563320"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.",
            "score": 0.5575741972781011,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "264590727",
            "title": "SoK: Memorization in General-Purpose Large Language Models",
            "text": "Privacy researchers often consider what information is stored in a model through inference attacks that are designed to make inferences about the training data. Two common types are membership inference and attribute inference [185]. In the membership inference task, an adversary is given access to a model and knowledge of a candidate record, and wants to determine whether or not that record was included in the model's training data. In attribute inference [182], the adversary is given incomplete information about a record from the training set and has to use its access to the model to infer some missing information about this record. A successful membership inference attack implies that the target model has memorized something about the candidate record, since the model reveals whether or not it was trained on that record, but not what precisely. \n\nA successful attribute inference attack, on the other hand, enables inferences about data not previously known to the adversary. 1 This implies that the model has stored the information learned in some way. This does not necessarily mean that the model has memorized these attributes, though. It could be that it has merely learned the data distribution well enough to predict the attributes based on other attributes of the record [68]. If, however, the attribute inference attack on a counterfactual model trained on the same data with the record removed is unsuccessful in recovering the attributes, then we have evidence that the attack's success was due to memorization of the target attributes by the original model. When successful and done carefully, this can give a much more precise characterization than membership inference of what the model has memorized about a record. We consider memorization in the sense of both membership and attribute inference, since both types of memorization can have significant implications. For example, in the case of verbatim text, knowing that a particular text was used for training an LLM (a successful membership inference attack) might give an auditor information about whether a particular user's data was used in training. Having a model output the first chapter of a book when prompted with its first sentence (a successful attribute inference attack) might constitute a copyright violation. \n\nWhile membership and attribute inference are concerned with individual data points, we consider several pieces of information that may affect multiple documents, such as writing styles or parameters of preprocessing methods.",
            "score": 0.5575376895437997,
            "section_title": "Inference attacks",
            "char_start_offset": 9979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2306
                },
                {
                    "start": 2309,
                    "end": 2533
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 232,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "252089942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "253370617",
            "title": "Unintended Memorization and Timing Attacks in Named Entity Recognition Models",
            "text": "inference attacks have been demonstrated on machine learning models other than neural networks [54]. Furthermore, membership inference attacks may use a different exploit other than neural memorization. One such example is overfitting due to over training which is different from neural memorization [7]. In this paper we have also shown how membership inference attacks can be launched via exploiting side channels, specifically, timing side channels. Note that timing side channels have previously only been demonstrated on stealing machine learning models (model parameters), and not on membership inference on natural language processing models [17]. We have shown how this can be done by exploiting the built-in dictionary of member words in NER systems.",
            "score": 0.5574780447218002,
            "section_title": "RELATED WORK",
            "char_start_offset": 66560,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "272423578",
            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
            "text": "Membership Inference Attacks (MIA) [20,31,50,55,65,74] have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. [55] introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. [50] relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research [12,31] has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang [31] proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. [7] leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. [34] demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. [61] highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks.",
            "score": 0.557445684152923,
            "section_title": "K.1 Membership Inference Attack",
            "char_start_offset": 65364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 45,
                    "end": 48,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "272367776"
                },
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "264820351"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "247922814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "273385410",
            "title": "PPDNN-CRP: privacy-preserving deep neural network processing for credit risk prediction in cloud: a homomorphic encryption-based approach",
            "text": "Objective: Membership inference attacks attempt to determine whether a specific data point was part of the training set. \n\nDefense Mechanisms: \n\n-Homomorphic Encryption: Ensures that even if an attacker gains access to the model's output, they cannot infer whether a particular data point was included in the training data. -Differential Privacy (DP): Adds noise to the training process to obscure individual records and protect against membership inference.",
            "score": 0.5568895267788507,
            "section_title": "Membership inference attacks",
            "char_start_offset": 22360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 458
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "We compare our attack against the attack by Shokri et al. [38]. Following the original configuration of the authors' code, 9 we train 10 shadow models and multiple attack models (one for each class). \n\nAs membership inference is a binary classification, we adopt precision and recall as our evaluation metrics. Moreover, we use accuracy to measure the target model's prediction performance. \n\nResults. Figure 1 depicts the first adversary's performance. In general, we observe that our attack has a very similar membership inference as the previous work [38]. For instance, our attack on the CIFAR-100 dataset achieves 0.95 for both precision and recall, while the attack by Shokri et al. has a 0.95 precision and 0.94 recall. It is also interesting to see that our attack works for both balanced datasets, such as CIFAR-10, and unbalanced datasets, such as Face. \n\nWe also observe variations of the attack performance on different datasets. We relate this to the overfitting level of ML models on different datasets, similar to previous works [38], [48]. We quantify the overfitting level of a target model as the difference between its prediction accuracy on the training set and testing set. Through investigation, we discover that if an ML model is more overfitted, then it is more vulnerable  to membership inference attack (see Figure 2). For instance, our attack on the Adult dataset achieves a relatively weak performance (around 0.5 precision and recall), and there is only a 2% difference between the target model's training and testing accuracy. On the other hand, the membership inference attack achieves a 0.95 precision and recall on the CIFAR-100 dataset. Meanwhile, the corresponding target model provides a much better prediction performance on the training set than on the testing set, i.e., 78% difference. \n\nTo further demonstrate the relationship between overfitting and membership inference, we perform another -more controlled -experiment on the Location and Purchase-100 datasets. Concretely, we focus on the number of epochs used in training, larger number leads to higher overfitting. We vary the number of epochs used from 10 to 100 and report the result in Figure 3.",
            "score": 0.5558165934940286,
            "section_title": "Methodology",
            "char_start_offset": 18569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1044,
                    "end": 1048,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1050,
                    "end": 1054,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "259375769",
            "title": "Scalable Membership Inference Attacks via Quantile Regression",
            "text": "We have introduced a new family of membership inference attacks that are competitive with the state of the art (and in our ImageNet experiments, substantially and uniformly better), while requiring substantially fewer computational resources and less knowledge of the target model. Moreover, we have identified pinball loss as a key target objective: uniformly across all of our experiments, the methods that produce thresholds minimizing pinball loss are the most effective attacks. Together, this brings membership inference closer to practicality on large commercial models. This serves to highlight a growing risk to privacy-but also provides a more efficient means to audit models by subjecting them to our attacks. We hope that our methods encourage and enable a more widespread practice of auditing models for privacy violations by subjecting them to membership inference attacks before deployment.",
            "score": 0.5555136690516268,
            "section_title": "Discussion",
            "char_start_offset": 29866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 905
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "235358658",
            "title": "Formalizing Distribution Inference Risks",
            "text": "Inference attacks seek to infer sensitive information about the training process of a revealed machine-learned model, most often about the training data. For example, in a membership inference attack [13], the adversary aims to infer whether a particular datum was part of the training data. In a property inference attack, an adversary aims to infer some statistical property of the training dataset, such as the proportion of women in a smile-detection dataset [1]. \n\nThe research community lacks a good understanding of property inference attacks-for example, different notions are used in Gopinath et al. [6] and Zhang et al. [17]. Many formal privacy notions have been proposed, including numerous variations on differential privacy, but most notions focus on protecting specific data elements, not statistical properties of a dataset. One notable exception is the Pufferfish framework [11], which introduces notions that allow capturing aggregates of records via specifications of potential secrets and their relations. Although Pufferfish supports aggregate secrets, it is unclear how it can extend to distribution-level properties. A recent attempt to formalize property inference [3] consists of a framework that reduces property inference to Boolean functions of individual members, posing the ratio of members satisfying the given function in a dataset as the property. However, such a formulation limits the threat model since it cannot capture many other kinds of statistical properties of the training distribution that may be sensitive, like the degree distribution of a graph [8]. \n\nIn this work 1 , we formalize property inference attacks based on the critical insight that the key difference between property inference attacks and other inference attacks is that the adversary's goal in the former is to learn about the training distribution, not the specific training dataset. Dataset inference attacks, such as membership inference [13] and attribute inference [4], operate on the level of training records. They are directly connected to the definition of differential privacy which bounds the ability to distinguish neighboring datasets. By contrast, distribution inference attacks attempt to learn statistical properties of the underlying distribution from which the training dataset is sampled. \n\nContributions. We propose a simple and general experiment to formalize property inference attacks, inspired by Yeom et al. [16]'s membership inference definition (Section 2).",
            "score": 0.5552302977531842,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2318
                },
                {
                    "start": 2321,
                    "end": 2335
                },
                {
                    "start": 2336,
                    "end": 2495
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "202577825"
                },
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "2572996"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1981,
                    "end": 1984,
                    "matchedPaperCorpusId": "2148083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "271334179",
            "title": "The Shadow of Fraud: The Emerging Danger of AI-powered Social Engineering and its Possible Cure",
            "text": "Other works have examined \"jailbreaking\" attacks, whereby an adversary manipulates an LLM into generating undesirable content without authorization. Chu  Additional research has profiled privacy risks, such as data theft [80], [105] or enabling membership inference attacks. For example, Greshake et al. ( 2023) [104], by strategically injecting prompts into data likely to be retrieved during inference, adversaries can exploit applications to perform arbitrary code execution, data theft, and manipulate functionality. Membership inference is able to reveal privacy dataset membership information, and thereby judge personal preferences, behaviors, etc [107]. Fu et al. (2023) [106] proposed a new membership inference attack, showing privacy leakage remains a challenge for LLMs trained on sensitive data. Duan et al. (2024) [108] evaluated whether traditional membership inference attacks work on pre-training data of large language models.",
            "score": 0.5547908920960058,
            "section_title": "Data Collection and Preparation",
            "char_start_offset": 32534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 232,
                    "matchedPaperCorpusId": "269015964"
                },
                {
                    "start": 312,
                    "end": 317,
                    "matchedPaperCorpusId": "258546941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "208247985",
            "title": "Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability",
            "text": "Membership inference attacks seek to infer membership of individual training instances of a privately trained model through black-box access to the prediction API of a MLaaS provider. We develop MPLens to expose membership inference vulnerability and perform privacy analysis and privacy compliance evaluation. We demonstrate how membership inference attack methods can be used in the development of representative datasets which can facilitate the generation of substitute models with respect to the attack target model, therefore heightening the risk for adversarial attacks. We investigate training data skewness and its impact on membership inference vulnerability. We also evaluate differential privacy as a mitigation technique for membership inference against DL models. Our empirical results show that (1) minority groups within skewed datasets display increased risk for membership inference and (2) differential privacy presents many trade-offs as a mitigation technique to membership inference risk.",
            "score": 0.5542863401453949,
            "section_title": "VII. CONCLUSION",
            "char_start_offset": 42859,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "In this section, we first summarize the metrics for evaluating attack and defense performance of membership inference. Then, we summarize common datasets used in membership inference attack and defense works on machine learning models. Lastly, we provide links to the open-source implementation of popular methods.",
            "score": 0.5540504495891085,
            "section_title": "METRICS, DATASETS, AND OPEN-SOURCE IMPLEMENTATIONS",
            "char_start_offset": 70553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 314
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "53237764",
            "title": "Illuminating the Dark or how to recover what should not be seen in FE-based classifiers",
            "text": "called leakage-abuse attacks. The goal of leakage-abuse attack is to exploit the leakage explicitly allowed by a cryptographic scheme in order to reveal/obtain information about input data, which is similar to our work except that the cryptographic construction is different. In any case, the leakage-abuse attack does not attack the cryptographic primitive itself but only its use in an operational context (use case).\n\nOther types of attacks have been run on machine learning use cases, such as membership inference attacks [5,19,32,38,39] or model extraction attacks [4,40]. On one hand, membership inference attacks -also referred as tracing attacks -aim to determine whether or not an input was a part of the dataset used during the training phase of a target model. In this paper, we do not focus on this membership privacy concern. On the other hand, model extraction attacks aim to extract the parameters of a target model trained on a private dataset. The main motivation is to construct a model whose predictive performance is similar to the targeted model. Compared to this attack in our case the model is already available to the attacker.",
            "score": 0.5540147473995146,
            "section_title": "Related works",
            "char_start_offset": 13169,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "199546273",
            "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
            "text": "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model.",
            "score": 0.5531448229382867,
            "section_title": "Background of Membership Inference",
            "char_start_offset": 6352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "251765197",
            "title": "Auditing Membership Leakages of Multi-Exit Networks",
            "text": "Membership leakages in ML models emerge when an adversary aims to identify whether a data sample was used to train a certain model or not. It can raise severe privacy risks as the membership can reveal an individual's private information. For example, identifying an individual's participation in a hospital's health analytic training set reveals that this individual was once a patient in that hospital. \n\nTo evaluate the vulnerability of a given ML model to membership leakages, membership inference attacks are used as an auditing tool to quantify the private information that a model leaks about the individual data samples in its training set. In this work, we focus on auditing the membership leakages of multi-exit networks.",
            "score": 0.5529721316462821,
            "section_title": "Membership Leakages in Machine Learning Models",
            "char_start_offset": 10263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "paperId": "9016617a9176942aa532d762461c89d3e605ccf5",
            "corpusId": 272432251,
            "title": "Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack",
            "venue": "IEEE Symposium on Security and Privacy",
            "year": 2024,
            "referenceCount": 107,
            "citationCount": 23,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SP54263.2024.00120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SP54263.2024.00120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319897476",
                    "name": "Han Liu"
                },
                {
                    "authorId": "2198477977",
                    "name": "Yuhao Wu"
                },
                {
                    "authorId": "2118174028",
                    "name": "Zhiyuan Yu"
                },
                {
                    "authorId": "2290708246",
                    "name": "Ning Zhang"
                }
            ],
            "abstract": "Explainability is increasingly recognized as an enabling technology for the broader adoption of machine learning (ML), particularly for safety-critical applications. This has given rise to explainable ML, which seeks to enhance the explainability of neural networks through the use of explanators. Yet, the pursuit for better explainability inadvertently leads to increased security and privacy risks. While there has been considerable research into the security risks of explainable ML, its potential privacy risks remain under-explored.To bridge this gap, we present a systematic study of privacy risks in explainable ML through the lens of membership inference. Building on the observation that, besides the accuracy of the model, robustness also exhibits observable differences among member samples and non-member samples, we develop a new membership inference attack. This attack extracts additional membership features from changes in model confidence under different levels of perturbations guided by the importance highlighted by the attribution maps in the explanators. Intuitively, perturbing important features generally results in a bigger loss in confidence for member samples. Using the member-non-member differences in both model performance and robustness, an attack model is trained to distinguish the membership. We evaluated our approach with seven popular explanators across various benchmark models and datasets. Our attack demonstrates there is non-trivial privacy leakage in current explainable ML methods. Furthermore, such leakage issue persists even if the attacker lacks the knowledge of training datasets or target model architectures. Lastly, we also found existing model and output-based defense mechanisms are not effective in mitigating this new attack.",
            "corpus_id": "272432251",
            "text": "Explainability is increasingly recognized as an enabling technology for the broader adoption of machine learning (ML), particularly for safety-critical applications. This has given rise to explainable ML, which seeks to enhance the explainability of neural networks through the use of explanators. Yet, the pursuit for better explainability inadvertently leads to increased security and privacy risks. While there has been considerable research into the security risks of explainable ML, its potential privacy risks remain under-explored.To bridge this gap, we present a systematic study of privacy risks in explainable ML through the lens of membership inference. Building on the observation that, besides the accuracy of the model, robustness also exhibits observable differences among member samples and non-member samples, we develop a new membership inference attack. This attack extracts additional membership features from changes in model confidence under different levels of perturbations guided by the importance highlighted by the attribution maps in the explanators. Intuitively, perturbing important features generally results in a bigger loss in confidence for member samples. Using the member-non-member differences in both model performance and robustness, an attack model is trained to distinguish the membership. We evaluated our approach with seven popular explanators across various benchmark models and datasets. Our attack demonstrates there is non-trivial privacy leakage in current explainable ML methods. Furthermore, such leakage issue persists even if the attacker lacks the knowledge of training datasets or target model architectures. Lastly, we also found existing model and output-based defense mechanisms are not effective in mitigating this new attack.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99169921875
        },
        {
            "paperId": "b48a8c226efb52844c9b97309a654a383f33b823",
            "corpusId": 256274774,
            "title": "Interaction-level Membership Inference Attack Against Federated Recommender Systems",
            "venue": "The Web Conference",
            "year": 2023,
            "referenceCount": 51,
            "citationCount": 70,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2301.10964",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.10964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2106755543",
                    "name": "Wei Yuan"
                },
                {
                    "authorId": "2177256804",
                    "name": "Chao-Peng Yang"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "101457473",
                    "name": "Li-zhen Cui"
                },
                {
                    "authorId": "2281761",
                    "name": "Tieke He"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ],
            "abstract": "The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy concerns in personalized recommendation services. In FedRecs, users\u2019 attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it is considered a fairly secure approach to protect user privacy. As a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to user attribute inference attacks, highlighting the privacy concerns of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs. Specifically, we perform the first systematic study on interaction-level membership inference attacks on FedRecs. An interaction-level membership inference attacker is first designed, and then the classical privacy protection mechanism, Local Differential Privacy (LDP), is adopted to defend against the membership inference attack. Unfortunately, the empirical analysis shows that LDP is not effective against such new attacks unless the recommendation performance is largely compromised. To mitigate the interaction-level membership attack threats, we design a simple yet effective defense method to significantly reduce the attacker\u2019s inference accuracy without losing recommendation performance. Extensive experiments are conducted with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on three real-world recommendation datasets (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the experimental results show the effectiveness of our solutions.",
            "corpus_id": "256274774",
            "text": "The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy concerns in personalized recommendation services. In FedRecs, users\u2019 attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it is considered a fairly secure approach to protect user privacy. As a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to user attribute inference attacks, highlighting the privacy concerns of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs. Specifically, we perform the first systematic study on interaction-level membership inference attacks on FedRecs. An interaction-level membership inference attacker is first designed, and then the classical privacy protection mechanism, Local Differential Privacy (LDP), is adopted to defend against the membership inference attack. Unfortunately, the empirical analysis shows that LDP is not effective against such new attacks unless the recommendation performance is largely compromised. To mitigate the interaction-level membership attack threats, we design a simple yet effective defense method to significantly reduce the attacker\u2019s inference accuracy without losing recommendation performance. Extensive experiments are conducted with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on three real-world recommendation datasets (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the experimental results show the effectiveness of our solutions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9912109375
        },
        {
            "paperId": "f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
            "corpusId": 10488675,
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Symposium on Security and Privacy",
            "year": 2016,
            "referenceCount": 41,
            "citationCount": 4160,
            "influentialCitationCount": 619,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1610.05820",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.05820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "34828439",
                    "name": "M. Stronati"
                },
                {
                    "authorId": "3469125",
                    "name": "Congzheng Song"
                },
                {
                    "authorId": "1723945",
                    "name": "Vitaly Shmatikov"
                }
            ],
            "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
            "corpus_id": "10488675",
            "text": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.998046875
        },
        {
            "paperId": "ecb0cb2fceb713d8d760baf979fc9f3190f965db",
            "corpusId": 258967241,
            "title": "An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "referenceCount": 52,
            "citationCount": 34,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.18355",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151079740",
                    "name": "Fei Kong"
                },
                {
                    "authorId": "2004228925",
                    "name": "Jinhao Duan"
                },
                {
                    "authorId": "2154200750",
                    "name": "Ruipeng Ma"
                },
                {
                    "authorId": "2148452614",
                    "name": "Hengtao Shen"
                },
                {
                    "authorId": "46875503",
                    "name": "Xiaofeng Zhu"
                },
                {
                    "authorId": "2766473",
                    "name": "Xiaoshuang Shi"
                },
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                }
            ],
            "abstract": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. {Code is available at \\url{https://github.com/kong13661/PIA}}.",
            "corpus_id": "258967241",
            "text": "Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by $\\epsilon$ initialized in $t=0$ and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries on both discrete-time and continuous-time diffusion models. Moreover, previous works on the privacy of diffusion models have focused on vision tasks without considering audio tasks. Therefore, we also explore the robustness of diffusion models to MIA in the text-to-speech (TTS) task, which is an audio generation task. To the best of our knowledge, this work is the first to study the robustness of diffusion models to MIA in the TTS task. Experimental results indicate that models with mel-spectrogram (image-like) output are vulnerable to MIA, while models with audio output are relatively robust to MIA. {Code is available at \\url{https://github.com/kong13661/PIA}}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99169921875
        },
        {
            "paperId": "569d3890058cc6ca0032c42b19dac5bf44e189f7",
            "corpusId": 271270708,
            "title": "Unveiling Structural Memorization: Structural Membership Inference Attack for Text-to-Image Diffusion Models",
            "venue": "ACM Multimedia",
            "year": 2024,
            "referenceCount": 43,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291082890",
                    "name": "Qiao Li"
                },
                {
                    "authorId": "2186621957",
                    "name": "Xiaomeng Fu"
                },
                {
                    "authorId": "2108249591",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "2108457215",
                    "name": "Jin Liu"
                },
                {
                    "authorId": "2264638827",
                    "name": "Xingyu Gao"
                },
                {
                    "authorId": "2108984",
                    "name": "Jiao Dai"
                },
                {
                    "authorId": "2269685669",
                    "name": "Jizhong Han"
                }
            ],
            "abstract": "With the rapid advancements of large-scale text-to-image diffusion models, various practical applications have emerged, bringing significant convenience to society. However, model developers may misuse the unauthorized data to train diffusion models. These data are at risk of being memorized by the models, thus potentially violating citizens' privacy rights. Therefore, in order to judge whether a specific image is utilized as a member of a model's training set, Membership Inference Attack (MIA) is proposed to serve as a tool for privacy protection. Current MIA methods predominantly utilize pixel-wise comparisons as distinguishing clues, considering the pixel-level memorization characteristic of diffusion models. However, it is practically impossible for text-to-image models to memorize all the pixel-level information in massive training sets. Therefore, we move to the more advanced structure-level memorization. Observations on the diffusion process show that the structures of members are better preserved compared to those of nonmembers, indicating that diffusion models possess the capability to remember the structures of member images from training sets. Drawing on these insights, we propose a simple yet effective MIA method tailored for text-to-image diffusion models. Extensive experimental results validate the efficacy of our approach. Compared to current pixel-level baselines, our approach not only achieves state-of-the-art performance but also demonstrates remarkable robustness against various distortions.",
            "corpus_id": "271270708",
            "text": "With the rapid advancements of large-scale text-to-image diffusion models, various practical applications have emerged, bringing significant convenience to society. However, model developers may misuse the unauthorized data to train diffusion models. These data are at risk of being memorized by the models, thus potentially violating citizens' privacy rights. Therefore, in order to judge whether a specific image is utilized as a member of a model's training set, Membership Inference Attack (MIA) is proposed to serve as a tool for privacy protection. Current MIA methods predominantly utilize pixel-wise comparisons as distinguishing clues, considering the pixel-level memorization characteristic of diffusion models. However, it is practically impossible for text-to-image models to memorize all the pixel-level information in massive training sets. Therefore, we move to the more advanced structure-level memorization. Observations on the diffusion process show that the structures of members are better preserved compared to those of nonmembers, indicating that diffusion models possess the capability to remember the structures of member images from training sets. Drawing on these insights, we propose a simple yet effective MIA method tailored for text-to-image diffusion models. Extensive experimental results validate the efficacy of our approach. Compared to current pixel-level baselines, our approach not only achieves state-of-the-art performance but also demonstrates remarkable robustness against various distortions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.994140625
        },
        {
            "paperId": "a9619d3286cef8bf57d291da5fb0007db6dc2e44",
            "corpusId": 270521266,
            "title": "Semantic Membership Inference Attack against Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10218, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306780363",
                    "name": "Hamid Mozaffari"
                },
                {
                    "authorId": "1751411",
                    "name": "Virendra J. Marathe"
                }
            ],
            "abstract": "Membership Inference Attacks (MIAs) determine whether a specific data point was included in the training set of a target model. In this paper, we introduce the Semantic Membership Inference Attack (SMIA), a novel approach that enhances MIA performance by leveraging the semantic content of inputs and their perturbations. SMIA trains a neural network to analyze the target model's behavior on perturbed inputs, effectively capturing variations in output probability distributions between members and non-members. We conduct comprehensive evaluations on the Pythia and GPT-Neo model families using the Wikipedia dataset. Our results show that SMIA significantly outperforms existing MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B, compared to 58.90% by the second-best attack.",
            "corpus_id": "270521266",
            "text": "Membership Inference Attacks (MIAs) determine whether a specific data point was included in the training set of a target model. In this paper, we introduce the Semantic Membership Inference Attack (SMIA), a novel approach that enhances MIA performance by leveraging the semantic content of inputs and their perturbations. SMIA trains a neural network to analyze the target model's behavior on perturbed inputs, effectively capturing variations in output probability distributions between members and non-members. We conduct comprehensive evaluations on the Pythia and GPT-Neo model families using the Wikipedia dataset. Our results show that SMIA significantly outperforms existing MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B, compared to 58.90% by the second-best attack.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99658203125
        },
        {
            "paperId": "52c0754b7db33114b548e55b05be59bb1f53e102",
            "corpusId": 270199864,
            "title": "Towards Black-Box Membership Inference Attack for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 56,
            "citationCount": 5,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218965391",
                    "name": "Jingwei Li"
                },
                {
                    "authorId": "2243466919",
                    "name": "Jingyi Dong"
                },
                {
                    "authorId": "2304405568",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2261355597",
                    "name": "Jingzhao Zhang"
                }
            ],
            "abstract": "Given the rising popularity of AI-generated art and the associated copyright concerns, identifying whether an artwork was used to train a diffusion model is an important research topic. The work approaches this problem from the membership inference attack (MIA) perspective. We first identify the limitation of applying existing MIA methods for proprietary diffusion models: the required access of internal U-nets. To address the above problem, we introduce a novel membership inference attack method that uses only the image-to-image variation API and operates without access to the model's internal U-net. Our method is based on the intuition that the model can more easily obtain an unbiased noise prediction estimate for images from the training set. By applying the API multiple times to the target image, averaging the outputs, and comparing the result to the original image, our approach can classify whether a sample was part of the training set. We validate our method using DDIM and Stable Diffusion setups and further extend both our approach and existing algorithms to the Diffusion Transformer architecture. Our experimental results consistently outperform previous methods.",
            "corpus_id": "270199864",
            "text": "Given the rising popularity of AI-generated art and the associated copyright concerns, identifying whether an artwork was used to train a diffusion model is an important research topic. The work approaches this problem from the membership inference attack (MIA) perspective. We first identify the limitation of applying existing MIA methods for proprietary diffusion models: the required access of internal U-nets. To address the above problem, we introduce a novel membership inference attack method that uses only the image-to-image variation API and operates without access to the model's internal U-net. Our method is based on the intuition that the model can more easily obtain an unbiased noise prediction estimate for images from the training set. By applying the API multiple times to the target image, averaging the outputs, and comparing the result to the original image, our approach can classify whether a sample was part of the training set. We validate our method using DDIM and Stable Diffusion setups and further extend both our approach and existing algorithms to the Diffusion Transformer architecture. Our experimental results consistently outperform previous methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99365234375
        },
        {
            "paperId": "398c04880f61ec2491911b7c2b25e197c3956ae6",
            "corpusId": 271860154,
            "title": "Membership Inference Attack Against Masked Image Modeling",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2314118585",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "2258708596",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition. The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks. However, most existing works focus on improving the performance of MIM.In this work, we take a different angle by studying the pre-training data privacy of MIM. Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset. The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors. These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors. Extensive evaluations are conducted on three model architectures and three benchmark datasets. Empirical results show that our attack outperforms baseline methods. Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack.",
            "corpus_id": "271860154",
            "text": "Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition. The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks. However, most existing works focus on improving the performance of MIM.In this work, we take a different angle by studying the pre-training data privacy of MIM. Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset. The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors. These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors. Extensive evaluations are conducted on three model architectures and three benchmark datasets. Empirical results show that our attack outperforms baseline methods. Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.98876953125
        },
        {
            "paperId": "3256c83b21f8feb344667711a25b8b596553f8de",
            "corpusId": 267617030,
            "title": "FedMIA: An Effective Membership Inference Attack Exploiting\"All for One\"Principle in Federated Learning",
            "venue": "",
            "year": 2024,
            "referenceCount": 50,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.06289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354891",
                    "name": "Gongxi Zhu"
                },
                {
                    "authorId": "1423705511",
                    "name": "Donghao Li"
                },
                {
                    "authorId": "2064783785",
                    "name": "Hanlin Gu"
                },
                {
                    "authorId": "2277047294",
                    "name": "Yuxing Han"
                },
                {
                    "authorId": "2283880963",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2087083369",
                    "name": "Lixin Fan"
                },
                {
                    "authorId": "2166949653",
                    "name": "Qiang Yang"
                }
            ],
            "abstract": "Federated Learning (FL) is a promising approach for training machine learning models on decentralized data while preserving privacy. However, privacy risks, particularly Membership Inference Attacks (MIAs), which aim to determine whether a specific data point belongs to a target client's training set, remain a significant concern. Existing methods for implementing MIAs in FL primarily analyze updates from the target client, focusing on metrics such as loss, gradient norm, and gradient difference. However, these methods fail to leverage updates from non-target clients, potentially underutilizing available information. In this paper, we first formulate a one-tailed likelihood-ratio hypothesis test based on the likelihood of updates from non-target clients. Building upon this formulation, we introduce a three-step Membership Inference Attack (MIA) method, called FedMIA, which follows the\"all for one\"--leveraging updates from all clients across multiple communication rounds to enhance MIA effectiveness. Both theoretical analysis and extensive experimental results demonstrate that FedMIA outperforms existing MIAs in both classification and generative tasks. Additionally, it can be integrated as an extension to existing methods and is robust against various defense strategies, Non-IID data, and different federated structures. Our code is available in https://github.com/Liar-Mask/FedMIA.",
            "corpus_id": "267617030",
            "text": "Federated Learning (FL) is a promising approach for training machine learning models on decentralized data while preserving privacy. However, privacy risks, particularly Membership Inference Attacks (MIAs), which aim to determine whether a specific data point belongs to a target client's training set, remain a significant concern. Existing methods for implementing MIAs in FL primarily analyze updates from the target client, focusing on metrics such as loss, gradient norm, and gradient difference. However, these methods fail to leverage updates from non-target clients, potentially underutilizing available information. In this paper, we first formulate a one-tailed likelihood-ratio hypothesis test based on the likelihood of updates from non-target clients. Building upon this formulation, we introduce a three-step Membership Inference Attack (MIA) method, called FedMIA, which follows the\"all for one\"--leveraging updates from all clients across multiple communication rounds to enhance MIA effectiveness. Both theoretical analysis and extensive experimental results demonstrate that FedMIA outperforms existing MIAs in both classification and generative tasks. Additionally, it can be integrated as an extension to existing methods and is robust against various defense strategies, Non-IID data, and different federated structures. Our code is available in https://github.com/Liar-Mask/FedMIA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99365234375
        },
        {
            "paperId": "8cf8d81ce2a52dab7aa185941e631b3c706fd845",
            "corpusId": 271064786,
            "title": "Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 8,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311386033",
                    "name": "Qizhang Feng"
                },
                {
                    "authorId": "89604589",
                    "name": "Siva Rajesh Kasa"
                },
                {
                    "authorId": "2310438793",
                    "name": "Hyokun Yun"
                },
                {
                    "authorId": "37077406",
                    "name": "C. Teo"
                },
                {
                    "authorId": "40696276",
                    "name": "S. Bodapati"
                }
            ],
            "abstract": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.",
            "corpus_id": "271064786",
            "text": "Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9765625
        },
        {
            "paperId": "19b08f0742f3180503f12bbb246b366dc8b66196",
            "corpusId": 274869679,
            "title": "Leveraging Multiple Adversarial Perturbation Distances for Enhanced Membership Inference Attack in Federated Learning",
            "venue": "Symmetry",
            "year": 2024,
            "referenceCount": 28,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/sym16121677",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/sym16121677?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/sym16121677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336171446",
                    "name": "Fan Xia"
                },
                {
                    "authorId": "2336189090",
                    "name": "Yuhao Liu"
                },
                {
                    "authorId": "2336172334",
                    "name": "Bo Jin"
                },
                {
                    "authorId": "2297431838",
                    "name": "Zheng Yu"
                },
                {
                    "authorId": "2336264478",
                    "name": "Xingwei Cai"
                },
                {
                    "authorId": "2336144088",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2318042338",
                    "name": "Zhiyong Zha"
                },
                {
                    "authorId": "2336109193",
                    "name": "Dai Hou"
                },
                {
                    "authorId": "2323202813",
                    "name": "Kai Peng"
                }
            ],
            "abstract": "In recent years, federated learning (FL) has gained significant attention for its ability to protect data privacy during distributed training. However, it also introduces new privacy leakage risks. Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning. Existing research on membership inference attacks in federated learning has primarily focused on leveraging intrinsic model parameters or manipulating the training process. However, the widespread adoption of privacy-preserving frameworks in federated learning has significantly diminished the effectiveness of traditional attack methods. To overcome this limitation, this paper aims to explore an efficient Membership Inference Attack algorithm tailored for encrypted federated learning scenarios, providing new perspectives for optimizing privacy-preserving technologies. Specifically, this paper proposes a novel Membership Inference Attack algorithm based on multiple adversarial perturbation distances (MAPD_MIA) by leveraging the asymmetry in adversarial perturbation distributions near decision boundaries between member and non-member samples. By analyzing these asymmetric perturbation characteristics, the algorithm achieves accurate membership identification. Experimental results demonstrate that the proposed algorithm achieves accuracy rates of 63.0%, 68.7%, and 59.5%, and precision rates of 59.0%, 65.9%, and 55.8% on CIFAR10, CIFAR100, and MNIST datasets, respectively, outperforming three mainstream Membership Inference Attack methods. Furthermore, the algorithm exhibits robust attack performance against two common defense mechanisms, MemGuard and DP-SGD. This study provides new benchmarks and methodologies for evaluating membership privacy leakage risks in federated learning scenarios.",
            "corpus_id": "274869679",
            "text": "In recent years, federated learning (FL) has gained significant attention for its ability to protect data privacy during distributed training. However, it also introduces new privacy leakage risks. Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning. Existing research on membership inference attacks in federated learning has primarily focused on leveraging intrinsic model parameters or manipulating the training process. However, the widespread adoption of privacy-preserving frameworks in federated learning has significantly diminished the effectiveness of traditional attack methods. To overcome this limitation, this paper aims to explore an efficient Membership Inference Attack algorithm tailored for encrypted federated learning scenarios, providing new perspectives for optimizing privacy-preserving technologies. Specifically, this paper proposes a novel Membership Inference Attack algorithm based on multiple adversarial perturbation distances (MAPD_MIA) by leveraging the asymmetry in adversarial perturbation distributions near decision boundaries between member and non-member samples. By analyzing these asymmetric perturbation characteristics, the algorithm achieves accurate membership identification. Experimental results demonstrate that the proposed algorithm achieves accuracy rates of 63.0%, 68.7%, and 59.5%, and precision rates of 59.0%, 65.9%, and 55.8% on CIFAR10, CIFAR100, and MNIST datasets, respectively, outperforming three mainstream Membership Inference Attack methods. Furthermore, the algorithm exhibits robust attack performance against two common defense mechanisms, MemGuard and DP-SGD. This study provides new benchmarks and methodologies for evaluating membership privacy leakage risks in federated learning scenarios.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.998046875
        },
        {
            "paperId": "f3045240c224e4f797b5ca1b67d3fd3d4c07bba6",
            "corpusId": 270243234,
            "title": "Deeper Leakage from Gradients through Membership Inference Attack",
            "venue": "International Congress on Information and Communication Technology",
            "year": 2024,
            "referenceCount": 14,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICICT62343.2024.00054?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICICT62343.2024.00054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304634826",
                    "name": "Biying Fang"
                },
                {
                    "authorId": "2304762369",
                    "name": "Tao Zhang"
                }
            ],
            "abstract": "Several prior studies underscore the vulnerability of individual participants\u2019 local data in collaborative learning settings, such as federated learning, to exposure through a small subset of gradients used in updating the local model. This deep leakage from gradients (DLG) empowers attackers to reconstruct images with high pixel-wise accuracy and texts with a high probability of token-wise matching. Additionally, research has revealed that a black-box machine learning model is susceptible to a membership inference attack (MIA). In this scenario, an attacker can develop a separate machine learning model, termed an MIA model, capable of accurately determining whether a given input belongs to the training dataset of the black-box machine learning model, achieving notably high prediction accuracy. Building on these understandings, our work proposes an enhanced DLG attack to further explore the depth of gradient leakage through the MIA attack, termed as DLG-MIA attack. Specifically, we enhance the performance of the attack compared to the standard DLG attack by leveraging the insight that a higher probability of the reconstructed sample belonging to the training dataset corresponds to better performance for the DLG attack. By incorporating additional constraints from the MIA attack and employing various techniques to extract more assistive information, the DLG-MIA attack achieves superior performance. We present a mathematical illustration demonstrating how our method enhances the DLG attack through the incorporation of the MIA attack and how it extracts additional assistive information. Additionally, we empirically showcase the advantages of our approach over the standard DLG attack.",
            "corpus_id": "270243234",
            "text": "Several prior studies underscore the vulnerability of individual participants\u2019 local data in collaborative learning settings, such as federated learning, to exposure through a small subset of gradients used in updating the local model. This deep leakage from gradients (DLG) empowers attackers to reconstruct images with high pixel-wise accuracy and texts with a high probability of token-wise matching. Additionally, research has revealed that a black-box machine learning model is susceptible to a membership inference attack (MIA). In this scenario, an attacker can develop a separate machine learning model, termed an MIA model, capable of accurately determining whether a given input belongs to the training dataset of the black-box machine learning model, achieving notably high prediction accuracy. Building on these understandings, our work proposes an enhanced DLG attack to further explore the depth of gradient leakage through the MIA attack, termed as DLG-MIA attack. Specifically, we enhance the performance of the attack compared to the standard DLG attack by leveraging the insight that a higher probability of the reconstructed sample belonging to the training dataset corresponds to better performance for the DLG attack. By incorporating additional constraints from the MIA attack and employing various techniques to extract more assistive information, the DLG-MIA attack achieves superior performance. We present a mathematical illustration demonstrating how our method enhances the DLG attack through the incorporation of the MIA attack and how it extracts additional assistive information. Additionally, we empirically showcase the advantages of our approach over the standard DLG attack.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99462890625
        },
        {
            "paperId": "e57aee28b70104dcbb8d969b37f6a56439a0d90d",
            "corpusId": 278604633,
            "title": "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning",
            "venue": "International Conference on Computing, Networking and Communications",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152052355",
                    "name": "Sayyed Farid Ahamed"
                },
                {
                    "authorId": "2273648426",
                    "name": "Soumya Banerjee"
                },
                {
                    "authorId": "2269453052",
                    "name": "Sandip Roy"
                },
                {
                    "authorId": "2219035314",
                    "name": "Devin Quinn"
                },
                {
                    "authorId": "2187681894",
                    "name": "Marc Vucovich"
                },
                {
                    "authorId": "2107150694",
                    "name": "Kevin Choi"
                },
                {
                    "authorId": "2269267114",
                    "name": "Abdul Rahman"
                },
                {
                    "authorId": "2313652406",
                    "name": "Alison Hu"
                },
                {
                    "authorId": "2151795097",
                    "name": "E. Bowen"
                },
                {
                    "authorId": "2269144650",
                    "name": "Sachin Shetty"
                }
            ],
            "abstract": "Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.",
            "corpus_id": "278604633",
            "text": "Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97998046875
        },
        {
            "paperId": "d6fc86d4998bc4b18a902a52dbfd280838cb11a4",
            "corpusId": 270764538,
            "title": "A Zero Auxiliary Knowledge Membership Inference Attack on Aggregate Location Data",
            "venue": "Proceedings on Privacy Enhancing Technologies",
            "year": 2024,
            "referenceCount": 74,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308467228",
                    "name": "Vincent Guan"
                },
                {
                    "authorId": "2193077081",
                    "name": "Florent Gu'epin"
                },
                {
                    "authorId": "2119728000",
                    "name": "Ana-Maria Cre\u0163u"
                },
                {
                    "authorId": "7400876",
                    "name": "Yves-Alexandre de Montjoye"
                }
            ],
            "abstract": "Location data is frequently collected from populations and shared in aggregate form to guide policy and decision making. However, the prevalence of aggregated data also raises the privacy concern of membership inference attacks (MIAs). MIAs infer whether an individual's data contributed to the aggregate release. Although effective MIAs have been developed for aggregate location data, these require access to an extensive auxiliary dataset of individual traces over the same locations, which are collected from a similar population. This assumption is often impractical given common privacy practices surrounding location data. To measure the risk of an MIA performed by a realistic adversary, we develop the first Zero Auxiliary Knowledge (ZK) MIA on aggregate location data, which eliminates the need for an auxiliary dataset of real individual traces. Instead, we develop a novel synthetic approach, such that suitable synthetic traces are generated from the released aggregate. We also develop methods to correct for bias and noise, to show that our synthetic-based attack is still applicable when privacy mechanisms are applied prior to release. Using two large-scale location datasets, we demonstrate that our ZK MIA matches the state-of-the-art Knock-Knock (KK) MIA across a wide range of settings, including popular implementations of differential privacy (DP) and suppression of small counts. Furthermore, we show that ZK MIA remains highly effective even when the adversary only knows a small fraction (10%) of their target's location history. This demonstrates that effective MIAs can be performed by realistic adversaries, highlighting the need for strong DP protection.",
            "corpus_id": "270764538",
            "text": "Location data is frequently collected from populations and shared in aggregate form to guide policy and decision making. However, the prevalence of aggregated data also raises the privacy concern of membership inference attacks (MIAs). MIAs infer whether an individual's data contributed to the aggregate release. Although effective MIAs have been developed for aggregate location data, these require access to an extensive auxiliary dataset of individual traces over the same locations, which are collected from a similar population. This assumption is often impractical given common privacy practices surrounding location data. To measure the risk of an MIA performed by a realistic adversary, we develop the first Zero Auxiliary Knowledge (ZK) MIA on aggregate location data, which eliminates the need for an auxiliary dataset of real individual traces. Instead, we develop a novel synthetic approach, such that suitable synthetic traces are generated from the released aggregate. We also develop methods to correct for bias and noise, to show that our synthetic-based attack is still applicable when privacy mechanisms are applied prior to release. Using two large-scale location datasets, we demonstrate that our ZK MIA matches the state-of-the-art Knock-Knock (KK) MIA across a wide range of settings, including popular implementations of differential privacy (DP) and suppression of small counts. Furthermore, we show that ZK MIA remains highly effective even when the adversary only knows a small fraction (10%) of their target's location history. This demonstrates that effective MIAs can be performed by realistic adversaries, highlighting the need for strong DP protection.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97705078125
        },
        {
            "paperId": "8d50dfa4c9a5a55487a3f0ebf5596ec80a30d793",
            "corpusId": 273498133,
            "title": "Interaction-level Membership Inference Attack against Recommender Systems with Long-tailed Distribution",
            "venue": "International Conference on Information and Knowledge Management",
            "year": 2024,
            "referenceCount": 44,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3627673.3679804?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3627673.3679804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166066869",
                    "name": "Da Zhong"
                },
                {
                    "authorId": "2118419150",
                    "name": "Xiuling Wang"
                },
                {
                    "authorId": "2327289323",
                    "name": "Zhichao Xu"
                },
                {
                    "authorId": "2239091778",
                    "name": "Jun Xu"
                },
                {
                    "authorId": "2260452779",
                    "name": "Wendy Hui Wang"
                }
            ],
            "abstract": "Recommender systems (RSs) are susceptible to Interaction-level Membership Inference Attacks (IMIAs), which aim to determine whether specific user-item interactions are present in the training data of the target RS. However, existing IMIAs struggle with inferring the membership of tail interactions, i.e., the interactions involving tail items, due to the limited information available about these items. This paper introduces MINER, a new IMIA designed to enhance attack performance against RSs with long-tailed item distribution. MINER addresses the information scarcity of tail items at both the feature and sample levels. At the feature level, MINER leverages the Knowledge Graphs (KGs) to obtain the auxiliary knowledge of tail items. At the sample level, MINER designs a Bilateral-Branch Network (BBN) as the attack model. The BBN trains two branches independently, with one branch trained on interaction samples with the original long-tailed item distribution and the other on interaction samples with a more balanced item distribution. The outputs of the two branches are aggregated using a cumulative learning component. Our experimental results demonstrate that MINER significantly enhances the attack accuracy of IMIA, especially for tail interactions. Beyond attack design, we design a defense mechanism named RGL to defend against MINER. Empirical evaluations demonstrate that RGL effectively mitigates the privacy risks posed by MINER while preserving recommendation accuracy. Our code is available at https://github.com/dzhong2/MINER.",
            "corpus_id": "273498133",
            "text": "Recommender systems (RSs) are susceptible to Interaction-level Membership Inference Attacks (IMIAs), which aim to determine whether specific user-item interactions are present in the training data of the target RS. However, existing IMIAs struggle with inferring the membership of tail interactions, i.e., the interactions involving tail items, due to the limited information available about these items. This paper introduces MINER, a new IMIA designed to enhance attack performance against RSs with long-tailed item distribution. MINER addresses the information scarcity of tail items at both the feature and sample levels. At the feature level, MINER leverages the Knowledge Graphs (KGs) to obtain the auxiliary knowledge of tail items. At the sample level, MINER designs a Bilateral-Branch Network (BBN) as the attack model. The BBN trains two branches independently, with one branch trained on interaction samples with the original long-tailed item distribution and the other on interaction samples with a more balanced item distribution. The outputs of the two branches are aggregated using a cumulative learning component. Our experimental results demonstrate that MINER significantly enhances the attack accuracy of IMIA, especially for tail interactions. Beyond attack design, we design a defense mechanism named RGL to defend against MINER. Empirical evaluations demonstrate that RGL effectively mitigates the privacy risks posed by MINER while preserving recommendation accuracy. Our code is available at https://github.com/dzhong2/MINER.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.98828125
        },
        {
            "paperId": "a2a455602f71ff60cec621901855d910ab667a18",
            "corpusId": 272348398,
            "title": "Defending Against Membership Inference Attack for Counterfactual Federated Recommendation With Differentially Private Representation Learning",
            "venue": "IEEE Transactions on Information Forensics and Security",
            "year": 2024,
            "referenceCount": 55,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIFS.2024.3453031?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIFS.2024.3453031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109304508",
                    "name": "Xiuwen Liu"
                },
                {
                    "authorId": "2109196522",
                    "name": "Yanjiao Chen"
                },
                {
                    "authorId": "1750138",
                    "name": "Shanchen Pang"
                }
            ],
            "abstract": "When it comes to the marriage of federated learning and personalized recommendation services (FedRec), characterizing user-item interaction behaviors is a long-standing and unresolved issue, highlighting the growing data privacy concerns due to the inherent openness of recommender systems. As the new interaction-level membership inference attacks on FedRecs have recently surfaced, quite possibly such adversarial attacks act as the hidden confounders lying behind the interactive recommendation, resulting in the obstruction of the causal effect disentanglement on long-term user satisfaction. As such, tailored to the specifics of private learning, we propose a counterfactual interactive recommendation system that builds a differentially private representation learning based defender (CIRDP) to capture and mitigate the adversarial threats, augmenting causal inference-based interactive recommendation of FedRecs. When characterizing interaction-level membership inference attacks of the hidden eavesdropping adversary as the primary cause of adversarial effect on user satisfaction, CIRDP incorporates causal inference-augmented offline reinforcement learning (offline RL) into FedRecs. CIRDP innovatively provides counterfactual satisfaction by optimizing a sensitivity-guided disentangled representation module with an innovative two-fold mutual information objective. As such, CIRDP introduces a differentially private representation learning based defender, guaranteeing interaction behavior-level differential privacy (DP) with a significant reduction in privacy costs. Extensive comparisons demonstrate CIRDP\u2019s superiority over the state-of-the-art baselines in reducing inference attack threats and improving long-term success in the interactive recommendation.",
            "corpus_id": "272348398",
            "text": "When it comes to the marriage of federated learning and personalized recommendation services (FedRec), characterizing user-item interaction behaviors is a long-standing and unresolved issue, highlighting the growing data privacy concerns due to the inherent openness of recommender systems. As the new interaction-level membership inference attacks on FedRecs have recently surfaced, quite possibly such adversarial attacks act as the hidden confounders lying behind the interactive recommendation, resulting in the obstruction of the causal effect disentanglement on long-term user satisfaction. As such, tailored to the specifics of private learning, we propose a counterfactual interactive recommendation system that builds a differentially private representation learning based defender (CIRDP) to capture and mitigate the adversarial threats, augmenting causal inference-based interactive recommendation of FedRecs. When characterizing interaction-level membership inference attacks of the hidden eavesdropping adversary as the primary cause of adversarial effect on user satisfaction, CIRDP incorporates causal inference-augmented offline reinforcement learning (offline RL) into FedRecs. CIRDP innovatively provides counterfactual satisfaction by optimizing a sensitivity-guided disentangled representation module with an innovative two-fold mutual information objective. As such, CIRDP introduces a differentially private representation learning based defender, guaranteeing interaction behavior-level differential privacy (DP) with a significant reduction in privacy costs. Extensive comparisons demonstrate CIRDP\u2019s superiority over the state-of-the-art baselines in reducing inference attack threats and improving long-term success in the interactive recommendation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96923828125
        },
        {
            "paperId": "8b881138225af0a0a50cdf6120fcac7f40c9166a",
            "corpusId": 274130924,
            "title": "Membership Inference Attack against Long-Context Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.11424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259065760",
                    "name": "Zixiong Wang"
                },
                {
                    "authorId": "2019143562",
                    "name": "Gaoyang Liu"
                },
                {
                    "authorId": "2320837306",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2323160757",
                    "name": "Chen Wang"
                }
            ],
            "abstract": "Recent advances in Large Language Models (LLMs) have enabled them to overcome their context window limitations, and demonstrate exceptional retrieval and reasoning capacities on longer context. Quesion-answering systems augmented with Long-Context Language Models (LCLMs) can automatically search massive external data and incorporate it into their contexts, enabling faithful predictions and reducing issues such as hallucinations and knowledge staleness. Existing studies targeting LCLMs mainly concentrate on addressing the so-called lost-in-the-middle problem or improving the inference effiencicy, leaving their privacy risks largely unexplored. In this paper, we aim to bridge this gap and argue that integrating all information into the long context makes it a repository of sensitive information, which often contains private data such as medical records or personal identities. We further investigate the membership privacy within LCLMs external context, with the aim of determining whether a given document or sequence is included in the LCLMs context. Our basic idea is that if a document lies in the context, it will exhibit a low generation loss or a high degree of semantic similarity to the contents generated by LCLMs. We for the first time propose six membership inference attack (MIA) strategies tailored for LCLMs and conduct extensive experiments on various popular models. Empirical results demonstrate that our attacks can accurately infer membership status in most cases, e.g., 90.66% attack F1-score on Multi-document QA datasets with LongChat-7b-v1.5-32k, highlighting significant risks of membership leakage within LCLMs input contexts. Furthermore, we examine the underlying reasons why LCLMs are susceptible to revealing such membership information.",
            "corpus_id": "274130924",
            "text": "Recent advances in Large Language Models (LLMs) have enabled them to overcome their context window limitations, and demonstrate exceptional retrieval and reasoning capacities on longer context. Quesion-answering systems augmented with Long-Context Language Models (LCLMs) can automatically search massive external data and incorporate it into their contexts, enabling faithful predictions and reducing issues such as hallucinations and knowledge staleness. Existing studies targeting LCLMs mainly concentrate on addressing the so-called lost-in-the-middle problem or improving the inference effiencicy, leaving their privacy risks largely unexplored. In this paper, we aim to bridge this gap and argue that integrating all information into the long context makes it a repository of sensitive information, which often contains private data such as medical records or personal identities. We further investigate the membership privacy within LCLMs external context, with the aim of determining whether a given document or sequence is included in the LCLMs context. Our basic idea is that if a document lies in the context, it will exhibit a low generation loss or a high degree of semantic similarity to the contents generated by LCLMs. We for the first time propose six membership inference attack (MIA) strategies tailored for LCLMs and conduct extensive experiments on various popular models. Empirical results demonstrate that our attacks can accurately infer membership status in most cases, e.g., 90.66% attack F1-score on Multi-document QA datasets with LongChat-7b-v1.5-32k, highlighting significant risks of membership leakage within LCLMs input contexts. Furthermore, we examine the underlying reasons why LCLMs are susceptible to revealing such membership information.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9951171875
        }
    ],
    "quotes": {
        "cost": 0.23699699999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[10488675 | Shokri et al. | 2016 | Citations: 4160]",
                "snippets": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[165163934 | Song et al. | 2019 | Citations: 244]",
                "snippets": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1369,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "52211986",
                            "133091488",
                            "46933970",
                            "10488675",
                            "2656445",
                            "10488675",
                            "2656445"
                        ],
                        "quote": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[195346528 | Yeom et al. | 2017 | Citations: 40]",
                "snippets": "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 715,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 147
                            },
                            {
                                "start": 148,
                                "end": 326
                            },
                            {
                                "start": 327,
                                "end": 613
                            },
                            {
                                "start": 614,
                                "end": 715
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[195699554 | Leino et al. | 2019 | Citations: 272]",
                "snippets": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 163,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[210942951 | Farokhi et al. | 2020 | Citations: 24]",
                "snippets": "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention (Shokri et al., 2016)(Truex et al., 2018)(Salem et al., 2018)(Sablayrolles et al., 2019). These attacks have been deployed on various machine learning models; see, e.g., (Shokri et al., 2016)(Hayes et al., 2017)(Chen et al., 2019)(Hilprecht et al., 2019)(Liu et al., 2019)(Backes et al., 2016). The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[50778569 | Truex et al. | 2018 | Citations: 112]": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[195477472 | Liu et al. | 2019 | Citations: 82]": "Social media networks have shown rapid growth in the past, and massive social data are generated which can reveal behavior or emotion propensities of users. Numerous social researchers leverage machine learning technology to build social media analytic models which can detect the abnormal behaviors or mental illnesses from the social media data effectively. Although the researchers only public the prediction interfaces of the machine learning models, in general, these interfaces may leak information about the individual data records on which the models were trained. Knowing a certain user\u2019s social media record was used to train a model can breach user privacy. In this paper, we present SocInf and focus on the fundamental problem known as membership inference. The key idea of SocInf is to construct a mimic model which has a similar prediction behavior with the public model, and then we can disclose the prediction differences between the training and testing data set by abusing the mimic model. With elaborated analytics on the predictions of the mimic model, SocInf can thus infer whether a given record is in the victim model\u2019s training set or not. We empirically evaluate the attack performance of SocInf on machine learning models trained by Xgboost, logistics, and online cloud platform. Using the realistic data, the experiment results show that SocInf can achieve an inference accuracy and precision of 73% and 84%, respectively, in average, and of 83% and 91% at best.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[202539472 | Chen et al. | 2019 | Citations: 52]": "In recent years, the success of deep learning has carried over from discriminative models to generative models. In particular, generative adversarial networks (GANs) have facilitated a new level of performance ranging from media manipulation to dataset re-generation. Despite the success, the potential risks of privacy breach stemming from GANs are less well explored. In this paper, we focus on membership inference attack against GANs that has the potential to reveal information about victim models' training data. Specifically, we present the first taxonomy of membership inference attacks, which encompasses not only existing attacks but also our novel ones. We also propose the first generic attack model that can be instantiated in various settings according to adversary's knowledge about the victim model. We complement our systematic analysis of attack vectors with a comprehensive experimental study, that investigates the effectiveness of these attacks w.r.t. model type, training configurations, and attack type across three diverse application scenarios ranging from images, over medical data to location data. We show consistent effectiveness in all the setups, which bridges the assumption gap and performance gap in previous study with a complete spectrum of performance across settings. We conclusively remind users to think over before publicizing any part of their models.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 544,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 193
                            },
                            {
                                "start": 194,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 544
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "50778569",
                            "46933970",
                            "174799799",
                            "10488675",
                            "52211986",
                            "202539472",
                            "199546273",
                            "195477472",
                            "1154437"
                        ],
                        "quote": "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention (Shokri et al., 2016)(Truex et al., 2018)(Salem et al., 2018)(Sablayrolles et al., 2019). These attacks have been deployed on various machine learning models; see, e.g., (Shokri et al., 2016)(Hayes et al., 2017)(Chen et al., 2019)(Hilprecht et al., 2019)(Liu et al., 2019)(Backes et al., 2016). The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[214623088 | Song et al. | 2020 | Citations: 375]",
                "snippets": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[218473513 | Xue et al. | 2020 | Citations: 124]",
                "snippets": "Liu et al. (Liu et al., 2015) illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. (Shokri et al., 2016) propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model (Shokri et al., 2016). According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in (Shokri et al., 2016) is generic, but the success of member inference attacks depends on the overfitting of the model (Yeom et al., 2017), (Shokri et al., 2016). If it is a well-generalized model, the success rate of the membership inference attack is low.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                },
                "metadata": [
                    {
                        "section_title": "2) MEMBERSHIP INFERENCE ATTACK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1015,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 65
                            },
                            {
                                "start": 66,
                                "end": 237
                            },
                            {
                                "start": 238,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 536
                            },
                            {
                                "start": 537,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 920
                            },
                            {
                                "start": 921,
                                "end": 1015
                            }
                        ],
                        "ref_mentions": [
                            "14847504",
                            "10488675",
                            "10488675",
                            "10488675",
                            "2656445",
                            "10488675"
                        ],
                        "quote": "Liu et al. (Liu et al., 2015) illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. (Shokri et al., 2016) propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model (Shokri et al., 2016). According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in (Shokri et al., 2016) is generic, but the success of member inference attacks depends on the overfitting of the model (Yeom et al., 2017), (Shokri et al., 2016). If it is a well-generalized model, the success rate of the membership inference attack is low."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[218674569 | Huang et al. | 2020 | Citations: 25]",
                "snippets": "Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19]20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 738,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 173
                            },
                            {
                                "start": 174,
                                "end": 365
                            },
                            {
                                "start": 366,
                                "end": 576
                            },
                            {
                                "start": 577,
                                "end": 738
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19]20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]",
                "snippets": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[221586480 | Zou et al. | 2020 | Citations: 32]",
                "snippets": "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference against ML Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 867,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 143
                            },
                            {
                                "start": 144,
                                "end": 237
                            },
                            {
                                "start": 238,
                                "end": 510
                            },
                            {
                                "start": 511,
                                "end": 590
                            },
                            {
                                "start": 593,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 812
                            },
                            {
                                "start": 813,
                                "end": 867
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[221725546 | Tanuwidjaja et al. | 2020 | Citations: 82]",
                "snippets": "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. (Shokri et al., 2016) provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "B. MEMBERSHIP INFERENCE ATTACK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 975,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 129
                            },
                            {
                                "start": 130,
                                "end": 269
                            },
                            {
                                "start": 270,
                                "end": 409
                            },
                            {
                                "start": 410,
                                "end": 499
                            },
                            {
                                "start": 502,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 975
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. (Shokri et al., 2016) provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[227152059 | Liu et al. | 2020 | Citations: 290]",
                "snippets": "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not (Melis et al., 2018)(Shokri et al., 2016). An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. (Shokri et al., 2016) introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. (Yeom et al., 2017) investigated \"the relationship between overfitting and privacy leakage\". Salem et al. (Salem et al., 2018) proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\".",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[53099247 | Melis et al. | 2018 | Citations: 1482]": "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack.",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1005,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 169
                            },
                            {
                                "start": 170,
                                "end": 230
                            },
                            {
                                "start": 233,
                                "end": 381
                            },
                            {
                                "start": 382,
                                "end": 515
                            },
                            {
                                "start": 516,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 786
                            },
                            {
                                "start": 787,
                                "end": 1005
                            }
                        ],
                        "ref_mentions": [
                            "53099247",
                            "10488675",
                            "10488675",
                            "2656445",
                            "46933970"
                        ],
                        "quote": "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not (Melis et al., 2018)(Shokri et al., 2016). An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. (Shokri et al., 2016) introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. (Yeom et al., 2017) investigated \"the relationship between overfitting and privacy leakage\". Salem et al. (Salem et al., 2018) proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\"."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[231802143 | Liu et al. | 2021 | Citations: 132]",
                "snippets": "Membership Inference (MemInf) (Shokri et al., 2016) against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature (Chen et al., 2019)(Chen et al., 2020)(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Nasr et al., 2018)(Sablayrolles et al., 2019)(Salem et al., 2018)(Shokri et al., 2016). Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[202734167 | Jia et al. | 2019 | Citations: 394]": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
                    "[218502126 | Chen et al. | 2020 | Citations: 232]": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. \\footnoteOur code is available at \\urlhttps://github.com/MinChen00/UnlearningLeaks.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 877,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 167
                            },
                            {
                                "start": 168,
                                "end": 337
                            },
                            {
                                "start": 340,
                                "end": 396
                            },
                            {
                                "start": 397,
                                "end": 488
                            },
                            {
                                "start": 489,
                                "end": 758
                            },
                            {
                                "start": 759,
                                "end": 877
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "221203089",
                            "218502126",
                            "202734167",
                            "195699554",
                            "237563320",
                            "49863840",
                            "174799799",
                            "46933970",
                            "10488675"
                        ],
                        "quote": "Membership Inference (MemInf) (Shokri et al., 2016) against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature (Chen et al., 2019)(Chen et al., 2020)(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Nasr et al., 2018)(Sablayrolles et al., 2019)(Salem et al., 2018)(Shokri et al., 2016). Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[231846491 | He et al. | 2021 | Citations: 52]",
                "snippets": "Membership inference attack is one of the most popular privacy attacks against ML models (Chen et al., 2019)8,10,19,(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020). The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[202734167 | Jia et al. | 2019 | Citations: 394]": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data)."
                },
                "metadata": [
                    {
                        "section_title": "Attack Definition and Threat Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 341,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 246
                            },
                            {
                                "start": 247,
                                "end": 341
                            }
                        ],
                        "ref_mentions": [
                            "221203089",
                            "202734167",
                            "195699554",
                            "237563320",
                            "46933970",
                            "10488675",
                            "214623088"
                        ],
                        "quote": "Membership inference attack is one of the most popular privacy attacks against ML models (Chen et al., 2019)8,10,19,(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020). The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[231861713 | He et al. | 2021 | Citations: 98]",
                "snippets": "Membership inference attacks aim at inferring membership of individual training samples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39]51].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[170076423 | Carlini et al. | 2018 | Citations: 1148]": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 25,
                        "end": 229,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "170076423",
                            "49863840",
                            "133091488",
                            "46933970",
                            "10488675"
                        ],
                        "quote": "Membership inference attacks aim at inferring membership of individual training samples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39]51]."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[237563320 | Li et al. | 2020 | Citations: 246]",
                "snippets": "Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[244809678 | Shin et al. | 2021 | Citations: 4]",
                "snippets": "Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack (Shokri et al., 2016), the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1715,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 230
                            },
                            {
                                "start": 231,
                                "end": 404
                            },
                            {
                                "start": 405,
                                "end": 625
                            },
                            {
                                "start": 626,
                                "end": 753
                            },
                            {
                                "start": 754,
                                "end": 898
                            },
                            {
                                "start": 899,
                                "end": 1107
                            },
                            {
                                "start": 1108,
                                "end": 1339
                            },
                            {
                                "start": 1340,
                                "end": 1456
                            },
                            {
                                "start": 1457,
                                "end": 1574
                            },
                            {
                                "start": 1575,
                                "end": 1715
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack (Shokri et al., 2016), the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[244920593 | Carlini et al. | 2021 | Citations: 708]",
                "snippets": "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "III. MEMBERSHIP INFERENCE ATTACKS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 261,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[246706163 | Zhou et al. | 2022 | Citations: 23]",
                "snippets": "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[244077745 | Hu et al. | 2021 | Citations: 28]": "Generative adversarial networks (GANs) have made unprecedented performance in image synthesis and play a key role in various downstream applications of computer vision. However, GAN models trained on sensitive data also pose a distinct threat to privacy. In this poster, we present a novel over-representation based membership inference attack. Unlike prior attacks against GANs which focus on the overall metrics, such as the attack accuracy, our attack aims to make inference from the high-precision perspective, which allows the adversary to concentrate on inferring a sample as a member confidently. Initial experimental results demonstrate that the adversary can achieve a high precision attack even if the overall attack accuracy is about 50% for a well-trained GAN model. Our work will raise awareness of the importance of precision when GAN owners evaluate the privacy risks of their models.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "B. Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1332,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 268
                            },
                            {
                                "start": 269,
                                "end": 529
                            },
                            {
                                "start": 530,
                                "end": 611
                            },
                            {
                                "start": 612,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 992
                            },
                            {
                                "start": 993,
                                "end": 1212
                            },
                            {
                                "start": 1213,
                                "end": 1332
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "133091488",
                            "221203089",
                            "195699554",
                            "3570809",
                            "244077745",
                            "52211986",
                            "199546273"
                        ],
                        "quote": "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[250048543 | Wang et al. | 2022 | Citations: 14]",
                "snippets": "The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. (Zhang et al., 2021) investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[237532289 | Zhang et al. | 2021 | Citations: 88]": "Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems. In this paper, we make the first attempt on quantifying the privacy leakage of recommender systems through the lens of membership inference. In contrast with traditional membership inference against machine learning classifiers, our attack faces two main differences. First, our attack is on the user-level but not on the data sample-level. Second, the adversary can only observe the ordered recommended items from a recommender system instead of prediction results in the form of posterior probabilities. To address the above challenges, we propose a novel method by representing users from relevant items. Moreover, a shadow recommender is established to derive the labeled training data for training the attack model. Extensive experimental results show that our attack framework achieves a strong performance. In addition, we design a defense mechanism to effectively mitigate the membership inference threat of recommender systems.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                },
                "metadata": [
                    {
                        "section_title": "Membership inference attacks",
                        "pdf_hash": "",
                        "start": 162,
                        "end": 1415,
                        "sentence_offsets": [
                            {
                                "start": 162,
                                "end": 280
                            },
                            {
                                "start": 281,
                                "end": 378
                            },
                            {
                                "start": 379,
                                "end": 558
                            },
                            {
                                "start": 559,
                                "end": 653
                            },
                            {
                                "start": 654,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1030
                            },
                            {
                                "start": 1031,
                                "end": 1245
                            },
                            {
                                "start": 1246,
                                "end": 1415
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "10488675",
                            "2656445",
                            "133091488",
                            "220831381",
                            "237563320",
                            "237532289"
                        ],
                        "quote": "The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. (Zhang et al., 2021) investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[251066729 | He et al. | 2022 | Citations: 16]",
                "snippets": "In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conventional Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 529,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 196
                            },
                            {
                                "start": 197,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 529
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[251664530 | Lu et al. | 2022 | Citations: 21]",
                "snippets": "Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model , the adversary  can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of , otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "| Membership inference",
                        "pdf_hash": "",
                        "start": 436,
                        "end": 992,
                        "sentence_offsets": [
                            {
                                "start": 388,
                                "end": 529
                            },
                            {
                                "start": 530,
                                "end": 637
                            },
                            {
                                "start": 638,
                                "end": 784
                            },
                            {
                                "start": 787,
                                "end": 856
                            },
                            {
                                "start": 857,
                                "end": 1089
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "221203089",
                            "52211986",
                            "199546273"
                        ],
                        "quote": "Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model , the adversary  can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of , otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27"
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[251719178 | He et al. | 2022 | Citations: 25]",
                "snippets": "In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[165163934 | Song et al. | 2019 | Citations: 244]": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[3052834 | Hayes et al. | 2017 | Citations: 104]": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 30,
                        "end": 292,
                        "sentence_offsets": [
                            {
                                "start": 30,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 390
                            }
                        ],
                        "ref_mentions": [
                            "3052834",
                            "195699554",
                            "133091488",
                            "46933970",
                            "10488675",
                            "165163934",
                            "2656445"
                        ],
                        "quote": "In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[253244424 | Chen et al. | 2022 | Citations: 32]",
                "snippets": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time (Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016). Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13](Li et al., 2013).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 638,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 161
                            },
                            {
                                "start": 162,
                                "end": 242
                            },
                            {
                                "start": 243,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 501
                            },
                            {
                                "start": 502,
                                "end": 638
                            }
                        ],
                        "ref_mentions": [
                            "237563320",
                            "46933970",
                            "10488675",
                            "17295025"
                        ],
                        "quote": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time (Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016). Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13](Li et al., 2013)."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[253446930 | Pawelczyk et al. | 2022 | Citations: 31]",
                "snippets": "In this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 163,
                        "end": 618,
                        "sentence_offsets": [
                            {
                                "start": 163,
                                "end": 374
                            },
                            {
                                "start": 375,
                                "end": 618
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[256627812 | Matsumoto et al. | 2023 | Citations: 60]",
                "snippets": "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model (Shokri et al., 2016)- (Carlini et al., 2021). A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models (Conti et al., 2022), (Li et al., 2022), (Hayes et al., 2017). There are two settings (Shokri et al., 2016), i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[244920593 | Carlini et al. | 2021 | Citations: 708]": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                    "[251135031 | Conti et al. | 2022 | Citations: 25]": "Graph Neural Networks (GNNs), inspired by Convolutional Neural Networks (CNNs), aggregate the message of nodes' neighbors and structure information to acquire expressive representations of nodes for node classification, graph classification, and link prediction. Previous studies have indicated that node-level GNNs are vulnerable to Membership Inference Attacks (MIAs), which infer whether a node is in the training data of GNNs and leak the node's private information, like the patient's disease history. The implementation of previous MIAs takes advantage of the models' probability output, which is infeasible if GNNs only provide the prediction label (label-only) for the input. In this paper, we propose a label-only MIA against GNNs for node classification with the help of GNNs' flexible prediction mechanism, e.g., obtaining the prediction label of one node even when neighbors' information is unavailable. Our attacking method achieves around 60% accuracy, precision, and Area Under the Curve (AUC) for most datasets and GNN models, some of which are competitive or even better than state-of-the-art probability-based MIAs implemented under our environment and settings. Additionally, we analyze the influence of the sampling method, model selection approach, and overfitting level on the attack performance of our label-only MIA. All of those three factors have an impact on the attack performance. Then, we consider scenarios where assumptions about the adversary's additional dataset (shadow dataset) and extra information about the target model are relaxed. Even in those scenarios, our label-only MIA achieves a better attack performance in most cases. Finally, we explore the effectiveness of possible defenses, including Dropout, Regularization, Normalization, and Jumping knowledge. None of those four defenses prevent our attack completely.",
                    "[251765197 | Li et al. | 2022 | Citations: 34]": "Relying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks. In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "A. Membership Inference",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 545,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "10488675",
                            "244920593",
                            "251135031",
                            "251765197",
                            "52211986",
                            "10488675"
                        ],
                        "quote": "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model (Shokri et al., 2016)- (Carlini et al., 2021). A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models (Conti et al., 2022), (Li et al., 2022), (Hayes et al., 2017). There are two settings (Shokri et al., 2016), i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[258236265 | Li et al. | 2023 | Citations: 26]",
                "snippets": "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by (Shokri et al., 2016). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 561,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 126
                            },
                            {
                                "start": 127,
                                "end": 285
                            },
                            {
                                "start": 286,
                                "end": 384
                            },
                            {
                                "start": 385,
                                "end": 561
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by (Shokri et al., 2016). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[259375769 | Bertran et al. | 2023 | Citations: 50]",
                "snippets": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[265220927 | Cilloni et al. | 2023 | Citations: 3]",
                "snippets": "The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 270,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 270
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[266191072 | Pang et al. | 2023 | Citations: 20]",
                "snippets": "Membership inference attacks (MIAs) primarily aim to determine whether a target data point x is within the training dataset, often referred to as the member set, of a given target model. The motivation behind these attacks is twofold: to ensure that models are not trained in a manner that misappropriates data and to safeguard against potential privacy breaches. MIA's underlying principle hinges on exploiting machine learning models' overfitting and memorization properties. Discerning the model's different reactions to member and nonmember samples makes it feasible to infer the membership of the target point x.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 617,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 186
                            },
                            {
                                "start": 187,
                                "end": 363
                            },
                            {
                                "start": 364,
                                "end": 477
                            },
                            {
                                "start": 478,
                                "end": 617
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Membership inference attacks (MIAs) primarily aim to determine whether a target data point x is within the training dataset, often referred to as the member set, of a given target model. The motivation behind these attacks is twofold: to ensure that models are not trained in a manner that misappropriates data and to safeguard against potential privacy breaches. MIA's underlying principle hinges on exploiting machine learning models' overfitting and memorization properties. Discerning the model's different reactions to member and nonmember samples makes it feasible to infer the membership of the target point x."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[266362863 | Al-Kaswan et al. | 2023 | Citations: 17]",
                "snippets": "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself.\n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[119302127 | Hisamoto et al. | 2019 | Citations: 107]": "Data privacy is an important issue for \u201cmachine learning as a service\u201d providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model\u2019s API, determine whether the sample existed in the model\u2019s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 768,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 165
                            },
                            {
                                "start": 166,
                                "end": 317
                            },
                            {
                                "start": 320,
                                "end": 517
                            },
                            {
                                "start": 518,
                                "end": 632
                            },
                            {
                                "start": 633,
                                "end": 769
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "199546273",
                            "119302127"
                        ],
                        "quote": "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself.\n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[267069190 | Liu et al. | 2024 | Citations: 3]",
                "snippets": "Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 866,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 66
                            },
                            {
                                "start": 67,
                                "end": 235
                            },
                            {
                                "start": 236,
                                "end": 299
                            },
                            {
                                "start": 300,
                                "end": 432
                            },
                            {
                                "start": 433,
                                "end": 609
                            },
                            {
                                "start": 610,
                                "end": 866
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[269605305 | Zhao et al. | 2024 | Citations: 3]",
                "snippets": "The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 538,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 140
                            },
                            {
                                "start": 141,
                                "end": 265
                            },
                            {
                                "start": 266,
                                "end": 538
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[269756904 | Galichin et al. | 2024 | Citations: 2]",
                "snippets": "The objective of the membership inference attack (MIA) (Shokri et al., 2016) is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "B. Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 517,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 167
                            },
                            {
                                "start": 167,
                                "end": 340
                            },
                            {
                                "start": 340,
                                "end": 517
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "The objective of the membership inference attack (MIA) (Shokri et al., 2016) is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[270199828 | Anderson et al. | 2024 | Citations: 19]",
                "snippets": "Membership inference attacks (Shokri et al., 2016)(Hu et al., 2021) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2021). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[244920593 | Carlini et al. | 2021 | Citations: 708]": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                    "[232233426 | Hu et al. | 2021 | Citations: 440]": "Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1065,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 400
                            },
                            {
                                "start": 403,
                                "end": 545
                            },
                            {
                                "start": 546,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 849
                            },
                            {
                                "start": 850,
                                "end": 1065
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "232233426",
                            "244920593"
                        ],
                        "quote": "Membership inference attacks (Shokri et al., 2016)(Hu et al., 2021) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2021). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[271328769 | Li et al. | 2024 | Citations: 9]",
                "snippets": "Membership inference attack is one of the most popular privacy attacks against ML models.The goal of the membership inference attack is to determine whether a data sample is used to train a target model.We consider data samples as members if they are used to train the target model, otherwise, non-members.Formally, considering a data sample\n\nx, a trained ML model M , and background knowledge of an adversary, denoted by I , the membership inference attack A can be defined as the following:\n\nHere, 0 means the data sample x is not a member of M 's training dataset, and 1 otherwise.The attack model A is essentially a binary classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 638,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 89
                            },
                            {
                                "start": 89,
                                "end": 203
                            },
                            {
                                "start": 203,
                                "end": 306
                            },
                            {
                                "start": 306,
                                "end": 341
                            },
                            {
                                "start": 343,
                                "end": 492
                            },
                            {
                                "start": 494,
                                "end": 584
                            },
                            {
                                "start": 584,
                                "end": 638
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Membership inference attack is one of the most popular privacy attacks against ML models.The goal of the membership inference attack is to determine whether a data sample is used to train a target model.We consider data samples as members if they are used to train the target model, otherwise, non-members.Formally, considering a data sample\n\nx, a trained ML model M , and background knowledge of an adversary, denoted by I , the membership inference attack A can be defined as the following:\n\nHere, 0 means the data sample x is not a member of M 's training dataset, and 1 otherwise.The attack model A is essentially a binary classifier."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[272423578 | Wen et al. | 2024 | Citations: 2]",
                "snippets": "Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability.\n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7]).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[244920593 | Carlini et al. | 2021 | Citations: 708]": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[244345608 | Ye et al. | 2021 | Citations: 255]": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensivehypothesis testing framework that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explainwhy different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform adifferential analysis between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of thePrivacy Meter software tool.",
                    "[251953448 | Liu et al. | 2022 | Citations: 111]": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 661,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 167
                            },
                            {
                                "start": 168,
                                "end": 284
                            },
                            {
                                "start": 287,
                                "end": 446
                            },
                            {
                                "start": 447,
                                "end": 662
                            }
                        ],
                        "ref_mentions": [
                            "220831381",
                            "237563320",
                            "251953448",
                            "46933970",
                            "10488675",
                            "244345608",
                            "244920593"
                        ],
                        "quote": "Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability.\n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7])."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[274869679 | Xia et al. | 2024 | Citations: 1]",
                "snippets": "Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[275471364 | Hong et al. | 2025 | Citations: 0]",
                "snippets": "Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy (Dwork, 2006) as a worst-case certification.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 555,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 106
                            },
                            {
                                "start": 107,
                                "end": 251
                            },
                            {
                                "start": 252,
                                "end": 555
                            }
                        ],
                        "ref_mentions": [
                            "2565493"
                        ],
                        "quote": "Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy (Dwork, 2006) as a worst-case certification."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[276928094 | Liu et al. | 2025 | Citations: 1]",
                "snippets": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 124,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[3619789 | Long et al. | 2018 | Citations: 224]",
                "snippets": "Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood.\n\nIn a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood.\n\nIn a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[46933970 | Salem et al. | 2018 | Citations: 950]",
                "snippets": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[50778569 | Truex et al. | 2018 | Citations: 112]",
                "snippets": "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy (Blum et al., 2005), (Vaidya et al., 2014), (Dwork, 2008) or secure multiparty computation (Wu et al., 2016), (Cock et al., 2019), [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[18290778 | Cock et al. | 2019 | Citations: 122]": "Many data-driven personalized services require that private data of users is scored against a trained machine learning model. In this paper we propose a novel protocol for privacy-preserving classification of decision trees, a popular machine learning model in these scenarios. Our solutions is composed out of building blocks, namely a secure comparison protocol, a protocol for obliviously selecting inputs, and a protocol for multiplication. By combining some of the building blocks for our decision tree classification protocol, we also improve previously proposed solutions for classification of support vector machines and logistic regression models. Our protocols are information theoretically secure and, unlike previously proposed solutions, do not require modular exponentiations. We show that our protocols for privacy-preserving classification lead to more efficient results from the point of view of computational and communication complexities. We present accuracy and runtime results for seven classification benchmark datasets from the UCI repository.",
                    "[6942138 | Wu et al. | 2016 | Citations: 169]": "Abstract Decision trees and random forests are common classifiers with widespread use. In this paper, we develop two protocols for privately evaluating decision trees and random forests. We operate in the standard two-party setting where the server holds a model (either a tree or a forest), and the client holds an input (a feature vector). At the conclusion of the protocol, the client learns only the model\u2019s output on its input and a few generic parameters concerning the model; the server learns nothing. The first protocol we develop provides security against semi-honest adversaries. We then give an extension of the semi-honest protocol that is robust against malicious adversaries. We implement both protocols and show that both variants are able to process trees with several hundred decision nodes in just a few seconds and a modest amount of bandwidth. Compared to previous semi-honest protocols for private decision tree evaluation, we demonstrate a tenfold improvement in computation and bandwidth."
                },
                "metadata": [
                    {
                        "section_title": "MEMBERSHIP INFERENCE ATTACKS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 778,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 306
                            },
                            {
                                "start": 307,
                                "end": 440
                            },
                            {
                                "start": 441,
                                "end": 623
                            },
                            {
                                "start": 624,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "15331398",
                            "589685",
                            "2887752",
                            "6942138",
                            "18290778"
                        ],
                        "quote": "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy (Blum et al., 2005), (Vaidya et al., 2014), (Dwork, 2008) or secure multiparty computation (Wu et al., 2016), (Cock et al., 2019), [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[91184074 | Salem et al. | 2019 | Citations: 257]",
                "snippets": "Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%).",
                    "[52205573 | Hagestedt et al. | 2019 | Citations: 42]": "The advancement of molecular profiling techniques \nfuels biomedical research with a deluge of data. To facilitate \ndata sharing, the Global Alliance for Genomics and Health \nestablished the Beacon system, a search engine designed to help \nresearchers find datasets of interest. While the current Beacon \nsystem only supports genomic data, other types of biomedical \ndata, such as DNA methylation, are also essential for advancing \nour understanding in the field. In this paper, we propose the first \nBeacon system for DNA methylation data sharing: MBeacon. As \nthe current genomic Beacon is vulnerable to privacy attacks, such \nas membership inference, and DNA methylation data is highly \nsensitive, we take a privacy-by-design approach to construct \nMBeacon. \nFirst, we demonstrate the privacy threat, by proposing a \nmembership inference attack tailored specifically to unprotected \nmethylation Beacons. Our experimental results show that 100 \nqueries are sufficient to achieve a successful attack with AUC \n(area under the ROC curve) above 0.9. To remedy this situation, \nwe propose a novel differential privacy mechanism, namely SVT2 \n, \nwhich is the core component of MBeacon. Extensive experiments \nover multiple datasets show that SVT2 \ncan successfully mitigate \nmembership privacy risks without significantly harming utility. \nWe further implement a fully functional prototype of MBeacon \nwhich we make available to the research community"
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 22,
                        "end": 780,
                        "sentence_offsets": [
                            {
                                "start": 22,
                                "end": 105
                            },
                            {
                                "start": 106,
                                "end": 220
                            },
                            {
                                "start": 221,
                                "end": 318
                            },
                            {
                                "start": 319,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 780
                            }
                        ],
                        "ref_mentions": [
                            "52205573",
                            "3570809",
                            "10488675",
                            "49863840",
                            "133091488",
                            "46933970",
                            "2656445"
                        ],
                        "quote": "Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.033432,
        "cot": "After analyzing the query \"is membership inference attack\" and the provided quotes, I'll plan the dimensions that would best structure a comprehensive answer. \n\nThe query is asking about what membership inference attacks are, so I need to organize the information to provide a clear and comprehensive explanation. Based on the quotes, I can identify several key aspects of membership inference attacks that should be covered:\n\n1. Introduction/Background: This section will provide a basic definition and explanation of what membership inference attacks are. This is essential to establish the foundation for understanding the topic.\n\n2. Attack Mechanism: This section will explain how membership inference attacks work, including methodologies, techniques, and approaches. Many quotes detail specific attack methods and mechanisms.\n\n3. Privacy Implications: This section will discuss the privacy risks and implications of membership inference attacks, which are mentioned in several quotes as a significant concern.\n\n4. Attack Settings: This section will cover the different settings in which membership inference attacks can be conducted (e.g., black-box, white-box), as mentioned in several quotes.\n\n5. Defenses and Mitigations: While not extensively covered in the quotes, there are mentions of factors affecting attack success and potential mitigations.\n\nFor each dimension, I'll choose the appropriate format:\n\n- Introduction/Background: Synthesis is appropriate to provide a coherent explanation of what membership inference attacks are at a fundamental level.\n- Attack Mechanism: Synthesis is appropriate to explain the mechanisms in a flowing, connected manner.\n- Privacy Implications: Synthesis is appropriate to present a cohesive discussion of the privacy concerns.\n- Attack Settings: List format is appropriate to clearly distinguish between different attack settings.\n- Defenses and Mitigations: List format is appropriate to present distinct mitigation strategies or factors.\n\nThis structure will provide a comprehensive answer to the query, covering the key aspects of membership inference attacks while ensuring all the provided quotes are incorporated.",
        "plan": {
            "Introduction/Background - What is a Membership Inference Attack (synthesis)": [
                0,
                3,
                5,
                8,
                9,
                12,
                13,
                15,
                17,
                22,
                28,
                32,
                39,
                40,
                42
            ],
            "Attack Mechanism and Methodologies (synthesis)": [
                1,
                2,
                4,
                7,
                10,
                11,
                18,
                19,
                20,
                21,
                26,
                27,
                29,
                30,
                34,
                35,
                36,
                41,
                43
            ],
            "Privacy Implications and Risks (synthesis)": [
                6,
                23,
                24,
                25,
                31,
                33,
                37,
                38
            ],
            "Attack Settings and Scenarios (list)": [
                14,
                16
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background - What is a Membership Inference Attack",
                "tldr": "Membership inference attacks determine whether a specific data sample was used to train a machine learning model. These attacks represent a fundamental privacy risk, as they can reveal sensitive information about individuals whose data was included in training datasets. (15 sources)",
                "text": "\nMembership inference attacks (MIAs) are a significant privacy concern in machine learning, focusing on determining whether a particular data sample was part of a model's training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>. These attacks exploit the fact that machine learning models can inadvertently leak information about their training data through their predictions and behavior <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper> <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>. The goal of a membership inference attack is straightforward: given a data record and access to a model, determine if that record was used to train the model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\nFormally, membership inference attacks can be defined as a function that takes a target data sample, the target model, and potentially auxiliary knowledge to produce a binary output indicating whether the sample was in the training dataset (member) or not (non-member) <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"231846491\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>. This can be represented as A_MemInf: x, T \u2192 {member, non-member}, where x is the data sample and T is the target model <Paper corpusId=\"231846491\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>.\n\nThe privacy implications of these attacks are significant. If an attacker can determine that an individual's data was used to train a specific model, it may reveal sensitive personal information <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper>. For example, if a patient's clinical records were used to train a model associated with a particular disease, this could reveal that the patient has that disease <Paper corpusId=\"221586480\" paperTitle=\"(Zou et al., 2020)\" isShortName></Paper> <Paper corpusId=\"269605305\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Similarly, membership in datasets related to sensitive attributes like health status, financial information, or personal preferences can leak private information about individuals <Paper corpusId=\"231802143\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nMembership inference attacks have become a standard method for evaluating the privacy risks of machine learning models <Paper corpusId=\"251719178\" paperTitle=\"(He et al., 2022)\" isShortName></Paper> <Paper corpusId=\"265220927\" paperTitle=\"(Cilloni et al., 2023)\" isShortName></Paper>. As the simplest and most widely deployed attack for auditing training data privacy <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>, they serve as a fundamental benchmark for assessing how much information models leak about their training data. These attacks operate at the local level, focusing on individual data points rather than attempting to recover the entire training dataset <Paper corpusId=\"50778569\" paperTitle=\"(Truex et al., 2018)\" isShortName></Paper>.\n\nUnderstanding membership inference attacks is crucial for developing privacy-preserving machine learning models, as these attacks represent a fundamental privacy risk that must be addressed to protect the confidentiality of training data <Paper corpusId=\"3619789\" paperTitle=\"(Long et al., 2018)\" isShortName></Paper> <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"276928094\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Choquette-Choo et al., 2020)",
                        "snippets": [
                            "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model."
                        ],
                        "paper": {
                            "corpus_id": 220831381,
                            "title": "Label-Only Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "1415982317",
                                    "name": "Christopher A. Choquette-Choo"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "1967156",
                                    "name": "Nicolas Papernot"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 516
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Leino et al., 2019)",
                        "snippets": [
                            "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model."
                        ],
                        "paper": {
                            "corpus_id": 195699554,
                            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                            "authors": [
                                {
                                    "authorId": "35802340",
                                    "name": "Klas Leino"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                }
                            ],
                            "year": 2019,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 272
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Song et al., 2020)",
                        "snippets": [
                            "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model."
                        ],
                        "paper": {
                            "corpus_id": 214623088,
                            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 375
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Carlini et al., 2021)",
                        "snippets": [
                            "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy."
                        ],
                        "paper": {
                            "corpus_id": 244920593,
                            "title": "Membership Inference Attacks From First Principles",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2059189068",
                                    "name": "Steve Chien"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "144206374",
                                    "name": "Shuang Song"
                                },
                                {
                                    "authorId": "1763579",
                                    "name": "A. Terzis"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 708
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Zou et al., 2020)",
                        "snippets": [
                            "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier."
                        ],
                        "paper": {
                            "corpus_id": 221586480,
                            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2113958751",
                                    "name": "Yang Zou"
                                },
                                {
                                    "authorId": "48806102",
                                    "name": "Zhikun Zhang"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 32
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(He et al., 2021)",
                        "snippets": [
                            "Membership inference attack is one of the most popular privacy attacks against ML models (Chen et al., 2019)8,10,19,(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020). The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}."
                        ],
                        "paper": {
                            "corpus_id": 231846491,
                            "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
                            "authors": [
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 52
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "The goal of membership inference attacks is to infer whether any particular data instance has been used in the training of a specific model. If a particular data instance has been used in the training, this instance is called a member, otherwise it is a non-member. Knowing the membership of one particular data point could result in revealing private information, for example, if someone's data is known to be in a cancer dataset (used to train a cancer prediction model), then it is highly likely that this particular person has cancer."
                        ],
                        "paper": {
                            "corpus_id": 269605305,
                            "title": "The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape",
                            "authors": [
                                {
                                    "authorId": "2133363467",
                                    "name": "Joshua C. Zhao"
                                },
                                {
                                    "authorId": "2268859358",
                                    "name": "Saurabh Bagchi"
                                },
                                {
                                    "authorId": "121011351",
                                    "name": "S. Avestimehr"
                                },
                                {
                                    "authorId": "2300143852",
                                    "name": "Kevin S. Chan"
                                },
                                {
                                    "authorId": "2228303",
                                    "name": "S. Chaterji"
                                },
                                {
                                    "authorId": "2300092534",
                                    "name": "Dimitris Dimitriadis"
                                },
                                {
                                    "authorId": "2300130544",
                                    "name": "Jiacheng Li"
                                },
                                {
                                    "authorId": "2265278308",
                                    "name": "Ninghui Li"
                                },
                                {
                                    "authorId": "2300093540",
                                    "name": "Arash Nourian"
                                },
                                {
                                    "authorId": "2302008236",
                                    "name": "Holger Roth"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 3
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Membership Inference (MemInf) (Shokri et al., 2016) against ML models involves an adversary aiming to determine whether or not a target data sample was used to train a target ML model. More formally, given a target data sample x target , (the access to) a target model M , and an auxiliary dataset D aux , a membership inference attack can be defined as: \n\nwhere M \u2208 {M B , M W } and D aux \u2208 {D P aux , D S aux }. Membership inference has been extensively studied in literature (Chen et al., 2019)(Chen et al., 2020)(Jia et al., 2019)(Leino et al., 2019)(Li et al., 2020)(Nasr et al., 2018)(Sablayrolles et al., 2019)(Salem et al., 2018)(Shokri et al., 2016). Inferring membership of a target sample prompts severe privacy threats; for instance, if an ML model for drug dose prediction is trained using data from patients with a certain disease, then inclusion in the training set inherently leaks the individuals' health status. Overall, MemInf is also often a signal that a target model is \"leaky\" and can be a gateway to additional attacks [10]."
                        ],
                        "paper": {
                            "corpus_id": 231802143,
                            "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2108101945",
                                    "name": "Yugeng Liu"
                                },
                                {
                                    "authorId": "2054749404",
                                    "name": "Rui Wen"
                                },
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "48806102",
                                    "name": "Zhikun Zhang"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "1728207",
                                    "name": "Emiliano De Cristofaro"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 132
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(He et al., 2022)",
                        "snippets": [
                            "In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)."
                        ],
                        "paper": {
                            "corpus_id": 251719178,
                            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2110631311",
                                    "name": "Weilin Xu"
                                },
                                {
                                    "authorId": "35372584",
                                    "name": "Cory Cornelius"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 25
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Cilloni et al., 2023)",
                        "snippets": [
                            "The goal of a membership inference attack (MIA) is to recognize what data samples were used in training a model, ignoring those that were not. MI attacks are the most fundamental attack on data privacy and are widely used for measuring the privacy of a training dataset."
                        ],
                        "paper": {
                            "corpus_id": 265220927,
                            "title": "Privacy Threats in Stable Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1900852898",
                                    "name": "Thomas Cilloni"
                                },
                                {
                                    "authorId": "2061097936",
                                    "name": "Charles Fleming"
                                },
                                {
                                    "authorId": "2057153656",
                                    "name": "Charles Walter"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Truex et al., 2018)",
                        "snippets": [
                            "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy (Blum et al., 2005), (Vaidya et al., 2014), (Dwork, 2008) or secure multiparty computation (Wu et al., 2016), (Cock et al., 2019), [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety."
                        ],
                        "paper": {
                            "corpus_id": 50778569,
                            "title": "Towards Demystifying Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "25121568",
                                    "name": "Stacey Truex"
                                },
                                {
                                    "authorId": "46458150",
                                    "name": "Ling Liu"
                                },
                                {
                                    "authorId": "2327300",
                                    "name": "M. E. Gursoy"
                                },
                                {
                                    "authorId": "2112532900",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "47747953",
                                    "name": "Wenqi Wei"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 112
                        },
                        "score": 0.99951171875
                    },
                    {
                        "id": "(Long et al., 2018)",
                        "snippets": [
                            "Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood.\n\nIn a membership inference attack, the adversary's goal is to infer the membership status of a target individuals data in the input dataset to some computation. For a survey, the adversary wishes to ascertain, from aggregate survey responses, whether the individual participated in the survey. For machine learning, the adversary wishes to ascertain whether the target's record was part of the dataset used to train a specific model."
                        ],
                        "paper": {
                            "corpus_id": 3619789,
                            "title": "Understanding Membership Inferences on Well-Generalized Learning Models",
                            "authors": [
                                {
                                    "authorId": "3147214",
                                    "name": "Yunhui Long"
                                },
                                {
                                    "authorId": "3094927",
                                    "name": "Vincent Bindschaedler"
                                },
                                {
                                    "authorId": "2152507640",
                                    "name": "Lei Wang"
                                },
                                {
                                    "authorId": "3203018",
                                    "name": "Diyue Bu"
                                },
                                {
                                    "authorId": "50141047",
                                    "name": "Xiaofeng Wang"
                                },
                                {
                                    "authorId": "2112389071",
                                    "name": "Haixu Tang"
                                },
                                {
                                    "authorId": "1785347",
                                    "name": "Carl A. Gunter"
                                },
                                {
                                    "authorId": "145126969",
                                    "name": "Kai Chen"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 224
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision."
                        ],
                        "paper": {
                            "corpus_id": 237563320,
                            "title": "Membership Leakage in Label-Only Exposures",
                            "authors": [
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 246
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model."
                        ],
                        "paper": {
                            "corpus_id": 276928094,
                            "title": "Efficient Membership Inference Attacks by Bayesian Neural Network",
                            "authors": [
                                {
                                    "authorId": "2283439172",
                                    "name": "Zhenlong Liu"
                                },
                                {
                                    "authorId": "2283510083",
                                    "name": "Wenyu Jiang"
                                },
                                {
                                    "authorId": "2350314174",
                                    "name": "Feng Zhou"
                                },
                                {
                                    "authorId": "2283524907",
                                    "name": "Hongxin Wei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9970703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Attack Mechanism and Methodologies",
                "tldr": "Membership inference attacks employ various methodologies including shadow modeling, metric-based comparisons, and machine learning classifiers to detect differences in model behavior between training and non-training data. These attacks range from black-box approaches that only access model outputs to white-box approaches that leverage internal model parameters. (24 sources)",
                "text": "\nMembership inference attacks operate on a fundamental principle: machine learning models tend to behave differently on data they were trained on versus data they haven't seen before. This difference forms the basis for detecting whether a specific data sample was part of the training dataset <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"210942951\" paperTitle=\"(Farokhi et al., 2020)\" isShortName></Paper>. Formally, a membership inference attack can be defined as a function that takes a data point, access to a target model, and potentially some background knowledge to determine whether the data point was used to train the model <Paper corpusId=\"195346528\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper> <Paper corpusId=\"251066729\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>.\n\nOne of the most influential attack methodologies was introduced by Shokri et al., who designed an approach based on \"shadow training\" <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. This technique involves three main steps: (1) training multiple shadow models that simulate the behavior of the target model, (2) creating a labeled dataset of member versus non-member predictions based on these shadow models, and (3) training an inference model (typically a neural network) to distinguish between members and non-members based on the target model's prediction vector <Paper corpusId=\"227152059\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. This approach essentially treats membership inference as a binary classification problem <Paper corpusId=\"258236265\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nSubsequent research has refined and simplified this approach. Salem et al. optimized the attack by reducing the number of required shadow models from multiple to just one, making the attack more practical <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. Yeom et al. proposed a simpler inference method that compares the prediction confidence value of a target example with a threshold (which can be learned through shadow training), where higher confidence suggests membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al._1, 2017)\" isShortName></Paper>.\n\nThe attack settings can be categorized into black-box and white-box approaches:\n\n1. Black-box attacks: These rely solely on the outputs (predictions or confidence scores) of the target model <Paper corpusId=\"250048543\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. The attacker queries the model with the target data sample and analyzes the response patterns to infer membership <Paper corpusId=\"218674569\" paperTitle=\"(Huang et al., 2020)\" isShortName></Paper>. Recent advances in black-box attacks include label-only membership inference attacks that can operate even when only the predicted labels (without confidence scores) are available <Paper corpusId=\"250048543\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\n2. White-box attacks: These leverage access to the model's internal parameters and architecture <Paper corpusId=\"259375769\" paperTitle=\"(Bertran et al., 2023)\" isShortName></Paper>. Nasr et al. designed white-box membership inference attacks that exploit the privacy vulnerabilities of the stochastic gradient descent algorithm used to train deep neural networks <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Leino et al. utilized the model's idiosyncratic use of features to develop white-box attacks that outperform black-box methods <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper> <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\nThe success of these attacks is often attributed to model overfitting, where the model memorizes training data rather than generalizing from it <Paper corpusId=\"221725546\" paperTitle=\"(Tanuwidjaja et al., 2020)\" isShortName></Paper> <Paper corpusId=\"210942951\" paperTitle=\"(Farokhi et al., 2020)\" isShortName></Paper>. However, research has shown that overfitting, while sufficient, is not necessary for successful attacks <Paper corpusId=\"227152059\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al._1, 2017)\" isShortName></Paper>.\n\nMembership inference attacks have been applied to various model types beyond standard classification models, including generative models like GANs (Generative Adversarial Networks) and VAEs (Variational Autoencoders) <Paper corpusId=\"266362863\" paperTitle=\"(Al-Kaswan et al., 2023)\" isShortName></Paper> <Paper corpusId=\"199546273\" paperTitle=\"(Hilprecht et al., 2019)\" isShortName></Paper>, as well as language models and image diffusion models <Paper corpusId=\"266362863\" paperTitle=\"(Al-Kaswan et al., 2023)\" isShortName></Paper>. Each model type may require specialized attack techniques, but the fundamental principles remain similar <Paper corpusId=\"251664530\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\nRecent developments have focused on improving attack efficacy through more sophisticated methods. Carlini et al. developed the Likelihood Ratio Attack (LiRA) that combines multiple techniques to achieve better performance, especially at low false-positive rates <Paper corpusId=\"270199828\" paperTitle=\"(Anderson et al., 2024)\" isShortName></Paper> <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>. Other advances include attacks that utilize the entire training trajectory of a model rather than just its final state <Paper corpusId=\"272423578\" paperTitle=\"(Wen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Song et al., 2019)",
                        "snippets": [
                            "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership."
                        ],
                        "paper": {
                            "corpus_id": 165163934,
                            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 244
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Farokhi et al., 2020)",
                        "snippets": [
                            "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention (Shokri et al., 2016)(Truex et al., 2018)(Salem et al., 2018)(Sablayrolles et al., 2019). These attacks have been deployed on various machine learning models; see, e.g., (Shokri et al., 2016)(Hayes et al., 2017)(Chen et al., 2019)(Hilprecht et al., 2019)(Liu et al., 2019)(Backes et al., 2016). The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting."
                        ],
                        "paper": {
                            "corpus_id": 210942951,
                            "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
                            "authors": [
                                {
                                    "authorId": "1803792",
                                    "name": "F. Farokhi"
                                },
                                {
                                    "authorId": "1708760",
                                    "name": "M. K\u00e2afar"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 24
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Yeom et al., 2017)",
                        "snippets": [
                            "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access."
                        ],
                        "paper": {
                            "corpus_id": 195346528,
                            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "26378728",
                                    "name": "Samuel Yeom"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                },
                                {
                                    "authorId": "1680133",
                                    "name": "S. Jha"
                                }
                            ],
                            "year": 2017,
                            "venue": "arXiv.org",
                            "n_citations": 40
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(He et al._1, 2022)",
                        "snippets": [
                            "In membership inference attack, the adversary aims to determine whether a given data sample x belongs to the target model T 's training dataset or not given the adversary's background knowledge K. A data sample x is called member (or non-member ) if it belongs to (or does not belong to) the training dataset of the target model T . Formally, we define the membership inference attack as A : x, T , K \u2192 {0, 1}, where the attack A is essentially a mapping function and 1 (or 0) means the data sample x is a member (or non-member)."
                        ],
                        "paper": {
                            "corpus_id": 251066729,
                            "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
                            "authors": [
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "2110279247",
                                    "name": "Hongbin Liu"
                                },
                                {
                                    "authorId": "144516687",
                                    "name": "N. Gong"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 16
                        },
                        "score": 0.99951171875
                    },
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Liu et al., 2020)",
                        "snippets": [
                            "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not (Melis et al., 2018)(Shokri et al., 2016). An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. (Shokri et al., 2016) introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. (Yeom et al., 2017) investigated \"the relationship between overfitting and privacy leakage\". Salem et al. (Salem et al., 2018) proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\"."
                        ],
                        "paper": {
                            "corpus_id": 227152059,
                            "title": "When Machine Learning Meets Privacy",
                            "authors": [
                                {
                                    "authorId": "145306564",
                                    "name": "B. Liu"
                                },
                                {
                                    "authorId": "145633124",
                                    "name": "Ming Ding"
                                },
                                {
                                    "authorId": "40221713",
                                    "name": "Sina Shaham"
                                },
                                {
                                    "authorId": "2352525282",
                                    "name": "Wenny Rahayu"
                                },
                                {
                                    "authorId": "1803792",
                                    "name": "F. Farokhi"
                                },
                                {
                                    "authorId": "1740858",
                                    "name": "Zihuai Lin"
                                }
                            ],
                            "year": 2020,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 290
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by (Shokri et al., 2016). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model."
                        ],
                        "paper": {
                            "corpus_id": 258236265,
                            "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning",
                            "authors": [
                                {
                                    "authorId": "1527113700",
                                    "name": "Yuyuan Li"
                                },
                                {
                                    "authorId": "1694815",
                                    "name": "Chaochao Chen"
                                },
                                {
                                    "authorId": "1687974",
                                    "name": "Xiaolin Zheng"
                                },
                                {
                                    "authorId": "2214803049",
                                    "name": "Yizhao Zhang"
                                },
                                {
                                    "authorId": "2211782518",
                                    "name": "Biao Gong"
                                },
                                {
                                    "authorId": "2152811080",
                                    "name": "Jun Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Expert systems with applications",
                            "n_citations": 26
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Zhou et al., 2022)",
                        "snippets": [
                            "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."
                        ],
                        "paper": {
                            "corpus_id": 246706163,
                            "title": "PPA: Preference Profiling Attack Against Federated Learning",
                            "authors": [
                                {
                                    "authorId": "1845880105",
                                    "name": "Chunyi Zhou"
                                },
                                {
                                    "authorId": "39922366",
                                    "name": "Yansong Gao"
                                },
                                {
                                    "authorId": "2068511826",
                                    "name": "Anmin Fu"
                                },
                                {
                                    "authorId": "2157740727",
                                    "name": "Kai Chen"
                                },
                                {
                                    "authorId": "151498397",
                                    "name": "Zhiyang Dai"
                                },
                                {
                                    "authorId": "2116763991",
                                    "name": "Zhi Zhang"
                                },
                                {
                                    "authorId": "2837434",
                                    "name": "Minhui Xue"
                                },
                                {
                                    "authorId": "2155342827",
                                    "name": "Yuqing Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 23
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Salem et al., 2018)",
                        "snippets": [
                            "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status."
                        ],
                        "paper": {
                            "corpus_id": 46933970,
                            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "144887171",
                                    "name": "Mathias Humbert"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                }
                            ],
                            "year": 2018,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 950
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Yeom et al._1, 2017)",
                        "snippets": [
                            "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                        ],
                        "paper": {
                            "corpus_id": 2656445,
                            "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                            "authors": [
                                {
                                    "authorId": "26378728",
                                    "name": "Samuel Yeom"
                                },
                                {
                                    "authorId": "3025831",
                                    "name": "Irene Giacomelli"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                },
                                {
                                    "authorId": "1680133",
                                    "name": "S. Jha"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Computer Security Foundations Symposium",
                            "n_citations": 1133
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. (Zhang et al., 2021) investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders."
                        ],
                        "paper": {
                            "corpus_id": 250048543,
                            "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Zihan Wang"
                                },
                                {
                                    "authorId": "2173325427",
                                    "name": "Na Huang"
                                },
                                {
                                    "authorId": "143770118",
                                    "name": "Fei Sun"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "2889630",
                                    "name": "Hengliang Luo"
                                },
                                {
                                    "authorId": "1696030",
                                    "name": "M. de Rijke"
                                },
                                {
                                    "authorId": "2780667",
                                    "name": "Z. Ren"
                                }
                            ],
                            "year": 2022,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 14
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Huang et al., 2020)",
                        "snippets": [
                            "Membership inference attack is a type of attack against deep learning models, which can be deployed to determine whether a sample is from the training set of a victim model. The basic idea of the attack is that the information exposed by the model contains the abundant information of the training data, based on which an attacker may perform membership inferences. Theoretically, all characteristics of the victim model such as activation values, affine outputs, gradients or even the model's transparency report, can be utilized by attackers to deploy the attack [15,19]20]. Given that most of the above characteristics are not publicly accessible, attackers may solely rely on the outputs of the model to deploy the attack in practice."
                        ],
                        "paper": {
                            "corpus_id": 218674569,
                            "title": "DAMIA: Leveraging Domain Adaptation as a Defense Against Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "2146281955",
                                    "name": "Hongwei Huang"
                                },
                                {
                                    "authorId": "144644709",
                                    "name": "Weiqi Luo"
                                },
                                {
                                    "authorId": "48035994",
                                    "name": "Guoqiang Zeng"
                                },
                                {
                                    "authorId": "145369053",
                                    "name": "J. Weng"
                                },
                                {
                                    "authorId": "2145913542",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "2145970",
                                    "name": "Anjia Yang"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Transactions on Dependable and Secure Computing",
                            "n_citations": 25
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Choquette-Choo et al., 2020)",
                        "snippets": [
                            "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model."
                        ],
                        "paper": {
                            "corpus_id": 220831381,
                            "title": "Label-Only Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "1415982317",
                                    "name": "Christopher A. Choquette-Choo"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "1967156",
                                    "name": "Nicolas Papernot"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 516
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Bertran et al., 2023)",
                        "snippets": [
                            "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data."
                        ],
                        "paper": {
                            "corpus_id": 259375769,
                            "title": "Scalable Membership Inference Attacks via Quantile Regression",
                            "authors": [
                                {
                                    "authorId": "37335063",
                                    "name": "Mart\u00edn Bertr\u00e1n"
                                },
                                {
                                    "authorId": "46321498",
                                    "name": "Shuai Tang"
                                },
                                {
                                    "authorId": "81338045",
                                    "name": "Michael Kearns"
                                },
                                {
                                    "authorId": "144848816",
                                    "name": "Jamie Morgenstern"
                                },
                                {
                                    "authorId": "1682008",
                                    "name": "Aaron Roth"
                                },
                                {
                                    "authorId": "1768074",
                                    "name": "Zhiwei Steven Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 50
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Leino et al., 2019)",
                        "snippets": [
                            "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model."
                        ],
                        "paper": {
                            "corpus_id": 195699554,
                            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                            "authors": [
                                {
                                    "authorId": "35802340",
                                    "name": "Klas Leino"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                }
                            ],
                            "year": 2019,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 272
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Tanuwidjaja et al., 2020)",
                        "snippets": [
                            "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. (Shokri et al., 2016) provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data."
                        ],
                        "paper": {
                            "corpus_id": 221725546,
                            "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey",
                            "authors": [
                                {
                                    "authorId": "3385931",
                                    "name": "Harry Chandra Tanuwidjaja"
                                },
                                {
                                    "authorId": "36105968",
                                    "name": "Rakyong Choi"
                                },
                                {
                                    "authorId": "30708702",
                                    "name": "Seunggeun Baek"
                                },
                                {
                                    "authorId": "1741995",
                                    "name": "Kwangjo Kim"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 82
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Al-Kaswan et al., 2023)",
                        "snippets": [
                            "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself.\n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]."
                        ],
                        "paper": {
                            "corpus_id": 266362863,
                            "title": "Traces of Memorisation in Large Language Models for Code",
                            "authors": [
                                {
                                    "authorId": "2199249915",
                                    "name": "Ali Al-Kaswan"
                                },
                                {
                                    "authorId": "145774460",
                                    "name": "M. Izadi"
                                },
                                {
                                    "authorId": "10734708",
                                    "name": "Arie van Deursen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Software Engineering",
                            "n_citations": 17
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Hilprecht et al., 2019)",
                        "snippets": [
                            "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples."
                        ],
                        "paper": {
                            "corpus_id": 199546273,
                            "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
                            "authors": [
                                {
                                    "authorId": "81786870",
                                    "name": "Benjamin Hilprecht"
                                },
                                {
                                    "authorId": "2736329",
                                    "name": "Martin H\u00e4rterich"
                                },
                                {
                                    "authorId": "13047311",
                                    "name": "Daniel Bernau"
                                }
                            ],
                            "year": 2019,
                            "venue": "Proceedings on Privacy Enhancing Technologies",
                            "n_citations": 191
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model , the adversary  can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of , otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27"
                        ],
                        "paper": {
                            "corpus_id": 251664530,
                            "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
                            "authors": [
                                {
                                    "authorId": "2143551615",
                                    "name": "Zhaobo Lu"
                                },
                                {
                                    "authorId": "145844506",
                                    "name": "Hai Liang"
                                },
                                {
                                    "authorId": "2152527649",
                                    "name": "Minghao Zhao"
                                },
                                {
                                    "authorId": "2143047511",
                                    "name": "Qingzhe Lv"
                                },
                                {
                                    "authorId": "47716189",
                                    "name": "Tiancai Liang"
                                },
                                {
                                    "authorId": "3057294",
                                    "name": "Yilei Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Journal of Intelligent Systems",
                            "n_citations": 21
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Anderson et al., 2024)",
                        "snippets": [
                            "Membership inference attacks (Shokri et al., 2016)(Hu et al., 2021) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2021). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set."
                        ],
                        "paper": {
                            "corpus_id": 270199828,
                            "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "39868788",
                                    "name": "Maya Anderson"
                                },
                                {
                                    "authorId": "2291066051",
                                    "name": "Guy Amit"
                                },
                                {
                                    "authorId": "2652502",
                                    "name": "Abigail Goldsteen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Information Systems Security and Privacy",
                            "n_citations": 19
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Carlini et al., 2021)",
                        "snippets": [
                            "The objective of a membership inference attack (MIA) [60] is to predict if a specific training example was, or was not, used as training data in a particular model. This makes MIAs the simplest and most widely deployed attack for auditing training data privacy."
                        ],
                        "paper": {
                            "corpus_id": 244920593,
                            "title": "Membership Inference Attacks From First Principles",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2059189068",
                                    "name": "Steve Chien"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "144206374",
                                    "name": "Shuang Song"
                                },
                                {
                                    "authorId": "1763579",
                                    "name": "A. Terzis"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 708
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Wen et al., 2024)",
                        "snippets": [
                            "Membership Inference Attack (MIA) [12,31,34,50,55,70] is a prominent privacy attack utilized to determine whether a specific data sample belongs to a training dataset. This attack is widely employed to assess the privacy of training data due to its simplicity and broad applicability.\n\nIn the attack scenario, the adversary A is granted access to a target model and is tasked with determining the membership status of a given data sample (x, y). Formally, the membership inference attack can be defined as a security game, referred to as Membership Inference Security Game, which is described as follows: Definition 4.1 (Membership Inference Security Game [7])."
                        ],
                        "paper": {
                            "corpus_id": 272423578,
                            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
                            "authors": [
                                {
                                    "authorId": "2054749404",
                                    "name": "Rui Wen"
                                },
                                {
                                    "authorId": "2257034706",
                                    "name": "Michael Backes"
                                },
                                {
                                    "authorId": "2257291195",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 2
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."
                        ],
                        "paper": {
                            "corpus_id": 251953448,
                            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
                            "authors": [
                                {
                                    "authorId": "2182511319",
                                    "name": "Yiyong Liu"
                                },
                                {
                                    "authorId": "2277275",
                                    "name": "Zhengyu Zhao"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 111
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Privacy Implications and Risks",
                "tldr": "Membership inference attacks pose serious privacy risks by potentially revealing sensitive personal information about individuals whose data was used to train machine learning models. These attacks are particularly concerning in domains like healthcare, where revealing membership in a training dataset could expose an individual's medical conditions. (11 sources)",
                "text": "\nMembership inference attacks represent a significant privacy threat that extends beyond simply determining whether a data sample was in a training set. The privacy implications are particularly severe when the training data contains sensitive information about individuals <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. For example, if an attacker can infer that a clinical record or an individual was used to train a model associated with a specific disease, this directly exposes private health information about that individual <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThese attacks enable adversaries to access confidential training data or even replicate processing models by using only publicly accessible services <Paper corpusId=\"218473513\" paperTitle=\"(Xue et al., 2020)\" isShortName></Paper>. The fundamental privacy concern is that machine learning models can unintentionally memorize and leak information about the data they were trained on <Paper corpusId=\"267069190\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This \"unintended membership exposure\" can lead to \"catastrophic privacy loss for individuals\" in real-world applications <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.\n\nDue to these serious privacy implications, membership inference attacks have become widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms <Paper corpusId=\"253244424\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>. These attacks demonstrate that, under mild assumptions about the target model, it is possible to identify portions of the training dataset, potentially leading to leakage of private data <Paper corpusId=\"269756904\" paperTitle=\"(Galichin et al., 2024)\" isShortName></Paper>.\n\nRecent research has expanded the understanding of membership inference risks by exploring new attack vectors. Pawelczyk et al. introduced \"counterfactual distance-based attacks\" that leverage algorithmic recourse to determine if an instance belongs to a model's training data <Paper corpusId=\"253446930\" paperTitle=\"(Pawelczyk et al., 2022)\" isShortName></Paper>. Such advances highlight the evolving nature of privacy threats in machine learning.\n\nThe vulnerability to membership inference attacks also implies a model's potential to leak other private information outside the specific context of membership, an idea that aligns closely with the definition of differential privacy as a worst-case certification <Paper corpusId=\"275471364\" paperTitle=\"(Hong et al., 2025)\" isShortName></Paper>. This means that models vulnerable to membership inference may be susceptible to other types of privacy breaches as well.\n\nMembership inference attacks are particularly problematic in certain domains and settings. In federated learning environments, where data remains distributed across multiple devices or organizations, these attacks pose a significant threat to the confidentiality of local training data <Paper corpusId=\"274869679\" paperTitle=\"(Xia et al., 2024)\" isShortName></Paper>. Similarly, in healthcare applications, membership inference attacks could reveal whether a patient's data was used to train a model related to sensitive conditions <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThe privacy risks extend to various model types beyond standard classification models. Research has shown that generative models, including GANs (Generative Adversarial Networks), can also be vulnerable to membership inference attacks <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>. These attacks against generative models exploit the models' capacity to learn statistical differences in distributions, enabling adversaries to detect overfitting and recognize inputs that were part of training datasets <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>.\n\nAs machine learning models become more integrated into privacy-critical applications like face recognition and medical image analysis, the potential impact of membership inference attacks grows <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. To address these risks, researchers have developed various defense mechanisms, though finding effective protections that maintain model utility remains an active area of research <Paper corpusId=\"256627812\" paperTitle=\"(Matsumoto et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time (Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016). Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13](Li et al., 2013)."
                        ],
                        "paper": {
                            "corpus_id": 253244424,
                            "title": "Amplifying Membership Exposure via Data Poisoning",
                            "authors": [
                                {
                                    "authorId": "1711982",
                                    "name": "Yufei Chen"
                                },
                                {
                                    "authorId": "2088079836",
                                    "name": "Chao Shen"
                                },
                                {
                                    "authorId": "2117688523",
                                    "name": "Yun Shen"
                                },
                                {
                                    "authorId": "2116638601",
                                    "name": "Cong Wang"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 32
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Xue et al., 2020)",
                        "snippets": [
                            "Liu et al. (Liu et al., 2015) illustrate security threats in cognitive systems. Specifically, they show that attacker can access to confidential training data or replicate the processing model by using only the public accessible services of the model. Shokri et al. (Shokri et al., 2016) propose the so called membership inference attack, in which the adversary can estimate whether a given data is in the training set of a target model. Particularly, they use the target model's prediction of training and nontraining data to train a membership inference model (Shokri et al., 2016). According to the output of the target model, the generated membership inference model can identify the differences in the prediction of the target model on its training data and the data that hasn't been used for its training. The membership inference attack proposed in (Shokri et al., 2016) is generic, but the success of member inference attacks depends on the overfitting of the model (Yeom et al., 2017), (Shokri et al., 2016). If it is a well-generalized model, the success rate of the membership inference attack is low."
                        ],
                        "paper": {
                            "corpus_id": 218473513,
                            "title": "Machine Learning Security: Threats, Countermeasures, and Evaluations",
                            "authors": [
                                {
                                    "authorId": "46301056",
                                    "name": "Mingfu Xue"
                                },
                                {
                                    "authorId": "1984508",
                                    "name": "Chengxiang Yuan"
                                },
                                {
                                    "authorId": "1421758751",
                                    "name": "Heyi Wu"
                                },
                                {
                                    "authorId": "2108051942",
                                    "name": "Yushu Zhang"
                                },
                                {
                                    "authorId": "2109335717",
                                    "name": "Weiqiang Liu"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 124
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Membership Inference Attacks have emerged as a noteworthy concern. At the core of membership inference attacks is the attacker's ability to infer whether a specific data point was part of the training dataset used to train an ML model. A visual representation of this process can be found in Fig. 6. In other words, membership inference attacks are based on methods in which the target model learns features in the training dataset. The attacker constructs his training dataset and observes the model's output on these samples to infer whether the samples in the test dataset belong to the membership samples. This might sound innocuous at first, but consider scenarios where the training data contains sensitive information -revealing whether a particular data point (like a patient's medical record) was used in training could lead to significant privacy breaches."
                        ],
                        "paper": {
                            "corpus_id": 267069190,
                            "title": "Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them",
                            "authors": [
                                {
                                    "authorId": "2281289264",
                                    "name": "Chao Liu"
                                },
                                {
                                    "authorId": "2280335814",
                                    "name": "Boxi Chen"
                                },
                                {
                                    "authorId": "2280146329",
                                    "name": "Wei Shao"
                                },
                                {
                                    "authorId": "2280200066",
                                    "name": "Chris Zhang"
                                },
                                {
                                    "authorId": "2264107865",
                                    "name": "Kelvin Wong"
                                },
                                {
                                    "authorId": "2280349425",
                                    "name": "Yi Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Galichin et al., 2024)",
                        "snippets": [
                            "The objective of the membership inference attack (MIA) (Shokri et al., 2016) is to determine whether a specific data sample was presented in the training data of the target model or no.MIAs demonstrate that, under mild assumptions about the target model, it is possible to identify a part of its training dataset, leading to possible leakage of private data.To broaden the scope of practical applications of neural networks, it is important to have a reliable tool to assess their vulnerability to the leakage of private training data."
                        ],
                        "paper": {
                            "corpus_id": 269756904,
                            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2301151068",
                                    "name": "Andrey V. Galichin"
                                },
                                {
                                    "authorId": "134451470",
                                    "name": "Mikhail Aleksandrovich Pautov"
                                },
                                {
                                    "authorId": "2301152807",
                                    "name": "Alexey Zhavoronkin"
                                },
                                {
                                    "authorId": "2279751953",
                                    "name": "Oleg Y. Rogov"
                                },
                                {
                                    "authorId": "2257279038",
                                    "name": "Ivan Oseledets"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Pawelczyk et al., 2022)",
                        "snippets": [
                            "In this work, we address the aforementioned gaps by initiating a study of if and how an adversary can leverage algorithmic recourses to leak sensitive information about the training data of the underlying model. To this end, we introduce a general class of membership inference attacks called counterfactual distance-based attacks which leverage algorithmic recourse to determine if an instance belongs to the training data of the underlying model or not."
                        ],
                        "paper": {
                            "corpus_id": 253446930,
                            "title": "On the Privacy Risks of Algorithmic Recourse",
                            "authors": [
                                {
                                    "authorId": "89583148",
                                    "name": "Martin Pawelczyk"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                },
                                {
                                    "authorId": "5880154",
                                    "name": "Seth Neel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Artificial Intelligence and Statistics",
                            "n_citations": 31
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Hong et al., 2025)",
                        "snippets": [
                            "Membership inference attacks aim to determine whether a specific example is a member of the training data. To do this, the attacker exploits the difference in the target model's response to the specific example when it is a member versus a non-member. Membership inference can be considered a threat on its own, but a model's vulnerability to inference attacks also implies its potential to leak other private information outside of this context, an idea which aligns closely with the definition of differential privacy (Dwork, 2006) as a worst-case certification."
                        ],
                        "paper": {
                            "corpus_id": 275471364,
                            "title": "Understanding and Mitigating Membership Inference Risks of Neural Ordinary Differential Equations",
                            "authors": [
                                {
                                    "authorId": "2275287112",
                                    "name": "Sanghyun Hong"
                                },
                                {
                                    "authorId": "2327330670",
                                    "name": "Fan Wu"
                                },
                                {
                                    "authorId": "2303406541",
                                    "name": "Anthony Gruber"
                                },
                                {
                                    "authorId": "2265425696",
                                    "name": "Kookjin Lee"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Xia et al., 2024)",
                        "snippets": [
                            "Membership inference attacks (MIAs), which aim to determine whether a specific sample is part of the training dataset, pose a significant threat to federated learning."
                        ],
                        "paper": {
                            "corpus_id": 274869679,
                            "title": "Leveraging Multiple Adversarial Perturbation Distances for Enhanced Membership Inference Attack in Federated Learning",
                            "authors": [
                                {
                                    "authorId": "2336171446",
                                    "name": "Fan Xia"
                                },
                                {
                                    "authorId": "2336189090",
                                    "name": "Yuhao Liu"
                                },
                                {
                                    "authorId": "2336172334",
                                    "name": "Bo Jin"
                                },
                                {
                                    "authorId": "2297431838",
                                    "name": "Zheng Yu"
                                },
                                {
                                    "authorId": "2336264478",
                                    "name": "Xingwei Cai"
                                },
                                {
                                    "authorId": "2336144088",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "2318042338",
                                    "name": "Zhiyong Zha"
                                },
                                {
                                    "authorId": "2336109193",
                                    "name": "Dai Hou"
                                },
                                {
                                    "authorId": "2323202813",
                                    "name": "Kai Peng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Symmetry",
                            "n_citations": 1
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Hayes et al., 2017)",
                        "snippets": [
                            "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                        ],
                        "paper": {
                            "corpus_id": 52211986,
                            "title": "LOGAN: Membership Inference Attacks Against Generative Models",
                            "authors": [
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "145557680",
                                    "name": "Luca Melis"
                                },
                                {
                                    "authorId": "1722262",
                                    "name": "G. Danezis"
                                },
                                {
                                    "authorId": "1728207",
                                    "name": "Emiliano De Cristofaro"
                                }
                            ],
                            "year": 2017,
                            "venue": "Proceedings on Privacy Enhancing Technologies",
                            "n_citations": 515
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision."
                        ],
                        "paper": {
                            "corpus_id": 237563320,
                            "title": "Membership Leakage in Label-Only Exposures",
                            "authors": [
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 246
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Matsumoto et al., 2023)",
                        "snippets": [
                            "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model (Shokri et al., 2016)- (Carlini et al., 2021). A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models (Conti et al., 2022), (Li et al., 2022), (Hayes et al., 2017). There are two settings (Shokri et al., 2016), i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model."
                        ],
                        "paper": {
                            "corpus_id": 256627812,
                            "title": "Membership Inference Attacks against Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2166181346",
                                    "name": "Tomoya Matsumoto"
                                },
                                {
                                    "authorId": "2204651737",
                                    "name": "Takayuki Miura"
                                },
                                {
                                    "authorId": "34800838",
                                    "name": "Naoto Yanai"
                                }
                            ],
                            "year": 2023,
                            "venue": "2023 IEEE Security and Privacy Workshops (SPW)",
                            "n_citations": 60
                        },
                        "score": 0.9990234375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Attack Settings and Scenarios",
                "tldr": "Membership inference attacks can occur in various settings including black-box scenarios where attackers only access model outputs, white-box attacks where model parameters are available, and collaborative learning environments where semi-honest participants can exploit shared model information. (7 sources)",
                "text": "\nMembership inference attacks can be executed in several distinct settings, each with different adversarial capabilities and access levels:\n\n1. **Black-box Attack Setting**: In this most common scenario, the adversary only has access to the target model through a prediction API without knowledge of the model's architecture or parameters. The attacker queries the model with data samples and analyzes the prediction outputs to determine membership status <Paper corpusId=\"231861713\" paperTitle=\"(He et al._1, 2021)\" isShortName></Paper> <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. This setting is particularly relevant for Machine Learning as a Service (MLaaS) platforms where users can only interact with models through APIs.\n\n2. **White-box Attack Setting**: Here, the adversary has complete access to the target model's architecture and parameters. This provides significantly more information for the attack, allowing for analysis of internal model behavior beyond just the final predictions. White-box attacks can exploit vulnerabilities in training algorithms like stochastic gradient descent <Paper corpusId=\"231861713\" paperTitle=\"(He et al._1, 2021)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. These attacks are particularly powerful as they can detect subtle patterns in how the model processes different data points.\n\n3. **Collaborative/Federated Learning Setting**: In scenarios where multiple parties jointly train a model while keeping their data private, a semi-honest participant can perform membership inference attacks against other participants' data. The semi-honest party follows the protocol correctly but attempts to learn about others' training data <Paper corpusId=\"244809678\" paperTitle=\"(Shin et al., 2021)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Since all participants share model architecture and parameters, a semi-honest party can create ideal shadow models that closely mimic the target model's behavior.\n\n4. **Active vs. Passive Attacks**: Membership inference can be conducted passively by simply observing model outputs, or actively by manipulating inputs or training processes to extract more information. Active attackers in federated learning can even modify their contributions to enhance the success of membership inference attacks <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>.\n\n5. **Generative Model Setting**: Beyond classification models, membership inference attacks target generative sequence models that may memorize and potentially leak training data. These attacks are particularly concerning when such models are trained on sensitive data like private messages or personal information <Paper corpusId=\"170076423\" paperTitle=\"(Carlini et al., 2018)\" isShortName></Paper>.\n\n6. **Defense-aware Setting**: As defenses against membership inference have emerged, researchers have developed more sophisticated attacks designed to circumvent these protections. This creates an ongoing arms race between attack and defense mechanisms <Paper corpusId=\"49863840\" paperTitle=\"(Nasr et al._1, 2018)\" isShortName></Paper>.\n\n7. **Transfer Attack Setting**: In this scenario, the attacker doesn't have direct access to the target model but instead trains shadow models on data from a similar distribution to simulate the target model's behavior. This approach, pioneered by Shokri et al., allows for attacks even with limited knowledge about the target model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(He et al._1, 2021)",
                        "snippets": [
                            "Membership inference attacks aim at inferring membership of individual training samples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39]51]."
                        ],
                        "paper": {
                            "corpus_id": 231861713,
                            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "2054749404",
                                    "name": "Rui Wen"
                                },
                                {
                                    "authorId": "2127727861",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2117688523",
                                    "name": "Yun Shen"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 98
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Salem et al., 2018)",
                        "snippets": [
                            "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications.\n\nIn this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status."
                        ],
                        "paper": {
                            "corpus_id": 46933970,
                            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "144887171",
                                    "name": "Mathias Humbert"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                }
                            ],
                            "year": 2018,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 950
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shin et al., 2021)",
                        "snippets": [
                            "Similar to a reconstruction attack, the membership inference attack requires some model information such as model architecture and hyper-parameters. Therefore, a membership inference attack cannot be performed in usual situations. However, when we consider a semi-honest party in the training phase that is operating honestly but curious about others data, a membership inference attack can be performed. As shown in Figure 3, when three parties including one semi-honest party train a model, these parties share all information about the model except their own data and train the model when all parties agree with the model. In this scenario, the semi-honest party can gain access to all the information needed to perform a membership inference attack. After the training is completed, the semi-honest party can obtain extra data including some data similar to the data used in the training phase. As the semi-honest party already knows the model information, they can generate the most desirable shadow model, which reproduces the target model's behavior and performs an ideal membership inference attack. For example, when a semi-honest adversary performs Shokri et al.'s membership inference attack (Shokri et al., 2016), the adversary can construct the ideal shadow model because the model parameters and architecture are already shared to all parties. After constructing the shadow model, the adversary trains the shadow model and an attack model using their own data. Then, the adversary can analyze the classification result of arbitrary data to perform a membership inference attack. Furthermore, the membership inference attack performed by the adversary cannot be detected since there is no interaction with other parties."
                        ],
                        "paper": {
                            "corpus_id": 244809678,
                            "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
                            "authors": [
                                {
                                    "authorId": "2111246337",
                                    "name": "Jinmyeong Shin"
                                },
                                {
                                    "authorId": "2100675",
                                    "name": "Seok-Hwan Choi"
                                },
                                {
                                    "authorId": "2111227849",
                                    "name": "Yoon-Ho Choi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 4
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Carlini et al., 2018)",
                        "snippets": [
                            "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages."
                        ],
                        "paper": {
                            "corpus_id": 170076423,
                            "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2288725419",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2118484320",
                                    "name": "Chang Liu"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "36426383",
                                    "name": "Jernej Kos"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                }
                            ],
                            "year": 2018,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1148
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nasr et al._1, 2018)",
                        "snippets": [
                            "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                        ],
                        "paper": {
                            "corpus_id": 49863840,
                            "title": "Machine Learning with Membership Privacy using Adversarial Regularization",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 474
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.196206
    }
}