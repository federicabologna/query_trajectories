{
    "query": "What are the empirical differences in model accuracy between 4-bit and higher precision (such as 8-bit or 16-bit) quantization for deep learning models on NLP benchmarks?",
    "user_id": "lib_user",
    "task_id": "c7534673-ad39-402a-9882-0d44cd8d00ed",
    "timestamp": "2025-06-23T21:55:14.460501",
    "n_retrieval": 256,
    "n_retrieved": 259,
    "n_candidates": 41,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.563715,
    "decomposed_query": {
        "rewritten_query": "Empirical differences in model accuracy between 4-bit and higher precision (such as 8-bit or 16-bit) quantization for deep learning models on NLP benchmarks.",
        "keyword_query": "empirical differences model accuracy 4-bit 8-bit 16-bit quantization deep learning NLP benchmarks",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010242,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Quantized Neural Network Inference with Precision Batching",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 39,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.00822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347284",
                    "name": "Maximilian Lam"
                },
                {
                    "authorId": "1515569180",
                    "name": "Zachary Yedidia"
                },
                {
                    "authorId": "103876904",
                    "name": "Colby R. Banbury"
                },
                {
                    "authorId": "1805668",
                    "name": "V. Reddi"
                }
            ],
            "abstract": "We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.",
            "corpus_id": 211677681,
            "sentences": [
                {
                    "corpus_id": "211677681",
                    "title": "Quantized Neural Network Inference with Precision Batching",
                    "text": "During evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision. \n\nFor natural language inference, we train a model with a 1layer 3072 unit LSTM encoder and a 3-layer 3072 unit fully connected decoder (a larger version of that seen in (Bowman et al., 2015)). We train on the SNLI dataset (Bowman et al., 2015) for 10 epochs and reach a baseline accuracy of 78%. \n\nDuring evaluation of quantization on model accuracy, we uniformly quantize both the weights and activations of the LSTM encoder and the fully connected decoder to the target precisions. \n\nTable 2 shows model performance (accuracy for MNIST and natural language inference, perplexity for language modeling) for different weight and activations precisions. For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;Xu et al., 2018).",
                    "score": 0.6760109641609542,
                    "section_title": "Benefits of Higher Precision Activations",
                    "char_start_offset": 26530,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 214,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 508
                        },
                        {
                            "start": 511,
                            "end": 696
                        },
                        {
                            "start": 699,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1649
                        },
                        {
                            "start": 1652,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2093
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 382,
                            "end": 403,
                            "matchedPaperCorpusId": "14604520"
                        },
                        {
                            "start": 435,
                            "end": 456,
                            "matchedPaperCorpusId": "14604520"
                        },
                        {
                            "start": 2076,
                            "end": 2092,
                            "matchedPaperCorpusId": "8257350"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                },
                {
                    "corpus_id": "211677681",
                    "title": "Quantized Neural Network Inference with Precision Batching",
                    "text": "Next we show that using higher precision for activations leads to significantly better model accuracy at low bitwidths. We benchmark model accuracy across three applications: MNIST, language modeling and natural language inference. For each we train one baseline full precision model and evaluate the effects of various levels of weight and activation quantization on the model's end performance. For each model/application we quantize weights and activations to 1, 4, 8, 16 and 32 bits. \n\nFor the MNIST task (LeCun & Cortes, 2010), we train a 3-layer fully connected neural network with a hidden size of 4096 for 20 epochs, reaching a baseline accuracy of 98%. We uniformly quantize the weights and activations of each layer to the target precisions. Table 2. Benefits of using more precision for activations on model quality, evaluated on MNIST, language modeling (Wikitext-2) and natural language inference (SNLI). Generally, using higher level activations allows quantizing twice as many bits (e.g: from 8-bits to 4-bits) with little degradation of model accuracy. Note that for accuracy (acc), higher is better, whereas for perplexity (ppl) lower is better (the best score for each weight precision is bolded). \n\nFor language modeling, we train a model with a 1-layer 2048 unit LSTM (Hochreiter & Schmidhuber, 1997) as the encoder, and a 1-layer 2048 unit fully connected as the decoder (a common architecture used in language modeling (Melis et al., 2017)). We apply dropout with a factor of .5 to the inputs of the encoder LSTM's recurrence, and to the encoder LSTM's output. We train the model on the Wikitext-2 dataset (Merity et al., 2016) for 40 epochs, reaching a baseline perplexity of 93. During evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision.",
                    "score": 0.6160333945417501,
                    "section_title": "Benefits of Higher Precision Activations",
                    "char_start_offset": 24827,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 487
                        },
                        {
                            "start": 490,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1215
                        },
                        {
                            "start": 1218,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1702
                        },
                        {
                            "start": 1703,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1288,
                            "end": 1320,
                            "matchedPaperCorpusId": "1915014"
                        },
                        {
                            "start": 1441,
                            "end": 1461,
                            "matchedPaperCorpusId": "33513311"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Quantized Neural Network Inference with Precision Batching\n# Venue: arXiv.org\n# Authors: Maximilian Lam, Zachary Yedidia, Colby R. Banbury, V. Reddi\n## Abstract\nWe present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.\n## Benefits of Higher Precision Activations\nNext we show that using higher precision for activations leads to significantly better model accuracy at low bitwidths. We benchmark model accuracy across three applications: MNIST, language modeling and natural language inference. For each we train one baseline full precision model and evaluate the effects of various levels of weight and activation quantization on the model's end performance. For each model/application we quantize weights and activations to 1, 4, 8, 16 and 32 bits. \n\nFor the MNIST task (LeCun & Cortes, 2010), we train a 3-layer fully connected neural network with a hidden size of 4096 for 20 epochs, reaching a baseline accuracy of 98%. We uniformly quantize the weights and activations of each layer to the target precisions. Table 2. Benefits of using more precision for activations on model quality, evaluated on MNIST, language modeling (Wikitext-2) and natural language inference (SNLI). Generally, using higher level activations allows quantizing twice as many bits (e.g: from 8-bits to 4-bits) with little degradation of model accuracy. Note that for accuracy (acc), higher is better, whereas for perplexity (ppl) lower is better (the best score for each weight precision is bolded). \n\nFor language modeling, we train a model with a 1-layer 2048 unit LSTM (Hochreiter & Schmidhuber, 1997) as the encoder, and a 1-layer 2048 unit fully connected as the decoder (a common architecture used in language modeling (Melis et al., 2017)). We apply dropout with a factor of .5 to the inputs of the encoder LSTM's recurrence, and to the encoder LSTM's output. We train the model on the Wikitext-2 dataset (Merity et al., 2016) for 40 epochs, reaching a baseline perplexity of 93. During evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision.\n...\nDuring evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision. \n\nFor natural language inference, we train a model with a 1layer 3072 unit LSTM encoder and a 3-layer 3072 unit fully connected decoder (a larger version of that seen in (Bowman et al., 2015)). We train on the SNLI dataset (Bowman et al., 2015) for 10 epochs and reach a baseline accuracy of 78%. \n\nDuring evaluation of quantization on model accuracy, we uniformly quantize both the weights and activations of the LSTM encoder and the fully connected decoder to the target precisions. \n\nTable 2 shows model performance (accuracy for MNIST and natural language inference, perplexity for language modeling) for different weight and activations precisions. For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;Xu et al., 2018).",
            "reference_string": "[211677681 | Lam et al. | 2020 | Citations: 1]"
        },
        {
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 35,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.08072",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.08072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108129670",
                    "name": "Peiyu Liu"
                },
                {
                    "authorId": "2119618242",
                    "name": "Zikang Liu"
                },
                {
                    "authorId": "9136116",
                    "name": "Ze-Feng Gao"
                },
                {
                    "authorId": "2162036220",
                    "name": "Dawei Gao"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2110479359",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ],
            "abstract": "Despite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.",
            "corpus_id": 259937594,
            "sentences": [
                {
                    "corpus_id": "259937594",
                    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                    "text": "In this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
                    "score": 0.8143811390715305,
                    "section_title": "Results and Analysis",
                    "char_start_offset": 10205,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 81
                        },
                        {
                            "start": 84,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 977
                        },
                        {
                            "start": 978,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1704
                        },
                        {
                            "start": 1707,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1990
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                },
                {
                    "corpus_id": "259937594",
                    "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                    "text": "For the first question, we evaluate the LLaMA models at four sizes (i.e., 7B, 13B, 30B, and 65B), examining their performance across a range of precision levels: 2-bit, 4-bit, 8-bit, and 16-bit. Our experiments indicate that 4-bit precision yields the most favorable trade-off between model performance and memory footprint, achieving superior results with the same amount of allocated total bits. However, all models at different sizes suffer from a severe decline at 2-bit precision. \n\nRegarding the second question, we carefully examine the quantization sensitivity of different model components (or substructures), specifically attention and feed-forward networks (FFN). In our experiments, we find that FFN plays a crucial role in retaining the model performance for low-bit quantization. We also evaluated the effects of outlier dimensions, which are specific dimensions that exhibit significantly higher values compared to others in feature activations. We find the outlier dimensions affecting most Transformer layers are primarily responsible for the decline in the quantization performance, and they mainly concentrate on the down projections of FFN. These observa-tions motivate us to design more fine-grained substructure quantization strategies for improving the performance of low-bit models. \n\nFurthermore, we study how to enhance the performance of quantization models through finetuning. We evaluate the impacts of different finetuning methods executed before and after quantization. Our experimental results reveal that parameter-efficient fine-tuning after quantization can achieve commendable performance with significantly reduced computational resources. Our approach can fine-tune a 2-bit LLaMA-65B model on a single NVIDIA A100, surpassing the performance of a 16-bit LLaMA-13B model on zero-shot MMLU dataset.",
                    "score": 0.5584842295748702,
                    "section_title": "Introduction",
                    "char_start_offset": 3712,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 485
                        },
                        {
                            "start": 488,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1306
                        },
                        {
                            "start": 1309,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1500
                        },
                        {
                            "start": 1501,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1834
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84228515625
                }
            ],
            "relevance_judgement": 0.94970703125,
            "relevance_judgment_input_expanded": "# Title: Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-rong Wen\n## Abstract\nDespite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.\n## Introduction\nFor the first question, we evaluate the LLaMA models at four sizes (i.e., 7B, 13B, 30B, and 65B), examining their performance across a range of precision levels: 2-bit, 4-bit, 8-bit, and 16-bit. Our experiments indicate that 4-bit precision yields the most favorable trade-off between model performance and memory footprint, achieving superior results with the same amount of allocated total bits. However, all models at different sizes suffer from a severe decline at 2-bit precision. \n\nRegarding the second question, we carefully examine the quantization sensitivity of different model components (or substructures), specifically attention and feed-forward networks (FFN). In our experiments, we find that FFN plays a crucial role in retaining the model performance for low-bit quantization. We also evaluated the effects of outlier dimensions, which are specific dimensions that exhibit significantly higher values compared to others in feature activations. We find the outlier dimensions affecting most Transformer layers are primarily responsible for the decline in the quantization performance, and they mainly concentrate on the down projections of FFN. These observa-tions motivate us to design more fine-grained substructure quantization strategies for improving the performance of low-bit models. \n\nFurthermore, we study how to enhance the performance of quantization models through finetuning. We evaluate the impacts of different finetuning methods executed before and after quantization. Our experimental results reveal that parameter-efficient fine-tuning after quantization can achieve commendable performance with significantly reduced computational resources. Our approach can fine-tune a 2-bit LLaMA-65B model on a single NVIDIA A100, surpassing the performance of a 16-bit LLaMA-13B model on zero-shot MMLU dataset.\n\n## Results and Analysis\nIn this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
            "reference_string": "[259937594 | Liu et al. | 2023 | Citations: 35]"
        },
        {
            "title": "QReg: On Regularization Effects of Quantization",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 24,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2206.12372",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.12372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1610857529",
                    "name": "Mohammadhossein Askarihemmat"
                },
                {
                    "authorId": "7872299",
                    "name": "Reyhane Askari Hemmat"
                },
                {
                    "authorId": "2062267546",
                    "name": "Alexander Hoffman"
                },
                {
                    "authorId": "4481888",
                    "name": "I. Lazarevich"
                },
                {
                    "authorId": "1682819",
                    "name": "Ehsan Saboori"
                },
                {
                    "authorId": "3422889",
                    "name": "Olivier Mastropietro"
                },
                {
                    "authorId": "1756949",
                    "name": "Y. Savaria"
                },
                {
                    "authorId": "145719986",
                    "name": "J. David"
                }
            ],
            "abstract": "In this paper we study the effects of quantization in DNN training. We hypothesize that weight quantization is a form of regularization and the amount of regularization is correlated with the quantization level (precision). We con\ufb01rm our hypothesis by providing analytical study and empirical results. By modeling weight quantization as a form of additive noise to weights, we explore how this noise propagates through the network at training time. We then show that the magnitude of this noise is correlated with the level of quantization. To con\ufb01rm our analytical study, we performed an extensive list of experiments sum-marized in this paper in which we show that the regularization effects of quantization can be seen in various vision tasks and models, over various datasets. Based on our study, we propose that 8-bit quantization provides a reliable form of regularization in different vision tasks and models.",
            "corpus_id": 250048704,
            "sentences": [
                {
                    "corpus_id": "250048704",
                    "title": "QReg: On Regularization Effects of Quantization",
                    "text": "Figure 2 illustrates part of the experiments that we ran to test our hypothesis. Figure 2 shows results of nine different test configuration in nine squares. Each square consists of 20 cells, which as we discussed in Section 4.1.2, corresponds to the test accuracy difference between full precision model and quantized models when different augmentation is used. \n\nFrom left to right, each column shows the quantization levels (2-bit, 4-bit and 8-bit) that was used for our tests. From top to bottom, we have tested our hypothesis on Resnet20 on cifar10, Resnet18 on cifar100 and YOLOv5n on VOC dataset respectively. As it can be seen (i.e. the overall cell colors in the right column squares are green), regardless of model, dataset, augmentation or quantization technique, 8-bit quantized models generally have better generalization compared to full precision models. As we discussed in 2.3, we believe that this performance improvement corresponds to the regularization effect of quantizing the models. As we predicted, this regularization effect is correlated with the quantization level. Moving from left to right columns, the overall color codes for each cell changes from red to green. Which indicates that quantized models are generalizing better compared to their full precision counterparts as we use higher precision quantized models (from 2-bit to 8-bit). Regardless of the model, dataset, data augmentation or quantization technique, the generalization difference shrinks and even get worse as we use less precision for quantization. Another way to evaluate how quantization is helping with better generalization, is to compare the error of quantized and full precision models. Table 1 shows the performance improvement relative to model error. We averaged the accuracy of each square in Figure 2. We then, calculated the performance improvement relative to error using the following formula: \n\nWhere f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups.",
                    "score": 0.5106102336194183,
                    "section_title": "RESULTS",
                    "char_start_offset": 13149,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 362
                        },
                        {
                            "start": 365,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1192
                        },
                        {
                            "start": 1193,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1905
                        },
                        {
                            "start": 1908,
                            "end": 2030
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                },
                {
                    "corpus_id": "250048704",
                    "title": "QReg: On Regularization Effects of Quantization",
                    "text": "Deep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.",
                    "score": 0.5042140477623003,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1267
                        },
                        {
                            "start": 1270,
                            "end": 1394
                        },
                        {
                            "start": 1395,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.912109375
                },
                {
                    "corpus_id": "250048704",
                    "title": "QReg: On Regularization Effects of Quantization",
                    "text": "Where f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups. Since models on small datasets like cifar10 generally have small errors compared to models on more complex datasets (like cifar100 or VOC), we used a logarithm in Equation 5. Once again, according to Table 1, in all cases 8-bit quantized models reduce the error more (i.e, generalizing better) compared to full precision models. On all three models tested over 19 different augmentation setups, all 8-bit quantized models reduce the error when compared to full precision models (i.e last column for 8-bit quantization levels is green for all models). This effect (reducing error) is less clear as we use lower precision models . \n\nOur results presented in Figure 2, Table 1 and Appendix B empirically confirm our hypothesis. Unlike (Courbariaux et al., 2015), we believe that the regularization effect of quantization is in fact correlated with the quantization level. \n\nOur empirical study shows that moderate quantization (8bit) generally helps models generalize better. In addition to these results, specially for more complex models, we observe that quantized models overfit less at training time. \n\nFigure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model. This, confirms our hypothesis. For the full list of experiments, please refer to Appendix B.",
                    "score": 0.5813736031671899,
                    "section_title": "RESULTS",
                    "char_start_offset": 15057,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 751
                        },
                        {
                            "start": 754,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 991
                        },
                        {
                            "start": 994,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1574
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 855,
                            "end": 881,
                            "matchedPaperCorpusId": "1518846"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9111328125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: QReg: On Regularization Effects of Quantization\n# Venue: arXiv.org\n# Authors: Mohammadhossein Askarihemmat, Reyhane Askari Hemmat, Alexander Hoffman, I. Lazarevich, Ehsan Saboori, Olivier Mastropietro, Y. Savaria, J. David\n## Abstract\nIn this paper we study the effects of quantization in DNN training. We hypothesize that weight quantization is a form of regularization and the amount of regularization is correlated with the quantization level (precision). We con\ufb01rm our hypothesis by providing analytical study and empirical results. By modeling weight quantization as a form of additive noise to weights, we explore how this noise propagates through the network at training time. We then show that the magnitude of this noise is correlated with the level of quantization. To con\ufb01rm our analytical study, we performed an extensive list of experiments sum-marized in this paper in which we show that the regularization effects of quantization can be seen in various vision tasks and models, over various datasets. Based on our study, we propose that 8-bit quantization provides a reliable form of regularization in different vision tasks and models.\n## Introduction\nDeep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.\n\n## RESULTS\nFigure 2 illustrates part of the experiments that we ran to test our hypothesis. Figure 2 shows results of nine different test configuration in nine squares. Each square consists of 20 cells, which as we discussed in Section 4.1.2, corresponds to the test accuracy difference between full precision model and quantized models when different augmentation is used. \n\nFrom left to right, each column shows the quantization levels (2-bit, 4-bit and 8-bit) that was used for our tests. From top to bottom, we have tested our hypothesis on Resnet20 on cifar10, Resnet18 on cifar100 and YOLOv5n on VOC dataset respectively. As it can be seen (i.e. the overall cell colors in the right column squares are green), regardless of model, dataset, augmentation or quantization technique, 8-bit quantized models generally have better generalization compared to full precision models. As we discussed in 2.3, we believe that this performance improvement corresponds to the regularization effect of quantizing the models. As we predicted, this regularization effect is correlated with the quantization level. Moving from left to right columns, the overall color codes for each cell changes from red to green. Which indicates that quantized models are generalizing better compared to their full precision counterparts as we use higher precision quantized models (from 2-bit to 8-bit). Regardless of the model, dataset, data augmentation or quantization technique, the generalization difference shrinks and even get worse as we use less precision for quantization. Another way to evaluate how quantization is helping with better generalization, is to compare the error of quantized and full precision models. Table 1 shows the performance improvement relative to model error. We averaged the accuracy of each square in Figure 2. We then, calculated the performance improvement relative to error using the following formula: \n\nWhere f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups.\n...\nWhere f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups. Since models on small datasets like cifar10 generally have small errors compared to models on more complex datasets (like cifar100 or VOC), we used a logarithm in Equation 5. Once again, according to Table 1, in all cases 8-bit quantized models reduce the error more (i.e, generalizing better) compared to full precision models. On all three models tested over 19 different augmentation setups, all 8-bit quantized models reduce the error when compared to full precision models (i.e last column for 8-bit quantization levels is green for all models). This effect (reducing error) is less clear as we use lower precision models . \n\nOur results presented in Figure 2, Table 1 and Appendix B empirically confirm our hypothesis. Unlike (Courbariaux et al., 2015), we believe that the regularization effect of quantization is in fact correlated with the quantization level. \n\nOur empirical study shows that moderate quantization (8bit) generally helps models generalize better. In addition to these results, specially for more complex models, we observe that quantized models overfit less at training time. \n\nFigure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model. This, confirms our hypothesis. For the full list of experiments, please refer to Appendix B.",
            "reference_string": "[250048704 | Askarihemmat et al. | 2022 | Citations: 5]"
        },
        {
            "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 72,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.03675",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.03675, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1644333695",
                    "name": "Martin Schiemer"
                },
                {
                    "authorId": "1592769267",
                    "name": "Clemens J. S. Schaefer"
                },
                {
                    "authorId": "2163400188",
                    "name": "Jayden Parker Vap"
                },
                {
                    "authorId": "51184096",
                    "name": "M. Horeni"
                },
                {
                    "authorId": "2256185768",
                    "name": "Yu Emma Wang"
                },
                {
                    "authorId": "2255995570",
                    "name": "Juan Ye"
                },
                {
                    "authorId": "2254256505",
                    "name": "Siddharth Joshi"
                }
            ],
            "abstract": "Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting. We achieve less than 0.5% and 3% accuracy degradation while we quantize all matrix multiplications inputs down to 4-bits with 8-bit accumulators.",
            "corpus_id": 263671632,
            "sentences": [
                {
                    "corpus_id": "263671632",
                    "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
                    "text": "For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits. We observe that different degrees of quantization still result in similar learning over different tasks (similar change in accuracy per step). Again this confirms that HDQT reduces initial training accuracy, but minimally impacts CIL. \n\nThe strong similarity in the training curves at each incremental learning step suggests that despite initial accuracy differences, both the FP and HDQT models have similar learning performance. Yet, differences in their forgetting trajectories provide insight into changes in the model's representational quality at each incremental step. Further, by comparing the learning and forgetting patterns of both models, we can infer qualitative differences in the features learned through FQT. For instance, since both the HDQT and FP models exhibit the same change in accuracy between two tasks (correlation of 0.99-1.00), we infer that they learned at the same rate. To better anaylze their difference, we categorize forgetting behaviors into distinct scenarios. Scenario 1: the quantized model forgets more than the unquantized model, indicating that the quantized model has reached its representational limit. Scenario 2: the quantized model forgets less, indicating it has not yet reached its representational limit. We analyze forgetting curves in Fig. 3 to further understand how FQT on CIL algorithms impacts the representational limit.",
                    "score": 0.5583128466534205,
                    "section_title": "RESULTS",
                    "char_start_offset": 20191,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 916
                        },
                        {
                            "start": 919,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1581
                        },
                        {
                            "start": 1582,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1934
                        },
                        {
                            "start": 1935,
                            "end": 2057
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: Hadamard Domain Training with Integers for Class Incremental Quantized Learning\n# Venue: arXiv.org\n# Authors: Martin Schiemer, Clemens J. S. Schaefer, Jayden Parker Vap, M. Horeni, Yu Emma Wang, Juan Ye, Siddharth Joshi\n## Abstract\nContinual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. We demonstrate the effectiveness of our technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting. We achieve less than 0.5% and 3% accuracy degradation while we quantize all matrix multiplications inputs down to 4-bits with 8-bit accumulators.\n## RESULTS\nFor all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits. We observe that different degrees of quantization still result in similar learning over different tasks (similar change in accuracy per step). Again this confirms that HDQT reduces initial training accuracy, but minimally impacts CIL. \n\nThe strong similarity in the training curves at each incremental learning step suggests that despite initial accuracy differences, both the FP and HDQT models have similar learning performance. Yet, differences in their forgetting trajectories provide insight into changes in the model's representational quality at each incremental step. Further, by comparing the learning and forgetting patterns of both models, we can infer qualitative differences in the features learned through FQT. For instance, since both the HDQT and FP models exhibit the same change in accuracy between two tasks (correlation of 0.99-1.00), we infer that they learned at the same rate. To better anaylze their difference, we categorize forgetting behaviors into distinct scenarios. Scenario 1: the quantized model forgets more than the unquantized model, indicating that the quantized model has reached its representational limit. Scenario 2: the quantized model forgets less, indicating it has not yet reached its representational limit. We analyze forgetting curves in Fig. 3 to further understand how FQT on CIL algorithms impacts the representational limit.",
            "reference_string": "[263671632 | Schiemer et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 36,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2105.03536",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.03536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1938271",
                    "name": "AmirAli Abdolrashidi"
                },
                {
                    "authorId": "2108552747",
                    "name": "Lisa Wang"
                },
                {
                    "authorId": "3504647",
                    "name": "Shivani Agrawal"
                },
                {
                    "authorId": "3274291",
                    "name": "J. Malmaud"
                },
                {
                    "authorId": "145573927",
                    "name": "Oleg Rybakov"
                },
                {
                    "authorId": "108381331",
                    "name": "Chas Leichner"
                },
                {
                    "authorId": "2065594000",
                    "name": "Lukasz Lew"
                }
            ],
            "abstract": "Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.",
            "corpus_id": 234337594,
            "sentences": [
                {
                    "corpus_id": "234337594",
                    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                    "text": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
                    "score": 0.5413420394519166,
                    "section_title": "Conclusion",
                    "char_start_offset": 23412,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                },
                {
                    "corpus_id": "234337594",
                    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                    "text": "is the added engineering complexity introduced by quantization, e.g. some quantization techniques add hyperparameters for clipping bounds, which would require tuning as well. This motivates us to focus on approaches that offer clear benefits while minimizing the amount of added complexity.\n\nIn this work, using ResNet [13] as an example, we seek to understand how different quantization precisions affect the compute cost-accuracy tradeoff curves, and find a simple strategy to compress models at different compute cost and quality requirements.\n\nAfter running experiments on ResNet using different precisions and numbers of parameters, we determined that 4-bit and 8-bit models strongly Pareto-dominate bfloat16 models, and mostly-4-bit models outperform 8-bit models. We present our results using two compute cost models (linear and quadratic), based on different assumptions about the compute speedups when the number of bits are reduced. Both cost models will be defined and justified in Section 4. While no hardware with the quadratic cost model is available yet to our knowledge, our results provide strong motivation for the development of such hardware.",
                    "score": 0.5317361181319763,
                    "section_title": "Introduction",
                    "char_start_offset": 1964,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 319,
                            "end": 323,
                            "matchedPaperCorpusId": "206594692"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.802734375
                },
                {
                    "corpus_id": "234337594",
                    "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
                    "text": "While the primary focus of our work is to evaluate the impact of quantization on Pareto curves, we want to provide evidence for the competitiveness of our quantization method compared to prior work. Table 2 shows the top-1 accuracy of a ResNet50 model quantized using various methods, differing by goals and constraints, which we did our best to take into account as much as possible. As can be seen, our quantization method achieves or beats the results found in prior work on ResNet-50 quantization. E.g. our 4-bit model (with first and last layers in 8-bit) achieves a higher accuracy compared to results in the PACT paper [6], as well as the XILINX paper [33].\n\nOne may notice that even though all the results in Table 2 are obtained on ResNet50, they differ significantly in Top-1 accuracy, potentially due to differences in ResNet versions and hyperparameter choices. Therefore, we focus on quantization loss, i.e. the difference between Top-1 of unquantized and quantized model, as the main metric to evaluate quality of the quantization algorithms. In our case, both our 4-bit model (with first and last layers in 8-bit) and our fully 8-bit model outperform the bfloat16 baseline model, highlighting a regularizing effect of quantization. This regularizing effect can be clearly seen in the last three rows of Table 2. Differences in the generalization gap show that the unquantized model is overfitting significantly more to the training data than the quantized models do. We would like 3089 to point out that other works may differ slightly due to quantization of ops other than Conv2D or MatMul, BatchNorm folding, etc.. We did not attempt to catch all the differences.",
                    "score": 0.5061901136192006,
                    "section_title": "Comparison to Prior Work",
                    "char_start_offset": 17765,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79150390625
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n# Venue: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: AmirAli Abdolrashidi, Lisa Wang, Shivani Agrawal, J. Malmaud, Oleg Rybakov, Chas Leichner, Lukasz Lew\n## Abstract\nQuantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher ac-curacy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n## Introduction\nis the added engineering complexity introduced by quantization, e.g. some quantization techniques add hyperparameters for clipping bounds, which would require tuning as well. This motivates us to focus on approaches that offer clear benefits while minimizing the amount of added complexity.\n\nIn this work, using ResNet [13] as an example, we seek to understand how different quantization precisions affect the compute cost-accuracy tradeoff curves, and find a simple strategy to compress models at different compute cost and quality requirements.\n\nAfter running experiments on ResNet using different precisions and numbers of parameters, we determined that 4-bit and 8-bit models strongly Pareto-dominate bfloat16 models, and mostly-4-bit models outperform 8-bit models. We present our results using two compute cost models (linear and quadratic), based on different assumptions about the compute speedups when the number of bits are reduced. Both cost models will be defined and justified in Section 4. While no hardware with the quadratic cost model is available yet to our knowledge, our results provide strong motivation for the development of such hardware.\n\n## Comparison to Prior Work\nWhile the primary focus of our work is to evaluate the impact of quantization on Pareto curves, we want to provide evidence for the competitiveness of our quantization method compared to prior work. Table 2 shows the top-1 accuracy of a ResNet50 model quantized using various methods, differing by goals and constraints, which we did our best to take into account as much as possible. As can be seen, our quantization method achieves or beats the results found in prior work on ResNet-50 quantization. E.g. our 4-bit model (with first and last layers in 8-bit) achieves a higher accuracy compared to results in the PACT paper [6], as well as the XILINX paper [33].\n\nOne may notice that even though all the results in Table 2 are obtained on ResNet50, they differ significantly in Top-1 accuracy, potentially due to differences in ResNet versions and hyperparameter choices. Therefore, we focus on quantization loss, i.e. the difference between Top-1 of unquantized and quantized model, as the main metric to evaluate quality of the quantization algorithms. In our case, both our 4-bit model (with first and last layers in 8-bit) and our fully 8-bit model outperform the bfloat16 baseline model, highlighting a regularizing effect of quantization. This regularizing effect can be clearly seen in the last three rows of Table 2. Differences in the generalization gap show that the unquantized model is overfitting significantly more to the training data than the quantized models do. We would like 3089 to point out that other works may differ slightly due to quantization of ops other than Conv2D or MatMul, BatchNorm folding, etc.. We did not attempt to catch all the differences.\n\n## Conclusion\nIn this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
            "reference_string": "[234337594 | Abdolrashidi et al. | 2021 | Citations: 36]"
        },
        {
            "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/s25010083",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11722680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2206188178",
                    "name": "Tianheng Ling"
                },
                {
                    "authorId": "2055414584",
                    "name": "Chao Qian"
                },
                {
                    "authorId": "2337596562",
                    "name": "Theodor Mario Klann"
                },
                {
                    "authorId": "2290013217",
                    "name": "Julian Hoever"
                },
                {
                    "authorId": "2124687187",
                    "name": "Lukas Einhaus"
                },
                {
                    "authorId": "2267602521",
                    "name": "Gregor Schiele"
                }
            ],
            "abstract": "This study presents a comprehensive workflow for developing and deploying Multi-Layer Perceptron (MLP)-based soft sensors on embedded FPGAs, addressing diverse deployment objectives. The proposed workflow extends our prior research by introducing greater model adaptability. It supports various configurations\u2014spanning layer counts, neuron counts, and quantization bitwidths\u2014to accommodate the constraints and capabilities of different FPGA platforms. The workflow incorporates a custom-developed, open-source toolchain ElasticAI.Creator that facilitates quantization-aware training, integer-only inference, automated accelerator generation using VHDL templates, and synthesis alongside performance estimation. A case study on fluid flow estimation was conducted on two FPGA platforms: the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. For precision-focused and latency-sensitive deployments, a six-layer, 60-neuron MLP accelerator quantized to 8 bits on the XC7S15 achieved an MSE of 56.56, an MAPE of 1.61%, and an inference latency of 23.87 \u03bcs. Moreover, for low-power and energy-constrained deployments, a five-layer, 30-neuron MLP accelerator quantized to 8 bits on the iCE40UP5K achieved an inference latency of 83.37 \u03bcs, a power consumption of 2.06 mW, and an energy consumption of just 0.172 \u03bcJ per inference. These results confirm the workflow\u2019s ability to identify optimal FPGA accelerators tailored to specific deployment requirements, achieving a balanced trade-off between precision, inference latency, and energy efficiency.",
            "corpus_id": 275104652,
            "sentences": [
                {
                    "corpus_id": "275104652",
                    "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
                    "text": "In this experiment, we assess the effect of quantization on model precision, specifically focusing on 8-bit, 6-bit, and 4-bit quantization across the model configurations explored in Experiment 1. The analysis is conducted across three datasets (DS1, DS2, and DS3), with DS1 selected as the representative example due to similar trends observed across all datasets. \n\nFigure 11 illustrates the Test MSE distribution of quantized models with various configurations across different quantization bitwidths. The results highlight that bitwidth is the dominant factor influencing the performance of quantized models. Models quantized at 4-bit exhibit significantly higher Test MSE values and a broader distribution than those quantized at 6-bit or 8-bit, indicating a substantial loss introduced by lower bitwidths. As bitwidth increases, the Test MSE distribution narrows, particularly for 8-bit models, which achieve performance levels close to the FP32 benchmark. The neuron count significantly impacts the Test MSE under quantization. Models with a larger neuron count (e.g., 120 neurons) generally demonstrate better performance, with lower median Test MSE values and more centralized distributions. This improvement is particularly evident under 6-bit and 8-bit quantization, showing that larger models are better equipped to absorb quantization noise. However, under 4-bit quantization, the relationship becomes less predictable. While configurations with higher neuron count occasionally achieve better median precision, their overall distribution widens, indicating less stability and a reduced ability to absorb errors introduced by low bitwidth quantization consistently. \n\nThe layer count has a more mixed effect on performance. In deeper models (e.g., six or seven layers), the Test MSE distribution under 4-bit quantization becomes narrower, with lower medians than shallower models (e.g., four or five layers). This suggests that deeper models are better equipped to handle the errors introduced by quantization, leveraging their additional complexity to absorb and mitigate quantization noise. In contrast, shallower models exhibit broader and less consistent Test MSE distributions under lower bitwidths, underscoring their vulnerability to quantization-induced errors.",
                    "score": 0.5351835601043213,
                    "section_title": "Experiments 2: Quantized Models Analysis",
                    "char_start_offset": 36949,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 365
                        },
                        {
                            "start": 368,
                            "end": 504
                        },
                        {
                            "start": 505,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1432
                        },
                        {
                            "start": 1433,
                            "end": 1678
                        },
                        {
                            "start": 1681,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1921
                        },
                        {
                            "start": 1922,
                            "end": 2105
                        },
                        {
                            "start": 2106,
                            "end": 2282
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91015625
                }
            ],
            "relevance_judgement": 0.91015625,
            "relevance_judgment_input_expanded": "# Title: Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation\n# Venue: Italian National Conference on Sensors\n# Authors: Tianheng Ling, Chao Qian, Theodor Mario Klann, Julian Hoever, Lukas Einhaus, Gregor Schiele\n## Abstract\nThis study presents a comprehensive workflow for developing and deploying Multi-Layer Perceptron (MLP)-based soft sensors on embedded FPGAs, addressing diverse deployment objectives. The proposed workflow extends our prior research by introducing greater model adaptability. It supports various configurations\u2014spanning layer counts, neuron counts, and quantization bitwidths\u2014to accommodate the constraints and capabilities of different FPGA platforms. The workflow incorporates a custom-developed, open-source toolchain ElasticAI.Creator that facilitates quantization-aware training, integer-only inference, automated accelerator generation using VHDL templates, and synthesis alongside performance estimation. A case study on fluid flow estimation was conducted on two FPGA platforms: the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. For precision-focused and latency-sensitive deployments, a six-layer, 60-neuron MLP accelerator quantized to 8 bits on the XC7S15 achieved an MSE of 56.56, an MAPE of 1.61%, and an inference latency of 23.87 \u03bcs. Moreover, for low-power and energy-constrained deployments, a five-layer, 30-neuron MLP accelerator quantized to 8 bits on the iCE40UP5K achieved an inference latency of 83.37 \u03bcs, a power consumption of 2.06 mW, and an energy consumption of just 0.172 \u03bcJ per inference. These results confirm the workflow\u2019s ability to identify optimal FPGA accelerators tailored to specific deployment requirements, achieving a balanced trade-off between precision, inference latency, and energy efficiency.\n## Experiments 2: Quantized Models Analysis\nIn this experiment, we assess the effect of quantization on model precision, specifically focusing on 8-bit, 6-bit, and 4-bit quantization across the model configurations explored in Experiment 1. The analysis is conducted across three datasets (DS1, DS2, and DS3), with DS1 selected as the representative example due to similar trends observed across all datasets. \n\nFigure 11 illustrates the Test MSE distribution of quantized models with various configurations across different quantization bitwidths. The results highlight that bitwidth is the dominant factor influencing the performance of quantized models. Models quantized at 4-bit exhibit significantly higher Test MSE values and a broader distribution than those quantized at 6-bit or 8-bit, indicating a substantial loss introduced by lower bitwidths. As bitwidth increases, the Test MSE distribution narrows, particularly for 8-bit models, which achieve performance levels close to the FP32 benchmark. The neuron count significantly impacts the Test MSE under quantization. Models with a larger neuron count (e.g., 120 neurons) generally demonstrate better performance, with lower median Test MSE values and more centralized distributions. This improvement is particularly evident under 6-bit and 8-bit quantization, showing that larger models are better equipped to absorb quantization noise. However, under 4-bit quantization, the relationship becomes less predictable. While configurations with higher neuron count occasionally achieve better median precision, their overall distribution widens, indicating less stability and a reduced ability to absorb errors introduced by low bitwidth quantization consistently. \n\nThe layer count has a more mixed effect on performance. In deeper models (e.g., six or seven layers), the Test MSE distribution under 4-bit quantization becomes narrower, with lower medians than shallower models (e.g., four or five layers). This suggests that deeper models are better equipped to handle the errors introduced by quantization, leveraging their additional complexity to absorb and mitigate quantization noise. In contrast, shallower models exhibit broader and less consistent Test MSE distributions under lower bitwidths, underscoring their vulnerability to quantization-induced errors.",
            "reference_string": "[275104652 | Ling et al. | 2024 | Citations: 2]"
        },
        {
            "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 95,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211424900",
                    "name": "Saqib Javed"
                },
                {
                    "authorId": "2310800861",
                    "name": "Hieu Le"
                },
                {
                    "authorId": "2243289145",
                    "name": "Mathieu Salzmann"
                }
            ],
            "abstract": "Domain Generalization (DG) aims to train models that perform well not only on the training (source) domains but also on novel, unseen target data distributions. A key challenge in DG is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both theoretical insights and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Our extensive experiments demonstrate that QT-DoG generalizes across various datasets, architectures, and quantization algorithms, and can be combined with other DG methods, establishing its versatility and robustness.",
            "corpus_id": 273228873,
            "sentences": [
                {
                    "corpus_id": "273228873",
                    "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
                    "text": "Figure 4: Bit precision analysis for efficient quantization. We show results on out-of-domain test accuracy with two different datasets, i.e., PACS and TerraIncognita. For each bit precision, we report the increase in the test domain accuracy averaged across all domains. The 7-bit quantized model exhibits the maximum increase for both datasets. We quantize the model at 2000 steps. \n\nHere, we empirically analyze the effect of different bitprecisions for quantization on the generalization of the model. We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy.",
                    "score": 0.6132854721230616,
                    "section_title": "PACS (ERM Baseline) Terra Incognito (ERM Baseline)",
                    "char_start_offset": 25260,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1167
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 608,
                            "end": 625,
                            "matchedPaperCorpusId": "6037691"
                        },
                        {
                            "start": 645,
                            "end": 665,
                            "matchedPaperCorpusId": "49744838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: QT-DoG: Quantization-aware Training for Domain Generalization\n# Venue: arXiv.org\n# Authors: Saqib Javed, Hieu Le, Mathieu Salzmann\n## Abstract\nDomain Generalization (DG) aims to train models that perform well not only on the training (source) domains but also on novel, unseen target data distributions. A key challenge in DG is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both theoretical insights and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Our extensive experiments demonstrate that QT-DoG generalizes across various datasets, architectures, and quantization algorithms, and can be combined with other DG methods, establishing its versatility and robustness.\n## PACS (ERM Baseline) Terra Incognito (ERM Baseline)\nFigure 4: Bit precision analysis for efficient quantization. We show results on out-of-domain test accuracy with two different datasets, i.e., PACS and TerraIncognita. For each bit precision, we report the increase in the test domain accuracy averaged across all domains. The 7-bit quantized model exhibits the maximum increase for both datasets. We quantize the model at 2000 steps. \n\nHere, we empirically analyze the effect of different bitprecisions for quantization on the generalization of the model. We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy.",
            "reference_string": "[273228873 | Javed et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 97,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2229196235",
                    "name": "Leonardo Berti"
                },
                {
                    "authorId": "2325903265",
                    "name": "Flavio Giorgi"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                }
            ],
            "abstract": "Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.",
            "corpus_id": 276903421,
            "sentences": [
                {
                    "corpus_id": "276903421",
                    "title": "Emergent Abilities in Large Language Models: A Survey",
                    "text": "Liu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
                    "score": 0.5182505185436381,
                    "section_title": "D. The Impact of Quantization on Emergent Abilities",
                    "char_start_offset": 31198,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 645
                        },
                        {
                            "start": 648,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 822
                        },
                        {
                            "start": 823,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1838
                        },
                        {
                            "start": 1841,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2248
                        },
                        {
                            "start": 2249,
                            "end": 2384
                        },
                        {
                            "start": 2385,
                            "end": 2421
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: Emergent Abilities in Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Leonardo Berti, Flavio Giorgi, Gjergji Kasneci\n## Abstract\nLarge Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.\n## D. The Impact of Quantization on Emergent Abilities\nLiu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
            "reference_string": "[276903421 | Berti et al. | 2025 | Citations: 6]"
        },
        {
            "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 73,
            "citation_count": 37,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16775, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2184143149",
                    "name": "Renren Jin"
                },
                {
                    "authorId": "2287758280",
                    "name": "Jiangcun Du"
                },
                {
                    "authorId": "1588102980",
                    "name": "Wuwei Huang"
                },
                {
                    "authorId": "2257333016",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2257013742",
                    "name": "Jian Luan"
                },
                {
                    "authorId": "2257388949",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2263617513",
                    "name": "Deyi Xiong"
                }
            ],
            "abstract": "Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\&capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.",
            "corpus_id": 268032411,
            "sentences": [
                {
                    "corpus_id": "268032411",
                    "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
                    "text": "We have presented a comprehensive evaluation of quantization strategies for LLMs, demonstrating the trade-offs between model efficiency and performance degradation across various benchmarks. By employing a structured evaluation framework that assesses models in terms of knowledge & capacity, alignment, and efficiency, we aim to offer valuable insights into the scalability and practical application of quantized LLMs. Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower. Moreover, the results suggest that perplexity can be a reliable performance indicator for quantized LLMs on various evaluation benchmarks. SpQR effectively quantizes LLMs to an extreme level of 2 bits by isolating outlier weights and maintaining high precision during computation. When memory constraints exist and inference speed is a secondary concern, LLMs quantized to lower bit precision with a larger parameter scale can be preferred over smaller models. Additionally, we highlight the need for engineering effort and hardware support to efficiently deploy quantized LLMs in real-world scenarios.",
                    "score": 0.5110896201316686,
                    "section_title": "Conclusion",
                    "char_start_offset": 19055,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1230
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Evaluation of Quantization Strategies for Large Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong\n## Abstract\nIncreasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\&capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.\n## Conclusion\nWe have presented a comprehensive evaluation of quantization strategies for LLMs, demonstrating the trade-offs between model efficiency and performance degradation across various benchmarks. By employing a structured evaluation framework that assesses models in terms of knowledge & capacity, alignment, and efficiency, we aim to offer valuable insights into the scalability and practical application of quantized LLMs. Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower. Moreover, the results suggest that perplexity can be a reliable performance indicator for quantized LLMs on various evaluation benchmarks. SpQR effectively quantizes LLMs to an extreme level of 2 bits by isolating outlier weights and maintaining high precision during computation. When memory constraints exist and inference speed is a secondary concern, LLMs quantized to lower bit precision with a larger parameter scale can be preferred over smaller models. Additionally, we highlight the need for engineering effort and hardware support to efficiently deploy quantized LLMs in real-world scenarios.",
            "reference_string": "[268032411 | Jin et al. | 2024 | Citations: 37]"
        },
        {
            "title": "Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation",
            "venue": "IEEE Wireless Communications and Networking Conference",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2406.03402",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305216766",
                    "name": "Jinsheng Yuan"
                },
                {
                    "authorId": "98553648",
                    "name": "Zhuangkun Wei"
                },
                {
                    "authorId": "48544782",
                    "name": "Weisi Guo"
                }
            ],
            "abstract": "Over-the-Air Federated Learning (OTA-FL) is a privacy-preserving distributed learning mechanism, by aggregating updates in the electromagnetic channel rather than at the server. A critical research gap in existing OTA - FL research is the assumption of homogeneous client computational bit precision. While in real world application, clients with varying hardware resources may exploit approximate computing (AxC) to operate at different bit precisions optimized for energy and computational efficiency. Model updates with varying precisions among clients present a significant challenge for OTA - FL, as they are incompatible with the wireless modulation superposition process. Here, we propose an mixed-precision OTA-FL framework of clients with multiple bit precisions, demonstrating the following innovations: (i) the superior trade-off for both server and clients within the constraints of varying edge computing capabilities, energy efficiency, and learning accuracy requirements compared to homogeneous client bit precision, and (ii) a multi-precision gradient modulation scheme to ensure compatibility with OTA aggregation and eliminate the overheads of precision conversion. Through case study with real world data, we validate our modulation scheme that enables AxC based mixed-precision OTA-FL. In comparison to homogeneous standard precision of 32-bit and 16-bit, our framework presents more than 10% in 4-bit ultra low precision client performance and over 65 % and 13 % of energy savings respectively. This demonstrates the great potential of our mixed-precision OTA-FL approach in heterogeneous edge computing environments.",
            "corpus_id": 270257725,
            "sentences": [
                {
                    "corpus_id": "270257725",
                    "title": "Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation",
                    "text": "2) Federated Training and Server Performance: The convergence velocity, as depicted in Fig. 3, indicates that setups of uniform 4-bit clients, or a mixed-precision schema of [12,4,4] bits, exhibit slower and more erratic initial convergence, even when the latter has a better random start in training. In contrast, setups incorporating clients with 16-bit precision or higher demonstrate more rapid and stable convergence, achieving approximately 90% accuracy within 10 communication rounds. Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients. 3) Client Performance: After 100 rounds, the final global model,  100 , is broadcast to clients. Here, we focus on the clients at the lowest precision, 4-bit, for higher counterparts have better performance and minor degradation from well converged global model as illustrated in Table I and Fig. 3. \n\nAs shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision  clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy.",
                    "score": 0.5488811298368629,
                    "section_title": "B. Results",
                    "char_start_offset": 17537,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1929
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 178,
                            "end": 180,
                            "matchedPaperCorpusId": "246890627"
                        },
                        {
                            "start": 180,
                            "end": 182,
                            "matchedPaperCorpusId": "246890627"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89794921875
                }
            ],
            "relevance_judgement": 0.89794921875,
            "relevance_judgment_input_expanded": "# Title: Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation\n# Venue: IEEE Wireless Communications and Networking Conference\n# Authors: Jinsheng Yuan, Zhuangkun Wei, Weisi Guo\n## Abstract\nOver-the-Air Federated Learning (OTA-FL) is a privacy-preserving distributed learning mechanism, by aggregating updates in the electromagnetic channel rather than at the server. A critical research gap in existing OTA - FL research is the assumption of homogeneous client computational bit precision. While in real world application, clients with varying hardware resources may exploit approximate computing (AxC) to operate at different bit precisions optimized for energy and computational efficiency. Model updates with varying precisions among clients present a significant challenge for OTA - FL, as they are incompatible with the wireless modulation superposition process. Here, we propose an mixed-precision OTA-FL framework of clients with multiple bit precisions, demonstrating the following innovations: (i) the superior trade-off for both server and clients within the constraints of varying edge computing capabilities, energy efficiency, and learning accuracy requirements compared to homogeneous client bit precision, and (ii) a multi-precision gradient modulation scheme to ensure compatibility with OTA aggregation and eliminate the overheads of precision conversion. Through case study with real world data, we validate our modulation scheme that enables AxC based mixed-precision OTA-FL. In comparison to homogeneous standard precision of 32-bit and 16-bit, our framework presents more than 10% in 4-bit ultra low precision client performance and over 65 % and 13 % of energy savings respectively. This demonstrates the great potential of our mixed-precision OTA-FL approach in heterogeneous edge computing environments.\n## B. Results\n2) Federated Training and Server Performance: The convergence velocity, as depicted in Fig. 3, indicates that setups of uniform 4-bit clients, or a mixed-precision schema of [12,4,4] bits, exhibit slower and more erratic initial convergence, even when the latter has a better random start in training. In contrast, setups incorporating clients with 16-bit precision or higher demonstrate more rapid and stable convergence, achieving approximately 90% accuracy within 10 communication rounds. Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients. 3) Client Performance: After 100 rounds, the final global model,  100 , is broadcast to clients. Here, we focus on the clients at the lowest precision, 4-bit, for higher counterparts have better performance and minor degradation from well converged global model as illustrated in Table I and Fig. 3. \n\nAs shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision  clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy.",
            "reference_string": "[270257725 | Yuan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352948034",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2274202084",
                    "name": "Zuchao Li"
                },
                {
                    "authorId": "2269488794",
                    "name": "Lefei Zhang"
                },
                {
                    "authorId": "2343636012",
                    "name": "Gui-Song Xia"
                },
                {
                    "authorId": "2306994733",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2268745050",
                    "name": "Liangpei Zhang"
                },
                {
                    "authorId": "2275194788",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Large language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.",
            "corpus_id": 277452419,
            "sentences": [
                {
                    "corpus_id": "277452419",
                    "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                    "text": "Fig. 4: GGUF Progressive quantization. Different network architectures exhibit varying sensitivities to quantization. Certain layers, such as initial and output layers, are highly sensitive to quantization errors and require higher precision representation, whereas some intermediate layers, such as the Feedforward layers in Transformers, are more robust to low-precision quantization. For example, studies have shown that Attention layers in Transformers are particularly sensitive to precision degradation during quantization, while Feedforward layers maintain acceptable performance even after INT8 quantization. Similarly, in vision models, shallow convolutional kernels are more sensitive to quantization errors than deeper feature mappings [44].Further investigations from ZeroQuant-V2 reveal that activation quantization is generally more sensitive than weight quantization, especially in larger models. For models exceeding 10B parameters (e.g., OPT-66B [45] and BLOOM-176B [46]), activation quantization leads to more significant accuracy degradation compared to smaller models [47]. This phenomenon highlights the need for careful handling of activations during quantization in large-scale language models. \n\nOur experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5). This advantage amplifies at extreme compression (70%+), where quantization preserves functional coherence while pruning induces hemorrhage.",
                    "score": 0.5128573861718937,
                    "section_title": "2-bits 4-bits 16-bits",
                    "char_start_offset": 30817,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 38
                        },
                        {
                            "start": 39,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1217
                        },
                        {
                            "start": 1220,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1519
                        },
                        {
                            "start": 1520,
                            "end": 1645
                        },
                        {
                            "start": 1646,
                            "end": 1834
                        },
                        {
                            "start": 1835,
                            "end": 1974
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 747,
                            "end": 751,
                            "matchedPaperCorpusId": "249395624"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: Model Hemorrhage and the Robustness Limits of Large Language Models\n# Venue: arXiv.org\n# Authors: Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, Dacheng Tao\n## Abstract\nLarge language models (LLMs) demonstrate strong performance across natural language processing tasks, yet undergo significant performance degradation when modified for deployment through quantization, pruning, or decoding strategy adjustments. We define this phenomenon as model hemorrhage - performance decline caused by parameter alterations and architectural changes. Through systematic analysis of various LLM frameworks, we identify key vulnerability patterns: layer expansion frequently disrupts attention mechanisms, compression techniques induce information loss cascades, and decoding adjustments amplify prediction divergences. Our investigation reveals transformer architectures exhibit inherent robustness thresholds that determine hemorrhage severity across modification types. We propose three mitigation strategies: gradient-aware pruning preserves critical weight pathways, dynamic quantization scaling maintains activation integrity, and decoding calibration aligns generation trajectories with original model distributions. This work establishes foundational metrics for evaluating model stability during adaptation, providing practical guidelines for maintaining performance while enabling efficient LLM deployment. Our findings advance understanding of neural network resilience under architectural transformations, particularly for large-scale language models.\n## 2-bits 4-bits 16-bits\nFig. 4: GGUF Progressive quantization. Different network architectures exhibit varying sensitivities to quantization. Certain layers, such as initial and output layers, are highly sensitive to quantization errors and require higher precision representation, whereas some intermediate layers, such as the Feedforward layers in Transformers, are more robust to low-precision quantization. For example, studies have shown that Attention layers in Transformers are particularly sensitive to precision degradation during quantization, while Feedforward layers maintain acceptable performance even after INT8 quantization. Similarly, in vision models, shallow convolutional kernels are more sensitive to quantization errors than deeper feature mappings [44].Further investigations from ZeroQuant-V2 reveal that activation quantization is generally more sensitive than weight quantization, especially in larger models. For models exceeding 10B parameters (e.g., OPT-66B [45] and BLOOM-176B [46]), activation quantization leads to more significant accuracy degradation compared to smaller models [47]. This phenomenon highlights the need for careful handling of activations during quantization in large-scale language models. \n\nOur experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5). This advantage amplifies at extreme compression (70%+), where quantization preserves functional coherence while pruning induces hemorrhage.",
            "reference_string": "[277452419 | Ma et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Post-Training 4-bit Quantization on Embedding Tables",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 29,
            "citation_count": 34,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.02079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2055337890",
                    "name": "Hui Guan"
                },
                {
                    "authorId": "143682293",
                    "name": "Andrey Malevich"
                },
                {
                    "authorId": "2791531",
                    "name": "Jiyan Yang"
                },
                {
                    "authorId": "1686843",
                    "name": "Jongsoo Park"
                },
                {
                    "authorId": "30891915",
                    "name": "Hector Yuen"
                }
            ],
            "abstract": "Continuous representations have been widely adopted in recommender systems where a large number of entities are represented using embedding vectors. As the cardinality of the entities increases, the embedding components can easily contain millions of parameters and become the bottleneck in both storage and inference due to large memory consumption. This work focuses on post-training 4-bit quantization on the continuous embeddings. We propose row-wise uniform quantization with greedy search and codebook-based quantization that consistently outperforms state-of-the-art quantization approaches on reducing accuracy degradation. We deploy our uniform quantization technique on a production model in Facebook and demonstrate that it can reduce the model size to only 13.89% of the single-precision version while the model quality stays neutral.",
            "corpus_id": 207769430,
            "sentences": [
                {
                    "corpus_id": "207769430",
                    "title": "Post-Training 4-bit Quantization on Embedding Tables",
                    "text": "Post-training quantization is simple to use and convenient for rapid deployment. Recent studies have shown post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]. \n\nSeveral state-of-the-art post-training quantization techniques that rely on the clipping have been proposed to mitigate accuracy degradation. Shin et al. [24] and Sung et al. [25] approximated the inputs as a histogram and adopt a clipping threshold that minimizes the 2 norm of the quantization error. Migacz et al. [19] proposed an iterative approach to search for the clipping threshold based on Kullback-Leibler Divergence measure for quantizing activations. Later, Banner et al. [3] proposed ACIQ, an analytic solution that computes the optimal clip threshold by assuming the input values are sampled from a Gaussian or Laplacian distribution. Although these approaches are demonstrated to reduce the accuracy drops to some extent, the problem of post-training 4-bit quantization without accuracy drop is still unsolved yet. Empirically, we also observe that the above-mentioned approaches can result in significant accuracy drops when applied to embedding table quantization. \n\nIn this paper, we explore a variety of post-training 4-bit quantization methods on embedding tables and propose novel quantization approaches that can reduce model size while incurring negligible accuracy degradation. Quantization on embedding tables is usually applied to row vectors (row-wise quantization) to reduce quantization error. Throughout the paper, quantization is applied to row vectors unless noted differently. We notice that the prior post-training quantization approaches approximate the inputs to quantize using either a histogram or some distributions.",
                    "score": 0.4980271217964394,
                    "section_title": "Introduction",
                    "char_start_offset": 2011,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1215
                        },
                        {
                            "start": 1216,
                            "end": 1367
                        },
                        {
                            "start": 1370,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 540,
                            "end": 544,
                            "matchedPaperCorpusId": "7583752"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Post-Training 4-bit Quantization on Embedding Tables\n# Venue: arXiv.org\n# Authors: Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, Hector Yuen\n## Abstract\nContinuous representations have been widely adopted in recommender systems where a large number of entities are represented using embedding vectors. As the cardinality of the entities increases, the embedding components can easily contain millions of parameters and become the bottleneck in both storage and inference due to large memory consumption. This work focuses on post-training 4-bit quantization on the continuous embeddings. We propose row-wise uniform quantization with greedy search and codebook-based quantization that consistently outperforms state-of-the-art quantization approaches on reducing accuracy degradation. We deploy our uniform quantization technique on a production model in Facebook and demonstrate that it can reduce the model size to only 13.89% of the single-precision version while the model quality stays neutral.\n## Introduction\nPost-training quantization is simple to use and convenient for rapid deployment. Recent studies have shown post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]. \n\nSeveral state-of-the-art post-training quantization techniques that rely on the clipping have been proposed to mitigate accuracy degradation. Shin et al. [24] and Sung et al. [25] approximated the inputs as a histogram and adopt a clipping threshold that minimizes the 2 norm of the quantization error. Migacz et al. [19] proposed an iterative approach to search for the clipping threshold based on Kullback-Leibler Divergence measure for quantizing activations. Later, Banner et al. [3] proposed ACIQ, an analytic solution that computes the optimal clip threshold by assuming the input values are sampled from a Gaussian or Laplacian distribution. Although these approaches are demonstrated to reduce the accuracy drops to some extent, the problem of post-training 4-bit quantization without accuracy drop is still unsolved yet. Empirically, we also observe that the above-mentioned approaches can result in significant accuracy drops when applied to embedding table quantization. \n\nIn this paper, we explore a variety of post-training 4-bit quantization methods on embedding tables and propose novel quantization approaches that can reduce model size while incurring negligible accuracy degradation. Quantization on embedding tables is usually applied to row vectors (row-wise quantization) to reduce quantization error. Throughout the paper, quantization is applied to row vectors unless noted differently. We notice that the prior post-training quantization approaches approximate the inputs to quantize using either a histogram or some distributions.",
            "reference_string": "[207769430 | Guan et al. | 2019 | Citations: 34]"
        },
        {
            "title": "Model compression via distillation and quantization",
            "venue": "International Conference on Learning Representations",
            "year": 2018,
            "reference_count": 43,
            "citation_count": 732,
            "influential_citation_count": 42,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "36060478",
                    "name": "A. Polino"
                },
                {
                    "authorId": "1996134",
                    "name": "Razvan Pascanu"
                },
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                }
            ],
            "abstract": "Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.",
            "corpus_id": 3323727,
            "sentences": [
                {
                    "corpus_id": "3323727",
                    "title": "Model compression via distillation and quantization",
                    "text": "In terms of size, this model is more than 2\u00d7 smaller than ResNet18 (but has higher accuracy), and is 4\u00d7 smaller than ResNet34, and about 1.5\u00d7 faster on inference, as it has fewer layers. This is state-of-the-art for 4bit models with 18 layers; to our knowledge, no such model has been able to surpass the accuracy of ResNet18. \n\nWe re-iterated this experiment using a 4-bit quantized 2xResNet34 student transferring from a ResNet50 full-precision teacher. We obtain a 4-bit quantized student of almost the same accuracy, which is 50% shallower and has a 2.5\u00d7 smaller size. One key question we are interested in is whether distillation loss is a consistently better metric when quantizing, compared to standard loss. We tested this for CIFAR-10, comparing the performance of quantized training with respect to each loss. At 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss. This strongly suggests that distillation loss is superior when quantizing. For details, see Section A.4.1 in the Appendix. \n\nImpact of Heuristics on Differentiable Quantization. We also performed an in-depth study of how the various heuristics impact accuracy. We found that, for differentiable quantization, redistributing bits according to the gradient norm of the layers is absolutely essential for good accuracy; quantiles and distillation loss also seem to provide an improvement, albeit smaller. Due to space constraints, we defer the results and their discussion to Section A.4.2 of the Appendix. \n\nInference Speed.",
                    "score": 0.5073581656222779,
                    "section_title": "LARGER DATASETS",
                    "char_start_offset": 26005,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 326
                        },
                        {
                            "start": 329,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1417
                        },
                        {
                            "start": 1420,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1898
                        },
                        {
                            "start": 1901,
                            "end": 1917
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.880859375
                },
                {
                    "corpus_id": "3323727",
                    "title": "Model compression via distillation and quantization",
                    "text": "Table 1 contains the results for full-precision training, PM quantization with and without bucketing, as well as our methods. The percentages on the left below the student models definition are the accuracy of the normal and the distilled model respectively (trained with full precision). More details are reported in table 11 in the appendix. We also tried an additional model where the student is deeper than the teacher, where we obtained that the student quantized to 4 bits is able to achieve significantly better accuracy than the teacher, with a compression factor of more than 7\u00d7. \n\nWe performed additional experiments for differentiable quantization using a wide residual network (Zagoruyko & Komodakis, 2016) that gets to higher accuracies; see table 3. \n\nOverall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization. Differentiable quantization is a close second on all experiments, but it has much faster convergence. Further, we highlight the good accuracy of the much simpler PM quantization method with bucketing at higher bit width (4 and 8 bits). \n\nCIFAR-100 Experiments. Next, we perform image classification with the full 100 classes. Here, we focus on 2bit and 4bit quantization, and on a single student architecture. The baseline architecture is a wide residual network with 28 layers, and 36.5M parameters, which is state-of-the-art for its The results confirm the trend from the previous dataset, with distilled and differential quantization preserving accuracy within less than 1% at 4bit precision. However, we note that accuracy loss is catastrophic at 2bit precision, probably because of reduced model capacity. We note that differentiable quantization is able to best recover accuracy for this harder task. \n\nOpenNMT Experiments. The OpenNMT integration test dataset (Ope) consists of 200K train sentences and 10K test sentences for a German-English translation task.",
                    "score": 0.5767109241291286,
                    "section_title": "COMPRESSION",
                    "char_start_offset": 20471,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 588
                        },
                        {
                            "start": 591,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1338
                        },
                        {
                            "start": 1341,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2009
                        },
                        {
                            "start": 2012,
                            "end": 2032
                        },
                        {
                            "start": 2033,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85205078125
                }
            ],
            "relevance_judgement": 0.880859375,
            "relevance_judgment_input_expanded": "# Title: Model compression via distillation and quantization\n# Venue: International Conference on Learning Representations\n# Authors: A. Polino, Razvan Pascanu, Dan Alistarh\n## Abstract\nDeep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n## COMPRESSION\nTable 1 contains the results for full-precision training, PM quantization with and without bucketing, as well as our methods. The percentages on the left below the student models definition are the accuracy of the normal and the distilled model respectively (trained with full precision). More details are reported in table 11 in the appendix. We also tried an additional model where the student is deeper than the teacher, where we obtained that the student quantized to 4 bits is able to achieve significantly better accuracy than the teacher, with a compression factor of more than 7\u00d7. \n\nWe performed additional experiments for differentiable quantization using a wide residual network (Zagoruyko & Komodakis, 2016) that gets to higher accuracies; see table 3. \n\nOverall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization. Differentiable quantization is a close second on all experiments, but it has much faster convergence. Further, we highlight the good accuracy of the much simpler PM quantization method with bucketing at higher bit width (4 and 8 bits). \n\nCIFAR-100 Experiments. Next, we perform image classification with the full 100 classes. Here, we focus on 2bit and 4bit quantization, and on a single student architecture. The baseline architecture is a wide residual network with 28 layers, and 36.5M parameters, which is state-of-the-art for its The results confirm the trend from the previous dataset, with distilled and differential quantization preserving accuracy within less than 1% at 4bit precision. However, we note that accuracy loss is catastrophic at 2bit precision, probably because of reduced model capacity. We note that differentiable quantization is able to best recover accuracy for this harder task. \n\nOpenNMT Experiments. The OpenNMT integration test dataset (Ope) consists of 200K train sentences and 10K test sentences for a German-English translation task.\n\n## LARGER DATASETS\nIn terms of size, this model is more than 2\u00d7 smaller than ResNet18 (but has higher accuracy), and is 4\u00d7 smaller than ResNet34, and about 1.5\u00d7 faster on inference, as it has fewer layers. This is state-of-the-art for 4bit models with 18 layers; to our knowledge, no such model has been able to surpass the accuracy of ResNet18. \n\nWe re-iterated this experiment using a 4-bit quantized 2xResNet34 student transferring from a ResNet50 full-precision teacher. We obtain a 4-bit quantized student of almost the same accuracy, which is 50% shallower and has a 2.5\u00d7 smaller size. One key question we are interested in is whether distillation loss is a consistently better metric when quantizing, compared to standard loss. We tested this for CIFAR-10, comparing the performance of quantized training with respect to each loss. At 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss. This strongly suggests that distillation loss is superior when quantizing. For details, see Section A.4.1 in the Appendix. \n\nImpact of Heuristics on Differentiable Quantization. We also performed an in-depth study of how the various heuristics impact accuracy. We found that, for differentiable quantization, redistributing bits according to the gradient norm of the layers is absolutely essential for good accuracy; quantiles and distillation loss also seem to provide an improvement, albeit smaller. Due to space constraints, we defer the results and their discussion to Section A.4.2 of the Appendix. \n\nInference Speed.",
            "reference_string": "[3323727 | Polino et al. | 2018 | Citations: 732]"
        },
        {
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 31,
            "citation_count": 94,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.04191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46571359",
                    "name": "J. McKinstry"
                },
                {
                    "authorId": "2357931",
                    "name": "S. K. Esser"
                },
                {
                    "authorId": "2730753",
                    "name": "R. Appuswamy"
                },
                {
                    "authorId": "2064431971",
                    "name": "Deepika Bablani"
                },
                {
                    "authorId": "2248110488",
                    "name": "John V. Arthur"
                },
                {
                    "authorId": "3121907",
                    "name": "Izzet B. Yildiz"
                },
                {
                    "authorId": "1944330",
                    "name": "D. Modha"
                }
            ],
            "abstract": "To realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. \nWe find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.",
            "corpus_id": 52197199,
            "sentences": [
                {
                    "corpus_id": "52197199",
                    "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                    "text": "Fine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
                    "score": 0.5227735788850375,
                    "section_title": "Experiments",
                    "char_start_offset": 12749,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 675
                        },
                        {
                            "start": 678,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1655
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 825,
                            "end": 843,
                            "matchedPaperCorpusId": "195347490"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87548828125
                }
            ],
            "relevance_judgement": 0.87548828125,
            "relevance_judgment_input_expanded": "# Title: Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference\n# Venue: arXiv.org\n# Authors: J. McKinstry, S. K. Esser, R. Appuswamy, Deepika Bablani, John V. Arthur, Izzet B. Yildiz, D. Modha\n## Abstract\nTo realize the promise of ubiquitous embedded deep network inference, it is essential to seek limits of energy and area efficiency. To this end, low-precision networks offer tremendous promise because both energy and area scale down quadratically with the reduction in precision. Here we demonstrate ResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on the ImageNet classification benchmark that, at 8-bit precision exceed the accuracy of the full-precision baseline networks after one epoch of finetuning, thereby leveraging the availability of pretrained models. We also demonstrate ResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match the accuracy of the full-precision baseline networks -- the highest scores to date. Surprisingly, the weights of the low-precision networks are very close (in cosine similarity) to the weights of the corresponding baseline networks, making training from scratch unnecessary. \nWe find that gradient noise due to quantization during training increases with reduced precision, and seek ways to overcome this noise. The number of iterations required by SGD to achieve a given training error is related to the square of (a) the distance of the initial solution from the final plus (b) the maximum variance of the gradient estimates. Therefore, we (a) reduce solution distance by starting with pretrained fp32 precision baseline networks and fine-tuning, and (b) combat gradient noise introduced by quantization by training longer and reducing learning rates. Sensitivity analysis indicates that these simple techniques, coupled with proper activation function range calibration to take full advantage of the limited precision, are sufficient to discover low-precision networks, if they exist, close to fp32 precision baseline networks. The results herein provide evidence that 4-bits suffice for classification.\n## Experiments\nFine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
            "reference_string": "[52197199 | McKinstry et al. | 2018 | Citations: 94]"
        },
        {
            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 35,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49476045",
                    "name": "Jangho Kim"
                },
                {
                    "authorId": "1713608836",
                    "name": "Kiyoon Yoo"
                },
                {
                    "authorId": "3160425",
                    "name": "Nojun Kwak"
                }
            ],
            "abstract": "We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is very useful in model compression domains such as quantization and sparse training. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and Imagenet datasets show the effectiveness of the proposed PSG in both domains of sparse training and quantization even for extremely low bits.",
            "corpus_id": 218862856,
            "sentences": [
                {
                    "corpus_id": "218862856",
                    "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                    "text": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
                    "score": 0.5555345889503472,
                    "section_title": "Method",
                    "char_start_offset": 20351,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1359
                        },
                        {
                            "start": 1362,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1733
                        },
                        {
                            "start": 1734,
                            "end": 1853
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 429,
                            "end": 432,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 546,
                            "end": 549,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 1273,
                            "end": 1276,
                            "matchedPaperCorpusId": "211724357"
                        },
                        {
                            "start": 1728,
                            "end": 1732,
                            "matchedPaperCorpusId": "184487878"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                }
            ],
            "relevance_judgement": 0.86865234375,
            "relevance_judgment_input_expanded": "# Title: Position-based Scaled Gradient for Model Quantization and Sparse Training\n# Venue: arXiv.org\n# Authors: Jangho Kim, Kiyoon Yoo, Nojun Kwak\n## Abstract\nWe propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to a weight vector is very useful in model compression domains such as quantization and sparse training. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and Imagenet datasets show the effectiveness of the proposed PSG in both domains of sparse training and quantization even for extremely low bits.\n## Method\nOn the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
            "reference_string": "[218862856 | Kim et al. | 2020 | Citations: 7]"
        },
        {
            "title": "Generative AI on the Edge: Architecture and Performance Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 14,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.17712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "80998059",
                    "name": "Zeinab Nezami"
                },
                {
                    "authorId": "2290529852",
                    "name": "Maryam Hafeez"
                },
                {
                    "authorId": "2332536758",
                    "name": "Karim Djemame"
                },
                {
                    "authorId": "2285125525",
                    "name": "S. A. R. Zaidi"
                }
            ],
            "abstract": "6G's AI native vision of embedding advance intelligence in the network while bringing it closer to the user requires a systematic evaluation of Generative AI (GenAI) models on edge devices. Rapidly emerging solutions based on Open RAN (ORAN) and Network-in-a-Box strongly advocate the use of low-cost, off-the-shelf components for simpler and efficient deployment, e.g., in provisioning rural connectivity. In this context, conceptual architecture, hardware testbeds and precise performance quantification of Large Language Models (LLMs) on off-the-shelf edge devices remains largely unexplored. This research investigates computationally demanding LLM inference on a single commodity Raspberry Pi serving as an edge testbed for ORAN. We investigate various LLMs, including small, medium and large models, on a Raspberry Pi 5 Cluster using a lightweight Kubernetes distribution (K3s) with modular prompting implementation. We study its feasibility and limitations by analyzing throughput, latency, accuracy and efficiency. Our findings indicate that CPU-only deployment of lightweight models, such as Yi, Phi, and Llama3, can effectively support edge applications, achieving a generation throughput of 5 to 12 tokens per second with less than 50\\% CPU and RAM usage. We conclude that GenAI on the edge offers localized inference in remote or bandwidth-constrained environments in 6G networks without reliance on cloud infrastructure.",
            "corpus_id": 274305769,
            "sentences": [
                {
                    "corpus_id": "274305769",
                    "title": "Generative AI on the Edge: Architecture and Performance Evaluation",
                    "text": "The models studied in this research did not encounter outof-memory (OOM) errors or hardware restarts during cluster deployment, demonstrating robustness and stability in handling large workloads. While quantization effectively reduces memory usage and accelerates inference, it often sacrifices accuracy, especially at lower precision levels (e.g., 4 bits). \n\nTo evaluate accuracy, we use the Winogrande benchmark dataset 15 , designed for common-sense reasoning, and the LM-Evaluation Harness [16] framework with its inference server to serve our quantized models. The dataset includes Natural Language Inference (NLI) tasks, where models select the answer with the highest log likelihood, which is then compared to the correct label. Model accuracy on Winogrande benchmark varies significantly, with InternLM leading at 0.8, followed by Gemma at 0.7, and Llama3 at 0.69. Mistral, Llama2, and Phi all share an accuracy of 0.68. Yi and Zephyr score lower at 0.49 and 0.46, respectively. These differences largely stem from model architecture and size [5]. It is important to note that we evaluate the non-fine-tuned variants of the models, which typically serve as a proxy for the accuracy degradation in downstream models [5]. We selected pre-trained models rather than fine-tuned versions due to the alignment in fine-tuning processes, which often leads to performance enhancements that are not directly comparable to the base models.",
                    "score": 0.5071789972251938,
                    "section_title": "C. Deployment Stability and Accuracy",
                    "char_start_offset": 16969,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 357
                        },
                        {
                            "start": 360,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1436
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86474609375
                }
            ],
            "relevance_judgement": 0.86474609375,
            "relevance_judgment_input_expanded": "# Title: Generative AI on the Edge: Architecture and Performance Evaluation\n# Venue: arXiv.org\n# Authors: Zeinab Nezami, Maryam Hafeez, Karim Djemame, S. A. R. Zaidi\n## Abstract\n6G's AI native vision of embedding advance intelligence in the network while bringing it closer to the user requires a systematic evaluation of Generative AI (GenAI) models on edge devices. Rapidly emerging solutions based on Open RAN (ORAN) and Network-in-a-Box strongly advocate the use of low-cost, off-the-shelf components for simpler and efficient deployment, e.g., in provisioning rural connectivity. In this context, conceptual architecture, hardware testbeds and precise performance quantification of Large Language Models (LLMs) on off-the-shelf edge devices remains largely unexplored. This research investigates computationally demanding LLM inference on a single commodity Raspberry Pi serving as an edge testbed for ORAN. We investigate various LLMs, including small, medium and large models, on a Raspberry Pi 5 Cluster using a lightweight Kubernetes distribution (K3s) with modular prompting implementation. We study its feasibility and limitations by analyzing throughput, latency, accuracy and efficiency. Our findings indicate that CPU-only deployment of lightweight models, such as Yi, Phi, and Llama3, can effectively support edge applications, achieving a generation throughput of 5 to 12 tokens per second with less than 50\\% CPU and RAM usage. We conclude that GenAI on the edge offers localized inference in remote or bandwidth-constrained environments in 6G networks without reliance on cloud infrastructure.\n## C. Deployment Stability and Accuracy\nThe models studied in this research did not encounter outof-memory (OOM) errors or hardware restarts during cluster deployment, demonstrating robustness and stability in handling large workloads. While quantization effectively reduces memory usage and accelerates inference, it often sacrifices accuracy, especially at lower precision levels (e.g., 4 bits). \n\nTo evaluate accuracy, we use the Winogrande benchmark dataset 15 , designed for common-sense reasoning, and the LM-Evaluation Harness [16] framework with its inference server to serve our quantized models. The dataset includes Natural Language Inference (NLI) tasks, where models select the answer with the highest log likelihood, which is then compared to the correct label. Model accuracy on Winogrande benchmark varies significantly, with InternLM leading at 0.8, followed by Gemma at 0.7, and Llama3 at 0.69. Mistral, Llama2, and Phi all share an accuracy of 0.68. Yi and Zephyr score lower at 0.49 and 0.46, respectively. These differences largely stem from model architecture and size [5]. It is important to note that we evaluate the non-fine-tuned variants of the models, which typically serve as a proxy for the accuracy degradation in downstream models [5]. We selected pre-trained models rather than fine-tuned versions due to the alignment in fine-tuning processes, which often leads to performance enhancements that are not directly comparable to the base models.",
            "reference_string": "[274305769 | Nezami et al. | 2024 | Citations: 3]"
        },
        {
            "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 59,
            "citation_count": 227,
            "influential_citation_count": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.acl-long.334.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9583912",
                    "name": "Haoli Bai"
                },
                {
                    "authorId": "2155468228",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "48557122",
                    "name": "Lu Hou"
                },
                {
                    "authorId": "50812138",
                    "name": "Lifeng Shang"
                },
                {
                    "authorId": "2115757711",
                    "name": "Jing Jin"
                },
                {
                    "authorId": "145820291",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "1785083",
                    "name": "Michael R. Lyu"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ],
            "abstract": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released.",
            "corpus_id": 229923538,
            "sentences": [
                {
                    "corpus_id": "229923538",
                    "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
                    "text": "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This  is similar to the findings in Figure 1.",
                    "score": 0.5245855250236388,
                    "section_title": "C.1 Performance Drop by Binarization",
                    "char_start_offset": 29617,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83349609375
                }
            ],
            "relevance_judgement": 0.83349609375,
            "relevance_judgment_input_expanded": "# Title: BinaryBERT: Pushing the Limit of BERT Quantization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael R. Lyu, Irwin King\n## Abstract\nThe rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released.\n## C.1 Performance Drop by Binarization\nHere we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This  is similar to the findings in Figure 1.",
            "reference_string": "[229923538 | Bai et al. | 2020 | Citations: 227]"
        },
        {
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 42,
            "citation_count": 1021,
            "influential_citation_count": 115,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.08342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2065915235",
                    "name": "Raghuraman Krishnamoorthi"
                }
            ],
            "abstract": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
            "corpus_id": 49356451,
            "sentences": [
                {
                    "corpus_id": "49356451",
                    "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                    "text": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
                    "score": 0.5976333828150053,
                    "section_title": "Lower Precision Networks",
                    "char_start_offset": 19489,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 324
                        },
                        {
                            "start": 327,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2127
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82666015625
                },
                {
                    "corpus_id": "49356451",
                    "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                    "text": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
                    "score": 0.5329473343539133,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75341796875
                }
            ],
            "relevance_judgement": 0.82666015625,
            "relevance_judgment_input_expanded": "# Title: Quantizing deep convolutional networks for efficient inference: A whitepaper\n# Venue: arXiv.org\n# Authors: Raghuraman Krishnamoorthi\n## Abstract\nWe present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.\n## Lower Precision Networks\nWe note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
            "reference_string": "[49356451 | Krishnamoorthi | 2018 | Citations: 1021]"
        },
        {
            "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15799, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346980509",
                    "name": "Artyom Kharinaev"
                },
                {
                    "authorId": "2291142916",
                    "name": "Viktor Moskvoretskii"
                },
                {
                    "authorId": "2183482391",
                    "name": "Egor Shvetsov"
                },
                {
                    "authorId": "2334636989",
                    "name": "Kseniia Studenikina"
                },
                {
                    "authorId": "2346982876",
                    "name": "Bykov Mikhail"
                },
                {
                    "authorId": "2257279157",
                    "name": "E. Burnaev"
                }
            ],
            "abstract": "Large Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.",
            "corpus_id": 276575417,
            "sentences": [
                {
                    "corpus_id": "276575417",
                    "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
                    "text": "In this section we discuss obtained critical insights into the effects of quantization bit-range, methods, safety benchmarks, and model architectures on the safety and trustworthiness of language models. Below, we summarize the key implications: \n\n6.1 Quantization Bit-Range \u2022 Int4 Precision: At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral. \n\n\u2022 Int2 Precision: Reducing precision to 2 bits introduces more pronounced performance declines, particularly for LLaMA. Vector quantization methods like AQLM demonstrate greater stability compared to scalar methods like QUIP#, which struggles with maintaining safety and factual accuracy.",
                    "score": 0.5330188078774735,
                    "section_title": "Discussion",
                    "char_start_offset": 19046,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 245
                        },
                        {
                            "start": 248,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 608
                        },
                        {
                            "start": 611,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 899
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8193359375
                }
            ],
            "relevance_judgement": 0.8193359375,
            "relevance_judgment_input_expanded": "# Title: Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models\n# Venue: arXiv.org\n# Authors: Artyom Kharinaev, Viktor Moskvoretskii, Egor Shvetsov, Kseniia Studenikina, Bykov Mikhail, E. Burnaev\n## Abstract\nLarge Language Models (LLMs) have emerged as powerful tools for addressing modern challenges and enabling practical applications. However, their computational expense remains a significant barrier to widespread adoption. Quantization has emerged as a promising technique to democratize access and enable low resource device deployment. Despite these advancements, the safety and trustworthiness of quantized models remain underexplored, as prior studies often overlook contemporary architectures and rely on overly simplistic benchmarks and evaluations. To address this gap, we introduce OpenSafetyMini, a novel open-ended safety dataset designed to better distinguish between models. We evaluate 4 state-of-the-art quantization techniques across LLaMA and Mistral models using 4 benchmarks, including human evaluations. Our findings reveal that the optimal quantization method varies for 4-bit precision, while vector quantization techniques deliver the best safety and trustworthiness performance at 2-bit precision, providing foundation for future research.\n## Discussion\nIn this section we discuss obtained critical insights into the effects of quantization bit-range, methods, safety benchmarks, and model architectures on the safety and trustworthiness of language models. Below, we summarize the key implications: \n\n6.1 Quantization Bit-Range \u2022 Int4 Precision: At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral. \n\n\u2022 Int2 Precision: Reducing precision to 2 bits introduces more pronounced performance declines, particularly for LLaMA. Vector quantization methods like AQLM demonstrate greater stability compared to scalar methods like QUIP#, which struggles with maintaining safety and factual accuracy.",
            "reference_string": "[276575417 | Kharinaev et al. | 2025 | Citations: 0]"
        },
        {
            "title": "MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks",
            "venue": "Asia and South Pacific Design Automation Conference",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.09850",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1820826857",
                    "name": "Ruokai Yin"
                },
                {
                    "authorId": "2265595883",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "65951652",
                    "name": "Abhishek Moitra"
                },
                {
                    "authorId": "9352814",
                    "name": "P. Panda"
                }
            ],
            "abstract": "We propose Multiplier-less INTeger (MINT) quantization, a uniform quantization scheme that efficiently compresses weights and membrane potentials in spiking neural networks (SNNs). Unlike previous SNN quantization methods, MINT quantizes memory-intensive membrane potentials to an extremely low precision (2-bit), significantly reducing the memory footprint. MINT also shares the quantization scaling factor between weights and membrane potentials, eliminating the need for multipliers required in conventional uniform quantization. Experimental results show that our method matches the accuracy of full-precision models and other state-of-the-art SNN quantization techniques while surpassing them in memory footprint reduction and hardware cost efficiency at deployment. For example, 2-bit MINT VGG-16 achieves 90.6% accuracy on CIFAR-10, with roughly 93.8% reduction in memory footprint from the full-precision model and 90% reduction in computation energy compared to vanilla uniform quantization at deployment.11Code is available at https://github.com/Intelligent-Computing-Lab-Yale/MINT-Quantization",
            "corpus_id": 265043878,
            "sentences": [
                {
                    "corpus_id": "265043878",
                    "title": "MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks",
                    "text": "Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization. Memory Saving. In this section, we first present the memory footprint reduction with MINT with a batch size of 1. As shown in Fig. 4 (Left), our W2U2 models on average give us more than 93% reduction in the total memory footprint. However, to improve inference speed, prior SNN works [2], [3], [26] commonly use mini-batch sizes larger than 1. In such cases, membrane potential quantization is critical. As illustrated in Fig. 4 (Right), the membrane potential overhead becomes sizeable with increasing batch size. We find that for a MINT quantized VGG-16 model on TinyImageNet with a batch size of 16, the compression of weight from 32-bit to 4-bit alone reduces the total memory footprint by 15%. Whereas, quantizing the membrane potential to 4 bits can further reduce 72.4% of the total memory footprint. We can expect that a larger batch size (> 64) will amplify the benefits from membrane potential quantization. Energy Saving. We study the inference energy difference between MINT and the vanilla UQ method on our proposed accelerator design in Sec. V. We normalize the results with the energy cost of a 16-bit integer multiply-accumulate operation. \n\nAs illustrated in Fig. 5, the computation energy of the UQ model hardly reduces with decreasing operand size, due to the power-hungry full-precision multipliers for scaling factors.",
                    "score": 0.49196784780356134,
                    "section_title": "B. Experimental Results",
                    "char_start_offset": 17313,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 9
                        },
                        {
                            "start": 10,
                            "end": 50
                        },
                        {
                            "start": 51,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1916
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 861,
                            "end": 864,
                            "matchedPaperCorpusId": "52283296"
                        },
                        {
                            "start": 866,
                            "end": 869,
                            "matchedPaperCorpusId": "211258776"
                        },
                        {
                            "start": 871,
                            "end": 875,
                            "matchedPaperCorpusId": "222134047"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81005859375
                }
            ],
            "relevance_judgement": 0.81005859375,
            "relevance_judgment_input_expanded": "# Title: MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks\n# Venue: Asia and South Pacific Design Automation Conference\n# Authors: Ruokai Yin, Yuhang Li, Abhishek Moitra, P. Panda\n## Abstract\nWe propose Multiplier-less INTeger (MINT) quantization, a uniform quantization scheme that efficiently compresses weights and membrane potentials in spiking neural networks (SNNs). Unlike previous SNN quantization methods, MINT quantizes memory-intensive membrane potentials to an extremely low precision (2-bit), significantly reducing the memory footprint. MINT also shares the quantization scaling factor between weights and membrane potentials, eliminating the need for multipliers required in conventional uniform quantization. Experimental results show that our method matches the accuracy of full-precision models and other state-of-the-art SNN quantization techniques while surpassing them in memory footprint reduction and hardware cost efficiency at deployment. For example, 2-bit MINT VGG-16 achieves 90.6% accuracy on CIFAR-10, with roughly 93.8% reduction in memory footprint from the full-precision model and 90% reduction in computation energy compared to vanilla uniform quantization at deployment.11Code is available at https://github.com/Intelligent-Computing-Lab-Yale/MINT-Quantization\n## B. Experimental Results\nAccuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization. Memory Saving. In this section, we first present the memory footprint reduction with MINT with a batch size of 1. As shown in Fig. 4 (Left), our W2U2 models on average give us more than 93% reduction in the total memory footprint. However, to improve inference speed, prior SNN works [2], [3], [26] commonly use mini-batch sizes larger than 1. In such cases, membrane potential quantization is critical. As illustrated in Fig. 4 (Right), the membrane potential overhead becomes sizeable with increasing batch size. We find that for a MINT quantized VGG-16 model on TinyImageNet with a batch size of 16, the compression of weight from 32-bit to 4-bit alone reduces the total memory footprint by 15%. Whereas, quantizing the membrane potential to 4 bits can further reduce 72.4% of the total memory footprint. We can expect that a larger batch size (> 64) will amplify the benefits from membrane potential quantization. Energy Saving. We study the inference energy difference between MINT and the vanilla UQ method on our proposed accelerator design in Sec. V. We normalize the results with the energy cost of a 16-bit integer multiply-accumulate operation. \n\nAs illustrated in Fig. 5, the computation energy of the UQ model hardly reduces with decreasing operand size, due to the power-hungry full-precision multipliers for scaling factors.",
            "reference_string": "[265043878 | Yin et al. | 2023 | Citations: 13]"
        },
        {
            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
            "venue": "",
            "year": 2016,
            "reference_count": 37,
            "citation_count": 422,
            "influential_citation_count": 84,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3311387",
                    "name": "Dan Alistarh"
                },
                {
                    "authorId": "29916095",
                    "name": "Demjan Grubic"
                },
                {
                    "authorId": "2800851",
                    "name": "Jerry Li"
                },
                {
                    "authorId": "2870603",
                    "name": "Ryota Tomioka"
                },
                {
                    "authorId": "1782150",
                    "name": "M. Vojnovi\u0107"
                }
            ],
            "abstract": "Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. \nIn this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. \nIn particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.",
            "corpus_id": 1193239,
            "sentences": [
                {
                    "corpus_id": "1193239",
                    "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
                    "text": "Also, QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. \n\nSimilarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version. \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. Across all our experiments, 8-bit gradients with 512 bucket size have been sufficient to recover or improve upon the full precision target accuracy. \n\nOn AN4 using an LSTM, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. On a two-GPU system, it is able to converge around 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by about 10%). \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy.",
                    "score": 0.520006435511394,
                    "section_title": "Experiments",
                    "char_start_offset": 32510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 89,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1510
                        },
                        {
                            "start": 1513,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1786
                        },
                        {
                            "start": 1787,
                            "end": 1853
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                },
                {
                    "corpus_id": "1193239",
                    "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
                    "text": "We now empirically validate our approach on data-parallel GPU training of deep neural networks. \n\nOn ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]. QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. We note that we did not tune bucket size, number of bits used, number of epochs or learning rate for this experiment. \n\nOn AN4 using LSTMs, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. It is able to converge 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by less than 10%). \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if not trained further. However, increasing precision to 4-bit or 8-bit recovers accuracy. This finding suggests that modern architectures for vision tasks, such as ResNet or Inception, which are almost entirely convolutional, may benefit less from quantization than recurrent deep networks such as LSTMs.",
                    "score": 0.5329459748696229,
                    "section_title": "E Experiments",
                    "char_start_offset": 51010,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 98,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 489
                        },
                        {
                            "start": 492,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 825
                        },
                        {
                            "start": 828,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1327
                        },
                        {
                            "start": 1330,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1839
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80126953125
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks\n# Venue: \n# Authors: Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, M. Vojnovi\u0107\n## Abstract\nParallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compression heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal. \nIn this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. \nIn particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. For instance, on 16 GPUs, we are able to train a ResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show that there exist generic parameter settings under which all known network architectures preserve or slightly improve their full accuracy when using quantization.\n## Experiments\nAlso, QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. \n\nSimilarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version. \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. Across all our experiments, 8-bit gradients with 512 bucket size have been sufficient to recover or improve upon the full precision target accuracy. \n\nOn AN4 using an LSTM, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. On a two-GPU system, it is able to converge around 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by about 10%). \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy.\n\n## E Experiments\nWe now empirically validate our approach on data-parallel GPU training of deep neural networks. \n\nOn ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]. QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. We note that we did not tune bucket size, number of bits used, number of epochs or learning rate for this experiment. \n\nOn AN4 using LSTMs, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. It is able to converge 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by less than 10%). \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if not trained further. However, increasing precision to 4-bit or 8-bit recovers accuracy. This finding suggests that modern architectures for vision tasks, such as ResNet or Inception, which are almost entirely convolutional, may benefit less from quantization than recurrent deep networks such as LSTMs.",
            "reference_string": "[1193239 | Alistarh et al. | 2016 | Citations: 422]"
        },
        {
            "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2343501642",
                    "name": "Alessandro Giagnorio"
                },
                {
                    "authorId": "2079107343",
                    "name": "A. Mastropaolo"
                },
                {
                    "authorId": "2041943516",
                    "name": "Saima Afrin"
                },
                {
                    "authorId": "1719962",
                    "name": "M. D. Penta"
                },
                {
                    "authorId": "1801330",
                    "name": "G. Bavota"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.",
            "corpus_id": 276902966,
            "sentences": [
                {
                    "corpus_id": "276902966",
                    "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
                    "text": "In this section, we report and discuss the results of our study to address the research questions formulated in Section 3. When reporting the results of the statistical tests, we adopt the notation in Table 3 for the significance level. 4.1 How does low-bit quantization affect the model's code generation ability? \n\nFig. 1 provides a handy graphical representation of the memory saving (xaxis) versus the relative performance loss in terms of pass@1 score (y-axis) when comparing models quantized at different bit precisions (8, 4, 3, and 2 bits per parameter) and the baseline models using fp16 precision. The blue lines represent CodeLlama 7B while the orange lines DeepSeek-Coder 7B. The left charts relate to the Python benchmarks (from MultiPL-E and McEval) while the right one to the Java benchmarks. Table 4 reports the detailed numbers on top of which Fig. 1 has been built: For each LLM subject of RQ 1 (i.e., CodeLlama and DeepSeek-Coder 7B) we show, both for the baseline fp16 precision model as well as for all its quantized versions, (i) their memory footprint in terms of GB, (ii) the pass@1 score they achieved on both the Python and the Java benchmarks; and (iii) the results of the statistical tests (i.e., adjusted p-value and OR), in which we compare each quantized model against the baseline. For example, looking at CodeLlama 7B, the fp16-precision baseline requires 13.48GB of memory to achieve a 29.8% of pass@1 score on the Python MultiPL-E benchmark, while its 4-bit quantized version only uses 4.00GB of memory with a pass@1 of 29.1%. This difference in performance is not statistically significant. When mapping this back to Fig. 1 (a) (blue line in the left chart), we can indeed see that the 4-bit quantized model, when compared to the fp16-precision baseline, allows achieving a 70% of memory reduction (i.e., 13.48\u22124.00 13.48",
                    "score": 0.5295607187647628,
                    "section_title": "Study Results",
                    "char_start_offset": 36487,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 314
                        },
                        {
                            "start": 317,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 1857
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8017578125
                }
            ],
            "relevance_judgement": 0.8017578125,
            "relevance_judgment_input_expanded": "# Title: Quantizing Large Language Models for Code Generation: A Differentiated Replication\n# Venue: arXiv.org\n# Authors: Alessandro Giagnorio, A. Mastropaolo, Saima Afrin, M. D. Penta, G. Bavota\n## Abstract\nLarge Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.\n## Study Results\nIn this section, we report and discuss the results of our study to address the research questions formulated in Section 3. When reporting the results of the statistical tests, we adopt the notation in Table 3 for the significance level. 4.1 How does low-bit quantization affect the model's code generation ability? \n\nFig. 1 provides a handy graphical representation of the memory saving (xaxis) versus the relative performance loss in terms of pass@1 score (y-axis) when comparing models quantized at different bit precisions (8, 4, 3, and 2 bits per parameter) and the baseline models using fp16 precision. The blue lines represent CodeLlama 7B while the orange lines DeepSeek-Coder 7B. The left charts relate to the Python benchmarks (from MultiPL-E and McEval) while the right one to the Java benchmarks. Table 4 reports the detailed numbers on top of which Fig. 1 has been built: For each LLM subject of RQ 1 (i.e., CodeLlama and DeepSeek-Coder 7B) we show, both for the baseline fp16 precision model as well as for all its quantized versions, (i) their memory footprint in terms of GB, (ii) the pass@1 score they achieved on both the Python and the Java benchmarks; and (iii) the results of the statistical tests (i.e., adjusted p-value and OR), in which we compare each quantized model against the baseline. For example, looking at CodeLlama 7B, the fp16-precision baseline requires 13.48GB of memory to achieve a 29.8% of pass@1 score on the Python MultiPL-E benchmark, while its 4-bit quantized version only uses 4.00GB of memory with a pass@1 of 29.1%. This difference in performance is not statistically significant. When mapping this back to Fig. 1 (a) (blue line in the left chart), we can indeed see that the 4-bit quantized model, when compared to the fp16-precision baseline, allows achieving a 70% of memory reduction (i.e., 13.48\u22124.00 13.48",
            "reference_string": "[276902966 | Giagnorio et al. | 2025 | Citations: 1]"
        },
        {
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 2606,
            "influential_citation_count": 322,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14314",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3239480",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "51152502",
                    "name": "Artidoro Pagnoni"
                },
                {
                    "authorId": "14487640",
                    "name": "Ari Holtzman"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
            "corpus_id": 258841328,
            "sentences": [
                {
                    "corpus_id": "258841328",
                    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                    "text": "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision. \n\nSummary Our results consistently show that 4-bit QLORA with NF4 data type matches 16bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with wellestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that double quantization does not degrade performance. Combined, this forms compelling evidence that 4-bit QLORA tuning reliably yields results matching 16-bit methods. \n\nIn line with previous work on quantization [13], our MMLU and Elo results indicate that with a given finetuning and inference resource budget it is beneficial to increase the number of parameters in the base model while decreasing their precision. This highlights the importance of efficiency benefits from QLORA. Since we did not observe performance degradation compared to full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. \n\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. \n\n5 Pushing the Chatbot State-of-the-art with QLoRA \n\nHaving established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate on a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for real-world chatbot performance evaluation.",
                    "score": 0.6057468385364775,
                    "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
                    "char_start_offset": 20373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 454
                        },
                        {
                            "start": 457,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 776
                        },
                        {
                            "start": 777,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1672
                        },
                        {
                            "start": 1675,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 2111
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79931640625
                }
            ],
            "relevance_judgement": 0.79931640625,
            "relevance_judgment_input_expanded": "# Title: QLoRA: Efficient Finetuning of Quantized LLMs\n# Venue: Neural Information Processing Systems\n# Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n## Abstract\nWe present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n## 4-bit NormalFloat yields better performance than 4-bit Floating Point\nResults are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision. \n\nSummary Our results consistently show that 4-bit QLORA with NF4 data type matches 16bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with wellestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that double quantization does not degrade performance. Combined, this forms compelling evidence that 4-bit QLORA tuning reliably yields results matching 16-bit methods. \n\nIn line with previous work on quantization [13], our MMLU and Elo results indicate that with a given finetuning and inference resource budget it is beneficial to increase the number of parameters in the base model while decreasing their precision. This highlights the importance of efficiency benefits from QLORA. Since we did not observe performance degradation compared to full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. \n\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. \n\n5 Pushing the Chatbot State-of-the-art with QLoRA \n\nHaving established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate on a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for real-world chatbot performance evaluation.",
            "reference_string": "[258841328 | Dettmers et al. | 2023 | Citations: 2606]"
        },
        {
            "title": "Power-efficient gesture sensing for edge devices: mimicking fourier transforms with spiking neural networks",
            "venue": "Applied intelligence (Boston)",
            "year": 2022,
            "reference_count": 82,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10489-022-04258-w.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-022-04258-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-022-04258-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491637170",
                    "name": "Muhammad Arsalan"
                },
                {
                    "authorId": "49642325",
                    "name": "Avik Santra"
                },
                {
                    "authorId": "2142734179",
                    "name": "V. Issakov"
                }
            ],
            "abstract": "One of the key design requirements for any portable/mobile device is low power. To enable such a low powered device, we propose an embedded gesture detection system that uses spiking neural networks (SNNs) applied directly to raw ADC data of a 60GHz frequency modulated continuous wave radar. SNNs can facilitate low power systems because they are sparse in time and space and are event-driven. The proposed system, as opposed to earlier state-of-the-art methods, relies solely on the target\u2019s raw ADC data, thus avoiding the overhead of performing slow-time and fast-time Fourier transforms (FFTs) processing. The proposed architecture mimics the discrete Fourier transformation within the SNN itself avoiding the need for FFT accelerators and makes the FFT processing tailored to the specific application, in this case gesture sensing. The experimental results demonstrate that the proposed system is capable of classifying 8 different gestures with an accuracy of 98.7%. This result is comparable to the conventional approaches, yet it offers lower complexity, lower power consumption and faster computations comparable to the conventional approaches.",
            "corpus_id": 253535294,
            "sentences": [
                {
                    "corpus_id": "253535294",
                    "title": "Power-efficient gesture sensing for edge devices: mimicking fourier transforms with spiking neural networks",
                    "text": "Similarly, for 8 and 16 bits quantization, 1 bit for sign, 3 bits are assigned to integer part and the rest of the bits are used for fractional parts. As expected the accuracy drops for high quantization i.e., 4 bits for all the models and drops with a higher percentage for our proposed model (model 3). This drop is due to the higher number of neurons used in the proposed model and hence the high impact of quantization. However, we believe higher accuracy can be achieved with quantization aware training which currently the nengoDL framework does not support. Increasing the bits for quantization increases the accuracy as expected and with 8-bit and 16-bit, the proposed model achieves 92.68% and 96.46% of accuracy with good precision and recall as indicated by f1-scores. Where for \"f1-scores micro\" average is calculated by counting the total true positives, false positives and false negatives. For \"f1-scores macro\" the metric is calculated for each class using their unweighted arithmetic mean. \n\nConsidering the goal of having a system that is energyefficient, we looked into the energy consumption per classification of the proposed system. Since the actual hardware-based energy calculation is out of the scope of this research work,in the current study, we relied the hardware metrics of the \u03bcBrain chip defined in [74] to estimate the energy consumption. If SP N is the maximum number of spikes, SP E = 2.1pJ is the energy per spike and LK P = 73\u03bcW is the static leakage power, then the energy  consumption C E per classification using \u03bcBrain hardware metrics is given as: \n\nwhere \u03b4T is the inference time. Assuming the \u03b4T = 28 ms. The energy consumption per classification of our proposed is approx. C E = 2.1\u03bcJ. To see the energy efficiency for SNN hardware, readers can refer to [75], where the SNN hardware is compared with the other deep learning hardware in terms of energy efficiency. The performance of SNN hardware was tested on a keyword spotting application using a dynamic energy cost per inference on some energy-efficient accelerators commercially available as shown in Fig. 10.",
                    "score": 0.5551679851238918,
                    "section_title": "Discussion",
                    "char_start_offset": 23164,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1006
                        },
                        {
                            "start": 1009,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1589
                        },
                        {
                            "start": 1592,
                            "end": 1623
                        },
                        {
                            "start": 1624,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2109
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                }
            ],
            "relevance_judgement": 0.7958984375,
            "relevance_judgment_input_expanded": "# Title: Power-efficient gesture sensing for edge devices: mimicking fourier transforms with spiking neural networks\n# Venue: Applied intelligence (Boston)\n# Authors: Muhammad Arsalan, Avik Santra, V. Issakov\n## Abstract\nOne of the key design requirements for any portable/mobile device is low power. To enable such a low powered device, we propose an embedded gesture detection system that uses spiking neural networks (SNNs) applied directly to raw ADC data of a 60GHz frequency modulated continuous wave radar. SNNs can facilitate low power systems because they are sparse in time and space and are event-driven. The proposed system, as opposed to earlier state-of-the-art methods, relies solely on the target\u2019s raw ADC data, thus avoiding the overhead of performing slow-time and fast-time Fourier transforms (FFTs) processing. The proposed architecture mimics the discrete Fourier transformation within the SNN itself avoiding the need for FFT accelerators and makes the FFT processing tailored to the specific application, in this case gesture sensing. The experimental results demonstrate that the proposed system is capable of classifying 8 different gestures with an accuracy of 98.7%. This result is comparable to the conventional approaches, yet it offers lower complexity, lower power consumption and faster computations comparable to the conventional approaches.\n## Discussion\nSimilarly, for 8 and 16 bits quantization, 1 bit for sign, 3 bits are assigned to integer part and the rest of the bits are used for fractional parts. As expected the accuracy drops for high quantization i.e., 4 bits for all the models and drops with a higher percentage for our proposed model (model 3). This drop is due to the higher number of neurons used in the proposed model and hence the high impact of quantization. However, we believe higher accuracy can be achieved with quantization aware training which currently the nengoDL framework does not support. Increasing the bits for quantization increases the accuracy as expected and with 8-bit and 16-bit, the proposed model achieves 92.68% and 96.46% of accuracy with good precision and recall as indicated by f1-scores. Where for \"f1-scores micro\" average is calculated by counting the total true positives, false positives and false negatives. For \"f1-scores macro\" the metric is calculated for each class using their unweighted arithmetic mean. \n\nConsidering the goal of having a system that is energyefficient, we looked into the energy consumption per classification of the proposed system. Since the actual hardware-based energy calculation is out of the scope of this research work,in the current study, we relied the hardware metrics of the \u03bcBrain chip defined in [74] to estimate the energy consumption. If SP N is the maximum number of spikes, SP E = 2.1pJ is the energy per spike and LK P = 73\u03bcW is the static leakage power, then the energy  consumption C E per classification using \u03bcBrain hardware metrics is given as: \n\nwhere \u03b4T is the inference time. Assuming the \u03b4T = 28 ms. The energy consumption per classification of our proposed is approx. C E = 2.1\u03bcJ. To see the energy efficiency for SNN hardware, readers can refer to [75], where the SNN hardware is compared with the other deep learning hardware in terms of energy efficiency. The performance of SNN hardware was tested on a keyword spotting application using a dynamic energy cost per inference on some energy-efficient accelerators commercially available as shown in Fig. 10.",
            "reference_string": "[253535294 | Arsalan et al. | 2022 | Citations: 5]"
        },
        {
            "title": "Face Recognition and Machine Learning at the Edge",
            "venue": "IOP Conference Series: Materials Science and Engineering",
            "year": 2020,
            "reference_count": 24,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1088/1757-899x/884/1/012084",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1757-899X/884/1/012084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1757-899X/884/1/012084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2004954858",
                    "name": "Joanne Ling Sin Yee"
                },
                {
                    "authorId": "2412102",
                    "name": "U. U. Sheikh"
                },
                {
                    "authorId": "1951977",
                    "name": "M. Mokji"
                },
                {
                    "authorId": "2111988471",
                    "name": "S. Rahman"
                }
            ],
            "abstract": "The number of IoT is expected to reach 20 billion by year 2020. This is due to data that log in the sensors or cameras are all send to the cloud for further processing. Cloud computing is not able to support big data analytic anymore due to network bandwidth. Face recognition is chosen as a case study to demonstrate the challenges to shift the application to the edge. The objective of this project is to develop a face recognition system that is suitable to be used at the edge using a deep neural network. Secondly, investigate the performance in terms of model size, speed and inference time after different bit-width fixed point quantization on the weights of the network. Lastly, deploy the model to Raspberry Pi 3 and test the performance. The chosen data set is AT&T. MATLAB is used to train the network in laptop with i5-7300 CPU while OpenCV-python is used to load and test the network in Raspberry Pi3 and laptop. The proposed system is designed by doing transfer learning on SqueezeNet to classify face. Fixed-point quantization is being applied to the weights of the layers to reduce the size of the model. From the experiment result, it is recommended to use 8-bit fixed-point quantization to the weights in all the layers in the model to compress the size of the network up to 2.5 times while maintaining the original accuracy 90%. That is only 1.1\u00d7 speed up of the model on Raspberry Pi 3 after different bit-width weight quantization.",
            "corpus_id": 225583435,
            "sentences": [
                {
                    "corpus_id": "225583435",
                    "title": "Face Recognition and Machine Learning at the Edge",
                    "text": "Figure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
                    "score": 0.5677793086702971,
                    "section_title": "RESULTS AND DISCUSSION",
                    "char_start_offset": 15210,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.791015625
                }
            ],
            "relevance_judgement": 0.791015625,
            "relevance_judgment_input_expanded": "# Title: Face Recognition and Machine Learning at the Edge\n# Venue: IOP Conference Series: Materials Science and Engineering\n# Authors: Joanne Ling Sin Yee, U. U. Sheikh, M. Mokji, S. Rahman\n## Abstract\nThe number of IoT is expected to reach 20 billion by year 2020. This is due to data that log in the sensors or cameras are all send to the cloud for further processing. Cloud computing is not able to support big data analytic anymore due to network bandwidth. Face recognition is chosen as a case study to demonstrate the challenges to shift the application to the edge. The objective of this project is to develop a face recognition system that is suitable to be used at the edge using a deep neural network. Secondly, investigate the performance in terms of model size, speed and inference time after different bit-width fixed point quantization on the weights of the network. Lastly, deploy the model to Raspberry Pi 3 and test the performance. The chosen data set is AT&T. MATLAB is used to train the network in laptop with i5-7300 CPU while OpenCV-python is used to load and test the network in Raspberry Pi3 and laptop. The proposed system is designed by doing transfer learning on SqueezeNet to classify face. Fixed-point quantization is being applied to the weights of the layers to reduce the size of the model. From the experiment result, it is recommended to use 8-bit fixed-point quantization to the weights in all the layers in the model to compress the size of the network up to 2.5 times while maintaining the original accuracy 90%. That is only 1.1\u00d7 speed up of the model on Raspberry Pi 3 after different bit-width weight quantization.\n## RESULTS AND DISCUSSION\nFigure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
            "reference_string": "[225583435 | Yee et al. | 2020 | Citations: 2]"
        },
        {
            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
            "year": 2020,
            "reference_count": 30,
            "citation_count": 30,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6570653/9133209/09069242.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JXCDC.2020.2987605?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JXCDC.2020.2987605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "91781917",
                    "name": "Gouranga Charan"
                },
                {
                    "authorId": "2942785",
                    "name": "Abinash Mohanty"
                },
                {
                    "authorId": "3457252",
                    "name": "Xiaocong Du"
                },
                {
                    "authorId": "144828251",
                    "name": "Gokul Krishnan"
                },
                {
                    "authorId": "145553390",
                    "name": "R. Joshi"
                },
                {
                    "authorId": "1965873861",
                    "name": "Yu Cao"
                }
            ],
            "abstract": "Resistive random access memory (RRAM) is a promising technology for energy-efficient neuromorphic accelerators. However, when a pretrained deep neural network (DNN) model is programmed to an RRAM array for inference, the model suffers from accuracy degradation due to RRAM nonidealities, such as device variations, quantization error, and stuck-at-faults. Previous solutions involving multiple read\u2013verify\u2013write (R-V-W) to the RRAM cells require cell-by-cell compensation and, thus, an excessive amount of processing time. In this article, we propose a joint algorithm-design solution to mitigate the accuracy degradation. We first leverage knowledge distillation (KD), where the model is trained with the RRAM nonidealities to increase the robustness of the model under device variations. Furthermore, we propose random sparse adaptation (RSA), which integrates a small on-chip memory with the main RRAM array for postmapping adaptation. Only the on-chip memory is updated to recover the inference accuracy. The joint algorithm-design solution achieves the state-of-the-art accuracy of 99.41% for MNIST (LeNet-5) and 91.86% for CIFAR-10 (VGG-16) with up to 5% parameters as overhead while providing a 15\u2013 $150\\times $ speedup compared with R-V-W.",
            "corpus_id": 218788236,
            "sentences": [
                {
                    "corpus_id": "218788236",
                    "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                    "text": "In deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
                    "score": 0.6108655721722611,
                    "section_title": "B. DEVICE QUANTIZATION",
                    "char_start_offset": 7268,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 423
                        },
                        {
                            "start": 424,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 894
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 118,
                            "matchedPaperCorpusId": "44071489"
                        },
                        {
                            "start": 324,
                            "end": 328,
                            "matchedPaperCorpusId": "16877209"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78466796875
                }
            ],
            "relevance_judgement": 0.78466796875,
            "relevance_judgment_input_expanded": "# Title: Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution\n# Venue: IEEE Journal on Exploratory Solid-State Computational Devices and Circuits\n# Authors: Gouranga Charan, Abinash Mohanty, Xiaocong Du, Gokul Krishnan, R. Joshi, Yu Cao\n## Abstract\nResistive random access memory (RRAM) is a promising technology for energy-efficient neuromorphic accelerators. However, when a pretrained deep neural network (DNN) model is programmed to an RRAM array for inference, the model suffers from accuracy degradation due to RRAM nonidealities, such as device variations, quantization error, and stuck-at-faults. Previous solutions involving multiple read\u2013verify\u2013write (R-V-W) to the RRAM cells require cell-by-cell compensation and, thus, an excessive amount of processing time. In this article, we propose a joint algorithm-design solution to mitigate the accuracy degradation. We first leverage knowledge distillation (KD), where the model is trained with the RRAM nonidealities to increase the robustness of the model under device variations. Furthermore, we propose random sparse adaptation (RSA), which integrates a small on-chip memory with the main RRAM array for postmapping adaptation. Only the on-chip memory is updated to recover the inference accuracy. The joint algorithm-design solution achieves the state-of-the-art accuracy of 99.41% for MNIST (LeNet-5) and 91.86% for CIFAR-10 (VGG-16) with up to 5% parameters as overhead while providing a 15\u2013 $150\\times $ speedup compared with R-V-W.\n## B. DEVICE QUANTIZATION\nIn deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
            "reference_string": "[218788236 | Charan et al. | 2020 | Citations: 30]"
        },
        {
            "title": "Learned Step Size Quantization",
            "venue": "International Conference on Learning Representations",
            "year": 2019,
            "reference_count": 36,
            "citation_count": 810,
            "influential_citation_count": 185,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.08153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357931",
                    "name": "S. K. Esser"
                },
                {
                    "authorId": "46571359",
                    "name": "J. McKinstry"
                },
                {
                    "authorId": "2064431971",
                    "name": "Deepika Bablani"
                },
                {
                    "authorId": "2730753",
                    "name": "R. Appuswamy"
                },
                {
                    "authorId": "1944330",
                    "name": "D. Modha"
                }
            ],
            "abstract": "Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code.",
            "corpus_id": 67788003,
            "sentences": [
                {
                    "corpus_id": "67788003",
                    "title": "Learned Step Size Quantization",
                    "text": "We trained several networks using LSQ and compare accuracy with other quantized networks and full precision baselines (Table 1). To facilitate comparison, we only consider published models that quantize all convolution and fully connected layer weights and input activations to the specified precision, except for the first and last layers which may use higher precision (as for the LSQ models). \n\nIn some cases, we report slightly higher accuracy on full precision networks than in their original publications, which we attribute to our use of cosine learning rate decay (Loshchilov & Hutter, 2016). \n\nWe found that LSQ achieved a higher top-1 accuracy than all previous reported approaches for 2-, 3and 4-bit networks with the architectures considered here. For nearly all cases, LSQ also achieved the best-to-date top-5 accuracy on these networks, and best-to-date accuracy on 8-bit versions of these networks. In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest. \n\nIt is interesting to note that when comparing a full precision to a 2-bit precision model, top-1 accuracy drops only 2.9 for ResNet-18, but 14.0 for SqueezeNext-23-2x. One interpretation of this is that the SqueezeNext architecture was designed to maximize performance using as few parameters as possible, which may have placed it at a design point extremely sensitive to reductions in precision.",
                    "score": 0.5395377158807793,
                    "section_title": "COMPARISON WITH OTHER APPROACHES",
                    "char_start_offset": 12270,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 395
                        },
                        {
                            "start": 398,
                            "end": 600
                        },
                        {
                            "start": 603,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1375
                        },
                        {
                            "start": 1378,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1774
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.783203125
                }
            ],
            "relevance_judgement": 0.783203125,
            "relevance_judgment_input_expanded": "# Title: Learned Step Size Quantization\n# Venue: International Conference on Learning Representations\n# Authors: S. K. Esser, J. McKinstry, Deepika Bablani, R. Appuswamy, D. Modha\n## Abstract\nDeep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code.\n## COMPARISON WITH OTHER APPROACHES\nWe trained several networks using LSQ and compare accuracy with other quantized networks and full precision baselines (Table 1). To facilitate comparison, we only consider published models that quantize all convolution and fully connected layer weights and input activations to the specified precision, except for the first and last layers which may use higher precision (as for the LSQ models). \n\nIn some cases, we report slightly higher accuracy on full precision networks than in their original publications, which we attribute to our use of cosine learning rate decay (Loshchilov & Hutter, 2016). \n\nWe found that LSQ achieved a higher top-1 accuracy than all previous reported approaches for 2-, 3and 4-bit networks with the architectures considered here. For nearly all cases, LSQ also achieved the best-to-date top-5 accuracy on these networks, and best-to-date accuracy on 8-bit versions of these networks. In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest. \n\nIt is interesting to note that when comparing a full precision to a 2-bit precision model, top-1 accuracy drops only 2.9 for ResNet-18, but 14.0 for SqueezeNext-23-2x. One interpretation of this is that the SqueezeNext architecture was designed to maximize performance using as few parameters as possible, which may have placed it at a design point extremely sensitive to reductions in precision.",
            "reference_string": "[67788003 | Esser et al. | 2019 | Citations: 810]"
        },
        {
            "title": "Confounding Tradeoffs for Neural Network Quantization",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 46,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.06366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "9273400",
                    "name": "Sahaj Garg"
                },
                {
                    "authorId": "153408223",
                    "name": "Anirudh Jain"
                },
                {
                    "authorId": "134183581",
                    "name": "Joe Lou"
                },
                {
                    "authorId": "2060968473",
                    "name": "Mitchell Nahmias"
                }
            ],
            "abstract": "Many neural network quantization techniques have been developed to decrease the computational and memory footprint of deep learning. However, these methods are evaluated subject to confounding tradeoffs that may affect inference acceleration or resource complexity in exchange for higher accuracy. In this work, we articulate a variety of tradeoffs whose impact is often overlooked and empirically analyze their impact on uniform and mixed-precision post-training quantization, finding that these confounding tradeoffs may have a larger impact on quantized network accuracy than the actual quantization methods themselves. Because these tradeoffs constrain the attainable hardware acceleration for different use-cases, we encourage researchers to explicitly report these design choices through the structure of\"quantization cards.\"We expect quantization cards to help researchers compare methods more effectively and engineers determine the applicability of quantization techniques for their hardware.",
            "corpus_id": 231918483,
            "sentences": [
                {
                    "corpus_id": "231918483",
                    "title": "Confounding Tradeoffs for Neural Network Quantization",
                    "text": "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 5: The set of allowed bitwidths for mixedprecision quantization affects the ability of mixed precision to increase accuracy. We test the impact on performance of using any integer bitwidth versus the subset {2, 4, 8}. We implement a restricted version of the post-training quantization method that rounds the continuously parameterized bitwidth to the closest of the three discrete values {2, 4, 8} during training. As shown in Figure 6, restricting the set of bitwidths used by the mixed-precision method does not affect accuracy at high average bitwidths, while at lower bitwidths the degradation is large. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations. \n\nObservation 7: Post-training quantization methods that use labeled data can perform equally well with pseudolabels.",
                    "score": 0.5261423521391426,
                    "section_title": "Results",
                    "char_start_offset": 19337,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 208,
                            "end": 366
                        },
                        {
                            "start": 369,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 860
                        },
                        {
                            "start": 863,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1483
                        },
                        {
                            "start": 1486,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2131
                        },
                        {
                            "start": 2134,
                            "end": 2249
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 53,
                            "end": 57,
                            "matchedPaperCorpusId": "221506623"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78076171875
                }
            ],
            "relevance_judgement": 0.78076171875,
            "relevance_judgment_input_expanded": "# Title: Confounding Tradeoffs for Neural Network Quantization\n# Venue: arXiv.org\n# Authors: Sahaj Garg, Anirudh Jain, Joe Lou, Mitchell Nahmias\n## Abstract\nMany neural network quantization techniques have been developed to decrease the computational and memory footprint of deep learning. However, these methods are evaluated subject to confounding tradeoffs that may affect inference acceleration or resource complexity in exchange for higher accuracy. In this work, we articulate a variety of tradeoffs whose impact is often overlooked and empirically analyze their impact on uniform and mixed-precision post-training quantization, finding that these confounding tradeoffs may have a larger impact on quantized network accuracy than the actual quantization methods themselves. Because these tradeoffs constrain the attainable hardware acceleration for different use-cases, we encourage researchers to explicitly report these design choices through the structure of\"quantization cards.\"We expect quantization cards to help researchers compare methods more effectively and engineers determine the applicability of quantization techniques for their hardware.\n## Results\nResults in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 5: The set of allowed bitwidths for mixedprecision quantization affects the ability of mixed precision to increase accuracy. We test the impact on performance of using any integer bitwidth versus the subset {2, 4, 8}. We implement a restricted version of the post-training quantization method that rounds the continuously parameterized bitwidth to the closest of the three discrete values {2, 4, 8} during training. As shown in Figure 6, restricting the set of bitwidths used by the mixed-precision method does not affect accuracy at high average bitwidths, while at lower bitwidths the degradation is large. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations. \n\nObservation 7: Post-training quantization methods that use labeled data can perform equally well with pseudolabels.",
            "reference_string": "[231918483 | Garg et al. | 2021 | Citations: 18]"
        },
        {
            "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization",
            "venue": "Micro",
            "year": 2022,
            "reference_count": 100,
            "citation_count": 60,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2208.14286",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.14286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109865181",
                    "name": "Cong Guo"
                },
                {
                    "authorId": "145107889",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1831521",
                    "name": "Jingwen Leng"
                },
                {
                    "authorId": "2117940902",
                    "name": "Zihan Liu"
                },
                {
                    "authorId": "145338263",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2117415805",
                    "name": "Yun-Bo Liu"
                },
                {
                    "authorId": "2151671216",
                    "name": "Minyi Guo"
                },
                {
                    "authorId": "2124167",
                    "name": "Yuhao Zhu"
                }
            ],
            "abstract": "Quantization is a technique to reduce the computation and memory cost of DNN models, which are getting increasingly large. Existing quantization solutions use fixed-point integer or floating-point types, which have limited benefits, as both require more bits to maintain the accuracy of original models. On the other hand, variable-length quantization uses low-bit quantization for normal values and high-precision for a fraction of outlier values. Even though this line of work brings algorithmic benefits, it also introduces significant hardware overheads due to variable-length encoding and decoding.In this work, we propose a fixed-length a daptive n umerical data t ype called ANT to achieve low-bit quantization with tiny hardware overheads. Our data type ANT leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, we propose a particular data type, flint, that combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, we propose an adaptive framework that selects the best type for each tensor according to its distribution characteristics. We design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Our design results in $2.8\\times $ speedup and $2.5\\times $ energy efficiency improvement over the state-of-the-art quantization accelerators.",
            "corpus_id": 251928917,
            "sentences": [
                {
                    "corpus_id": "251928917",
                    "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization",
                    "text": "Finally, we observe that adding the float has the least impact on the quantization errors, whose role is replaced by other primitive types. \n\nAccuracy Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT. We also observe that our proposed flint data type is important for the accuracies of both vision and NLP tasks. Meanwhile, the PoT type is more important for Transformer-based models on NLP tasks than vision tasks.",
                    "score": 0.5270610020594271,
                    "section_title": "B. Quantization Accuracy",
                    "char_start_offset": 48751,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 142,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1073
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7783203125
                }
            ],
            "relevance_judgement": 0.7783203125,
            "relevance_judgment_input_expanded": "# Title: ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization\n# Venue: Micro\n# Authors: Cong Guo, Chen Zhang, Jingwen Leng, Zihan Liu, Fan Yang, Yun-Bo Liu, Minyi Guo, Yuhao Zhu\n## Abstract\nQuantization is a technique to reduce the computation and memory cost of DNN models, which are getting increasingly large. Existing quantization solutions use fixed-point integer or floating-point types, which have limited benefits, as both require more bits to maintain the accuracy of original models. On the other hand, variable-length quantization uses low-bit quantization for normal values and high-precision for a fraction of outlier values. Even though this line of work brings algorithmic benefits, it also introduces significant hardware overheads due to variable-length encoding and decoding.In this work, we propose a fixed-length a daptive n umerical data t ype called ANT to achieve low-bit quantization with tiny hardware overheads. Our data type ANT leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, we propose a particular data type, flint, that combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, we propose an adaptive framework that selects the best type for each tensor according to its distribution characteristics. We design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Our design results in $2.8\\times $ speedup and $2.5\\times $ energy efficiency improvement over the state-of-the-art quantization accelerators.\n## B. Quantization Accuracy\nFinally, we observe that adding the float has the least impact on the quantization errors, whose role is replaced by other primitive types. \n\nAccuracy Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT. We also observe that our proposed flint data type is important for the accuracies of both vision and NLP tasks. Meanwhile, the PoT type is more important for Transformer-based models on NLP tasks than vision tasks.",
            "reference_string": "[251928917 | Guo et al. | 2022 | Citations: 60]"
        },
        {
            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2023,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kaust.edu.sa/bitstreams/bc8c84a2-a993-43ea-b638-5f1cd83d56c4/download",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81112415",
                    "name": "Ayan Shymyrbay"
                },
                {
                    "authorId": "11702681",
                    "name": "M. Fouda"
                },
                {
                    "authorId": "2868015",
                    "name": "A. Eltawil"
                }
            ],
            "abstract": "Deep neural networks have been proven to be highly effective tools in various domains, yet their computational and memory costs restrict them from being widely deployed on portable devices. The recent rapid increase of edge computing devices has led to an active search for techniques to address the abovementioned limitations of machine learning frameworks. The quantization of artificial neural networks (ANNs), which converts the full-precision synaptic weights into low-bit versions, emerged as one of the solutions. At the same time, spiking neural networks (SNNs) have become an attractive alternative to conventional ANNs due to their temporal information processing capability, energy efficiency, and high biological plausibility. Despite being driven by the same motivation, the simultaneous utilization of both concepts has yet to be thoroughly studied. Therefore, this work aims to bridge the gap between recent progress in quantized neural networks and SNNs. It presents an extensive study on the performance of the quantization function, represented as a linear combination of sigmoid functions, exploited in low-bit weight quantization in SNNs. The presented quantization function demonstrates the state-of-the-art performance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture, N-Caltech101, and N-MNIST, for binary networks (64.05%, 95.45%, 68.71%, and 99.43% respectively) with small accuracy drops and up to 31 \u00d7 memory savings, which outperforms existing methods.",
            "corpus_id": 258990120,
            "sentences": [
                {
                    "corpus_id": "258990120",
                    "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
                    "text": "Since quantized models are not available in the case of the N-Caltech101 dataset, the models reported in this work are compared with other full-precision models in the literature. While the accuracy of our full-precision model is highest, our binarized model has slightly lower accuracy than the full-precision model in [31]. However, there is a substantial difference in memory size between our binarized model and other full-precision models while having comparable accuracy. B. Accuracy -bit-precision trade-off Fig. 2 illustrates the final test accuracy obtained for a particular combination of bit-precision value and dataset. The blue bar corresponds to the non-quantized, full-precision model and serves as a baseline for quantized models to be compared. Overall, the proposed quantization method provides similar performance for different classification tasks. Quantization has more impact on the SNN performance for more complex tasks, such as CIFAR10-DVS and N-Caltech101.\n\nQuantization of the SNN model for the CIFAR10-DVS dataset yields the highest degradation in accuracy compared to other datasets. While the accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively). Hence, for this dataset, the model can be quantized down to 1-bit with an accuracy degradation of around 1%.\n\nFor the N-Caltech101 dataset, on the other hand, the binarized SNN model has a more",
                    "score": 0.5315229985658378,
                    "section_title": "4) N-Caltech101:",
                    "char_start_offset": 19203,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 320,
                            "end": 324,
                            "matchedPaperCorpusId": "251648053"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77685546875
                }
            ],
            "relevance_judgement": 0.77685546875,
            "relevance_judgment_input_expanded": "# Title: Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Ayan Shymyrbay, M. Fouda, A. Eltawil\n## Abstract\nDeep neural networks have been proven to be highly effective tools in various domains, yet their computational and memory costs restrict them from being widely deployed on portable devices. The recent rapid increase of edge computing devices has led to an active search for techniques to address the abovementioned limitations of machine learning frameworks. The quantization of artificial neural networks (ANNs), which converts the full-precision synaptic weights into low-bit versions, emerged as one of the solutions. At the same time, spiking neural networks (SNNs) have become an attractive alternative to conventional ANNs due to their temporal information processing capability, energy efficiency, and high biological plausibility. Despite being driven by the same motivation, the simultaneous utilization of both concepts has yet to be thoroughly studied. Therefore, this work aims to bridge the gap between recent progress in quantized neural networks and SNNs. It presents an extensive study on the performance of the quantization function, represented as a linear combination of sigmoid functions, exploited in low-bit weight quantization in SNNs. The presented quantization function demonstrates the state-of-the-art performance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture, N-Caltech101, and N-MNIST, for binary networks (64.05%, 95.45%, 68.71%, and 99.43% respectively) with small accuracy drops and up to 31 \u00d7 memory savings, which outperforms existing methods.\n## 4) N-Caltech101:\nSince quantized models are not available in the case of the N-Caltech101 dataset, the models reported in this work are compared with other full-precision models in the literature. While the accuracy of our full-precision model is highest, our binarized model has slightly lower accuracy than the full-precision model in [31]. However, there is a substantial difference in memory size between our binarized model and other full-precision models while having comparable accuracy. B. Accuracy -bit-precision trade-off Fig. 2 illustrates the final test accuracy obtained for a particular combination of bit-precision value and dataset. The blue bar corresponds to the non-quantized, full-precision model and serves as a baseline for quantized models to be compared. Overall, the proposed quantization method provides similar performance for different classification tasks. Quantization has more impact on the SNN performance for more complex tasks, such as CIFAR10-DVS and N-Caltech101.\n\nQuantization of the SNN model for the CIFAR10-DVS dataset yields the highest degradation in accuracy compared to other datasets. While the accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively). Hence, for this dataset, the model can be quantized down to 1-bit with an accuracy degradation of around 1%.\n\nFor the N-Caltech101 dataset, on the other hand, the binarized SNN model has a more",
            "reference_string": "[258990120 | Shymyrbay et al. | 2023 | Citations: 7]"
        },
        {
            "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
            "venue": "",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.11055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2154961369",
                    "name": "Jemin Lee"
                },
                {
                    "authorId": "1936868",
                    "name": "Sihyeong Park"
                },
                {
                    "authorId": "3460677",
                    "name": "Jinse Kwon"
                },
                {
                    "authorId": "2321710033",
                    "name": "Jihun Oh"
                },
                {
                    "authorId": "3156595",
                    "name": "Yongin Kwon"
                }
            ],
            "abstract": "Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \\textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.",
            "corpus_id": 272694046,
            "sentences": [
                {
                    "corpus_id": "272694046",
                    "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
                    "text": "Further dataset details are provided in Appendix A (Table 3). \n\nTo compare our results with ongoing community efforts, we synchronize our benchmarks with Huggingface OpenLLM Leaderboard-v1 (covering April 2023 to June 2024) and Open-LLM Leaderboard-v2 (launched on June 26, 2024). However, both versions currently provide only limited data on quantized models, highlighting the need for our comprehensive evaluation. Our key findings are as follows: \n\n\u2022 Quantized LLMs generally perform better than smaller models on most benchmarks and maintain their advantage across different architectures, showing significant improvements in both large and small language models. However, they still struggle with instruction-following (IFEval) and detecting hallucinations (TruthfulQA). \u2022 FP8 is the most reliable method for all model sizes and tasks, especially for LLMs with 405B parameters, where SmoothQuant encounters problems. AWQ usually performs better than GPTQ in weight-only quantization, and hardware support makes FP8 even more advantageous. \u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits. While model size is the main factor affecting quantization difficulty, differences in LLM architecture within the same parameter size can also affect accuracy. Nevertheless, AWQ consistently outperforms GPTQ across different tasks and model types. \u2022 Difficult tasks do not always have the biggest accuracy drops when quantized. The impact depends on the model design and the quantization method used, causing some hard tasks to remain stable while some easy tasks see bigger decreases. Overall, quantization highlights a model's existing weaknesses, especially in commonsense, logical, or mathematical reasoning. \u2022 Quantization greatly reduces performance in Coding and STEM tasks, although it sometimes improves reasoning accuracy. Additionally, GPT4-based evaluators can sometimes incorrectly judge wrong answers as correct.",
                    "score": 0.5226000069836455,
                    "section_title": "Introduction",
                    "char_start_offset": 3512,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 61
                        },
                        {
                            "start": 64,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 449
                        },
                        {
                            "start": 452,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1962
                        },
                        {
                            "start": 1963,
                            "end": 2056
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.775390625
                }
            ],
            "relevance_judgement": 0.775390625,
            "relevance_judgment_input_expanded": "# Title: Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant\n# Venue: \n# Authors: Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon\n## Abstract\nQuantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \\textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.\n## Introduction\nFurther dataset details are provided in Appendix A (Table 3). \n\nTo compare our results with ongoing community efforts, we synchronize our benchmarks with Huggingface OpenLLM Leaderboard-v1 (covering April 2023 to June 2024) and Open-LLM Leaderboard-v2 (launched on June 26, 2024). However, both versions currently provide only limited data on quantized models, highlighting the need for our comprehensive evaluation. Our key findings are as follows: \n\n\u2022 Quantized LLMs generally perform better than smaller models on most benchmarks and maintain their advantage across different architectures, showing significant improvements in both large and small language models. However, they still struggle with instruction-following (IFEval) and detecting hallucinations (TruthfulQA). \u2022 FP8 is the most reliable method for all model sizes and tasks, especially for LLMs with 405B parameters, where SmoothQuant encounters problems. AWQ usually performs better than GPTQ in weight-only quantization, and hardware support makes FP8 even more advantageous. \u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits. While model size is the main factor affecting quantization difficulty, differences in LLM architecture within the same parameter size can also affect accuracy. Nevertheless, AWQ consistently outperforms GPTQ across different tasks and model types. \u2022 Difficult tasks do not always have the biggest accuracy drops when quantized. The impact depends on the model design and the quantization method used, causing some hard tasks to remain stable while some easy tasks see bigger decreases. Overall, quantization highlights a model's existing weaknesses, especially in commonsense, logical, or mathematical reasoning. \u2022 Quantization greatly reduces performance in Coding and STEM tasks, although it sometimes improves reasoning accuracy. Additionally, GPT4-based evaluators can sometimes incorrectly judge wrong answers as correct.",
            "reference_string": "[272694046 | Lee et al. | 2024 | Citations: 2]"
        },
        {
            "title": "SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks",
            "venue": "Neuro Inspired Computational Elements Workshop",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.19668",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2132187927",
                    "name": "Sreyes P. Venkatesh"
                },
                {
                    "authorId": "2261707840",
                    "name": "Razvan Marinescu"
                },
                {
                    "authorId": "3444950",
                    "name": "J. Eshraghian"
                }
            ],
            "abstract": "Weight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an \u2018event-driven\u2019 approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch: https://github.com/jeshraghian/snntorch.",
            "corpus_id": 269457259,
            "sentences": [],
            "relevance_judgement": 0.775390625,
            "relevance_judgment_input_expanded": "# Title: SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks\n# Venue: Neuro Inspired Computational Elements Workshop\n# Authors: Sreyes P. Venkatesh, Razvan Marinescu, J. Eshraghian\n## Abstract\nWeight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an \u2018event-driven\u2019 approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch: https://github.com/jeshraghian/snntorch.\n",
            "reference_string": "[269457259 | Venkatesh et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Quantization-Guided Training for Compact TinyML Models",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 37,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.06231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "103148446",
                    "name": "Sedigh Ghamari"
                },
                {
                    "authorId": "40408041",
                    "name": "Koray Ozcan"
                },
                {
                    "authorId": "40844014",
                    "name": "Thu Dinh"
                },
                {
                    "authorId": "38363587",
                    "name": "A. Melnikov"
                },
                {
                    "authorId": "2062260709",
                    "name": "Juan Carvajal"
                },
                {
                    "authorId": "39497207",
                    "name": "Jan Ernst"
                },
                {
                    "authorId": "1691331",
                    "name": "S. Chai"
                }
            ],
            "abstract": "We propose a Quantization Guided Training (QGT) method to guide DNN training towards optimized low-bit-precision targets and reach extreme compression levels below 8-bit precision. Unlike standard quantization-aware training (QAT) approaches, QGT uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. One of the main benefits of this approach is the ability to identify compression bottlenecks. We validate QGT using state-of-the-art model architectures on vision datasets. We also demonstrate the effectiveness of QGT with an 81KB tiny model for person detection down to 2-bit precision (representing 17.7x size reduction), while maintaining an accuracy drop of only 3% compared to a floating-point baseline.",
            "corpus_id": 231906389,
            "sentences": [
                {
                    "corpus_id": "231906389",
                    "title": "Quantization-Guided Training for Compact TinyML Models",
                    "text": "Table 1 presents the results of a number of QGT experiments for various architectures. Note that the main purpose of these experiments is to QGT's utility on small and large architectures in the context of a range of tasks. We used the asymmetric quantizer in all of the experiments. We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models. In the experiments, the quantizer was applied, both, in per-tensor and per-channel (for convolutional and depth-wise convolutional layers) fashions. We do not apply QGT to biases and the trainable parameters ( and  -see [16]) of the batch normalization layers. The reported top-1 accuracies were computed based on the dequantized weights with activations kept at four-byte floating point. Since the asymmetric quantization requires retaining slope and intercept, the sizes of per-channel-quantized models are slightly larger than their per-tensor counterparts quantized at the same bit-widths. \n\nThe results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy. We note that per-channel quantization provides higher accuracy than its per-tensor counterparts at the same bit-precision targets. At 2-bit precision target, our early results are not conclusive as the solution space might require a more comprehensive search, or the model architecture might reach its capacity for the task / dataset. \n\nTo see why QGT, in spite of the simplicity of its formulation is so effective, it is instructive to compare the histograms of a model parameters trained under QGT against those of its base floating-point model. Fig. 4 presents several histograms (from the 27 convolutional kernels plus the final dense layer) of the MobileNetV1 model trained on the ten-class ImageNette task.",
                    "score": 0.5023636650151021,
                    "section_title": "Benchmark Experiments",
                    "char_start_offset": 16545,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1075
                        },
                        {
                            "start": 1078,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1474
                        },
                        {
                            "start": 1475,
                            "end": 1678
                        },
                        {
                            "start": 1681,
                            "end": 1891
                        },
                        {
                            "start": 1892,
                            "end": 2056
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7744140625
                }
            ],
            "relevance_judgement": 0.7744140625,
            "relevance_judgment_input_expanded": "# Title: Quantization-Guided Training for Compact TinyML Models\n# Venue: arXiv.org\n# Authors: Sedigh Ghamari, Koray Ozcan, Thu Dinh, A. Melnikov, Juan Carvajal, Jan Ernst, S. Chai\n## Abstract\nWe propose a Quantization Guided Training (QGT) method to guide DNN training towards optimized low-bit-precision targets and reach extreme compression levels below 8-bit precision. Unlike standard quantization-aware training (QAT) approaches, QGT uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. One of the main benefits of this approach is the ability to identify compression bottlenecks. We validate QGT using state-of-the-art model architectures on vision datasets. We also demonstrate the effectiveness of QGT with an 81KB tiny model for person detection down to 2-bit precision (representing 17.7x size reduction), while maintaining an accuracy drop of only 3% compared to a floating-point baseline.\n## Benchmark Experiments\nTable 1 presents the results of a number of QGT experiments for various architectures. Note that the main purpose of these experiments is to QGT's utility on small and large architectures in the context of a range of tasks. We used the asymmetric quantizer in all of the experiments. We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models. In the experiments, the quantizer was applied, both, in per-tensor and per-channel (for convolutional and depth-wise convolutional layers) fashions. We do not apply QGT to biases and the trainable parameters ( and  -see [16]) of the batch normalization layers. The reported top-1 accuracies were computed based on the dequantized weights with activations kept at four-byte floating point. Since the asymmetric quantization requires retaining slope and intercept, the sizes of per-channel-quantized models are slightly larger than their per-tensor counterparts quantized at the same bit-widths. \n\nThe results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy. We note that per-channel quantization provides higher accuracy than its per-tensor counterparts at the same bit-precision targets. At 2-bit precision target, our early results are not conclusive as the solution space might require a more comprehensive search, or the model architecture might reach its capacity for the task / dataset. \n\nTo see why QGT, in spite of the simplicity of its formulation is so effective, it is instructive to compare the histograms of a model parameters trained under QGT against those of its base floating-point model. Fig. 4 presents several histograms (from the 27 convolutional kernels plus the final dense layer) of the MobileNetV1 model trained on the ten-class ImageNette task.",
            "reference_string": "[231906389 | Ghamari et al. | 2021 | Citations: 17]"
        },
        {
            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.zora.uzh.ch/id/eprint/217535/1/Wang_LiteEdge_Lightweight_Semantic_Edge_Detection_Network_ICCVW_2021_paper.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVW54120.2021.00300?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVW54120.2021.00300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Hao Wang"
                },
                {
                    "authorId": "118657651",
                    "name": "Hasan Al-Banna Mohamed"
                },
                {
                    "authorId": "15678675",
                    "name": "Zuowen Wang"
                },
                {
                    "authorId": "4765464",
                    "name": "Bodo Rueckauer"
                },
                {
                    "authorId": "1704961",
                    "name": "Shih-Chii Liu"
                }
            ],
            "abstract": "Scene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only \u2248 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device.",
            "corpus_id": 244531228,
            "sentences": [
                {
                    "corpus_id": "244531228",
                    "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
                    "text": "Quantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
                    "score": 0.5997216721128378,
                    "section_title": "Model compression study",
                    "char_start_offset": 17959,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 361,
                            "end": 365,
                            "matchedPaperCorpusId": "39867659"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77001953125
                }
            ],
            "relevance_judgement": 0.77001953125,
            "relevance_judgment_input_expanded": "# Title: LiteEdge: Lightweight Semantic Edge Detection Network\n# Venue: 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)\n# Authors: Hao Wang, Hasan Al-Banna Mohamed, Zuowen Wang, Bodo Rueckauer, Shih-Chii Liu\n## Abstract\nScene parsing is a critical component for understanding complex scenes in applications such as autonomous driving. Semantic segmentation networks are typically reported for scene parsing but semantic edge networks have also become of interest because of the sparseness of the segmented maps. This work presents an end-to-end trained lightweight deep semantic edge detection architecture called LiteEdge suitable for edge deployment. By utilizing hierarchical supervision and a new weighted multi-label loss function to balance different edge classes during training, LiteEdge predicts with high accuracy category-wise binary edges. Our LiteEdge network with only \u2248 3M parameters, has a semantic edge prediction accuracy of 52.9% mean maximum F (MF) score on the Cityscapes dataset. This accuracy was evaluated on the network trained to produce a low resolution edge map. The network can be quantized to 6-bit weights and 8-bit activations and shows only a 2% drop in the mean MF score. This quantization leads to a memory footprint savings of 6X for an edge device.\n## Model compression study\nQuantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
            "reference_string": "[244531228 | Wang et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Shrink and Eliminate: A Study of Post-Training Quantization and Repeated Operations Elimination in RNN Models",
            "venue": "Inf.",
            "year": 2022,
            "reference_count": 28,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/13/4/176/pdf?version=1650446015",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/info13040176?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/info13040176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1911208",
                    "name": "Nesma M. Rezk"
                },
                {
                    "authorId": "2541510",
                    "name": "T. Nordstr\u00f6m"
                },
                {
                    "authorId": "1403630586",
                    "name": "Z. Ul-Abdin"
                }
            ],
            "abstract": "Recurrent neural networks (RNNs) are neural networks (NN) designed for time-series applications. There is a growing interest in running RNNs to support these applications on edge devices. However, RNNs have large memory and computational demands that make them challenging to implement on edge devices. Quantization is used to shrink the size and the computational needs of such models by decreasing weights and activation precision. Further, the delta networks method increases the sparsity in activation vectors by relying on the temporal relationship between successive input sequences to eliminate repeated computations and memory accesses. In this paper, we study the effect of quantization on LSTM-, GRU-, LiGRU-, and SRU-based RNN models for speech recognition on the TIMIT dataset. We show how to apply post-training quantization on these models with a minimal increase in the error by skipping quantization of selected paths. In addition, we show that the quantization of activation vectors in RNNs to integer precision leads to considerable sparsity if the delta networks method is applied. Then, we propose a method for increasing the sparsity in the activation vectors while minimizing the error and maximizing the percentage of eliminated computations. The proposed quantization method managed to compress the four models more than 85%, with an error increase of 0.6, 0, 2.1, and 0.2 percentage points, respectively. By applying the delta networks method to the quantized models, more than 50% of the operations can be eliminated, in most cases with only a minor increase in the error. Comparing the four models to each other under the quantization and delta networks method, we found that compressed LSTM-based models are the most-optimum solutions at low-error-rates constraints. The compressed SRU-based models are the smallest in size, suitable when higher error rates are acceptable, and the compressed LiGRU-based models have the highest number of eliminated operations.",
            "corpus_id": 247880733,
            "sentences": [
                {
                    "corpus_id": "247880733",
                    "title": "Shrink and Eliminate: A Study of Post-Training Quantization and Repeated Operations Elimination in RNN Models",
                    "text": "In this section, we apply post-training quantization on the four models visualized in Figure 1 with selected path skipping as explained in Section 4. We first quantize the models' weights and activations to 16 bit-fixed and 8-bit, 4-bit, and 2-bit integers. Then we select eight mixed-precision configurations named M1 to M8. In the mixed-precisions configuration, we mix between 8-bit, 4-bit, and 2-bit weights and 8-bit and 4-bit activations. We select the precisions of the first layer input computations separately as we know it is the most sensitive layer [8]. The rest of the recurrent layer M \u00d7 V multiplication operations have the same precision. \n\nFor each quantization configuration, we show the resulting testing error rate and the size of the model in Table 2. As observed in the table, all models maintain low error rates at 8-bit quantization. The error rate increases reasonably at 4-bit quantization, But, at 2-bit quantization all the models fail. In the mixed-precision solutions, it is possible to quantize the models' weights to 2-bit and have the activation with higher precision to achieve a reasonable error increase. Furthermore, if the first layer input computation is kept in 8-bit integer precision, the lowest error rates are achieved (M1, M2, M3, M6, and M7). The LSTM-based and the SRU-based models keep the error increase between 0.5 p.p. (percentage points) and 1.7 p.p. (LSTM-based) and 2.5 p.p. (SRU-based) compared to their baseline errors. At the same time, the GRU model has a negligible increase in the error. In contrast, the increase of the baseline error of the LiGRU-based model is found higher. \n\nTable 2. The result of applying post-training quantization on the LSTM-, GRU-, LiGRU-, and SRUbased models. The first row is for the baseline model size and error rate. The second four rows are for non-mixed quantization.",
                    "score": 0.6032338180324553,
                    "section_title": "Post-Training Quantization of RNN Models",
                    "char_start_offset": 25581,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 654
                        },
                        {
                            "start": 657,
                            "end": 772
                        },
                        {
                            "start": 773,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1547
                        },
                        {
                            "start": 1548,
                            "end": 1637
                        },
                        {
                            "start": 1640,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1861
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 561,
                            "end": 564,
                            "matchedPaperCorpusId": "59413897"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76904296875
                }
            ],
            "relevance_judgement": 0.76904296875,
            "relevance_judgment_input_expanded": "# Title: Shrink and Eliminate: A Study of Post-Training Quantization and Repeated Operations Elimination in RNN Models\n# Venue: Inf.\n# Authors: Nesma M. Rezk, T. Nordstr\u00f6m, Z. Ul-Abdin\n## Abstract\nRecurrent neural networks (RNNs) are neural networks (NN) designed for time-series applications. There is a growing interest in running RNNs to support these applications on edge devices. However, RNNs have large memory and computational demands that make them challenging to implement on edge devices. Quantization is used to shrink the size and the computational needs of such models by decreasing weights and activation precision. Further, the delta networks method increases the sparsity in activation vectors by relying on the temporal relationship between successive input sequences to eliminate repeated computations and memory accesses. In this paper, we study the effect of quantization on LSTM-, GRU-, LiGRU-, and SRU-based RNN models for speech recognition on the TIMIT dataset. We show how to apply post-training quantization on these models with a minimal increase in the error by skipping quantization of selected paths. In addition, we show that the quantization of activation vectors in RNNs to integer precision leads to considerable sparsity if the delta networks method is applied. Then, we propose a method for increasing the sparsity in the activation vectors while minimizing the error and maximizing the percentage of eliminated computations. The proposed quantization method managed to compress the four models more than 85%, with an error increase of 0.6, 0, 2.1, and 0.2 percentage points, respectively. By applying the delta networks method to the quantized models, more than 50% of the operations can be eliminated, in most cases with only a minor increase in the error. Comparing the four models to each other under the quantization and delta networks method, we found that compressed LSTM-based models are the most-optimum solutions at low-error-rates constraints. The compressed SRU-based models are the smallest in size, suitable when higher error rates are acceptable, and the compressed LiGRU-based models have the highest number of eliminated operations.\n## Post-Training Quantization of RNN Models\nIn this section, we apply post-training quantization on the four models visualized in Figure 1 with selected path skipping as explained in Section 4. We first quantize the models' weights and activations to 16 bit-fixed and 8-bit, 4-bit, and 2-bit integers. Then we select eight mixed-precision configurations named M1 to M8. In the mixed-precisions configuration, we mix between 8-bit, 4-bit, and 2-bit weights and 8-bit and 4-bit activations. We select the precisions of the first layer input computations separately as we know it is the most sensitive layer [8]. The rest of the recurrent layer M \u00d7 V multiplication operations have the same precision. \n\nFor each quantization configuration, we show the resulting testing error rate and the size of the model in Table 2. As observed in the table, all models maintain low error rates at 8-bit quantization. The error rate increases reasonably at 4-bit quantization, But, at 2-bit quantization all the models fail. In the mixed-precision solutions, it is possible to quantize the models' weights to 2-bit and have the activation with higher precision to achieve a reasonable error increase. Furthermore, if the first layer input computation is kept in 8-bit integer precision, the lowest error rates are achieved (M1, M2, M3, M6, and M7). The LSTM-based and the SRU-based models keep the error increase between 0.5 p.p. (percentage points) and 1.7 p.p. (LSTM-based) and 2.5 p.p. (SRU-based) compared to their baseline errors. At the same time, the GRU model has a negligible increase in the error. In contrast, the increase of the baseline error of the LiGRU-based model is found higher. \n\nTable 2. The result of applying post-training quantization on the LSTM-, GRU-, LiGRU-, and SRUbased models. The first row is for the baseline model size and error rate. The second four rows are for non-mixed quantization.",
            "reference_string": "[247880733 | Rezk et al. | 2022 | Citations: 1]"
        },
        {
            "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
            "venue": "Design Automation Conference",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2402.14866",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.14866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2120170158",
                    "name": "Ziyi Guan"
                },
                {
                    "authorId": "2301516332",
                    "name": "Hantao Huang"
                },
                {
                    "authorId": "2286850679",
                    "name": "Yupeng Su"
                },
                {
                    "authorId": "2351687700",
                    "name": "Hong Huang"
                },
                {
                    "authorId": "2287187433",
                    "name": "Ngai Wong"
                },
                {
                    "authorId": "2288350991",
                    "name": "Hao Yu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24% and 70.48% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its effectiveness to produce high-quality quantized LLMs.",
            "corpus_id": 267897495,
            "sentences": [
                {
                    "corpus_id": "267897495",
                    "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
                    "text": "We assess the the performance of APTQ using the C4 [15] and WikiText-2 [13] benchmarks. We compare APTQ against three established PTQ methods: GPTQ [6], OWQ [10], and PB-LLM [16]. \n\nNotably, OWQ and PB-LLM extend upon GPTQ, with PB-LLM incorporating mixed-precision quantization. To ensure a balanced comparison, all methods are evaluated on a standardized platform. Moreover, we benchmark APTQ's performance with the leading QAT approach, LLM-QAT. Table 1 reveals that APTQ, at an average 4 bit, closely matches the full-precision model and attains SOTA performance on the C4 dataset, showing only a 0.01-point increase in perplexity. Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B. \n\nTo substantiate the robustness and broad applicability of the Hessian trace-based mixed-precision quantization posited in our study, we conducted a comparative analysis of various 4-bit utilization levels of APTQ against other prevalent PTQ methods applied to the LLaMa-7B model on the C4 dataset. The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy. These results unequivocally demonstrate the superior performance of APTQ, leveraging Hessian trace-driven precision allocation to optimize quantization outcomes. \n\nFigure 2 visually summarizes our findings.",
                    "score": 0.5962058044471796,
                    "section_title": "Evaluation of Perplexity performance",
                    "char_start_offset": 16177,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 179
                        },
                        {
                            "start": 182,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 941
                        },
                        {
                            "start": 944,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1707
                        },
                        {
                            "start": 1710,
                            "end": 1752
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 51,
                            "end": 55,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7685546875
                }
            ],
            "relevance_judgement": 0.7685546875,
            "relevance_judgment_input_expanded": "# Title: APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models\n# Venue: Design Automation Conference\n# Authors: Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao Yu\n## Abstract\nLarge Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24% and 70.48% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating its effectiveness to produce high-quality quantized LLMs.\n## Evaluation of Perplexity performance\nWe assess the the performance of APTQ using the C4 [15] and WikiText-2 [13] benchmarks. We compare APTQ against three established PTQ methods: GPTQ [6], OWQ [10], and PB-LLM [16]. \n\nNotably, OWQ and PB-LLM extend upon GPTQ, with PB-LLM incorporating mixed-precision quantization. To ensure a balanced comparison, all methods are evaluated on a standardized platform. Moreover, we benchmark APTQ's performance with the leading QAT approach, LLM-QAT. Table 1 reveals that APTQ, at an average 4 bit, closely matches the full-precision model and attains SOTA performance on the C4 dataset, showing only a 0.01-point increase in perplexity. Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B. \n\nTo substantiate the robustness and broad applicability of the Hessian trace-based mixed-precision quantization posited in our study, we conducted a comparative analysis of various 4-bit utilization levels of APTQ against other prevalent PTQ methods applied to the LLaMa-7B model on the C4 dataset. The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy. These results unequivocally demonstrate the superior performance of APTQ, leveraging Hessian trace-driven precision allocation to optimize quantization outcomes. \n\nFigure 2 visually summarizes our findings.",
            "reference_string": "[267897495 | Guan et al. | 2024 | Citations: 16]"
        },
        {
            "title": "An Empirical Study of Qwen3 Quantization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 19,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283439767",
                    "name": "Xingyu Zheng"
                },
                {
                    "authorId": "2359206148",
                    "name": "Yuye Li"
                },
                {
                    "authorId": "2359612976",
                    "name": "Haoran Chu"
                },
                {
                    "authorId": "2359148500",
                    "name": "Yue Feng"
                },
                {
                    "authorId": "2190791504",
                    "name": "Xudong Ma"
                },
                {
                    "authorId": "2283307736",
                    "name": "Jie Luo"
                },
                {
                    "authorId": "2261414086",
                    "name": "Jinyang Guo"
                },
                {
                    "authorId": "2329344125",
                    "name": "Haotong Qin"
                },
                {
                    "authorId": "2283140094",
                    "name": "Michele Magno"
                },
                {
                    "authorId": "2278053544",
                    "name": "Xianglong Liu"
                }
            ],
            "abstract": "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
            "corpus_id": 278326880,
            "sentences": [
                {
                    "corpus_id": "278326880",
                    "title": "An Empirical Study of Qwen3 Quantization",
                    "text": "The newly released Qwen3 series has emerged as one of the most capable open-source LLM families, garnering substantial attention from both academia and industry. In this study, we present the first systematic evaluation of Qwen3's robustness under various low-bit quantization schemes, with particular focus on post-training quantization methods. Our investigation seeks to establish practical boundaries for deploying Qwen3 in resource-constrained scenarios through comprehensive quantization analysis. \n\nOur experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios. \n\nThese findings underscore two critical implications: (1) current quantization techniques require further innovation to better preserve Qwen3's advanced capabilities, and (2) the trade-offs between model compression and performance retention need careful reconsideration for state-of-the-art LLMs. We believe our empirical analysis provides valuable guidance for future research directions in LLM quantization, particularly in developing methods that can maintain high accuracy at ultra-low  bit-widths. As the field progresses, we anticipate these insights will contribute to more efficient deployment of powerful models like Qwen3, ultimately advancing the practical applications of large language models while reducing their computational overhead. \n\nFuture Work. We plan to evaluate more advanced forms of quantization methods, such as channel reordering-based approaches [18] and rotation-based quantization strategies [11], to assess Qwen3's performance under these techniques, particularly regarding their impact on activation quantization. Table 1: 2 to 8-bits per-channel PTQ results of Qwen3-Base Models. We report ppl on Wikitext2 and c4, 0-shot reasoning tasks and 5-shot mmlu performance.",
                    "score": 0.4963973602685909,
                    "section_title": "Conclusion",
                    "char_start_offset": 4881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 503
                        },
                        {
                            "start": 506,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1881
                        },
                        {
                            "start": 1884,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2177
                        },
                        {
                            "start": 2178,
                            "end": 2244
                        },
                        {
                            "start": 2245,
                            "end": 2331
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75634765625
                }
            ],
            "relevance_judgement": 0.75634765625,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Qwen3 Quantization\n# Venue: arXiv.org\n# Authors: Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, Xianglong Liu\n## Abstract\nThe Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.\n## Conclusion\nThe newly released Qwen3 series has emerged as one of the most capable open-source LLM families, garnering substantial attention from both academia and industry. In this study, we present the first systematic evaluation of Qwen3's robustness under various low-bit quantization schemes, with particular focus on post-training quantization methods. Our investigation seeks to establish practical boundaries for deploying Qwen3 in resource-constrained scenarios through comprehensive quantization analysis. \n\nOur experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios. \n\nThese findings underscore two critical implications: (1) current quantization techniques require further innovation to better preserve Qwen3's advanced capabilities, and (2) the trade-offs between model compression and performance retention need careful reconsideration for state-of-the-art LLMs. We believe our empirical analysis provides valuable guidance for future research directions in LLM quantization, particularly in developing methods that can maintain high accuracy at ultra-low  bit-widths. As the field progresses, we anticipate these insights will contribute to more efficient deployment of powerful models like Qwen3, ultimately advancing the practical applications of large language models while reducing their computational overhead. \n\nFuture Work. We plan to evaluate more advanced forms of quantization methods, such as channel reordering-based approaches [18] and rotation-based quantization strategies [11], to assess Qwen3's performance under these techniques, particularly regarding their impact on activation quantization. Table 1: 2 to 8-bits per-channel PTQ results of Qwen3-Base Models. We report ppl on Wikitext2 and c4, 0-shot reasoning tasks and 5-shot mmlu performance.",
            "reference_string": "[278326880 | Zheng et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 15,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.02410",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152658577",
                    "name": "Young Jin Kim"
                },
                {
                    "authorId": "2191522437",
                    "name": "Raffy Fahim"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                }
            ],
            "abstract": "Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit expert weights can deliver better model performance than the dense model trained on the same dataset. As a result of low-bit quantization, we show the model size can be reduced by 79.6% of the original half precision floating point (fp16) MoE model. Combined with an optimized GPU runtime implementation, it also achieves 1.24X speed-up on A100 GPUs.",
            "corpus_id": 263620300,
            "sentences": [
                {
                    "corpus_id": "263620300",
                    "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
                    "text": "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores. Table 6 in Appendix shows the score differences by quantization for both MoE and dense models on 10 different language pairs translations.",
                    "score": 0.5255848758268349,
                    "section_title": "Robustness comparison between MoE and dense models",
                    "char_start_offset": 5122,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1095
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74951171875
                }
            ],
            "relevance_judgement": 0.74951171875,
            "relevance_judgment_input_expanded": "# Title: Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness\n# Venue: arXiv.org\n# Authors: Young Jin Kim, Raffy Fahim, H. Awadalla\n## Abstract\nLarge Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit expert weights can deliver better model performance than the dense model trained on the same dataset. As a result of low-bit quantization, we show the model size can be reduced by 79.6% of the original half precision floating point (fp16) MoE model. Combined with an optimized GPU runtime implementation, it also achieves 1.24X speed-up on A100 GPUs.\n## Robustness comparison between MoE and dense models\nWe compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores. Table 6 in Appendix shows the score differences by quantization for both MoE and dense models on 10 different language pairs translations.",
            "reference_string": "[263620300 | Kim et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Efficient Post-training Quantization with FP8 Formats",
            "venue": "Conference on Machine Learning and Systems",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 23,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.14592",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1921920",
                    "name": "Haihao Shen"
                },
                {
                    "authorId": "8792111",
                    "name": "Naveen Mellempudi"
                },
                {
                    "authorId": "2154166181",
                    "name": "Xin He"
                },
                {
                    "authorId": "2337868249",
                    "name": "Qun Gao"
                },
                {
                    "authorId": "87801490",
                    "name": "Chang\u2010Bao Wang"
                },
                {
                    "authorId": "2247654665",
                    "name": "Mengni Wang"
                }
            ],
            "abstract": "Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations. Furthermore, our findings suggest that E4M3 is better suited for NLP models, whereas E3M4 performs marginally better than E4M3 on computer vision tasks. The code is publicly available on Intel Neural Compressor: https://github.com/intel/neural-compressor.",
            "corpus_id": 262825537,
            "sentences": [
                {
                    "corpus_id": "262825537",
                    "title": "Efficient Post-training Quantization with FP8 Formats",
                    "text": "Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations. Furthermore, our findings suggest that E4M3 is better suited for NLP models, whereas E3M4 performs marginally better than E4M3 on computer vision tasks. The code is publicly available on Intel Neural Compressor: https://github.com/intel/neural-compressor.",
                    "score": 0.5440226048924168,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74169921875
                }
            ],
            "relevance_judgement": 0.74169921875,
            "relevance_judgment_input_expanded": "# Title: Efficient Post-training Quantization with FP8 Formats\n# Venue: Conference on Machine Learning and Systems\n# Authors: Haihao Shen, Naveen Mellempudi, Xin He, Qun Gao, Chang\u2010Bao Wang, Mengni Wang\n## Abstract\nRecent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations. Furthermore, our findings suggest that E4M3 is better suited for NLP models, whereas E3M4 performs marginally better than E4M3 on computer vision tasks. The code is publicly available on Intel Neural Compressor: https://github.com/intel/neural-compressor.\n",
            "reference_string": "[262825537 | Shen et al. | 2023 | Citations: 23]"
        },
        {
            "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2343738049",
                    "name": "Moses Ananta"
                },
                {
                    "authorId": "2191731497",
                    "name": "Farid Adilazuarda"
                },
                {
                    "authorId": "2306265527",
                    "name": "Z. M. K. Zuhri"
                },
                {
                    "authorId": "2257345523",
                    "name": "Ayu Purwarianti"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ],
            "abstract": "Fine-tuning large language models (LLMs) is often constrained by the computational costs of processing massive datasets. We propose \\textbf{QLESS} (Quantized Low-rank Gradient Similarity Search), which integrates gradient quantization with the LESS framework to enable memory-efficient data valuation and selection. QLESS employs a two-step compression process: first, it obtains low-dimensional gradient representations through LoRA-based random projection; then, it quantizes these gradients to low-bitwidth representations. Experiments on multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH, TyDiQA) show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Even 1-bit gradient quantization preserves data valuation quality. These findings underscore QLESS as a practical, scalable approach to identifying informative examples within strict memory constraints.",
            "corpus_id": 276106922,
            "sentences": [
                {
                    "corpus_id": "276106922",
                    "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
                    "text": "The results of our experiments, as summarized in Table 1 and Table 4, highlight the efficacy of QLESS in balancing storage efficiency and downstream model performance across various bit-width configurations. Our analysis focuses on the trade-offs between performance and storage, the generalizability of QLESS across models, and the surprising robustness of extreme quantization. \n\nOverall Performance. Our experiments demonstrate that QLESS achieves competitive performance across all evaluated models and benchmarks while significantly reducing storage requirements compared to LESS. On most benchmarks, QLESS results in comparable or better performance than LESS and the random baselines. Most notably, even with 1-bit quantization, Llama 3.1 8B for example achieves an average score of 65.93, exceeding the random 5% baseline of 64.71, demonstrating the robustness of our approach under extreme quantization. \n\nPerformance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation. \n\nRobustness Across Benchmarks. Across TyDiQA, MMLU, and BBH, QLESS at 8-bit precision frequently matches LESS, and 4-bit or 2-bit configurations remain within a tolerable performance gap. Although BBH can be more sensitive to quantization, 1-bit QLESS still performs competitively, confirming its resilience. These results underscore that even extreme compression maintains sufficient gradient information for robust data selection. \n\nInsights into 1-Bit Quantization. A particularly striking outcome is the effectiveness of 1-bit QLESS, which often attains performance near that of higher-bit variants. This finding challenges the assumption that low-precision representations inevitably undermine gradient-based methods.",
                    "score": 0.5347619309258194,
                    "section_title": "Main Results",
                    "char_start_offset": 13569,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 379
                        },
                        {
                            "start": 382,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 912
                        },
                        {
                            "start": 915,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1661
                        },
                        {
                            "start": 1664,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1850
                        },
                        {
                            "start": 1851,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2095
                        },
                        {
                            "start": 2098,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2266
                        },
                        {
                            "start": 2267,
                            "end": 2385
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7412109375
                }
            ],
            "relevance_judgement": 0.7412109375,
            "relevance_judgment_input_expanded": "# Title: QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning\n# Venue: arXiv.org\n# Authors: Moses Ananta, Farid Adilazuarda, Z. M. K. Zuhri, Ayu Purwarianti, Alham Fikri Aji\n## Abstract\nFine-tuning large language models (LLMs) is often constrained by the computational costs of processing massive datasets. We propose \\textbf{QLESS} (Quantized Low-rank Gradient Similarity Search), which integrates gradient quantization with the LESS framework to enable memory-efficient data valuation and selection. QLESS employs a two-step compression process: first, it obtains low-dimensional gradient representations through LoRA-based random projection; then, it quantizes these gradients to low-bitwidth representations. Experiments on multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH, TyDiQA) show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Even 1-bit gradient quantization preserves data valuation quality. These findings underscore QLESS as a practical, scalable approach to identifying informative examples within strict memory constraints.\n## Main Results\nThe results of our experiments, as summarized in Table 1 and Table 4, highlight the efficacy of QLESS in balancing storage efficiency and downstream model performance across various bit-width configurations. Our analysis focuses on the trade-offs between performance and storage, the generalizability of QLESS across models, and the surprising robustness of extreme quantization. \n\nOverall Performance. Our experiments demonstrate that QLESS achieves competitive performance across all evaluated models and benchmarks while significantly reducing storage requirements compared to LESS. On most benchmarks, QLESS results in comparable or better performance than LESS and the random baselines. Most notably, even with 1-bit quantization, Llama 3.1 8B for example achieves an average score of 65.93, exceeding the random 5% baseline of 64.71, demonstrating the robustness of our approach under extreme quantization. \n\nPerformance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation. \n\nRobustness Across Benchmarks. Across TyDiQA, MMLU, and BBH, QLESS at 8-bit precision frequently matches LESS, and 4-bit or 2-bit configurations remain within a tolerable performance gap. Although BBH can be more sensitive to quantization, 1-bit QLESS still performs competitively, confirming its resilience. These results underscore that even extreme compression maintains sufficient gradient information for robust data selection. \n\nInsights into 1-Bit Quantization. A particularly striking outcome is the effectiveness of 1-bit QLESS, which often attains performance near that of higher-bit variants. This finding challenges the assumption that low-precision representations inevitably undermine gradient-based methods.",
            "reference_string": "[276106922 | Ananta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 44,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.02031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164141997",
                    "name": "Lorenzo Papa"
                },
                {
                    "authorId": "2047330818",
                    "name": "Paolo Russo"
                },
                {
                    "authorId": "2281890830",
                    "name": "Irene Amerini"
                },
                {
                    "authorId": "2237944863",
                    "name": "Luping Zhou"
                }
            ],
            "abstract": "Vision Transformer (ViT) architectures are becoming increasingly popular and widely employed to tackle computer vision applications. Their main feature is the capacity to extract global information through the self-attention mechanism, outperforming earlier convolutional neural networks. However, ViT deployment and performance have grown steadily with their size, number of trainable parameters, and operations. Furthermore, self-attention's computational and memory cost quadratically increases with the image resolution. Generally speaking, it is challenging to employ these architectures in real-world applications due to many hardware and environmental restrictions, such as processing and computational capabilities. Therefore, this survey investigates the most efficient methodologies to ensure sub-optimal estimation performances. More in detail, four efficient categories will be analyzed: compact architecture, pruning, knowledge distillation, and quantization strategies. Moreover, a new metric called Efficient Error Rate has been introduced in order to normalize and compare models\u2019 features that affect hardware devices at inference time, such as the number of parameters, bits, FLOPs, and model size. Summarizing, this paper first mathematically defines the strategies used to make Vision Transformer efficient, describes and discusses state-of-the-art methodologies, and analyzes their performances over different application scenarios. Toward the end of this paper, we also discuss open challenges and promising research directions.",
            "corpus_id": 261531260,
            "sentences": [
                {
                    "corpus_id": "261531260",
                    "title": "A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking",
                    "text": "In this section, we compare the estimation performances of general-purpose quantization strategies, which have been introduced in Section 3.4 and formalized in Section 2.4. As commonly reported in the reference papers, the different strategies are compared to the same architecture structure; precisely, we report the results obtained over ViT, DeiT, and Swin architectures, respectively, in Table 5, Table 6, and Table 7. \n\nDifferently from previous studies, these tables report the bit-width (#Bit) in which the models are compressed for both weights (W) and activation functions (A), as well as the size (Size) of the model after the quantization. This choice is due to the fundamental effect of the Q efficient strategy, which tries to preserve the estimation performances of originally trained models with a lower data precision, i.e., compressing their data from 32-bit to 8/6/4-bit. As a result, after the Q methodology, the number of trainable parameters (#Pram.) will remain constant, but the data precision of each neuron, and hence the total size of the model, will be reduced. The first performed analysis regards the optimal bitwidth in order to generate efficient and accurate models. Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size.",
                    "score": 0.5347225025726516,
                    "section_title": "Results of Quantization strategies",
                    "char_start_offset": 67069,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 422
                        },
                        {
                            "start": 425,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1800
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Lorenzo Papa, Paolo Russo, Irene Amerini, Luping Zhou\n## Abstract\nVision Transformer (ViT) architectures are becoming increasingly popular and widely employed to tackle computer vision applications. Their main feature is the capacity to extract global information through the self-attention mechanism, outperforming earlier convolutional neural networks. However, ViT deployment and performance have grown steadily with their size, number of trainable parameters, and operations. Furthermore, self-attention's computational and memory cost quadratically increases with the image resolution. Generally speaking, it is challenging to employ these architectures in real-world applications due to many hardware and environmental restrictions, such as processing and computational capabilities. Therefore, this survey investigates the most efficient methodologies to ensure sub-optimal estimation performances. More in detail, four efficient categories will be analyzed: compact architecture, pruning, knowledge distillation, and quantization strategies. Moreover, a new metric called Efficient Error Rate has been introduced in order to normalize and compare models\u2019 features that affect hardware devices at inference time, such as the number of parameters, bits, FLOPs, and model size. Summarizing, this paper first mathematically defines the strategies used to make Vision Transformer efficient, describes and discusses state-of-the-art methodologies, and analyzes their performances over different application scenarios. Toward the end of this paper, we also discuss open challenges and promising research directions.\n## Results of Quantization strategies\nIn this section, we compare the estimation performances of general-purpose quantization strategies, which have been introduced in Section 3.4 and formalized in Section 2.4. As commonly reported in the reference papers, the different strategies are compared to the same architecture structure; precisely, we report the results obtained over ViT, DeiT, and Swin architectures, respectively, in Table 5, Table 6, and Table 7. \n\nDifferently from previous studies, these tables report the bit-width (#Bit) in which the models are compressed for both weights (W) and activation functions (A), as well as the size (Size) of the model after the quantization. This choice is due to the fundamental effect of the Q efficient strategy, which tries to preserve the estimation performances of originally trained models with a lower data precision, i.e., compressing their data from 32-bit to 8/6/4-bit. As a result, after the Q methodology, the number of trainable parameters (#Pram.) will remain constant, but the data precision of each neuron, and hence the total size of the model, will be reduced. The first performed analysis regards the optimal bitwidth in order to generate efficient and accurate models. Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size.",
            "reference_string": "[261531260 | Papa et al. | 2023 | Citations: 44]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "259937594",
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "text": "In this part, we present the experimental results and the corresponding analysis. \n\nOverall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets. \n\n4-bit precision exhibits a favorable trade-off in terms of both total bits and performance. As shown in Table 1, it can be observed that 4-bit quantization offers a notable reduction in memory cost. \n\nTo further examine the relation between model performance and resource usage, we follow Dettmers and Zettlemoyer (2022) to introduce the measure of total bits by multiplying the number of the parameters and the bits, and report the test results in Figure 1 by varying the number of total bits. s From the four accuracy curves corresponding to different bit precision, we can see that 4-bit preci-sion consistently exhibits higher model accuracy under the same amount of total model bits. Thus, 4-bit quantization is recommended to be used for a favorable balance between memory cost and model performance in practice. \n\nThe scaling effect depends on specific tasks, and increasing the model scale benefits the CoT task the most. We conducted an investigation, as depicted in Figure 1, to examine the impact of scaling the total number of bits on the performance of a low-bit model across multiple tasks.",
            "score": 0.8143811390715305,
            "section_title": "Results and Analysis",
            "char_start_offset": 10205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "277272083",
            "title": "Energy-Aware LLMs: A step towards sustainable AI for downstream applications",
            "text": "In the detailed training loss, the 16-bit model and the 32bit model were almost identical, with only negligible changes throughout the epochs. That means 16-bit precision performance is close to 32-bit performance, with energy efficiency; the best case reaches up to 80% in energy efficiency. While the performances of the 8-bit and 4-bit models are also similar to each other during the same range of epochs, they tend to show a bit higher training loss compared to 16-bit and 32-bit models. Also, the performance difference did exist between the 16-bit/32-bit models and the 8-bit/4-bit models, though small, and the 32-bit model was better than its low-precision version. Therefore, it underlines that the 16-bit model gives the best trade-off considering the power consumption and performance for the current task and hardware in this phase. Turning to the results from the inference phase, Fig. 4 and Fig. 5 highlight the effects of quantization levels during the inference phase. Similar to the fine-tuning phase, the 16-bit model stands out as one of the top candidates for energy efficiency (reduction up to 40.5% overall), offering a good compromise with model performance. In detail, the BERT score drops slightly while other metrics show a slight improvement. The 8-bit model demonstrates an improvement across most metrics, but this comes with a substantial rise in both energy consumption and the number of tokens generated per second. Thus, this suggests that it can maintain accuracy while using significantly fewer bits in a downstream application. However, this comes at a significant cost to energy efficiency, resulting in higher energy consumption and slower token generation. The comparison underscores that while the 8-bit Fig. 5: Impact of quantization levels 3 . Fig. 6: LLAMA3: impact of unstructured-base pruning and structured pruning 3 . model offers improved performance over the 32-bit model in the same task, the trade-off in resource usage needs to be carefully evaluated, especially when aiming for energy efficiency. One of the reasons for this is quantization also introduces quantization errors into the model, which adds extra computational cost as the model works to recover the weights during inference [22].",
            "score": 0.7037396866871768,
            "section_title": "IV. RESULTS AND FINDINGS",
            "char_start_offset": 18371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54541015625
        },
        {
            "corpus_id": "270870579",
            "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks",
            "text": "For all the three \"High\" DNNs (leftmost plot), most of the weight parameters are quantized at 8 bits, i.e. the highest possibile precision, as expected. In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages. For what concerns \"Low\" models, except when trained with the MPIC regularizer, the 4-bit precision is assigned more often than 8-bit. In particular, with a more deepened analysis, it is possible to see that the 8-bit precision is favored only in the final layer, which is a common finding [7], [28]. Size is the only cost regularizer leading to some weights' channels being quantized at 2 bits. The MPIC cost model mainly favors pruning and keeps most of the other weights at 8 bits, since there is not a sufficient cost difference between this and smaller bit-widths. The NE16 cost model, instead, encourages a more spread-out distribution between 4-and 8-bit but entirely avoids 2-bit precision. The reason is that the NE16 cost model does not scale linearly with the output channels, as each processing element (PE) handles groups of 32 output channels (Sec. 4.3.3). Consequently, running a single channel at one precision incurs the same cost as running 32 channels, implying that to enhance latency, the NE16 accelerator should execute at least 32 2-bit filters. However, this would lead to suboptimal solutions from an accuracy standpoint, and is thus avoided by the optimization.",
            "score": 0.6790881172950309,
            "section_title": "Models analysis",
            "char_start_offset": 59907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1737
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "215745195"
                },
                {
                    "start": 845,
                    "end": 849,
                    "matchedPaperCorpusId": "220363587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "211677681",
            "title": "Quantized Neural Network Inference with Precision Batching",
            "text": "During evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision. \n\nFor natural language inference, we train a model with a 1layer 3072 unit LSTM encoder and a 3-layer 3072 unit fully connected decoder (a larger version of that seen in (Bowman et al., 2015)). We train on the SNLI dataset (Bowman et al., 2015) for 10 epochs and reach a baseline accuracy of 78%. \n\nDuring evaluation of quantization on model accuracy, we uniformly quantize both the weights and activations of the LSTM encoder and the fully connected decoder to the target precisions. \n\nTable 2 shows model performance (accuracy for MNIST and natural language inference, perplexity for language modeling) for different weight and activations precisions. For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;Xu et al., 2018).",
            "score": 0.6760109641609542,
            "section_title": "Benefits of Higher Precision Activations",
            "char_start_offset": 26530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 403,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 435,
                    "end": 456,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 2076,
                    "end": 2092,
                    "matchedPaperCorpusId": "8257350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "251979721",
            "title": "Efficient Methods for Natural Language Processing: A Survey",
            "text": "Mapping high-precision data types to low-precision ones is referred to as quantization. Quantization can be applied at different stages in the NLP model-building pipeline to reduce training and inference costs. Various research has shown that low-precision data format can reduce memory consumption by 4x-24x and improve the throughput by 4.5x compared to 32-bit floating point format. Various works targeted specific precision-levels such as integers (Kim et al., 2021), 8-bit (Quinn and Ballesteros, 2018;Zafrir et al., 2019;Bhandare et al., 2019;Prato et al., 2020;Dettmers et al., 2022a), ternary (Zhang et al., 2020;Ji et al., 2021;Zadeh et al., 2022), and even binary representations (Bai et al., 2021). \n\nDifferent components may have a different sensitivities regarding their underlying precision, so there is a body of work on mixed-precision quantization. Shen et al. (2020) showed that embedding layers require more precise parameter representations than the attention layer, while Kim et al. (2021) showed that nonlinear functions require more bits than the general matrix multiplication. Others defined quantization as a constrained optimization problem to automatically identify layers where lower precision is sufficient (Hubara et al., 2021). Finally, several works proposed quantization during training to make them robust against performance loss after quantization (Zafrir et al., 2019;Kim et al., 2021;Stock et al., 2021). For instance, Bai et al. ( 2021) and Zhang et al. (2020) proposed using knowledge distillation to maintain the accuracy of binarized and ternarized models. These show that component-customized quantization can preserve accuracy while improving efficiency. To maximize the benefit from quantization, one should also consider the available underlying hardware and associated specialized kernels compatible with different bit representations (Noune et al., 2022;Kuzmin et al., 2022).",
            "score": 0.6663694054841036,
            "section_title": "Quantization",
            "char_start_offset": 30163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 478,
                    "end": 507,
                    "matchedPaperCorpusId": "4858508"
                },
                {
                    "start": 507,
                    "end": 527,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 527,
                    "end": 549,
                    "matchedPaperCorpusId": "173990430"
                },
                {
                    "start": 549,
                    "end": 568,
                    "matchedPaperCorpusId": "215862562"
                },
                {
                    "start": 621,
                    "end": 637,
                    "matchedPaperCorpusId": "235293987"
                },
                {
                    "start": 637,
                    "end": 656,
                    "matchedPaperCorpusId": "247627656"
                },
                {
                    "start": 866,
                    "end": 884,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1236,
                    "end": 1257,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 1384,
                    "end": 1405,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1422,
                    "end": 1441,
                    "matchedPaperCorpusId": "215814169"
                },
                {
                    "start": 1902,
                    "end": 1922,
                    "matchedPaperCorpusId": "251710272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.413818359375
        },
        {
            "corpus_id": "233324191",
            "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures",
            "text": "32-bit floating-point (FP32) has been the predominant numerical format for deep learning, however the current surge for reduced bandwidth and compute resources has propelled the implementation of lower-precision formats. It has been demonstrated that weights and activation representations via 8-bit integers (INT8) have not led to an evident accuracy loss. For instance, BERT's quantization to 16/8-bit weight format resulted in 4\u00d7 model compression with minimal accuracy loss, consequently, a scaled-up BERT serves a billion CPU requests daily.",
            "score": 0.6590327915209265,
            "section_title": "VI-C QUANTIZATION",
            "char_start_offset": 59471,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "273501713",
            "title": "Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks",
            "text": "The results suggest that 4-bit integer quantization provides the best balance between model performance and model size. \n\nIt is consistent with a conclusion made in the earlier study that evaluated quantized LLMs on general reasoning and knowledge tasks [16]. Furthermore, while still being smaller in size, quantized 4-bit models with 7 billion parameters performed better than non-quantized half-precision models with 3 billion or less parameters. \n\nOn the other hand, 2-bit integer quantization resulted in a significant performance degradation. In the extreme case of 2-bit CodeGemma, there was a complete breakdown in the model's ability to generate coherent responses. This is likely an effect of hallucination [38; 39]. The low precision rounding likely impacted the model's next token prediction ability (underlying probability distributions) resulting in a sequence of repetitive out-of-context tokens. \n\nAccording to [14], StarCoderBase 1B, StarCoderBase 15B, and CodeLlama 34B demonstrated MultiPL-E pass@1 percentages of 12.1, 26.6, and 43.9 for Lua. In another study [33], the InCoder 6.7B, CodeGen 16.1B, and Codex 12B models demonstrated MultiPL-HumanEval pass@1 rates of approximately 0.05, 0.08, and 0.41 for Lua. In the same study, the corresponding pass@1 rates in the MultiPL-MBPP Lua benchmark were 0.13, 0.09, and 0.49. These values can be compared with the pass@1 rates in Table 4. The 4-bit and 8-bit quantized models with 7B parameters generally do not perform much worse than the non-quantized models in [14; 33] 7: The results of the linear regression on inference time with benchmarks, model, q-bits, and correctness as nominal predictors. The regression model includes all two-way interactions but only the significant interactions are listed in the table. The overall results suggest that code LLMs quantized at 4-bit integer precision can be comfortably run on an average CPU-only consumer laptop while maintaining good performance relative to other quantized and non-quantized code LLMs.",
            "score": 0.6472750501049759,
            "section_title": "Discussion",
            "char_start_offset": 40279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "258205341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "258841328",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "text": "While the 4-bit NormalFloat (NF4) data type is informationtheoretically optimal, it still needs to be determined if this property translates to empirical advantages. We follow the setup from Dettmers and Zettlemoyer [13] where quantized LLMs (OPT [72], BLOOM [52], Pythia [7], LLaMA) of different sizes (125M to 65B) with different data types are evaluated on language modeling and a set of zero-shot tasks. In Figure 3 and Table 2 we see that NF4 improves performance significantly over FP4 and Int4 and that double quantization reduces the memory footprint without degrading performance. \n\nk-bit QLORA matches 16-bit full finetuning and 16-bit LoRA performance Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13,18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization. \n\nFor our second setup, since full finetuning models at and beyond 11B parameters requires more than one server of high memory GPUs, we continue to test whether 4-bit QLORA can match 16-bit LoRA at the 7B to 65B parameter scales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets, Alpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance.",
            "score": 0.631969683866836,
            "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
            "char_start_offset": 18538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1959
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "271161904",
            "title": "Accuracy is Not All You Need",
            "text": "We next perform a detailed qualitative examination of the performance of these models.Specifically, we choose the Llama2-70B-chat model since it has the highest MT-Bench score (Table 3).We compare the 16-bit baseline against 8-bit and 4-bit models, quantized using LLM.int8().We chose LLM.int8() as it matches the accuracy of the baseline on most tasks and also has the highest GPT4 scores among the W8A8 and 4-bit quantized models for this task (Table 3).\n\nWe summarize our findings of the qualitative analysis for a sample of ten questions (out of \u2248 30 that had similar issues) from MT-Bench in Table 4.The corresponding generated text of all three models for these questions are provided in Table 20.Overall, we find that the 4-bit and 8-bit models are significantly worse than the 16-bit baseline.Specifically, we find that the 4-bit model often does not follow the provided instruction, makes more mistakes, and rambles a lot more, with the 8-bit model performing in-between the 16-bit and 4-bit models.\n\nWe encourage the reader to look at the full model responses in Table 20 (at least the first one!) to convince themselves that, at least for this task, there is significant degradation due to quantization, despite these two compressed models matching baseline accuracy on various tasks (e.g., MMLU accuracy within 1%) and suffering only a 0.4 lower score on a scale of ten in the GPT4 evaluation.We believe that this qualitative analysis adds further evidence to our claim that benchmark accuracy alone, as is standard practise today, is a poor metric to evaluate compressed LLMs, especially, if they are likely to be used for generative tasks in downstream applications.",
            "score": 0.6274167552818286,
            "section_title": "Qualitative evaluation",
            "char_start_offset": 23145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 1008
                },
                {
                    "start": 1010,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "254408710",
            "title": "Approximations in Deep Learning",
            "text": "For fixed-point arithmetic, [LTA16] explored the use of various bit width combinations (4, 8 and 16 bits) of weights and activations. Notable results with integer arithmetic are presented in [JKC + 18], which showcases how 8-bit integer quantization on ARM CPUs can achieve nearidentical accuracy compared to baseline float32 models based on MobileNet architectures for classification and detection tasks, but with improved on-device latency. Good quantization results with 4-bit weights and activations are presented in [BNS19] by combining three complementary methods for minimizing quantization error at the tensor level. Heterogeneous/mixed-precision quantization approaches also heavily focus on integer/fixed-point formats [WWZ + 18, WLL + 19, DYG + 19, DYC + 19, CYD + 20]. \n\nOne problem with low precision integer/fixed-point formats is that they have limited dynamic range, which might make them inappropriate, especially for networks used in Natural Language Processing (NLP) tasks, where weights tend to have values that are more than 10\u00d7 larger than the largest magnitude values found in popular CNNs [TYW + 20, Fig. 1]. While not that widespread, there has been some work looking into low precision floating-point quantization for CNN inference. For instance, [SBD + 18] explores the use of up to 8-bit (scaled) floating-point formats for weight and activation quantization in classification networks such as GoogLeNet, ResNet, and MobileNet, without any accuracy degradation. More recently, [WWC + 20, WWL + 20] show how an 8-bit floating-point quantization format (4-bit mantissa and 3-bit exponent) can be used in FPGA-based accelerators for deep CNN inference, without any retraining. Another approach [TYW + 20] consists of an adaptive floating-point quantization method, where the exponent range of quantized values is dynamically shifted at each network layer (through changing the bias term of the exponent), yielding competitive results on NLP networks and tasks.",
            "score": 0.6248155134753837,
            "section_title": "Quantization formats.",
            "char_start_offset": 29752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 35,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 521,
                    "end": 528,
                    "matchedPaperCorpusId": "59292009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "211677681",
            "title": "Quantized Neural Network Inference with Precision Batching",
            "text": "Next we show that using higher precision for activations leads to significantly better model accuracy at low bitwidths. We benchmark model accuracy across three applications: MNIST, language modeling and natural language inference. For each we train one baseline full precision model and evaluate the effects of various levels of weight and activation quantization on the model's end performance. For each model/application we quantize weights and activations to 1, 4, 8, 16 and 32 bits. \n\nFor the MNIST task (LeCun & Cortes, 2010), we train a 3-layer fully connected neural network with a hidden size of 4096 for 20 epochs, reaching a baseline accuracy of 98%. We uniformly quantize the weights and activations of each layer to the target precisions. Table 2. Benefits of using more precision for activations on model quality, evaluated on MNIST, language modeling (Wikitext-2) and natural language inference (SNLI). Generally, using higher level activations allows quantizing twice as many bits (e.g: from 8-bits to 4-bits) with little degradation of model accuracy. Note that for accuracy (acc), higher is better, whereas for perplexity (ppl) lower is better (the best score for each weight precision is bolded). \n\nFor language modeling, we train a model with a 1-layer 2048 unit LSTM (Hochreiter & Schmidhuber, 1997) as the encoder, and a 1-layer 2048 unit fully connected as the decoder (a common architecture used in language modeling (Melis et al., 2017)). We apply dropout with a factor of .5 to the inputs of the encoder LSTM's recurrence, and to the encoder LSTM's output. We train the model on the Wikitext-2 dataset (Merity et al., 2016) for 40 epochs, reaching a baseline perplexity of 93. During evaluation of quantization on model accuracy, we quantize the LSTM's input and hidden layers to the same weight and activation levels; however, we keep the final fully connected decoder in full precision.",
            "score": 0.6160333945417501,
            "section_title": "Benefits of Higher Precision Activations",
            "char_start_offset": 24827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 1288,
                    "end": 1320,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1441,
                    "end": 1461,
                    "matchedPaperCorpusId": "33513311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "278367647",
            "title": "QStore: Quantization-Aware Compressed Model Storage",
            "text": "Quantization is commonly applied to models to achieve desired quality-resource consumption tradeoffs. In this section, we overview the pros and cons of common quantization techniques, and key differences between QStore and quantization. \n\nCommon Quantization Targets. While 32-bit floating-point (FP32) precision was once standard [46], the recent increases in model sizes and corresponding increases in computational and memory requirements have driven the adoption of lower-precision, quantized model formats. For example, 16-bit precision (FP16 [35], BF16 [7,40]) formats have become a de-facto standard for training and fine-tuning to balance between accuracy and resource consumption. For more resource-constrained scenarios or latency-sensitive applications (e.g., on-device processing [63]), further quantization is commontypically to 8-bit (INT8) [28,36], but sometimes more aggressively to 4-bit (INT4, NF4) [29,30,43] or even lower [56]. Recently, FP8 quantization has also been used during inference [45]. \n\nQuantization Methods. There exists several notable classes of quantization methods commonly applied to foundation models. \n\n(1) RTN (round to nearest) rounds weights to the nearest representable value in low-precision format (e.g., 42.25 \u2192 42), which is fast, but can significantly degrade model accuracy (e.g., with outlier weights). (2) Channel-wise quantization such as LLM.int8() [28] and SmoothQuant [62] apply per-channel scaling and quantization to model weights to better preserve outliers. (3) Reconstructionbased approaches such as AWQ [43] and GPTQ [30] are also applied on a per-channel or per-block level, but they aim to quantize in a fashion such that the original high-precision weights can be reconstructed with minimal error. While these methods are capable of quantizing to very low precisions such as INT4 and INT3, they incur higher computational overhead versus alternatives.",
            "score": 0.614745232852385,
            "section_title": "Quantization",
            "char_start_offset": 7449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "3297437"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "32010676"
                },
                {
                    "start": 855,
                    "end": 859,
                    "matchedPaperCorpusId": "258509304"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 917,
                    "end": 921,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "258509304"
                },
                {
                    "start": 1424,
                    "end": 1428,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "258999941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "268201894",
            "title": "An All-Optical General-Purpose CPU and Optical Computer Architecture",
            "text": "With the choice of bit width on data words being merely a choice on performance (and with floorspace being the tradeoff), the question is, what is the ideal width? 64-bit is a great general-purpose candidate, but requires an almost O(n 2 ) number of logic elements to implement even the simplest arithmetic function, making it less ideal for pure throughput. In deep learning models, the current push is towards AI quantization [66]. This quantization process reduces the typical 32-bit floating point precision of deep learning models all the way down to 4-bit values [67], [68], without a significant loss in precision of the model. Not only does this drastically reduce the size of the models, such as for LLaMA [69] and many other large language models [70], but also improves the performance of the computation. The current sweet spot for AI quantization is converging to a 4-bit and 8-bit integer paradigm [71]. \n\nThe use of 4-bit and 8-bit integers in AI over the classical 32-bit floating point representations make a very strong case for optical computing, as it would otherwise be quite unfeasible to implement the full IEEE 754 floating-point standard [72] in a PIC and software emulation too performance heavy. However, 4-and 8-bit integer operations can be implemented economically. For the few tasks (see also Figure 2) that require real numbers, alternatives to floating-point, such as posits, are proving to be more accurate and easier to implement in hardware [73] or software emulation is valid option. \n\nFor an all-optical processor using the current generation of PIC technologies, a 16-bit wide arithmetic unit strikes a good balance between performance and gate count. 16-bit integers are wide enough, that the majority of the numbers that are currently represented by 32-bits or 64-bits in software experience a minor performance hit, as they rarely exceed a value of 65,535 (examples include most counters in loops, array indices, Boolean values or characters). Should the need arise to represent a 32-bit or 64-bit value, the performance loss is on a manageable order of magnitude for individual arithmetic operations.",
            "score": 0.6140635310843523,
            "section_title": "B. \"2-Bit is not enough\"?",
            "char_start_offset": 22057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "39715445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2393798828125
        },
        {
            "corpus_id": "273228873",
            "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
            "text": "Figure 4: Bit precision analysis for efficient quantization. We show results on out-of-domain test accuracy with two different datasets, i.e., PACS and TerraIncognita. For each bit precision, we report the increase in the test domain accuracy averaged across all domains. The 7-bit quantized model exhibits the maximum increase for both datasets. We quantize the model at 2000 steps. \n\nHere, we empirically analyze the effect of different bitprecisions for quantization on the generalization of the model. We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy.",
            "score": 0.6132854721230616,
            "section_title": "PACS (ERM Baseline) Terra Incognito (ERM Baseline)",
            "char_start_offset": 25260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1167
                }
            ],
            "ref_mentions": [
                {
                    "start": 608,
                    "end": 625,
                    "matchedPaperCorpusId": "6037691"
                },
                {
                    "start": 645,
                    "end": 665,
                    "matchedPaperCorpusId": "49744838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "218788236",
            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
            "text": "In deep learning, a 32-bit floating-point (FP32) format is used for training and inference purposes. Recent works [26] have achieved an 8-bit fixed-point precision of weights and activations without incurring any inference accuracy loss. However, unlike GPUs and CPUs, RRAM devices are limited by finite quantization levels [13]. Hence, an 8-bit quantization of parameters is not a viable solution for RRAM-based crossbars. In this article, we perform a more conservative 4-bit quantization for the model parameters, i.e., weights and biases. We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
            "score": 0.6108655721722611,
            "section_title": "B. DEVICE QUANTIZATION",
            "char_start_offset": 7268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "16877209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "248834483",
            "title": "ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks",
            "text": "We compare ShiftAddNAS with SOTA language models on two NLP tasks to evaluate its efficacy. Fig. 5 shows that Shif-tAddNAS consistently outperforms all the baselines in terms of BLEU scores and FLOPs. Specifically, ShiftAddNAS with full precision achieves 11.8% \u223c 73.6% FLOPs reductions while offering a comparable or better BLEU score (-0.3 \u223c +1.1), over all the full precision baselines. To bench--64.4% +4.7 Acc. \n\n-93.0% Energy mark with Lite Transformer (8-bit) which is dedicated for mobile devices, we refer to a SOTA quantization technique (Banner et al., 2018) for quantizing ShiftAddNAS to 8-bit fixed point: ShiftAddNAS (8-bit) achieves +1.8 \u223c +4.9 BLEU scores improvements over Lite Transformer (8-bit), while offering 5.0% \u223c 82.7% FLOPs reductions, and aggressively reduces 91.6% \u223c 98.4% FLOPs as compared to all the full-precision baselines with comparable BLEU scores (-0.1 \u223c +0.3). Note that for quantized models, we follow (Zhou et al., 2016) to use FLOPs \u00d7 (Bit/32) 2 for calculating the effective FLOPs which is proportional to the number of bit-operations. We further compare various aspects of ShiftAddNAS with all baselines in Tab. 3. As illustrated in this  FBNet and leads to 33.8% and 38.6% latency savings on CIFAR-10 and CIFAR-100, respectively.",
            "score": 0.6058541174210511,
            "section_title": "ShiftAddNAS vs. SOTA Methods on NLP Tasks",
            "char_start_offset": 24366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "258841328",
            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
            "text": "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision. \n\nSummary Our results consistently show that 4-bit QLORA with NF4 data type matches 16bit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with wellestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that double quantization does not degrade performance. Combined, this forms compelling evidence that 4-bit QLORA tuning reliably yields results matching 16-bit methods. \n\nIn line with previous work on quantization [13], our MMLU and Elo results indicate that with a given finetuning and inference resource budget it is beneficial to increase the number of parameters in the base model while decreasing their precision. This highlights the importance of efficiency benefits from QLORA. Since we did not observe performance degradation compared to full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. \n\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. \n\n5 Pushing the Chatbot State-of-the-art with QLoRA \n\nHaving established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate on a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for real-world chatbot performance evaluation.",
            "score": 0.6057468385364775,
            "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
            "char_start_offset": 20373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2111
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "247880733",
            "title": "Shrink and Eliminate: A Study of Post-Training Quantization and Repeated Operations Elimination in RNN Models",
            "text": "In this section, we apply post-training quantization on the four models visualized in Figure 1 with selected path skipping as explained in Section 4. We first quantize the models' weights and activations to 16 bit-fixed and 8-bit, 4-bit, and 2-bit integers. Then we select eight mixed-precision configurations named M1 to M8. In the mixed-precisions configuration, we mix between 8-bit, 4-bit, and 2-bit weights and 8-bit and 4-bit activations. We select the precisions of the first layer input computations separately as we know it is the most sensitive layer [8]. The rest of the recurrent layer M \u00d7 V multiplication operations have the same precision. \n\nFor each quantization configuration, we show the resulting testing error rate and the size of the model in Table 2. As observed in the table, all models maintain low error rates at 8-bit quantization. The error rate increases reasonably at 4-bit quantization, But, at 2-bit quantization all the models fail. In the mixed-precision solutions, it is possible to quantize the models' weights to 2-bit and have the activation with higher precision to achieve a reasonable error increase. Furthermore, if the first layer input computation is kept in 8-bit integer precision, the lowest error rates are achieved (M1, M2, M3, M6, and M7). The LSTM-based and the SRU-based models keep the error increase between 0.5 p.p. (percentage points) and 1.7 p.p. (LSTM-based) and 2.5 p.p. (SRU-based) compared to their baseline errors. At the same time, the GRU model has a negligible increase in the error. In contrast, the increase of the baseline error of the LiGRU-based model is found higher. \n\nTable 2. The result of applying post-training quantization on the LSTM-, GRU-, LiGRU-, and SRUbased models. The first row is for the baseline model size and error rate. The second four rows are for non-mixed quantization.",
            "score": 0.6032338180324553,
            "section_title": "Post-Training Quantization of RNN Models",
            "char_start_offset": 25581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "59413897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "276813486",
            "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size",
            "text": "Our analysis encompasses six distinct variants for each model, focusing on quantization applied to the Linear and Embedding layers of transformer blocks. These variants include the raw unquantized model serving as our baseline reference. We evaluate global quantization approaches using both 4-bit and 8-bit precision applied uniformly across all transformer blocks. Additionally, we test an 8-bit mixed quantization scheme where transformer blocks with weighted entropy below the mean value are quantized to 8 bits, while preserving the remaining blocks in their unquantized state. The most sophisticated approach implements a 4-bit/8-bit mixed quantization strategy, where blocks with weighted entropy below a threshold value receive 4-bit quantization, blocks with entropy between the mean and threshold are assigned 8-bit quantization, and blocks above the mean remain unquantized. \n\nTable 6 presents comprehensive MMLU benchmarking results for these various quantization methods as applied to transformer blocks. The results include the distribution of quantized blocks across different precision levels and the total model size contributed by the transformer blocks, which constitute the majority of the model's overall size. The FastEWQ methodology incorporates three key criteria for transformer block quantization decisions: total parameter count, execution index position within the model architecture, and total number of transformer blocks. This schema-driven approach analyzes model architecture files to generate quantization plans in constant time complexity O(1), eliminating the need for weight downloads while maintaining compatibility across diverse LLM architectures. \n\nWe evaluate two distinct classifier configurations to validate the framework's robustness. The first variant utilizes a classifier trained on the complete dataset, achieving 99% accuracy through nearperfect capture of EWQ's entropy-weighting behavior. The second configuration employes a classifier trained on 70% of samples to assess generalization capabilities, maintaining 80% accuracy despite reduced training data exposure. Notably, the 4-bit/8-bit FastEWQ mixed quantization specifically targets final transformer blocks with the highest execution indices for maximal compression, capitalizing on our observation that late-stage semantic integration layers exhibit unexpected quantization tolerance.",
            "score": 0.6031458879045204,
            "section_title": "EWQ Test Results",
            "char_start_offset": 49101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "273963038",
            "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization",
            "text": "We have shown that BitNet models can be trained to yield commensurate accuracy with their standard 16/32-bit counterparts. In T5-like architectures, the BitNet models perform even better than the 16-bit models -which we attribute to a regularization effect. These results highlight that 1.58-bit quantization aware training methods could greatly reduce memory requirements and increase throughput at inference time for a wide range of models. \n\nThroughout Sections 3 and 4 we have shown that 1.58-bit models yield performance comparable to standard 16/32-bit models -both for nontransformer and transformer models. In some cases, the 1.58-bit models even outperform 16-bit models: on multilayer perceptrons (see Section 3.2), graph neural networks (Section 3.3, and encoder-decoder language models (Section 4.2). \n\nComparing the two weight quantization schemes for BitNet: AbsMedian and AbsMean, we have observed that AbsMedian is on par with AbsMean. Prior work has shown that AbsMedian performs better in some situations (Nielsen and Schneider-Kamp, 2024), conjecturing AbsMedian to be more resilient to weight-updates, which allows for higher variance without noticeable effect on the scaling factor. While this may be a factor for models with millions of parameters, the miniature models involved in solving the X-OR task seem to be extraordinarily sensitive when the scaling factor is computed on a much smaller sample. We conjecture that this is the source of the instability observed in some configurations (e.g, hidden size 8 and 16 with low learning rate), which we have observed to be dampened when increasing the hidden size or the learning rate. In the more practical scenarios, we see that AbsMean and AbsMedian quantization are close to each other with a 0.6% drop in accuracy for MLPs and within each other's confidence intervals for GNNs. This is in line with prior work for small language and vision models (Nielsen and Schneider-Kamp, 2024) and underlines the strong potential of b1.58 models.",
            "score": 0.601531302669976,
            "section_title": "Discussion",
            "char_start_offset": 19207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1057,
                    "matchedPaperCorpusId": "271212141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415771484375
        },
        {
            "corpus_id": "244531228",
            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
            "text": "Quantization has become a significant method for optimizing deep-learning models so that they can accelerate inference when deployed on embedded systems with restricted memory footprint and computing resource. In this work, we use the Neural Network Intelligence toolkit 4 . To increase the network accuracy, we use the quantization aware training method (QAT) [19], where we started with the trained model and further refined the model with quantized parameters for 100 epochs.\n\nThe results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
            "score": 0.5997216721128378,
            "section_title": "Model compression study",
            "char_start_offset": 17959,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "209515558",
            "title": "Towards Unified INT8 Training for Convolutional Neural Network",
            "text": "Compared to huge amount of studies on accelerating inference by model quantization [47,60,7,53,11,40], there are few works exploring quantized training including backward propagation comprehensively. DoReFa-Net [62] quantizes gradients to 4 and 6 bits, but only experiments AlexNet with low precision gradient. WAGE [56] and WAGEUBN [58] quantize gradient to 8-bit integer, but they both incur considerable loss of accuracy (greater than 5%). RangeBN [2] and FP8 training [54] achieve accuracy comparable to full-precision models, but they both use floatingpoint number in gradients, which is not beneficial for hardware optimization to boost the speed. Besides quantized training, most low-precision training research keeps gradient precision in 16-bit floating-point. Flexpoint [33], MPT [41] and DFP [9] all use 16-bit floating-point to train DNNs with accuracy comparable to full-precision model.",
            "score": 0.5993061744069508,
            "section_title": "Related Work",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 87,
                    "end": 90,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "102352789"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "31312287"
                },
                {
                    "start": 803,
                    "end": 806,
                    "matchedPaperCorpusId": "3517431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3603515625
        },
        {
            "corpus_id": "49356451",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "text": "We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations. \n\nWe perform the following experiments: \n\n\u2022 Experiment 1: Per-channel quantization is significantly better than perlayer quantization at 4 bits. We show that per-channel quantization provides big gains over per-layer quantization for all networks. At 8-bits, the gains were not significant as there were sufficient levels to represent the weights with high fidelity. At four bits, the benefits of per-channel quantization are apparent, even for post training quantization (columns 2 and 3 of Table 5). \n\n\u2022 Experiment 2: Fine tuning can provide substantial accuracy improvements at lower bitwidths. It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments. \n\n\u2022 Experiment 3: Lower precision activations: We investigate the accuracies obtained with 4-bit activations for all layers with and without fine tuning. Note that activations are quantized on a per-layer basis. The weights are quantized at 8bits of precision with per-channel granularity. We note that fine tuning improves accuracy in this case also. The losses due to activation quantization are more severe than that of weight quantization (see Table 6). Note that the quantization granularity is different for activations and weights, so this is not a fair comparison of the impact of quantization. We hypothesize that quantizing activations We experiment with several configurations for training quantized models: Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
            "score": 0.5976333828150053,
            "section_title": "Lower Precision Networks",
            "char_start_offset": 19489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "247470589",
            "title": "Customizable FPGA-Based Hardware Accelerator for Standard Convolution Processes Empowered with Quantization Applied to LiDAR Data",
            "text": "Deep learning algorithms are usually implemented in software using 32 bit floatingpoint values (FP32). Migrating a deep learning algorithm to an ASIC or an FPGA requires for a bit width reduction which is possible using the quantization technique [24][25][26]. A quantized model and a non-quantized model execute the same operations, however, a quantized model with bit-width reduction promotes a memory reduction and allows the execution of more operations per cycle. This memory reduction allows a more compact model representation, which leads to a better deployment in a hardware platform. For hardware, implementation is intended to convert a 32 bit floating-point value to a 16/8/4 bit fixed-point value INT (fixed-point expression), respectively [27]. The bit reduction may lead to a considerable accuracy gap on full precision models as suggested by [28]. \n\nTherefore, it is necessary to achieve a trade-off regarding model accuracy, model parameters, and hardware (HW) performance. The work in [29,30] presents a method that takes full advantage of a DSP block for 8 bit quantization. However, the trade-off between accuracy and inference time might be required and applied whenever possible, therefore, this study provides insights about the model degradation for various model depths, i.e., number of layers.",
            "score": 0.5962173453025017,
            "section_title": "Optimization Methods",
            "char_start_offset": 9443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "221474323"
                },
                {
                    "start": 251,
                    "end": 255,
                    "matchedPaperCorpusId": "207759458"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "216184556"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "207959781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370361328125
        },
        {
            "corpus_id": "267897495",
            "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
            "text": "We assess the the performance of APTQ using the C4 [15] and WikiText-2 [13] benchmarks. We compare APTQ against three established PTQ methods: GPTQ [6], OWQ [10], and PB-LLM [16]. \n\nNotably, OWQ and PB-LLM extend upon GPTQ, with PB-LLM incorporating mixed-precision quantization. To ensure a balanced comparison, all methods are evaluated on a standardized platform. Moreover, we benchmark APTQ's performance with the leading QAT approach, LLM-QAT. Table 1 reveals that APTQ, at an average 4 bit, closely matches the full-precision model and attains SOTA performance on the C4 dataset, showing only a 0.01-point increase in perplexity. Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B. \n\nTo substantiate the robustness and broad applicability of the Hessian trace-based mixed-precision quantization posited in our study, we conducted a comparative analysis of various 4-bit utilization levels of APTQ against other prevalent PTQ methods applied to the LLaMa-7B model on the C4 dataset. The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy. These results unequivocally demonstrate the superior performance of APTQ, leveraging Hessian trace-driven precision allocation to optimize quantization outcomes. \n\nFigure 2 visually summarizes our findings.",
            "score": 0.5962058044471796,
            "section_title": "Evaluation of Perplexity performance",
            "char_start_offset": 16177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "276117030",
            "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
            "text": "As deep learning continues to scale toward larger models and datasets, significant attention has been devoted to studying the scaling laws that trade-off between model and dataset size to optimize performance and computational efficiency (Hoffmann et al., 2022;Kumar et al., 2024;Dettmers & Zettlemoyer, 2023). In the meantime, the field is shifting toward lower-precision computation, particularly in large language models, driven by the substantial benefits of mem- ory savings and computational efficiency (Liu et al., 2023a;Ma et al., 2024). This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model performance. \n\nWhen allowing for lower-bit quantization, we can freely trade off the bit-width and the number of parameters. Keeping the amount of memory used the same, we could have an 8-bit model, or a 4-bit model twice the size. This begs the question: What is the optimal trade-off between bit-width and model size? Recent papers (Dettmers & Zettlemoyer, 2023;Kumar et al., 2024) on scaling laws for low-precision conclude that 4 or 6-bit quantization often resides on the Pareto frontier to balance accuracy and efficiency. Other studies (Ma et al., 2024;Kaushal et al., 2024a) suggest that bit-widths as low as 1.58-bit per parameter hold significant promise for the optimal scaling law trade-off. These opposing conclusions highlight the challenges of studying scaling laws in the low-precision domain. \n\nIn this paper, we demonstrate that previous conclusions on the low-bit scaling laws can be significantly sharpened by better quantization scheme design and training improvements. While previous works define the search space of the QAT scaling laws solely as a function of model parameters (N ), token count (D), and quantization precision (P), we emphasize the critical role that the training scheme (S train ) and the bit-specific quantization function (F) play in the equation. We formalize the search space as ParetoQ L(N , D, P, S train , F), comprising five dimensions.",
            "score": 0.5956799454502058,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 309,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 1005,
                    "end": 1035,
                    "matchedPaperCorpusId": "254853733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "231855747",
            "title": "VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference",
            "text": "Krishnamoorthi evaluates per-channel scaled quantization at various precisions for a set of convolutional neural networks (CNNs) (Krishnamoorthi, 2018). The paper finds that although PTQ can achieve good accuracy at 8 bits for these networks, QAT is required for getting good accuracy at lower precisions or for matching floating-point accuracy at 8-bits. McKinstry et al. shows that CNNs require only a small number of epochs of finetuning after carefully setting the learning rate schedule and fixing the quantization range (McKinstry et al., 2018). Instead of fixing the quantization range before QAT, PACT proposes to learn the range of weights and activations as part of training (Choi et al., 2018). Both papers achieve full-precision accuracy with only 4-bit precision. Other research has explored very low precision ternary (Zhu et al., 2016) and binary (Courbariaux et al., 2015;Hubara et al., 2016) weights and activations. These models required significant retraining to recover accuracy loss and do not reach full-precision accuracy for more difficult tasks. In addition to CNNs, recent work has proposed quantized transformer models for NLP (Zafrir et al., 2019;Shen et al., 2020) and for machine translation (Bhandare et al., 2019;Prato et al., 2019;Wu et al., 2016b). Wu et al. establishes a single 8-bit quantization workflow for maintaining less than 1% accuracy drop for many different types of networks (Wu et al., 2020). \n\nPrior work has proposed schemes for uniform quantization (Courbariaux et al., 2014;Zhou et al., 2016) and nonuniform quantization (Han et al., 2015;Zhu et al., 2016). Uniform quantization uses integer or fixed-point format which can be accelerated with specialized math pipelines and is the focus of this paper. Non-uniform quantization leverages codebook look-ups to enable model compression and memory bandwidth reduction.",
            "score": 0.5956709231815102,
            "section_title": "RELATED WORK",
            "char_start_offset": 5702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "195767218",
            "title": "Compression of Acoustic Event Detection Models With Quantized Distillation",
            "text": "In quantization experiments, we will experiment 8-bit and 4-bit quantization with training. We did not quantize the model with less than 4-bit, as it results in significant performance degradation. Both parameters and input are quantized to the target number of bit (8 or 4) except that the cell state (Ct in equation 5) will be quantized into 16 bits. Empirically we find quantizing Ct to low-bits leads to divergence in training. This may be related to its potential unbounded value. Such behavior on quantizing recurrent neural networks is also reported in [13]. \n\nFor all experiments we use Adam optimizer with learning rate of 0.001 and batch size of 64. We tuned penalty on positive loss (wc in equation 1) on validation set and found setting it to be the ratio between positive and negative examples of each class in the training set gives overall best results. This practice also prevents us from tuning wc for every class. \n\nEvaluation Metric We evaluate the performance of models based on AUC (area under curve) on ROC curve (true positive rate vs. false positive rate) and EER (equal error rate) on DET curve (false negative rate vs. false positive rate). Higher AUC and lower EER indicate better performance. We report their values on individual events as well as average over all three events. To evaluate the compression effect, we measure number of parameters (in Millions), size of parameters (in MB) and number of floating point operations (FLOPs). FLOPs measure the amount of computation while the first two metrics measure storage size of the model.",
            "score": 0.5935689133019868,
            "section_title": "Experimental Setting",
            "char_start_offset": 11731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "235780511",
            "title": "Mixed Precision Quantization of Transformer Language Models for Speech Recognition",
            "text": "Deep Transformer models in recent years have defined state-of-theart language modelling performance across a range of applications including automatic speech recognition (ASR). The Transformer model architecture features a deep stacking of multiple self-attention layers [1,2,3] with residual connections [4] and layer normalization [5] Additional positional encoding layers [6,7] can be used to further augment the self-attention layers with sequence order information. Performance improvements over the conventional long short-term memory recurrent neural network (LSTM-RNN) language models have been widely reported [8,9]. However, the deeper architecture design of Transformers not only leads to a large increase in overall system complexity, memory footprint and computational cost when operating on the cloud, but also creates difficulty when deploying them on edge devices to enhance privacy and reduce latency, in common with many other computational intensive deep learning applications that are currently facing similar challenges. \n\nTo this end, one powerful solution recently drawing increasing interest in the machine learning and speech technology community is to use low-bit deep neural network (DNN) quantization techniques [10,11]. By replacing floating point weights with low precision values, for example, binary numbers, quantization can dramatically reduce the model size without modifying the network ar-chitecture [12,13,14]. Further model size reduction can be obtained when low-precision quantization is used in combination with neural architecture search (NAS) methods, for example, in the SqueezeNet system [15]. In contrast to the extensive research works on low-bit quantization methods conducted on computer vision tasks [16,17], only limited previous research in this direction has been conducted in the context of language modelling [18] and ASR systems. \n\nTwo issues are associated with current low-bit DNN quantization methods. First, these quantization approaches are predominantly based on uniform precision, where an identical bit-width is applied to all weight parameters for quantization. This fails to account for the varying performance sensitivity at different parts of the system to quantization errors. In practice, this often leads to large performance degradation against full precision models.",
            "score": 0.5905030365519593,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 276,
                    "matchedPaperCorpusId": "15280949"
                },
                {
                    "start": 276,
                    "end": 278,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 378,
                    "end": 380,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 622,
                    "end": 624,
                    "matchedPaperCorpusId": "204749986"
                },
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "14245558"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 1751,
                    "end": 1755,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1755,
                    "end": 1758,
                    "matchedPaperCorpusId": "53746082"
                },
                {
                    "start": 1865,
                    "end": 1869,
                    "matchedPaperCorpusId": "221275765"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "258990120",
            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
            "text": "12% and 1.18%, respectively). Hence, for this dataset, the model can be quantized down to 1-bit with an accuracy degradation of around 1%.\n\nFor the N-Caltech101 dataset, on the other hand, the binarized SNN model has a more significant accuracy drop compared to other bit-precision models. While 8-bit, 4-bit, and 2-bit quantized SNN models have 1.07 %, 1.39 %, and 1.8 % accuracy drops, the 1-bit model has 3.47 % accuracy degradation compared to its full-precision counterpart.\n\nFor the N-MNIST dataset, the accuracy degradation is the lowest among the presented datasets. The accuracy drop is negligible and fluctuates between different bit-precisions. The accuracy drop in the < 0.2% range is achieved compared to the full-precision model.",
            "score": 0.5901636032100976,
            "section_title": "4) N-Caltech101:",
            "char_start_offset": 21003,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54150390625
        },
        {
            "corpus_id": "244391498",
            "title": "Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy",
            "text": "\u2022 \n\nWe present an extensive literature overview of uniform and nonuniform quantization for fixed-point inference; \u2022 A novel modification to a neural network compute engine is introduced to improve the accuracy of models with 4 bit weights and 8 bit activations, in conjunction with bit-shift-based scaling, through the aid of a lookup-table; \u2022 A quantization-aware training method is proposed to optimize the models that need to run on our proposed compute engine; \u2022 We are the first to make a fair empirical comparison between the performance of (uniform) quantized models with full-precision and power-of-two scales with either per-layer or per-channel quantization using 4 bit weights; \u2022 Our source code has been made publicly available https://gitlab.com/EAVISE/lutmodel-quantization (accessed on 16 November 2021). \n\nThe remainder of this paper is organized as follows: Section 2 presents an extensive literature overview of quantization in greater detail, organized into different topics for convenience. For each topic, we also highlight the choices we made for our own approach. Our proposed method is explained in Section 3; our results are presented in Section 4; conclusions are made in final Section 5.",
            "score": 0.5900019611480563,
            "section_title": "Introduction",
            "char_start_offset": 4330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 1
                },
                {
                    "start": 4,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "268531657",
            "title": "MELTing Point: Mobile Evaluation of Language Transformers",
            "text": "A prominent method for reducing the memory traffic between main and on-chip memory is to decrease the precision of the weights and activations of the Neural Network [32,61,110]. However, this often comes at the expense of model accuracy, especially at sub 4-bit weight precision. Benchmark for common-sense reasoning, designed not to be easily solvable by statistical models and plain word associations. ThuthfulQA [62] Knowledge NLG 817 Benchmark for measuring truthfulness in a model's generated answers. ARC-{E,C} [16] Reasoning NLI 5.2k, 2.6k \n\nScience and language exam questions from a variety of sources. E: Easy; C: Complex Moreover, the hardware needs to support operations at these precisions, to avoid dequantization before computation. By leveraging the supported quantization schemes in the two LLM frameworks MELT supports (Tab. 3), we measure the impact of quantization in various tasks on the pretrained models. We use pretrained instead of fine-tuned models for this because the latter's fine-tuning and RLHF [75] alignment can affect the original performance. A description of the employed quantization schemes is presented in Sec. 7. We use the benchmark datasets depicted in Tab. 4, which consist of Natural Language Inference (NLI) and Natural Language Generation (NLG) tasks. In the former case, it comprises multiple choice questions, and the most likely answer -expressed by cumulative log likelihood of the model's output -is selected and matched against the correct label. In the latter case, the model's output is evaluated against template answers over BLEURT [92] score. \n\nResults are depicted in Fig. 10 across datasets and models. From the data we can see that the most evident performance difference comes from the model architecture and parameter size, and this performance difference persists across datasets. In terms of quantization schemes, it is obvious that bitwidth is correlated to model size, but also to accuracy, i.e., lower bitwidth means higher error rate.",
            "score": 0.5877799336152263,
            "section_title": "Impact of Quantization",
            "char_start_offset": 37968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1588,
                    "end": 1592,
                    "matchedPaperCorpusId": "215548699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "269502679",
            "title": "Efficient Compression of Multitask Multilingual Speech Models",
            "text": "Quantization is a well-established technique in the field of Deep Learning, employed to increase the efficiency of neural networks.Historically, neural networks were often trained using low-precision numerical representations (Hubara et al., 2017).However, a recent trend, particularly in NLP, involves post-training quantization.This technique entails applying quantization to models after they have been trained with regular, higher precision.This approach has gained traction as it offers the dual benefits of reducing inference latency and model size.\n\nPost-training quantization has found widespread use in various domains, including machine translation and language models (Bondarenko et al., 2021;Liang et al., 2021;Menghani, 2023;Wu et al., 2020).Quantized NLP models have yielded promising results, making it an appealing approach.\n\nOne of the most widely adopted techniques for post-training quantization in both NLP and speech communities is the LLM.int8() algorithm (Dettmers et al., 2022).This method implements quantization in the feed-forward and attention projection layers of the Transformer architecture.The method has two parts: vector-wise quantization and mixed precision decomposition.In the vector-wise quantization, it is determined conversion constants that allow for the recovery of original numbers from 8-bit to 16-bit floating-point representations.This enables matrix multiplication to be carried out in the lower 8-bit precision.Moreover, in the mixed precision decomposition, it identifies potential outliers that could be adversely impacted by reduced precision and then executes this part of the matrix multiplication in 16-bit precision.\n\nWhile initially designed for decoder-only large language models (LLMs), this quantization method, along with its 4-bit variation (Dettmers & Zettlemoyer, 2023), has gained widespread adoption for various Transformer-based models.It has been made readily available in the Transformers library by Hugging Face (Wolf et al., 2020), contributing to its popularity.Additionally, it is becoming common to combine this quantization technique with domain adaptation methods.",
            "score": 0.5863582582320592,
            "section_title": "Quantization",
            "char_start_offset": 7163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 555
                },
                {
                    "start": 557,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 840
                },
                {
                    "start": 842,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1672
                },
                {
                    "start": 1674,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 679,
                    "end": 704,
                    "matchedPaperCorpusId": "237940329"
                },
                {
                    "start": 704,
                    "end": 723,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 978,
                    "end": 1001,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1982,
                    "end": 2001,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "170078885",
            "title": "Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers",
            "text": "Quantized Neural Networks. Early works on quantization of deep networks targeted 16 bits fixed-point implementations [15], which result in an almost lossless approximation of full-precision trained networks, or extreme binarized networks, which, despite the fascinating low-computational and memory requirements, showed major accuracy losses when applied on image classification benchmarks [4,19]. Several studies demonstrated that 8 bit quantization of weights and activations results in a good trade-off between latency, compression and a near-zero accuracy degradation, also if applied to efficient Imagenet classification networks [11,18,12]. Among the employed methodologies, TensorRT [18] approximates the parameters tensor by the minimization of the KL divergence metric between quantized and full-precision values. On the contrary, [11] quantizes values within a range defined by the tensor min and max values. Concerning activations, the PACT approach [2] demonstrated the highest efficiency by leveraging backpropagation to learn the quantization ranges. Recently, to fit stringent memory requirements, more aggressive sub-byte precision quantization approaches, i.e. less than 8 bit, are under investigation [3,12,6,13,16]. The works [12,6] exploits learning-based approaches for determining the quantization ranges of activation and weights at low-bitwidth precision. State-of-the-art accuracy level on the efficient MobilenetV1 model has been reported by [13,16], by making use of per-channel quantization when moving to 4 bits precision. It is also worth to mention as non-uniform quantizers have resulted as the best approximators when reducing the bit precision [24,22,9]. However, a high-precision (floating point) arithmetic is needed on uncompressed values within the datapath, hence these methods results not suitable for the microcontroller domain. In this work, we leverage existing techniques and show the insights, concerning either computational and memory aspects, when bringing fake-quantized networks to the integer-only arithmetic domain, which is not taken into consideration by this class of works. \n\nMixed Low Precision Quantization.",
            "score": 0.5843088272141037,
            "section_title": "Related Work",
            "char_start_offset": 5129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1678,
                    "end": 1682,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61669921875
        },
        {
            "corpus_id": "252198916",
            "title": "FP8 Formats for Deep Learning",
            "text": "Both papers investigate 16-bit weight updates as well as stochastic rounding. Use of two FP8 formats, 4-and 5-bit exponent fields, for training is introduced in [20], studying a wider range of CNNs as well as speech and language translation models. [20] also investigates FP8 inference of networks trained in higher precision and introduces returning of batch normalization statistics to improve result accuracy. Noune et al [16] propose a modified FP8 representation that dedicates a single encoding to special values in order to increase the represented dynamic range and present an extensive study of exponent bias effect on result quality. 8-bit inference with various formats, including FP8, with networks trained in higher precision is the focus of [10]. \n\nIn this paper we describe an 8-bit binary format for floating point representation, using two encodings for FP8. Basic principles of using FP8 for deep learning are summarized in Section 2. In Section 3 we describe the bit encodings and reasoning behind them. Empirical evaluation of training and inference with a variety of tasks and models is presented in Section 4. We show that FP8 training matches FP16 or bfloat16 training results for a variety of tasks and neural network model architectures and sizes, without changing any model or optimizer hyperparameters. Our study includes the training of very large language models, up to 175B parameters. It is important to consider a wide range of model sizes since it has been shown that models of different sizes may have different numerical behaviors (for example, the different behavior of Resnet-18 and Resnet-50 observed in [12].",
            "score": 0.5823099363547424,
            "section_title": "Introduction",
            "char_start_offset": 2007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1647
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 165,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "202779157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394287109375
        },
        {
            "corpus_id": "250048704",
            "title": "QReg: On Regularization Effects of Quantization",
            "text": "Where f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups. Since models on small datasets like cifar10 generally have small errors compared to models on more complex datasets (like cifar100 or VOC), we used a logarithm in Equation 5. Once again, according to Table 1, in all cases 8-bit quantized models reduce the error more (i.e, generalizing better) compared to full precision models. On all three models tested over 19 different augmentation setups, all 8-bit quantized models reduce the error when compared to full precision models (i.e last column for 8-bit quantization levels is green for all models). This effect (reducing error) is less clear as we use lower precision models . \n\nOur results presented in Figure 2, Table 1 and Appendix B empirically confirm our hypothesis. Unlike (Courbariaux et al., 2015), we believe that the regularization effect of quantization is in fact correlated with the quantization level. \n\nOur empirical study shows that moderate quantization (8bit) generally helps models generalize better. In addition to these results, specially for more complex models, we observe that quantized models overfit less at training time. \n\nFigure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model. This, confirms our hypothesis. For the full list of experiments, please refer to Appendix B.",
            "score": 0.5813736031671899,
            "section_title": "RESULTS",
            "char_start_offset": 15057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1574
                }
            ],
            "ref_mentions": [
                {
                    "start": 855,
                    "end": 881,
                    "matchedPaperCorpusId": "1518846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "244714363",
            "title": "Mixed Precision DNN Quantization for Overlapped Speech Separation and Recognition",
            "text": "The third solution to automatically learn the optimal local quantization precision settings is to use mixed precision based neural architecture search (NAS) [37,38] approaches. The super-network is constructed by first separately training the speech separation system using uniform precision, e.g. 2-bit, 4-bit, 8-bit and 16-bit, before connecting these uniform precision quantized models at each layer. \n\nIn order to avoid the trivial selection of the longest, most generous quantization bit width, these precision selection weights learning can be further constrained by a model complexity penalty term with respect to the number of bits retained after quantization, in order to obtain a target average quantization precision, for example, 4-bit, \n\nwhere LSI\u2212SNR(\u0398) is the scale-invariant signal to noise ratio (SI-SNR) objective function. a l n denotes the architecture weights using n-bit quantization for the l-th cluster of weight parameters. \u03b2 is a scaling factor empirically set as 0.5 in all experiments of this paper.",
            "score": 0.5797241469007222,
            "section_title": "Architecture Search Based Mixed Precision Quantization",
            "char_start_offset": 13056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1027
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "52016139"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "211252793"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467041015625
        },
        {
            "corpus_id": "3323727",
            "title": "Model compression via distillation and quantization",
            "text": "Table 1 contains the results for full-precision training, PM quantization with and without bucketing, as well as our methods. The percentages on the left below the student models definition are the accuracy of the normal and the distilled model respectively (trained with full precision). More details are reported in table 11 in the appendix. We also tried an additional model where the student is deeper than the teacher, where we obtained that the student quantized to 4 bits is able to achieve significantly better accuracy than the teacher, with a compression factor of more than 7\u00d7. \n\nWe performed additional experiments for differentiable quantization using a wide residual network (Zagoruyko & Komodakis, 2016) that gets to higher accuracies; see table 3. \n\nOverall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization. Differentiable quantization is a close second on all experiments, but it has much faster convergence. Further, we highlight the good accuracy of the much simpler PM quantization method with bucketing at higher bit width (4 and 8 bits). \n\nCIFAR-100 Experiments. Next, we perform image classification with the full 100 classes. Here, we focus on 2bit and 4bit quantization, and on a single student architecture. The baseline architecture is a wide residual network with 28 layers, and 36.5M parameters, which is state-of-the-art for its The results confirm the trend from the previous dataset, with distilled and differential quantization preserving accuracy within less than 1% at 4bit precision. However, we note that accuracy loss is catastrophic at 2bit precision, probably because of reduced model capacity. We note that differentiable quantization is able to best recover accuracy for this harder task. \n\nOpenNMT Experiments. The OpenNMT integration test dataset (Ope) consists of 200K train sentences and 10K test sentences for a German-English translation task.",
            "score": 0.5767109241291286,
            "section_title": "COMPRESSION",
            "char_start_offset": 20471,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2009
                },
                {
                    "start": 2012,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2170
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "16687744",
            "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point",
            "text": "Deep learning training and inferencing are highly compute intensive operations, however using full precision (F P 32) computations on conventional hardware is inefficient and not strictly warranted from functional point-of-view. To address this issue, there has been a lot of interest at using lower precision for deep learning, in an attempt to identify the minimum required precision to ensure functional correctness within acceptable thresholds.\n\nIn the past many researches have proposed low-precision alternatives to perform deep learning tasks. Vanhoucke et al. [12] showed that using 8-bit fixed-point arithmetic convolution networks can be sped up by up to 10x on speech recognition tasks on general purpose CPU hardware. Gupta et al. [4] have successfully trained networks using 16-bit fixed point on custom hardware. Miyashita et al. [8] used log quantization on pre-trained models and achieved good accuracy by tuning the bit length for each layer. More recently, Venkatesh et al. [13] achieved near state of the art results using 32b activations with 2-bit ternary weights on Imagenet dataset. Hubara et al. [5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task [9].",
            "score": 0.5758912517440218,
            "section_title": "Related Work",
            "char_start_offset": 3038,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 743,
                    "end": 746,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "2930547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46142578125
        },
        {
            "corpus_id": "220514471",
            "title": "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic",
            "text": "Modern high-performance computing (HPC) hardware continues to experience an ongoing shift towards supporting a variety reduced-precision formats for representing floating-point numbers in order to offer a much increased performance rate. However, portability is often of little concern as the hardware tends to serve only a specific set of workloads that are of special interest to the particular vendor. In general, the machine learning community has been more aggressive in evaluating multiple precision to the extent that even a 1-bit Stochastic Gradient Descent has been considered [98]. The typical use case in machine learning is to use the training with 32-bit arithmetic and use different precision for the inference task. The quantization for the inference is supported in popular frameworks like TensorFlow [99] and pyTorch [100]. Quantization is the approach to store the tensors and compute on them using bitwidths lower than floating point bitwidths. Even in machine learning frameworks, the support for quantizations is limited to just the key functionality needed for a convolutional neural networks or recurrent neural networks with some limited hardware support. For example, pyTorch and TensorFlow supports 8-bit quantization for activation and weights. This allows using 8-bits for inference where the additional 2-4x performance is necessary. On the training front, it has been shown that 16-bit training is sufficient for certain tasks [1,101]. The recent Gordon Bell winner demonstrated that lower-precison training can be used for scientific machine learning tasks as well [102]. \n\nThe analogous effort to the work in deep learning to the examples of our interest in scientific computing involves training the network in lower precision and performing inference in a higher one [103,104]. The compute imbalance between training and inference is even higher than that of factorization and the subsequent iterative refinement. Another difference is that in the context of neural network training, lowering the precision may be incorporated into the model as a regularizer. \n\n9 Multiprecision capabilities of xSDK Math Libraries and Interoperability",
            "score": 0.5751153886144167,
            "section_title": "Low precision and multiprecision technology for Machine Learning",
            "char_start_offset": 90044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "2189412"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "6287870"
                },
                {
                    "start": 1457,
                    "end": 1460,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1596,
                    "end": 1601,
                    "matchedPaperCorpusId": "52922914"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "2547043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "233459296",
            "title": "Quantization and Deployment of Deep Neural Networks on Microcontrollers",
            "text": "In this work, we presented a framework to perform quantization and then deployment of deep neural networks on microcontrollers. This framework represents an alternative to the STM32Cube.AI proprietary solution and TensorFlow Lite for Microcontrollers, an opensource but complex environment. Inference time and energy efficiency measured on two different embedded platforms demonstrated that our framework is a viable alternative to the aforementioned solutions to perform deep neural network inference. Our framework also introduces a fixed-point on 16-bit integer post-training quantization which is not available with the two other frameworks. We have shown that this 16-bit fixed-point quantization provides an improvement over a 32-bit floating-point inference, while being competitive with fixed-point on 8-bit integer quantization-aware training. It provides a reduced inference time compared to floating-point inference. Moreover, the memory footprint is divided by two while keeping the same accuracy. The 8-bit quantization provides further improvements in inference time and memory footprint but at the cost of a slight decrease in accuracy and a more complex implementation. \n\nWork is still in progress to implement some optimization techniques for fixed-point on 8-bit integer inference. Three optimizations are especially targeted: per-filter quantization, asymmetric range and non-power-of-two scale factor. In addition, using SIMD instructions in the inference engine should help further decrease the inference time. These optimizations would therefore make our framework more competitive in terms of inference time and accuracy. Another possible improvement for fixed-point on integers inference consists of using 8-bit quantization for the weights and 16-bit quantization for the activations. TensorFlow Lite for Microcontrollers is currently in the process of implementing this technique. Mixed precision can indeed provide a way to reduce the memory footprint of layers that do not need a high-precision representation (using 8 bits for weights and activations), while keeping a higher precision (16-bit representation) for layers that need it. The CMix-NN [58] library already provides an implementation of convolution functions for various data type configurations (in 2, 4 and 8 bits). To further improve power consumption and memory footprint, binary neural networks can also be considered.",
            "score": 0.5748999696160606,
            "section_title": "Conclusions",
            "char_start_offset": 69603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2307
                },
                {
                    "start": 2308,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 2176,
                    "end": 2180,
                    "matchedPaperCorpusId": "216187122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423583984375
        },
        {
            "corpus_id": "250644336",
            "title": "Accelerating Deep Learning Model Inference on Arm CPUs with Ultra-Low Bit Quantization and Runtime",
            "text": "Deep Learning model performance has progressed considerably in the past few years, especially with state-of-theart classification [3] and object detection models [1], [17]. These models perform well on computer vision problems and improve upon the state-of-the-art year-over-year. However, constantly improving performance of these models still does not make them suitable to be used on edge devices due to computational, power and memory requirements. These models use 32-bit precision which can be reduced to 16 bits or even to 8 bits by quantizing the weights and activations accordingly. \n\nExisting methods for neural network quantization can be classified in two categories [24], Post-training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require the original network to be re-trained on the target dataset and is therefore typically limited to FP16 or INT8 quantization, where sufficient information is preserved in the original training statistics to accurately quantize the network. On the other hand, QAT quantizes the network during training and preserves significantly more accuracy than PTQ. QAT methods have been tested at as low as INT4 precision [23] using 4 bits per neural network weight and 8 bits per activation. However, INT4 quantization in [23] is tested only on classification models. Low bit quantization is much more difficult in object detection problems. Significant work to quantize the model using just 1 bit is also available but such models suffer from accuracy loss beyond 10% [26] [27]. \n\nOn the other end of the research spectrum, low complexity light weight models like MobileNet [2] are available to help enable AI on edge devices. However, these models are handcrafted and lack the performance needed for many computer vision applications. Also, because of their handcrafted and compact architecture, compression techniques such as quantization are generally not effective [18] and custom modifications might be necessary to accommodate quantization [22]. \n\nEven when high accuracy quantized models with 1-2 bits of precision are available, they cannot be deployed on commercial off-the-shelf hardware [19] due to lack of support for 1 or 2-bit instructions and operators. Most commodity hardware only supports down to 8-bit operations.",
            "score": 0.5743139444736847,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 2701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 2018
                },
                {
                    "start": 2021,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "2141740"
                },
                {
                    "start": 1187,
                    "end": 1191,
                    "matchedPaperCorpusId": "53112855"
                },
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "53112855"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "6564560"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 1936,
                    "end": 1940,
                    "matchedPaperCorpusId": "233393900"
                },
                {
                    "start": 2013,
                    "end": 2017,
                    "matchedPaperCorpusId": "231584264"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.484375
        },
        {
            "corpus_id": "53977760",
            "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers",
            "text": "Over the past decade, Deep Learning has emerged as the dominant Machine Learning algorithm showing remarkable success in a wide spectrum of applications, including image processing [9], machine translation [21], speech recognition [22] and many others. \n\nIn each of these domains, Deep Neural Networks (DNNs) achieve superior accuracy through the use of very large and deep models -necessitating up to 100s of ExaOps of computation during training and Gigabytes of storage. Approximate computing techniques have been widely studied to minimize the computational complexity of these algorithms as well as to improve the throughput and energy efficiency of hardware platforms executing Deep Learning kernels [2]. These techniques trade off the inherent resilience of Machine Learning algorithms for improved computational efficiency. Towards this end, exploiting reduced numerical precision for data representation and computation has been extremely promising -since hardware energy efficiency improves quadratically with bit-precision. \n\nWhile reduced-precision methods have been studied extensively, recent work has mostly focused on exploiting them for DNN inference. It has shown that the bit-width for inference computations can be successfully scaled down to just a few bits (i.e., 2-4 bits) while (mostly) preserving accuracy [3]. However, reduced precision DNN training has been significantly more challenging due to the need to maintain fidelity of the gradients during the back-propagation step. Recent studies have empirically shown that at least 16 bits of precision is necessary to train DNNs without impacting model accuracy [6,16,4]. As a result, state-of-the-art training platforms have started to offer 16-bit floating point training hardware [8,5] with \u2265 4\u00d7 performance over equivalent 32-bit systems. \n\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.",
            "score": 0.5728998600981561,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "7464806"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "5052176"
                },
                {
                    "start": 1637,
                    "end": 1640,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1761,
                    "end": 1763,
                    "matchedPaperCorpusId": "53083553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "273822014",
            "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
            "text": "The high computational cost of serving LLMs has driven extensive research into inference acceleration techniques, including quantization (Frantar et al., 2022;Dettmers and Zettlemoyer, 2022;Lin et al., 2024a), speculative decoding (Chen et al., 2023;Leviathan et al., 2023), and pruning (Xia et al., 2023;Muralidharan et al., 2024). Among these, quantization-reducing the bitwidth of weights, activations, or both-has emerged as the most widely used approach. However, its key challenge lies in balancing efficiency and accuracy. \n\nDespite progress, systematic benchmarks and practical deployment guidelines remain scarce. This uncertainty has fueled speculation around quantized models, exemplified by the initial skepticism toward the Llama-3.1-405B quantized model release (Dubey et al., 2024), which was later found to be near-lossless in LMSYS Arena user evaluations (Chiang et al., 2024). To address this gap, we pose the following core question: \n\nWhat are the practical accuracy-performance trade-offs for popular quantization formats? \n\nIn this study, we focus on widely supported, computationally efficient quantization formats. Specifically, we examine 8-bit weights and activations (W8A8), using integer (INT) precision for NVIDIA Ampere and older GPUs and floating-point (FP) precision for NVIDIA Hopper and Ada Lovelace. Additionally, we consider 4-bit integer weights with 16-bit activations (W4A16-INT), a competitive low-bit alternative. To evaluate accuracy, we implement a broad automated evaluation suite, spanning both academic and real-world benchmarks.",
            "score": 0.5689369261985456,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1575
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 190,
                    "matchedPaperCorpusId": "259076379"
                },
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 250,
                    "end": 273,
                    "matchedPaperCorpusId": "272694046"
                },
                {
                    "start": 305,
                    "end": 331,
                    "matchedPaperCorpusId": "271328221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "225583435",
            "title": "Face Recognition and Machine Learning at the Edge",
            "text": "Figure 1 compares the difference in model size after different bit-width quantization are applied on the weights in a single layer. Model size reduced most after apply 4-bit fixed point quantization in all of the layers compared with 8-bit and 16bit because the weights are stored in lower precision format which means it consumes less memory storage. For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers. Hence, it is recommended to apply 8-bit or 16-bit fixed point quantization on the weights in layer 'fire-8-expand3x3' when implementing weight quantization in a single layer as they can help to reduce model size efficiency.",
            "score": 0.5677793086702971,
            "section_title": "RESULTS AND DISCUSSION",
            "char_start_offset": 15210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "261076459",
            "title": "SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization",
            "text": "With the success of deep learning, deep neural networks (DNNs) have been adopted for many artificial intelligence tasks such as image classification [18,20,35], object detection [33,34], and meanwhile, become the indispensable part of modern multimedia applications [37]. However, the large computational resource requirements of DNNs remain one of the most giant stumbling blocks for deploying deep learning models. There are several compression techniques to reduce the redundancy in a deep model, such as pruning [29], knowledge distillation [19] and quantization [6,26,41,53]. Quantization is a promising technique to reduce both the storage and computational resources overhead remarkably, by leveraging the fact that the inference precision is not strictly as high as training time. Therefore, quantization enables large models to run directly on the edge and mobile devices without redesigning a new model architecture, which empowers edge intelligence significantly. \n\nQuantization can be divided into two categories: fixed-precision quantization and mixed-precision quantization (MPQ). Fixed-precision quantization [12,28,53], where an identical bit-width is designated for all layers in a deep model. While such a paradigm is proven to make the quantized model achieve sufficiently good performance at high bit-width (e.g., \u2265 8 bits), a uniform bit-width is challenging for quantization in an ultra-low bit-width (i.e., \u2264 4 bits) scenario. For example, BRQ [17] reports that there is more than 20% top-1 accuracy degradation in a 2 bit quantization for the MobilNetV2 model as compared to its full-precision (FP) counterpart. \n\nMixed-precision quantization (MPQ) [11,14,21,46,51] offers a flexible and efficient way to quantize deep models by allocating varying bit-widths to individual layers based on their diverse redundancy levels. Unlike fixed-precision quantization, MPQ assigns specific precisions to different layers, with higher redundancy layers receiving less bit-width than lower redundancy ones, thereby achieving an optimal accuracy-efficiency trade-off.",
            "score": 0.5672239273131146,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "198985870"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "52978527"
                },
                {
                    "start": 570,
                    "end": 573,
                    "matchedPaperCorpusId": "246904819"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "244715141"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "244113718"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "matchedPaperCorpusId": "221568225"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "90262841"
                },
                {
                    "start": 1680,
                    "end": 1683,
                    "matchedPaperCorpusId": "249538319"
                },
                {
                    "start": 1683,
                    "end": 1686,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1686,
                    "end": 1689,
                    "matchedPaperCorpusId": "220646488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "256808431",
            "title": "Binarized Neural Machine Translation",
            "text": "The success of Transformer has spurred an active body of work to quantize it to lower precision. In this section, we review a subset of these efforts that inspired our approach. \n\nTransformer quantization. Much of the prior effort focused on 8-bit Transformer. Bhandare et al. (2019) reported a less than 0.5 BLEU drop on the WMT14 En-De translation task with 8 bits. Prato et al. (2019) showed an 8-bit Transformer preserved the translation quality. For non-generative tasks, Zafrir et al. (2019) quantized BERT to 8-bit with marginal quality loss. When pushed down to 4 bits, though Prato et al. (2019) reported an 8 BLEU degradation for MT, Aji & Heafield (2019) reported almost no BLEU loss by using a logarithmic quantization scheme. \n\nThe exploration on 1-bit Transformers centered around BERT. Usually binarization is directly applied and the focus is on improving the training recipe. Bai et al. (2020) initiated the attempt by splitting a ternary BERT into a binary one, then fine-tuning. It achieved 41% average accuracy on the GLUE benchmarks. Qin et al. (2022) proposed to distill each intermediate layer outputs from a floating-point model. Recently, Liu et al. (2022) proposed to incrementally quantize the model, e.g., from 32-bit to 4-bit to 2-bit, finally to 1-bit, and it improved the GLUE accuracy to 73.5%. Binarized vision models. Courbariaux et al. (2016) pioneered the investigation on binarized deep neural nets. Recently, PokeBNN (Zhang et al., 2022) established a pareto SOTA on the ImageNet recognition task. We inherit the binarization functions and training recipes from PokeBNN. \n\nGeneralizability. Hooker et al. (2019) show that compressed models do not generalize well on out-of-domain (OOD) data. We are particularly interested in evaluating BMT under OOD settings and analyze its generalizability.",
            "score": 0.5668899031585121,
            "section_title": "Related Work",
            "char_start_offset": 6573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1831
                }
            ],
            "ref_mentions": [
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1455,
                    "end": 1475,
                    "matchedPaperCorpusId": "244773000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "277824213",
            "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float",
            "text": "Data Formats for Model Weights LLM weights are typically stored in compact floating-point formats such as FP16 or BF16. FP16 allocates 1 sign bit, 5 exponent bits, and 10 mantissa bits, whereas BF16 uses 1 sign bit, 8 exponent bits, and 7 mantissa bits. Compared to FP16, BF16 offers a wider dynamic range at the cost of precision, which improves numerical stability and mitigates overflow issues during training [8,19]. \n\nCompressed data formats typically aim for lower bit-widths. For example, FP8-which comes in both E4M3 (4 exponent bits, 3 mantissa bits, plus 1 sign bit) and E5M2 configurations-has seen reasonable adoption in LLM training and development. Integer formats like INT8 have also been well explored, as in LLM.int8() [2] and its following works. Formats with a stronger emphasis on efficiency, such as FP4, INT4, NF4 [3], and AF4 [39], use only 4 bits. In this work, we primarily focus on formats with \u22658 bits, as benchmark literature [37,10,23] often suggests that 8-bit quantization results in negligible performance drop-though we show in Section 2 that this claim is likely skewed due to evaluation selectiveness and benchmark limitations. \n\nLossless Model Compression While lossy model compression techniques such as pruning and quantization [6,21,7] have received widespread attention, lossless model compression remains a relatively underexplored area. Upon careful investigation, we identified roughly four prior works that have made meaningful efforts in this space. Deep Compression [13] is a foundational work, applying Huffman coding [17] to quantized CNN models and achieving an additional \u223c22% compression gain for model checkpoints. ZipNN [16] extended this idea to language models, comparing its results to classic lossless compression tools such as zlib [4] and zstd3 and demonstrated superior compression gains. However, this line of work is limited in that its efficiency gains only apply to storage (reducing the size of model checkpoints) but offer no benefits during inference.",
            "score": 0.5656126247610482,
            "section_title": "Related Works",
            "char_start_offset": 32274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 739,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "273662240"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1269,
                    "end": 1272,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "10606404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3701171875
        },
        {
            "corpus_id": "235434086",
            "title": "MQBench: Towards Reproducible and Deployable Model Quantization Benchmark",
            "text": "In this paper, we mainly experiment with 8-bit post-training quantization (Appendix. C) and 4-bit quantization-aware training. To test the accuracy of the quantized model, we simulate the algorithm with fake quantization (see difference between fake and real quantization in Sec. 3.1). \n\nUnlike the reported results in other paper, 4-bit QAT in our benchmark could be very challenging. \n\nWe do not experiment with 3-bit quantization because it is undeployable on general hardware. As for 2-bit quantization, we find most of the algorithms do not converge on hardware settings.",
            "score": 0.5638398678310558,
            "section_title": "MQBench: Towards Reproducible Quantization",
            "char_start_offset": 8074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "275104652",
            "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
            "text": "This lack of a clear correlation suggests that other factors, such as the interplay between layer count, neuron count, and the inherent noise resilience of the dataset, may play a significant role in determining the quantization impact. In general, these findings reaffirm the effectiveness of 8-bit quantization for most configurations, with precision losses being minimal and occasionally yielding improvements due to potential regularization effects introduced by quantization noise. \n\nAt 6-bit quantization, models generally maintain acceptable performance, but the reduction in bitwidth introduces a more pronounced degradation in precision compared to 8-bit quantization. As shown in Figure 14, the Test MSE and Test MAPE values increase across nearly all configurations relative to their 8-bit counterparts. For instance, the seven-layer, 120-neuron model now has a Test MSE of 68.91 and a Test MAPE of 1.76%, representing an increase of 19.65% in Test MSE compared to the 8-bit version. Smaller models are also obviously affected. The four-layer, 10-neuron model exhibits a Test MSE of 88.41 and a Test MAPE of 2.06%, a substantial increase from its performance at 8-bit quantization. Figure 13 further highlights the differences in Test MSE between the 6-bit models and their FP32 counterparts. Across all configurations, the percentage difference in Test MSE ranges from 12.48% to 32.36%. However, as with the 8-bit results, the effects of model complexity (i.e., layer count and neuron count) on quantization sensitivity remain inconsistent. Despite these challenges, Test MAPE values remain around 2% for all configurations, indicating that 6-bit quantization can still meet industrially acceptable precision requirements. This makes it a practical choice for scenarios where resource efficiency is a priority, providing a viable trade-off between precision and computational cost. At 4-bit quantization, the effects of reduced precision become far more pronounced, leading to significant increases in Test MSE and MAPE across configurations. Figure 15 reveals that most configurations exhibit considerable performance degradation.",
            "score": 0.5636037678445716,
            "section_title": "Experiments 2: Quantized Models Analysis",
            "char_start_offset": 40969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "261049460",
            "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs",
            "text": "We compared robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits was applied to the even numbered FFN layers. Appendix C shows this is the best layer selection for the dense model. We used two different datasets to verify the proposed quantization method works in different model settings. Figure 8 presents the experiment with the model trained with the larger dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores. \n\nFigure 9 presents the experiment with the model trained with the smaller dataset. In this setting, each individual expert is smaller, but there are 4 times more experts in one MoE layer. And, they are trained with smaller dataset, so they do not have equivalent knowledge as the previous model trained on the larger dataset. As can be seen in the Figure, the quantization performance shows a similar pattern. The MoE model preserves accuracy even when it is quantized to 2 or 3 bits. However, dense model quickly loses the performance when it is quantized down to lower than 4-bit. Again, the MoE model is much more robust to quantization than the dense model.",
            "score": 0.562591003243891,
            "section_title": "F Robustness comparison between MoE and dense models",
            "char_start_offset": 27281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1595
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "244156385",
            "title": "Evaluation of Deep Neural Network Compression Methods for Edge Devices Using Weighted Score-Based Ranking Scheme",
            "text": "Bit precision reduction is an important concept in mathematics that has been widely adopted in different applications, including deep neural network compression. Quantization limits the width of the bit that is used to represent a digit or number. The bit width of the operands controls the precision level of the output when mathematical operations are performed. \n\nThe most common types of operations that are performed in CNNs are convolution operation, bias addition, and the dot product of the weights matrix and float input tensor, as described in Figure 4. These operations are computed in a 32-bit full precision floating point. Quantization aims to replace the 32-bit floating-point operations with low-precision number formats such as 1 bit, 4 bit, 8 bit, or 16 bit. Binarization transforms full precision models into a single-bit model (the weights and activations are encoded using 1 bit). Binarization can be described as an extreme case of quantization where the weights and activations are encoded using 4 bit, 8 bit, or 16 bit. We quantized the weights and activations of the baseline model using a symmetric mode 8-bit signed full integer quantizer implemented in Keras [38], using TensorFlow [39] as the computing engine. The mapping of the 32-bit input float tensors (i.e., weight matrix, activations) to the 8-bit quantized range is described in Figure 5. The mapping function (i.e., 8-bit quantizer) maps the input float tensors of the baseline model to the 8-bit quantized output. This function is defined in Equation (1): \n\nwhere q 8bit is the 8-bit quantizer, m f is the multiplier, and i f is the input float tensor. The multiplier is the quantization constant that is multiplied with the float input tensor, as expressed in Equation (2):",
            "score": 0.5608730979509291,
            "section_title": "Quantization",
            "char_start_offset": 19790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2978515625
        },
        {
            "corpus_id": "262825537",
            "title": "Efficient Post-training Quantization with FP8 Formats",
            "text": "Most common approach for handling outliers is to clip them using threshold values that are either obtained through calibration (Sung et al., 2015;Zhao et al., 2019b) or learned during training (Bhalgat et al., 2020;Choi et al., 2018;Esser et al., 2020;Zhang et al., 2018a). More recently (Wei et al., 2022;Xiao et al., 2022) have proposed applying mathematical transformations to redistribute the magnitude of outliers between weights and activation tensors to minimize their impact. Despite these advancements, INT8 methods remain ineffective for a wide range of language modeling tasks, where the presence of LayerNorm was shown to amplify the occurrence of outliers (Wei et al., 2022). Therefore, a significant percentage of these workloads falls back to using higher precision to preserve model accuracy. \n\nThis paper argues that 8-bit floating-point (FP8) formats are an efficient and more productive alternative to INT8 for deep neural network quantization. We evaluated three different representations (E5M2, E4M3, and E3M4) that offer varying degrees of trade-off between dynamic range and precision. Table 1 shows the details of the binary format and special value encoding. The study focused on the benefits of FP8 formats for post-training quantization as the preferred approach used in production. We developed quantization workflows that generalized across different network \u2022 Propose a unified and scalable FP8 quantization flow that works across application domains and different model sizes. To the best of our knowledge, our work is the first to study this problem across 200+ tasks and 75+ models demonstrating the scalability of our approach. \n\n\u2022 Demonstrate the advantages of FP8 formats over INT8, in terms of workload coverage, model accuracy and suitability for a broader range of operations. Our work is also the first study to showcase accuracy-driven automatic model tuning for quantization. \n\n\u2022 Suggest that E4M3 is better suited for NLP models, whereas E3M4 performs marginally better than E4M3 on computer vision tasks.",
            "score": 0.5604129696801295,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1917
                },
                {
                    "start": 1920,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 146,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 288,
                    "end": 306,
                    "matchedPaperCorpusId": "252545187"
                },
                {
                    "start": 669,
                    "end": 687,
                    "matchedPaperCorpusId": "252545187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "260681267",
            "title": "Survey on Computer Vision Techniques for Internet-of-Things Devices",
            "text": "By default, most deep learning frameworks such as Py-Torch and Jax represent floating-point values using 32 bits. These 32-bit floating point values may be needed during training to maintain high precision. However, during inference, a technique called quantization can be used to reduce the number of bits, and consequently reduce the number of memory operations required by a DNN. Quantization techniques are methods to map 32-bit floating point values to lower precision formats such as 16-bit floating point or 8-bit integer [4].\n\nSeveral hardware processors, including the NVIDIA Jetson Nano and Raspberry Pi 4B support low-precision arithmetic. On these processors, 8-bit integer operations can be close to 10\u00d7 faster than 32-bit floating point operations. Furthermore, research has shown that integer operations can consume close to 14\u00d7 less energy than floating point operations.\n\nQuantization Aware Training (QAT) and Post-Training Quantization (PTQ) are the two main methods to perform quantization. QAT methods perform quantization during DNN training to reduce accuracy losses. Generally, these techniques are more accurate but have expensive training processes [5]. As the name suggests, PTQ methods perform quantization once training is complete. These techniques use little or no labeled data to perform quantization. Thus, these techniques do not increase the training complexity but are generally less accurate. The difference in accuracy between QAT and PTQ is seen in Fig 2. We see that for most convolutional DNNs, QAT outperforms PTQ consistently [6]. The accuracy vs. complexity tradeoff of both QAT and PTQ techniques can be tuned by adjusting the symmetry, granularity, and uniformity. This creates a complex design landscape and often requires experts to carefully tune the different hyper-parameters.\n\nA significant amount of research has focused on using quantization to improve the efficiency of transformer-based DNN architectures. The work by Kim et al. [7]. Special quantization functions, such as power-of-two and log-intsoftmax, are used to approximate the GELU activation function, Softmax, and Layer Norm into integer arithmetic [8]. Liu et al. [9] present a method to use PTQ",
            "score": 0.5593694812618317,
            "section_title": "A. Quantization",
            "char_start_offset": 2789,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "9183542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "276575680",
            "title": "Energy-Efficient Transformer Inference: Optimization Strategies for Time Series Classification",
            "text": "Finally, the model is fine-tuned on the training set to recover any accuracy lost due to weight removal. This pruning strategy is particularly beneficial for reducing inference time and computational complexity, making the optimized model suitable for edge deployment and low-power computing environments. 3.4.2 Quantization. Quantization is a widely adopted method to reduce computational complexity and memory requirement. Typically, deep learning models are trained using high-precision 32-bit floating-point (FP32) representations. However, during inference, these representations can be converted to lower bit-width formats, such as 8-bit integers (INT8). \n\nwhere Q denotes the quantization factor. Reducing numerical precision can lead to substantial power savings while maintaining a minimal impact on model performance. In our study, two distinct quantization strategies are employed to decrease memory footprint and computational overhead while maintaining high model accuracy.",
            "score": 0.5587786653106886,
            "section_title": "Pruning.",
            "char_start_offset": 16032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49560546875
        },
        {
            "corpus_id": "259937594",
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "text": "For the first question, we evaluate the LLaMA models at four sizes (i.e., 7B, 13B, 30B, and 65B), examining their performance across a range of precision levels: 2-bit, 4-bit, 8-bit, and 16-bit. Our experiments indicate that 4-bit precision yields the most favorable trade-off between model performance and memory footprint, achieving superior results with the same amount of allocated total bits. However, all models at different sizes suffer from a severe decline at 2-bit precision. \n\nRegarding the second question, we carefully examine the quantization sensitivity of different model components (or substructures), specifically attention and feed-forward networks (FFN). In our experiments, we find that FFN plays a crucial role in retaining the model performance for low-bit quantization. We also evaluated the effects of outlier dimensions, which are specific dimensions that exhibit significantly higher values compared to others in feature activations. We find the outlier dimensions affecting most Transformer layers are primarily responsible for the decline in the quantization performance, and they mainly concentrate on the down projections of FFN. These observa-tions motivate us to design more fine-grained substructure quantization strategies for improving the performance of low-bit models. \n\nFurthermore, we study how to enhance the performance of quantization models through finetuning. We evaluate the impacts of different finetuning methods executed before and after quantization. Our experimental results reveal that parameter-efficient fine-tuning after quantization can achieve commendable performance with significantly reduced computational resources. Our approach can fine-tune a 2-bit LLaMA-65B model on a single NVIDIA A100, surpassing the performance of a 16-bit LLaMA-13B model on zero-shot MMLU dataset.",
            "score": 0.5584842295748702,
            "section_title": "Introduction",
            "char_start_offset": 3712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "240353756",
            "title": "RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions",
            "text": "In all the quantization methods with mixed schemes and multiple precisions, the combination is row-wise within a layer. The evaluated models on image classification tasks include ResNet-18, ResNet-50 [14] and MobileNet-v2 [27] on CIFAR-10, CIFAR-100 [17] and ImageNet ILSVRC-2012 [18] datasets. Baseline models in 32-bit floatingpoint representation for CIFAR-10 and CIFAR-100 datasets are trained from scratch for 150 epochs, then quantized for 150 epochs. For ImageNet dataset, pre-trained 32-bit floating-point models are quantized for 90 epochs. The initial learning rates are 8e \u2212 3 for CIFAR-10, 4e \u2212 3 for CIFAR-100, and 1e \u2212 2 for ImageNet. For NLP tasks with BERT [8] models, we evaluate on the Stanford Sentiment Treebank (SST-2) and Multi-Genre Natural Language Inference (MNLI) datasets from the General Language Understanding Evaluation (GLUE) [30] benchmark. The pre-trained BERT models are from HuggingFace Transformer [32]. Quantization and finetuning are simultaneously performed for 3 epochs with the initial learning rate of 2e \u2212 5. Besides, for all models, the maximum iteration  2. Comparisons with existing quantization works for ResNet-18 on ImageNet, using the (equivalent) 4-bit precision. The covered quantization approaches include linear, nonlinear (Non-Lin.), layer-wise multi-precision with linear (MP-Lin.), mixed-scheme (MS), and our row-wise multi-precision, mixed-scheme (MP-MS). The baseline is the unquantized model with 32-bit floating-point weights/activations, i.e. W32A32. For our method, W4A4* indicates that 5% of the weights are in 8bit (the activations are all in 4-bit).",
            "score": 0.5583278475225346,
            "section_title": "RMSMP Quantization Algorithm",
            "char_start_offset": 19564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1615
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40185546875
        },
        {
            "corpus_id": "263671632",
            "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
            "text": "For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits. We observe that different degrees of quantization still result in similar learning over different tasks (similar change in accuracy per step). Again this confirms that HDQT reduces initial training accuracy, but minimally impacts CIL. \n\nThe strong similarity in the training curves at each incremental learning step suggests that despite initial accuracy differences, both the FP and HDQT models have similar learning performance. Yet, differences in their forgetting trajectories provide insight into changes in the model's representational quality at each incremental step. Further, by comparing the learning and forgetting patterns of both models, we can infer qualitative differences in the features learned through FQT. For instance, since both the HDQT and FP models exhibit the same change in accuracy between two tasks (correlation of 0.99-1.00), we infer that they learned at the same rate. To better anaylze their difference, we categorize forgetting behaviors into distinct scenarios. Scenario 1: the quantized model forgets more than the unquantized model, indicating that the quantized model has reached its representational limit. Scenario 2: the quantized model forgets less, indicating it has not yet reached its representational limit. We analyze forgetting curves in Fig. 3 to further understand how FQT on CIL algorithms impacts the representational limit.",
            "score": 0.5583128466534205,
            "section_title": "RESULTS",
            "char_start_offset": 20191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "248240394",
            "title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression",
            "text": "Network quantization uses smaller bit-width integers to represent and compress parameters of deep neural networks. A typical deep learning model uses 32-bit floating point (FP32) format for its parameters. [8,29] demonstrate that weights and activations can be represented using 8-bit numbers without significant accuracy drop. The use of even lower bit-widths such as 4, 2, and 1-bits has also shown remarkable progress [30][31][32][33]. For example, binarized neural network (BNN) [32] uses 1-bit for each parameter to save storage and reduce computation. However, the quantized model, such as BNN, causes severe loss of precision and accuracy drop. \n\nQ8BERT. Q8BERT [15] applies 8-bit quantization-aware training during the fine-tuning process of BERT. In forward propagation, Q8BERT first quantizes the activation and weight matrices to INT8 format by multiplying two scaling factors of the two matrices, performs INT8 GEMM, which multiplies and accumulates two INT8 matrices to an INT32 matrix, and then de-quantizes the INT32 matrix to an FP32 matrix by dividing the scaling factors of the weight and activation matrices. In backward propagation, Q8BERT uses the 8-bit clip function to approximate the 8-bit round function for training the model. Q8BERT reduces the model size by a factor of 4 and maintains almost the same accuracy as the BERT with FP32 precision in eight different NLP tasks. However, Q8BERT does not consider the sensitivity of layers in BERT and does not apply mixed precision quantization strategy; on the other hand, our SENSI-MIX applies the mixed precision quantization considering the sensitivity and provides a superior performance compared to Q8BERT. \n\nKDLSQ-BERT & K-means quantization. Jin et al. [33] propose KDLSQ-BERT, a framework to combine quantization with knowledge distillation using BERT. KDLSQ-BERT adopts learned step size quantization (LSQ; [34]), which is a variant of the original quantizationaware training, which has been shown to be effective in computer vision.",
            "score": 0.5581602582124534,
            "section_title": "Network quantization",
            "char_start_offset": 12375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "204509218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "271212850",
            "title": "Quantized Prompt for Efficient Generalization of Vision-Language Models",
            "text": "Quantization is one of the most effective compression methods [4,8,10,21,38,47,70] for deep learning models.Generally, parameters such as weights and activations are typically stored as 32-bit floating-point numbers, which consume a significant amount of memory and require intensive computation during inference.Quantization [18,19,56] involves representing these parameters with reduced precision, such as 8-bit integers or even lower bit-widths.By doing so, quantization can significantly reduce the memory footprint and computational complexity of the model without significantly sacrificing accuracy.Quantization methods can be divided into two groups, Post-Training Quantization (PTQ) [2,16,26,41,46,53,55,72] that consumes few resources but suffers higher accuracy loss, and Quantization-Aware Training (QAT) [3,14,32,40,57] that relies on plenty of resources for training and shows better accuracy.Existing works aim to minimize quantization error to improve accuracy, while our work demonstrates that both excessive and insufficient errors are detrimental to model generalization.To achieve optimal generalization performance, a moderate error is required.\n\n3 Exploring Quantization in Model Generalization",
            "score": 0.5581586029890722,
            "section_title": "Quantization",
            "char_start_offset": 6113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1215
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "252968113"
                },
                {
                    "start": 65,
                    "end": 67,
                    "matchedPaperCorpusId": "102350555"
                },
                {
                    "start": 67,
                    "end": 70,
                    "matchedPaperCorpusId": "237108363"
                },
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "236635335"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "5993328"
                },
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "247475977"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "254823125"
                },
                {
                    "start": 703,
                    "end": 706,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "59606175"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 712,
                    "end": 715,
                    "matchedPaperCorpusId": "244527659"
                },
                {
                    "start": 816,
                    "end": 819,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "252873138"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "247595112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "218900673",
            "title": "Accelerating Neural Network Inference by Overflow Aware Quantization",
            "text": "Moreover, in addition to speeding up the MAC operations, quantization also achieves better parallel computing based on the capability of modern CPUs. By comparing Figure 1(b) against Figure 1(a), it can be shown that if 16-bit fixed-point variables are used to hold the MAC result, the degree of parallelism will be doubled and the I/O times will be halved. However, when 16-bit holder is used, numerical overflow on MAC results becomes a frequently-happening problem that must be explicitly considered. A straightforward solution is to use low-bit quantization (<8-bit) for all operands, which however leads to loss of quantization precision and significantly reduced the performance. In addition, low-bit quantization still needs to take up more physical bits (e.g., 4-bit quantization still requires 8-bit physical operands) in most modern CPUs, making the computational resources partially wasted. \n\nTo summarize, existing quantization methods utilize fixed number of bits to represent float values, while both highbit and low-bit representation have limitations. The former suffers from numerical overflow problems and the latter one leads to model precision degradation. To tackle this problem, we propose a novel method to adaptively determine the quantization precision in DNNs, by optimizing the number of bits for operands while prohibiting overflow on the low-bit MAC result holders. To achieve this, we introduce a trainable quantization range mapping factor \u03b1 into each layer of a DNN network, which automatically scales the quantized result to prevent the undesirable overflow. In addition, we propose a quantization-overflow aware training framework for learning the quantization parameters, to minimize the performance loss caused by post quantization [Krishnamoorthi, 2018]. To verify the effectiveness of our method, we conducted tests on a couple of state-of-the-art light-weighted DNNs for a variety of tasks on different benchmarking datasets. Specifically, our experiments include image classification, object detection, and semantic segmentation, which are tested on Ima-geNet, Pascal VOC, and COCO datasets, respectively.",
            "score": 0.5568439137246435,
            "section_title": "Introduction",
            "char_start_offset": 2001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "271946892",
            "title": "Robust iterative value conversion: Deep reinforcement learning for neurochip-driven edge robots",
            "text": "This experiment evaluated the effect of the number of quantization bits of the QNN on learning performances. We evaluated whether the learning performance is high when the quantization bits are close to those of SNNs. This experiment was inspired by previous work [50,51,52] that utilized conversion from binarized neural networks (BNNs) with 1-bit weights and 1-bit activation. This experiment evaluated the number of bits with the best learning performance, such that the quantization error was small and the approximation accuracy was not considerably reduced by quantization. Thus, we compared the learning performance by various quantization bits. The results (Fig. 6) \n\nshow that the highest performance (in total reward and convergence speed) was obtained in 4-or 8-bits, followed by 2-, 16-, and 32-bits. These results confirm that the higher the NN quantization weights, the lower is the performance of the converted SNN. \n\nOn the other hand, lower weights are not necessarily better; around 4-or 8-bits are appropriate. The most significant factor causing this result is that 8 is the upper limit of the number of quantization bits of the neurochip. Since the trained policies need to convert to the same bit as the SNN policies, the quantization of the policy conversion results in large changes in the parameters of the policies trained with weights over 8 bits. Therefore, the performances of the 16-and 32-bit policies were significantly degraded. The latter's performance was even worse than the former's due to the large difference in the number of quantization bits between the 32-bit policies and the SNN policies. \n\nThe other experiments used 4-bit quantization for training QNNs based on two aspects: 1) these experiments show that 4-or 8-bits achieved the best performance, and 2) a QNN with lower bits can be calculated quickly.",
            "score": 0.5564954338824599,
            "section_title": "Evaluation of Best Quantization-bit number from Learning Performance",
            "char_start_offset": 32159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "220403513"
                },
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "211259291"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "12356965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "270924438",
            "title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment",
            "text": "Recent advancements in PTQ have demonstrated that 4-bit quantized LLMs are effective for a variety of tasks, as evidenced by references such as AWQ and OPTQ (Lin et al., 2023;Frantar et al., 2023).However, our observations reveal that these quantized LLMs struggle to sustain engaging conversations, particularly in multi-turn chatbot interactions.For instance, Fig. 1 illustrates the contrast between the 16-bit baseline and 4-bit quantized LLMs in sentence generation.The baseline model begins its responses with \"The circle of fifths is a musical diagram,\" providing relevant answers.On the other hand, the 4-bit quantized model starts to deviate at the seventh token, switching its focus from \"musical\" to \"visual,\" and often generates limited and repetitive phrases.Although both models display similar task performance metrics, such as accuracy in multiple-choice benchmarks, there's a noticeable difference in the logit probability for the seventh token in the 4-bit model, causing a change in the token from \"musical\" to \"visual.\"This issue of altered text generation, observed across multiple examples (see A.9 for additional examples), prompts an investigation into its underlying causes.",
            "score": 0.5564152436320732,
            "section_title": "Observations",
            "char_start_offset": 11251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "225076422",
            "title": "A Statistical Framework for Low-bitwidth Training of Deep Neural Networks",
            "text": "Deep neural networks (DNNs) have a high computational cost and memory footprint that slow down their training and inference. By taking advantage of low-bitwidth computational units in hardware, neural network quantization methods provide promising approaches for reducing the cost of timing, memory, and energy consumption, for both training and inference. \n\nNotable quantization methods can be mainly categorized into two groups, inference quantization and training quantization. In inference quantization, the weights and the activations are quantized to speed up the inference phase. Among inference quantization approaches, post training quantization usually does not require access to the partial/full training dataset, and it does not need to re-train/fine-tune the quantized model [1,2,3,4,5]. To reduce the performance gap between the quantized model and its full precision counterpart, quantization-aware training (QAT) fine-tunes the quantized model on the training dataset [6,7,8,9,10,11,12,13,14]. However, QAT computes the gradients in full precision, so the training phase is not accelerated. \n\nTraining quantization methods, also known as fully quantized training (FQT), further quantize the gradients, compared with QAT. In FQT, all the activations, weights, and gradients are quantized in both the forward and backward propagation. Hence, training can be implemented efficiently on low-bitwidth computational units, such as tensor cores [15]. Low-bitwidth hardware is faster and more power-efficient, as compared to FP32 counterparts. As the need for training huge models continues to grow [16,17,18], there has been increasing attention on FQT. Earlier work on FQT includes mixed-precision FP16/FP32 training [19] and lossy 2-bit training [6]. Recently, 8-bit FQT has emerged as a sweet spot on the accuracy versus efficiency tradeoff. Various 8-bit numerical formats have been proposed, including INT8 [20,21,22,23], FP8 [24,25],",
            "score": 0.5559247543834198,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "67750088"
                },
                {
                    "start": 793,
                    "end": 795,
                    "matchedPaperCorpusId": "59413897"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 991,
                    "end": 993,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 1611,
                    "end": 1614,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "2547043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4267578125
        },
        {
            "corpus_id": "269741383",
            "title": "From Algorithm to Hardware: A Survey on Efficient and Safe Deployment of Deep Neural Networks",
            "text": "FracBits [43] generalized quantization bit width to an arbitrary real number to make it differentiable and learned channel-wise bit allocation during training.Distribution-aware Multi-Bit Quantization (DMBQ) [44] proposed a loss-guided bit width allocation strategy to adjust the bit width of weights and activations channel-wisely.Lastly, AutoQ [45] presented a hierarchical deep reinforcement learning approach to find the quantization bit width of channels while simultaneously optimizing hardware metrics such as latency and energy.Table II summarizes the mixed-precision quantization approaches discussed above.In Table II, the bit width of mixed-precision quantization approaches is the average bit width across all the layers.For example, if a two-layer model quantizes one layer to 4 bits and the other one to 8 bits, then the average bit width of this mixed-precision quantization is (4+8)/2 = 6 bits.The variance in reported accuracies for CNN architectures like MobileNet-V2 across literature [39], [45] complicates the evaluation of quantization's impact on model performance.This inconsistency could be due to the differences in dataset pre-processing, training protocols, or minor architectural modifications.To accurately evaluate quantization effects, it requires a standardized approach towards reporting the performance metrics of both original and quantized models.This would facilitate a more precise isolation of quantization's effects from other influencing factors.As a result, emphasizing the relative changes in model accuracy resulting from quantization-rather than focusing solely on absolute accuracy figures-offers a more robust measure for evaluating the effectiveness of quantization techniques.Such an approach would help in better understanding the trade-offs between model efficiency and performance in the context of quantization, thereby contributing to more informed decisions in model optimization and deployment.Therefore, we present the accuracies of the original and compressed models to demonstrate the relative change in model accuracy.\n\n3) Transform Quantization: Transform quantization decorrelates and quantizes the weights, which first rotates the space and then quantizes the data after rotation with mixed precision as shown in Fig. 1(d).",
            "score": 0.5557320386995699,
            "section_title": "A. Quantization and Entropy Coding",
            "char_start_offset": 11972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1951
                },
                {
                    "start": 1951,
                    "end": 2079
                },
                {
                    "start": 2081,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "218862856",
            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
            "text": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) [1], PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between [1] and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated. Consequently, as our 4-bit targeted model (PSGD@W4) achieves comparable accuracy for all higher bits of the other methods, PSGD outperforms conventional methods in all bit-widths. Compared to other regularization methods, PSGD is able to maintain reasonable performance across all bits by constraining the distribution of the full precision weight to resemble that of the quantized weight. This quantization-friendliness is achieved by the appropriately designed scaling function. Also, unlike [1], PSGD does not need additional overhead of calculating the double-backpropagation. \n\nPost-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits. For OCS, following the original paper, we chose the best clipping method for both weights and activation. DFQ also has a similar tendency of showing drastic accuracy drop under the 6 bit-widths depicted in Fig. 1 at the original paper of DFQ [26]. This is due to the fundamental discrepancy between FP and quantized weight distributions as stated in Sec 1 and Fig. 1.",
            "score": 0.5555345889503472,
            "section_title": "Method",
            "char_start_offset": 20351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "211724357"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "184487878"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "262062668",
            "title": "DeepliteRT: Computer Vision at the Edge",
            "text": "Deep learning models for computer vision are being extensively deployed in various domains and industries due to substantial improvements in the accuracy of deep convolutional neural networks (CNNs). CNN architectures including VGG [28], ResNet [18], Inception [29], DenseNet [20] and YOLO [27] have demonstrated exceptional performance on image classification and object detection tasks. The widespread adoption of deep learning solutions in computer vision has also coincided with the growth of edge computing [33], promising the potential of bringing machine learning to low-power edge devices. However, the enhancements in CNN model accuracy have come at the expense of increased model complexity leading to high power, compute, memory, and storage requirements, making such models highly impractical for most use cases on resource-constrained edge devices. \n\nSeveral compression techniques [4] [15] [19] have been explored to tackle this problem with the goal of decreasing model size while maintaining the baseline accuracy. Quantization is one such approach that realizes this goal by reducing the scale of model weights and activations from 32-bit floating-point (FP32) to lower precision representations. In addition to model compression, quantization also offers the benefits of fewer memory accesses, lower latency, and improved energy efficiency. 8-bit integer (INT8) has become the predominant bit-width for quantization and is widely supported in publicly available machine learning frameworks [1] [25] that perform quantization-aware training (QAT) and in open-source inference engines [10] [16] that execute the quantized models on commodity hardware. Recent advances have also been made in ultra low-bit quantization where the model weights and activations are quantized to less than 8 bits of precision. Using methods such as LSQ [11], a 2-bit quantized model can achieve a compression rate of up to 16\u00d7 with an accuracy drop of less than a few percent relative to the FP32 baseline. Moreover, compute-intensive nodes in the network, including dense and convolution layers, can also utilize inexpensive bitwise operations to perform the dot products on extremely low-bit data. The significant compression and speedup resulting from ultra low-bit quantization make it a compelling choice for CNN deployment on edge devices.",
            "score": 0.5553024287119677,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "52155776"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "202786778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470703125
        },
        {
            "corpus_id": "253535294",
            "title": "Power-efficient gesture sensing for edge devices: mimicking fourier transforms with spiking neural networks",
            "text": "Similarly, for 8 and 16 bits quantization, 1 bit for sign, 3 bits are assigned to integer part and the rest of the bits are used for fractional parts. As expected the accuracy drops for high quantization i.e., 4 bits for all the models and drops with a higher percentage for our proposed model (model 3). This drop is due to the higher number of neurons used in the proposed model and hence the high impact of quantization. However, we believe higher accuracy can be achieved with quantization aware training which currently the nengoDL framework does not support. Increasing the bits for quantization increases the accuracy as expected and with 8-bit and 16-bit, the proposed model achieves 92.68% and 96.46% of accuracy with good precision and recall as indicated by f1-scores. Where for \"f1-scores micro\" average is calculated by counting the total true positives, false positives and false negatives. For \"f1-scores macro\" the metric is calculated for each class using their unweighted arithmetic mean. \n\nConsidering the goal of having a system that is energyefficient, we looked into the energy consumption per classification of the proposed system. Since the actual hardware-based energy calculation is out of the scope of this research work,in the current study, we relied the hardware metrics of the \u03bcBrain chip defined in [74] to estimate the energy consumption. If SP N is the maximum number of spikes, SP E = 2.1pJ is the energy per spike and LK P = 73\u03bcW is the static leakage power, then the energy  consumption C E per classification using \u03bcBrain hardware metrics is given as: \n\nwhere \u03b4T is the inference time. Assuming the \u03b4T = 28 ms. The energy consumption per classification of our proposed is approx. C E = 2.1\u03bcJ. To see the energy efficiency for SNN hardware, readers can refer to [75], where the SNN hardware is compared with the other deep learning hardware in terms of energy efficiency. The performance of SNN hardware was tested on a keyword spotting application using a dynamic energy cost per inference on some energy-efficient accelerators commercially available as shown in Fig. 10.",
            "score": 0.5551679851238918,
            "section_title": "Discussion",
            "char_start_offset": 23164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "270710957",
            "title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels",
            "text": "Quantization techniques for large language models (LLMs) aim to reduce the precision of weights and activations to lower-bits without significantly compromising performance (Zhu et al., 2023). Popular approaches include Post-Training Quantization (PTQ) (Banner et al., 2019) and Quantization-Aware Training (QAT) (Liu et al., 2023). PTQ can be divided into static quantization, which uses a small dataset to calibrate scaling factors for weights and activations, and dynamic quantization, which quantizes activations on-the-fly during inference. Our study utilizes the static PTQ techniques such as GPT-Q (Frantar et al., 2023) in our experiments for practicality, but, importantly, it is agnostic to the actual quantization technique used. \n\nA few works have highlighted 4-bit precision as a robust quantization limit for wide variety of NLP tasks (Dettmers & Zettlemoyer, 2023). Our results (fig. 2 and 3) also show similar findings that performance drop is small from bf16 to 8-bits, and then to 4-bits. Hence, our experiments focus more on quantizing LLMs below 4-bits to highlight the effectiveness of layer-wise quantization based on layer importance. Specifically, our empirical results (section 5) show that variable layer-wise quantization can retain 90% of the performance with a notable compression up to 2.85-bits overall. Concurrent (ArXiv preprint) work by Tai et al. (2024) also show effectiveness of variable quantization in vision language models. \n\nA few previous works have studied layer importance in transformers (Vaswani et al., 2017;Simoulin & Crabb\u00e9, 2021). Some of the recent, (unpublished) concurrent works such as ShortGPT (Men et al., 2024) have also proposed utilizing layer importance but primarily focusing on pruning. Shen et al. (2020) had utilized hessian information from each layer as an importance measure to quantize specific weight matrices at different bits.",
            "score": 0.5550676490207698,
            "section_title": "RELATED WORK",
            "char_start_offset": 7026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 274,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1556,
                    "end": 1580,
                    "matchedPaperCorpusId": "237331496"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "168170068",
            "title": "Mixed Precision Training With 8-bit Floating Point",
            "text": "Reduced precision computation for deep neural networks is one of the key areas addressing the widening compute gap driven by an exponential growth in model size. In recent years, deep learning training has largely migrated to 16-bit precision, with significant gains in performance and energy efficiency. However, attempts to train DNNs at 8-bit precision have met with significant challenges because of the higher precision and dynamic range requirements of back-propagation. In this paper, we propose a method to train deep neural networks using 8-bit floating point representation for weights, activations, errors, and gradients. In addition to reducing compute precision, we also reduced the precision requirements for the master copy of weights from 32-bit to 16-bit. We demonstrate state-of-the-art accuracy across multiple data sets (imagenet-1K, WMT16) and a broader set of workloads (Resnet-18/34/50, GNMT, Transformer) than previously reported. We propose an enhanced loss scaling method to augment the reduced subnormal range of 8-bit floating point for improved error propagation. We also examine the impact of quantization noise on generalization and propose a stochastic rounding technique to address gradient noise. As a result of applying all these techniques, we report slightly higher validation accuracy compared to full precision baseline.",
            "score": 0.5538837829199745,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "269148715",
            "title": "SNN4Agents: a framework for developing energy-efficient embodied spiking neural networks for autonomous agents",
            "text": "To effectively compress the model size, we perform weight quantization through PTQ with TR rounding scheme.To do this, we first train the given network without quantization, while employing baseline settings for timestep, attention window, and training epoch.For this scenario, we employ 32-bit precision, 20 timestep, 100\u00d7100 attention window, and 200 training epoch.Once the training phase is finished, we perform quantization process to the trained network.Afterward, we perform DSE under different weight precision levels (i.e., 32, 16, 12, 10, 8, 6, and 4 bit) to evaluate their impact on the accuracy; see the parameter settings for DSE in Table 4. Experimental results of DSE are shown in Figure 7, from which we draw the following key observations.\n\n\u2022 In the early of training phase (e.g., \u2264 60 training epoch), the network is still learning new information, hence the accuracy curve is increasing for 16-, 12-, and 10-bit precision levels, as shown by 1 .\n\n\u2022 Employing 16-, 12-, and 10-bit precision levels for SNN weights lead to comparable accuracy to the original SNN model with 32-bit precision (no quantization) after running at least 80 training epoch, as shown by 2 .\n\n\u2022 Employing 8-, 6-, and 4-bit precision levels for SNN weights lead to significant accuracy degradation, as they can only reach about 50% accuracy across training epochs, as shown by 3 .These results indicate that the network is not properly trained.\n\nThese observations expose several key design guides that we should consider when applying quantization.First, selecting the precision level should be performed carefully, so that it does not lead to a significant accuracy degradation which diminishes the benefits of quantization.Second, a 10-bit precision level offers a good trade-off between accuracy and memory footprint as it can achieve comparable accuracy to that of the larger precision levels after running at least 80 training epoch., 32, 16, 12, 10, 8, and 4 bit).",
            "score": 0.5538708097012821,
            "section_title": "Model Compression through Quantization",
            "char_start_offset": 14288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 964
                },
                {
                    "start": 966,
                    "end": 1183
                },
                {
                    "start": 1185,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1435
                },
                {
                    "start": 1437,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1930
                },
                {
                    "start": 1930,
                    "end": 1962
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "235166247",
            "title": "Post-Training Sparsity-Aware Quantization",
            "text": "Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.",
            "score": 0.5537246771420874,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "263310765",
            "title": "MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search",
            "text": "In this paper we focus on mixed precision quantization. There are only a few prior works that focus on mixed precision quantization since most focus on single precision quantization, where the quantization bit-width of all weights are uniform and therefore; treated as a constant. [23] propose a framework for determining the quantization policy with mixed precision and reinforcement learning, but compared to MixQuant it requires significantly more overhead (hardware simulators and reinforcement learning). [24] focuses on mixed precision quantization of activations and distinguishes between key and non-key activations to assign 8-bit and 4-bit precision respectively. In contrast to MixQuant, which searches for weights mixed precision from 8 to 2 bits, [24] is limited to a choice between 4 and 8 bits and applies only to activations while all weights are quantized with 8-bit precision. The primary focus of [25] is neural architecture search, which can also be used for mixed precision quantization. However, their search on ResNet 18 for ImageNet takes 5 hours, while MixQuant runs in order of a few seconds. [26] use single precision for weights, where the mixed precision is represented only by selecting a different bit-width for weights than activations. [26] is the most most recent, and we show that MixQuant yields better accuracy. \n\nAnother mixed precision quantization work that we build on is [27], who identify optimal bit-width allocation across DNN layers. However, there are two primary differences between [27] and our work: (1) [27] focus on fixed-point precision, not integer precision, (2) [27] a different method for finding layer bitwidths based on predicted signal-quantization-to-noise -ratio. Moreover, while they find that on CIFAR-10 convolutional DNN is able to achieve 20 % model size reduction; their AlexNet experiments on ImageNet-1000 achieve less than 1% model reduction. In this work we are able to successfully leverage mixed precision optimal bit-width allocation on ImageNet-1000 models.",
            "score": 0.5535226867472248,
            "section_title": "Mixed Precision Quantization",
            "char_start_offset": 5620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369140625
        },
        {
            "corpus_id": "277621803",
            "title": "RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm",
            "text": "As in previous studies (Lin et al., 2024;Shao et al., 2023), we evaluate our method on the wikitext2 (Merity et al., 2016) and c4 (Raffel et al., 2020) datasets. We split the test / validation sets into sequences of length 2048 as test samples and measure the average perplexity of the quantized model on each test sample. For c4, since the full validation set is too large and model performance converges quickly, we use 500 samples as the test set. \n\nBaseline Methods. We compare RaanA with GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2024), OmniQuant (Shao et al., 2023), and Quip# no FT & no E 8 (Tseng et al., 2024) 5 . Additionally, for 4-bit quantization, we also compare with EasyQuant (Tang et al., 2024), a lightweight quantization method that does not require calibration data and targets 4-bit quantization. Note that most baseline models use tricks such as grouping and keeping fullprecision outliers that introduce extra bit costs6 , generally ranging from 0.1 to 0.3 bits. To make a fair comparison, we report RaanA performance with x + 0.1 bits and x + 0.3 bits for x \u2208 2, 3, 4.",
            "score": 0.5530938627718945,
            "section_title": "Datasets.",
            "char_start_offset": 19807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1096
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 41,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 130,
                    "end": 151,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 498,
                    "end": 520,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 526,
                    "end": 544,
                    "matchedPaperCorpusId": "258999941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "251564521",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "text": "Large pretrained language models are widely adopted in NLP (Vaswani et al., 2017;Radford et al., 2019;Brown et al., 2020;Zhang et al., 2022) but require significant memory for inference. For large transformer language models at and beyond 6.7B parameters, the feed-forward and attention projection layers and their matrix multiplication operations are responsible for 95% 2 of consumed parameters and 65-85% of all computation (Ilharco et al., 2020). One way to reduce the size of the parameters is to quantize them to less bits and use low-bit-precision matrix multiplication. With this goal in mind, 8-bit quantization methods for transformers have been developed (Chen et al., 2020;Lin et al., 2020;Zafrir et al., 2019;Shen et al., 2020). While these methods reduce memory use, they degrade performance, usually require tuning quantization further after training, and have only been studied for models with less than 350M parameters. Degradation-free quantization up to 350M parameters is poorly understood, and multi-billion parameter quantization remains an open challenge. \n\nIn this paper, we present the first multi-billion-scale Int8 quantization procedure for transformers that does not incur any performance degradation. Our procedure makes it possible to load a 175B parameter transformer with 16 or 32-bit weights, convert the feed-forward and attention projection layers to 8-bit, and use the resulting model immediately for inference without any performance degradation. We achieve this result by solving two key challenges: the need for higher quantization precision at scales beyond 1B parameters and the need to explicitly represent the sparse but systematic large magnitude outlier features that ruin quantization precision once they emerge in all transformer layers starting at scales of 6.7B parameters. This loss of precision is reflected in C4 evaluation perplexity (Section 3) as well as zeroshot accuracy as soon as these outlier features emerge, as shown in Figure 1. Figure 1: OPT model mean zeroshot accuracy for WinoGrande, HellaSwag, PIQA, and LAMBADA datasets.",
            "score": 0.5516304375833466,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 102,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 427,
                    "end": 449,
                    "matchedPaperCorpusId": "226283902"
                },
                {
                    "start": 666,
                    "end": 685,
                    "matchedPaperCorpusId": "225076422"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 722,
                    "end": 740,
                    "matchedPaperCorpusId": "202565587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404052734375
        },
        {
            "corpus_id": "254974105",
            "title": "Training Integer-Only Deep Recurrent Neural Networks",
            "text": "In recent years, neural network quantization research [13,[16][17][18] has enabled exploration into the quantization of RNN architectures. [19], which explores low-bit quantization of weights for RNNs, showed that binarizing weights leads to a massive accuracy drop, but ternarizing them keeps the model performance. [16] demonstrates that quantizing RNNs to extremely low bits is challenging; they quantize weights and matrix product to 4-bit, but other operations such activation functions are computed in full-precision.\n\nLow-precision RNNs has also been a recent focal point in industry research. [20] introduces Bit-RNN and improves 1-bit and 2-bit RNNs quantization by constraining values within a fixed range carefully; they keep activation computation and element-wise operations in full-precision. [21] builds upon Bit-RNN and propose a low-bit RNN with minimal performance drop, but they increase the number of neurons to compensate for performance drop; they run activation functions in floating-point precision as well. [22] quantizes LSTM weights to 1-bit and 2-bit and empirically show that low-bit quantized LSTMs suffer from exploding gradients. It has been shown that gradient explosion can be alleviated using normalization layers and leads to the successful training of low bit weights [23]. [24] studied the effect of normalization in low-bit networks theoretically and proved that low-bit training without normalization operation is mathematically impossible; their work demonstrates the fundamental importance of involving normalization layers in quantized networks.\n\n[25] is a pioneering work in LSTM quantization, which demonstrates speed-up inference of large-scale LSTM models with limited performance drop by partially quantizing RNN cells. Their proposed method is tailored towards specific hardware. They use an 8-bit integer for matrix multiplications and a 16-bit integer for tanh, sigmoid, and element-wise operations but do not quantize attention. [26] propose an effective 8-bit integeronly LSTM cell for Keyword Spotting application on microcontrollers. They enforce weights and activations to be symmetric on fixed ranges [\u22124, 4]",
            "score": 0.5499631753526628,
            "section_title": "Related Work",
            "char_start_offset": 9207,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "216641722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "270257725",
            "title": "Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation",
            "text": "2) Federated Training and Server Performance: The convergence velocity, as depicted in Fig. 3, indicates that setups of uniform 4-bit clients, or a mixed-precision schema of [12,4,4] bits, exhibit slower and more erratic initial convergence, even when the latter has a better random start in training. In contrast, setups incorporating clients with 16-bit precision or higher demonstrate more rapid and stable convergence, achieving approximately 90% accuracy within 10 communication rounds. Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients. 3) Client Performance: After 100 rounds, the final global model,  100 , is broadcast to clients. Here, we focus on the clients at the lowest precision, 4-bit, for higher counterparts have better performance and minor degradation from well converged global model as illustrated in Table I and Fig. 3. \n\nAs shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision  clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy.",
            "score": 0.5488811298368629,
            "section_title": "B. Results",
            "char_start_offset": 17537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 180,
                    "matchedPaperCorpusId": "246890627"
                },
                {
                    "start": 180,
                    "end": 182,
                    "matchedPaperCorpusId": "246890627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "257038209",
            "title": "Rethinking Data-Free Quantization as a Zero-Sum Game",
            "text": "Regarding the bit width n, we select 3-bit, 4-bit and 5-bit precision, which are representative for low-bit and highbit cases, particularly: 3-bit quantization actually leads to a huge performance loss, which is a major challenge for the existing DFQ methods; while 5-bit or higher-bit quantization usually causes a small performance loss, which is selected to validate the generalization ability. \n\nFor the maximization process, we construct the architecture of the generator (G) following ACGAN (Odena, Olah, and Shlens 2017), while P and Q play the role of discriminator, where G is trained with the loss function Eq.( 14) using Adam (Kingma and Ba 2014) as an optimizer with a momentum of 0.9 and a learning rate of 1e-3. For the minimization process, Q is optimized with the loss function Eq. ( 15), where SGD with Nesterov (Nesterov 1983) is adopted as an optimizer with a momentum of 0.9 and weight decay of 1e-4. For CIFAR, the learning rate is initialized to 1e-4 and decayed by 0.1 for every 100 epochs, while it is 1e-5 (1e-4 for ResNet-50) and divided by 10 at epoch 350 (at epoch 200 and 300 for ResNet-50) on ImageNet. The generator and quantized model are totally trained for 400 epochs. The batch size is set to 16. For the hyperparameters, \u03b1, \u03b2 and \u03b3 in Eq.( 14); \u03bb l and \u03bb u in Eq.( 12) are empirically set to be 0.1, 1, 1, 0.3 and 0.8. All experiments are implemented with pytorch (Paszke et al. 2019) based on the code of GDFQ (Xu et al. 2020) and run on an NVIDIA GeForce GTX 1080 Ti GPU and an Intel(R) Core(TM) i7-6950X CPU @ 3.00GHz.",
            "score": 0.5479834760303709,
            "section_title": "Experiment Experimental Settings and Implementation Details",
            "char_start_offset": 25308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1557
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 527,
                    "matchedPaperCorpusId": "202786778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409423828125
        },
        {
            "corpus_id": "264406079",
            "title": "Exploring the Potential of Flexible 8-bit Format: Design and Algorithm",
            "text": "For NIA format, there appears to be a significant fluctuation. For instance, the relatively large loss of precision that occurs on the MobileNetv2 is even lower than the precision of INT8(-3.04%). But then almost exceeds All Mix (-0.01%) on PSPNet. While in general, our format mixing solution shows high performance while also speeding up the search process and improving hardware efficiency with only a minor loss of precision. \n\nResults on NLP tasks. Table 3 presents the precision of various quantization formats on the BERT model. The FP32 baseline achieved an average precision of 83.93%. When quantized to INT8, a decline to 79.68% was noted. Impressively, all FP8-related formats, including NIA Format (83.51%), Mixed FP8 (83.69%), Mixed FP8 (r) (83.65%), and both All Mixed and Limited Mix (83.72%), performed consistently close to the FP32 baseline. On the other hand, the aggressive W4A8 format (4-bit weight and 8-bit activation), yielded a respectable average of 82.04%, marking only a modest 2.2% drop from the FP32 precision. In summary, while FP8 formats emulate the original performance admirably, the more radical W4A8 approach proves to be promising with minimal accuracy compromises.",
            "score": 0.5446529039411989,
            "section_title": "Main Results",
            "char_start_offset": 16536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "258990120",
            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
            "text": "1) DVS128 Gesture: For the DVS128 Gesture dataset, three quantization methodologies are available in the literature. Two of them reduce the bit precision to 2 bits, while the third binarizes the model. The full-precision model in this work shows higher accuracy than models reported in the literature. Also, our binarized model outperforms the 2-bit [10], [12] and 1-bit [11] cases shown in the table.\n\nApplying the proposed quantization method to the SNN model from [11] allows for evaluating its performance based   on accuracy. Table IV presents accuracy results corresponding to 1-bit and 32-bit precisions. In this work, accuracies of both full-precision and 1-bit quantized SNN models are higher than the ones reported in [11] by 1.44% and 1.5%, respectively.\n\n2) N-MNIST: As seen in Table III, our full-precision and binarized SNN model shows competitive performance compared to other full-precision and 4-bit quantized models. Only [28] with its 32-bit SNN model achieves slightly higher accuracy than our binarized model.\n\nThe accuracy comparison between the proposed method and [13] are shown in Table V, where the SNN network structure is identical. Our binarized and 4-bit precision models outperform the 4-bit one from [13] by 18.33% and 18.45%, respectively. At the same time, our full-precision model shows an accuracy that is 1.53% higher than the one reported in [13]. It is worth mentioning that although the network structure is identical, the SNN training methods and selected optimization techniques are different in our work, which can explain the significant difference in the obtained results.\n\n3) CIFAR10-DVS: No quantized SNN model can be found in the literature for the CIFAR10-DVS dataset. Hence, the performance of the proposed quantized models is compared with other full-precision models. Both the 32-bit and 1-bit models in this work significantly outperform other works in terms of accuracy. Binarized model, while requiring almost 32 times less model size, still performs better than fullprecision models",
            "score": 0.5444826220416075,
            "section_title": "A. Comparison with prior works",
            "char_start_offset": 17086,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "19496174"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "220794419"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "233443785"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "233443785"
                },
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "233443785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "262825537",
            "title": "Efficient Post-training Quantization with FP8 Formats",
            "text": "Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of operations. Furthermore, our findings suggest that E4M3 is better suited for NLP models, whereas E3M4 performs marginally better than E4M3 on computer vision tasks. The code is publicly available on Intel Neural Compressor: https://github.com/intel/neural-compressor.",
            "score": 0.5440226048924168,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "258657998",
            "title": "Systematic Literature Review on Cost-Efficient Deep Learning",
            "text": "Which bit-precision to use for quantization? 8-bit integers are a common choice for precision [S32], [S33], [S34]. 8-bit integers benefit from not requiring custom inference hardware but having a wide enough range to maintain model accuracy. Moderate accuracy loss has been achieved with 6bit precision [S35], 4-bit precision [S36], and ternary values [S37]. Quantization further reduces the model accuracy for simpler models [38]. The more complex the model and the more neurons the model has, the less quantization decreases the accuracy. When using low precision -binary or ternarymore complex models compensate for the accuracy loss. \n\nWhat is a good granularity level for bucketing? Bucketing can be performed for the entire model, by layer [S32] or by a CNN filter [S38]. Bucketing at the layer level can lead to fewer buckets, which requires smaller integer bit-width, which reduces the model size without significant accuracy loss [S32]. \n\nWhat to quantize: weights or activations? When quantizing the weights, the inference load is lowered. On the one hand, quantizing activations, including inputs, leads to inference-time bucketing, which adds computations. On the other hand, quantizing activations during inference can still be lighter than using floating point numbers for activations [S34]. \n\n3) BINARY NEURAL NETWORKS Binary neural networks (BNN) take the compression of weights and activations further by representing them with just one bit [S39]. Compared to 32-bit floating point DNNs, BNNs mean 32 times smaller models and radically reduced off-chip memory access during inference. Computations become light-weight as multiplications can be replaced by bit-wise operations, such as XNOR [39]. BNNs are promising for low latency and low energy inference, although optimal BNN execution benefits from custom hardware. Another significant drawback of BNNs is the reduced model accuracy. One suggested approach to increasing the BNN model accuracy is to combine ensemble methods with BNNs [S40].",
            "score": 0.5436626558295921,
            "section_title": "2) QUANTIZATION",
            "char_start_offset": 31297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "235709564",
            "title": "An Overview of Machine Learning within Embedded and Mobile Devices\u2013Optimizations and Applications",
            "text": "Furthermore, most machine-learning models, especially deep learning models, require huge amounts of multiply and accumulate (MAC) operations for effective training and inference. Figure 3 describes the power consumed by the MAC unit as a function of the bit precision adopted by the system. We may observe that the higher the number of bits, the higher the power consumed. Thus, to reduce the power consumed during computation, reduced bit precision arithmetic and data quantization may be utilized [151]. It may be deduced from the graph that lower number precisions consume less power than high precisions with no loss in prediction accuracy. However, we can observe that when precision is reduced below a particular threshold (16 bit fp), the accuracy of the model is greatly affected. Thus, quantization It may be deduced from the graph that lower number precisions consume less power than high precisions with no loss in prediction accuracy. However, we can observe that when precision is reduced below a particular threshold (16 bit fp), the accuracy of the model is greatly affected. Thus, quantization may be performed successfully to conserve energy but quantizing below 16-bit fp may require retraining and fine-tuning to restore the accuracy of the model.",
            "score": 0.5416964963401245,
            "section_title": "Power Consumption",
            "char_start_offset": 46039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1266
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 504,
                    "matchedPaperCorpusId": "15817277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330322265625
        },
        {
            "corpus_id": "273501713",
            "title": "Evaluating Quantized Large Language Models for Code Generation on Low-Resource Language Benchmarks",
            "text": "This section addressed the research question RQ1. The code LLMs were chosen based on licensing, comparative performance, and computational demand that can meet the limitations of the hardware specified in the preceding section. \n\nTable 1 summarizes the evaluated models. The following LLMs trained for code generation were selected for this study: DeepSeek Coder 6.7B Instruct [19], CodeQwen 1.5 7B Chat [18], CodeLlama 7B Instruct [17], StarCoder2 7b [21], and CodeGemma 7b [20]. As of August 14th, 2024, these models were ranked among the top in the Multilingual Code Models Evaluation leaderboard. This leaderboard ranks multilingual code-generation models based on their performance on HumanEval [28] and MultiPL-E [33] benchmarks. \n\nTo maximize the diversity of models, only original models were considered, and fine-tuned offshoots of these models were ignored. For example, Artigenz Coder DS 6.7B, while ranked high on the leaderboard, is a fine-tuned version of DeepSeek Coder 6.7B and was not included in this study. \n\nOnly small models with 7 billion parameters or less were considered to ensure that the quantized models can be run reasonably well on consumer devices. Lastly, all these models employ a free-to-use model, albeit with certain restrictions (e.g., output from CodeLlama cannot be used to train other models). \n\nFor each of the five models, we tested 2-, 4-, and 8-bit integer weights-only quantized versions in a GPT-Generated Unified Format (GGUF) 7 format. All models were downloaded from HuggingFace's model repository. If multiple similar versions of the same model were available then the version with the highest download count was used. 2-and 8-bit quantizations are the most common quantizations at the lower and higher precision ends. 4-bit quantization is often recommended as a well-balanced trade-off between quality and size [16]. We will test whether this observation also applies to code LLMs.",
            "score": 0.541572046694864,
            "section_title": "Choice of LLMs",
            "char_start_offset": 18980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "258205341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models. We found that quantization consistently improves the tradeoff regardless of where we are on the compute cost-quality tradeoff graph. In other words, for each bfloat16 model, we found quantized models with lower compute cost and higher accuracy. Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks. Based on our results, we proposed a simple practical approach to compress models given compute cost and quality constraints, consisting of two steps: quantize the model to 4-bit, then multiply the number of parameters in each layer by a global factor to achieve the desired tradeoff. We invite further research into comparing different numeric formats for quantization, and hope that this line of work will inform the development of future hardware. We also encourage more works in model compression to compare methods on cost-quality tradeoff graphs, as this provides a more nuanced and thorough analysis. Lastly, we open-sourced our quantization library, in the hopes that it will accelerate quantization research and deployment with JAX.",
            "score": 0.5413420394519166,
            "section_title": "Conclusion",
            "char_start_offset": 23412,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "220891847",
            "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge",
            "text": "Quantization is one of the most effective compression techniques used for reducing the weight and activation bit-width of a neural network model, thereby resulting in reduced model size and execution cost in terms of energy, time, and silicon area. The general matrix multiply (GEMM) operation is most commonly used to perform inferences for DNNs. Although CPUs and GPUs generally process this GEMM operation using floating-point 32-bit precision without applying a quantization technique, previous studies have proposed numerous methods for quantizing DNN models for a variety of tasks in a range of domains, and have successfully attained a nearly lossless inference model, using less than 8-bit integer (int8) precision. \n\nFor CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs [35], [37], as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset [27], [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 [10] with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained. \n\nAmong the recent studies concerned with the quantization on BERT [1], a model widely used in the field of NLP, [39] presented the state-of-the-art results. For example, in tasks of sentiment classification, natural language inference, named entity recognition, and machine reading comprehension with benchmarks of the general language understanding evaluation (GLUE) [40], Stanford Question Answering Dataset (SQuAD) [41], and CoNLL-03 [42], the activation 8-bit and weight 4-bit quantization precision yielded a comparable performance, without any significant quality degradation in the BERT model [1].",
            "score": 0.5408361148167267,
            "section_title": "B. LATEST QUANTIZATION RESULTS",
            "char_start_offset": 10232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "213621056"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "277667131",
            "title": "PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs",
            "text": "Quantized models replace full-precision (e.g., 32-bit float) weights with lowerprecision representations such as 4-bit integers. This significantly reduces: \n\n-Memory Footprint : 4\u00d7 fewer bits than 16-bit, or 8\u00d7 fewer than 32-bit. \n\n-Bandwidth Requirements: Cheaper to transmit off-chain or store in decentralized storage. -Compute Overheads: Specialized hardware (e.g., GPU tensor cores) can often process low-precision vectors at higher throughput, up to 8\u00d7 faster for 4-bit vs. 32-bit [7,9]. \n\nHence, if PoGO requires public availability of model parameters, it makes sense to use a compact 4-bit representation. However, to preserve full precision for actual training and gradient checks, we still keep a Merkle commitment on the 32-bit model.",
            "score": 0.540555899567454,
            "section_title": "Why Quantization?",
            "char_start_offset": 3996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "69619070",
            "title": "Efficient Deep Learning in Network Compression and Acceleration",
            "text": "Typically, DNNs apply floating-point (such as 32-bit) precision for training and inference, which may lead to a large cost in memory, storage, and computation. To save the cost, network quantization category uses reduced precision to approximate network parameters. These approaches consist of scalar or vector quantization and fixed-point quantization (see Figure 4). \n\nScalar or vector quantization techniques are originally designed for data compression, where a codebook and a set of quantization codes are used to represent the original data. Considering that the size of codebook is much smaller than the original data, the original data could be efficiently compressed via quantization. Inspired by that, scalar or vector quantization approaches are applied to represent the parameters or weights of a deep network for compressing. In [29], Gong et al. applied k-means clustering to the weights or conducting product quantization and achieved a very good balance between model size and recognition accuracy. They achieved 16-24\u00d7 compression of the network with only 1% loss of accuracy on ImageNet classification task. Wu et al. [30] proposed quantized CNN to simultaneously speedup the computation and reduce the storage and memory overhead of CNN models. This method obtains 4-6\u00d7 speedup and 15-20\u00d7 compression with 1% loss of accuracy on ImageNet. With the quantized CNN model, even mobile devices can accurately classify images within 1 second. Souli\u00e9 et al. [31] proposed compressing deep network during the learning phase by adding an extra regularization term and combining product quantization of the network parameters. \n\nDifferent from scalar and vector quantization approaches, fixed-point quantization approaches directly reduce the precision of parameters without codebooks. In [32], Dettmers proposed 8-bit approximation algorithms to make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations, which obtains a speedup of 50\u00d7. In [33], Gupta et al. used only 16-bit wide fixed-point number representation when using stochastic rounding and incur little to no degradation in the classification accuracy.",
            "score": 0.5404257242109699,
            "section_title": "Network quantization",
            "char_start_offset": 12088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "6251653"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "5498724"
                },
                {
                    "start": 1798,
                    "end": 1802,
                    "matchedPaperCorpusId": "15201887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "235434086",
            "title": "MQBench: Towards Reproducible and Deployable Model Quantization Benchmark",
            "text": "Modern deep learning is increasingly consuming larger memory and computation to pursue higher performance. While large-scale models can be trained on the cloud, transition to edge devices during deployment is notoriously hard due to the limited resource budget, including latency, energy and memory consumption. For this reason various techniques have been developed to accelerate the deep learning inference, including model quantization [1,2,3,4,5], pruning [6,7,8,9,10], neural network distillation [11,12], lightweight network design [13], and weight matrix decomposition [14]. \n\nIn this work, we focus on model quantization for efficient inference. Quantization targets to map the (nearly) continuous 32-bit floating-point (FP) numbers into discrete low-bit integers. As a result, the neural networks could rely on the integer-arithmetic units to speed up the inference. In academic research, there is a trend towards steadily reducing the bit-width and maintaining the accuracy across a range of quantized network architectures on ImageNet. It is incredible that the even 3-bit quantization of both weights and activations can reach FP-level accuracy [15]. Exciting though the breakthrough is, there lacks a systematic study that whether these research works can really be applied to practice, and whether the major improvement is brought by the algorithm rather than the training techniques. \n\nWe point out two long-neglected key factors in quantization research, namely reproducibility and deployability. First, we observe that the training hyper-parameters can significantly affect the performance of a quantized network. As an example, Esser et al. [15]   rate [16] and better weight decay choice, improving the Top-1 accuracy of 2-bit ResNet-18 [17] by 0.7% and 0.4% on ImageNet. Full precision network pre-training can also boost quantization results [15,18]. The reproducibility issue has received considerable attention in other areas as well, e.g. NAS-Bench-101 [19]. So far, there lacks a benchmark that unifies training pipelines and compares the quantization algorithms in a thorough and impartial sense.",
            "score": 0.5404049935251879,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 446,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 446,
                    "end": 448,
                    "matchedPaperCorpusId": "15817277"
                },
                {
                    "start": 448,
                    "end": 450,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "5750817"
                },
                {
                    "start": 463,
                    "end": 465,
                    "matchedPaperCorpusId": "7057040"
                },
                {
                    "start": 467,
                    "end": 469,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "11169209"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1659,
                    "end": 1663,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1756,
                    "end": 1760,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1863,
                    "end": 1867,
                    "matchedPaperCorpusId": "67788003"
                },
                {
                    "start": 1867,
                    "end": 1870,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1977,
                    "end": 1981,
                    "matchedPaperCorpusId": "67856022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "227127479",
            "title": "HAWQV3: Dyadic Neural Network Quantization",
            "text": "Models All the empirical results are performed using pretrained models from PyTorchCV (pyt, 2020) library. In particular, we do not make any architectural changes to the models, even though doing so might lead to better accuracy. We consider three NN models, ResNet18, ResNet50, and InceptionV3, trained on the ImageNet dataset (Deng et al., 2009). For all the NNs, we perform BN folding to speed up the inference. All the calculations during inference are performed using dyadic arithmetic (i.e., integer addition, multiplication, and bit shifting), with no floating point or integer division anywhere in the network, including requantization stages. \n\nTraining details We use PyTorch (version 1.6) for quantizing models with HAWQ-V3. For all the quantization results, we follow the standard practice of keeping the first and last layer in 8-bit (note that input data is encoded with 8-bits for the RGB channels, which is quantized with symmetric quantization). We only use uniform quantization along with channel-wise symmetric quantization for weights, and we use layer-wise asymmetric quantization for activations. In order to perform static quantization, we set our momentum factor of quantization range (i.e., minimum and maximum) of activations to be 0.99 during training. Although further hyperparameter tuning may achieve better accuracy, for uniformity, all our experiments are conducted using learning rate 1e-4, weight decay 1e-4, and batch size 128. \n\nDistillation As pointed out previously (Polino et al., 2018), for extra-low bit quantization (in our case uniform 4 bit and mixed 4/8 bit quantization), distillation may alleviate the performance degradation from quantization. Therefore, in addition to our basic results, we also present results with distillation (denoted with HAWQV3+DIST). Among other things, we do confirm the findings of previous work (Polino et al., 2018) that distillation can boost the accuracy of quantized models.",
            "score": 0.5399864368058882,
            "section_title": "H. Implementation Details",
            "char_start_offset": 41760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 347,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "67788003",
            "title": "Learned Step Size Quantization",
            "text": "We trained several networks using LSQ and compare accuracy with other quantized networks and full precision baselines (Table 1). To facilitate comparison, we only consider published models that quantize all convolution and fully connected layer weights and input activations to the specified precision, except for the first and last layers which may use higher precision (as for the LSQ models). \n\nIn some cases, we report slightly higher accuracy on full precision networks than in their original publications, which we attribute to our use of cosine learning rate decay (Loshchilov & Hutter, 2016). \n\nWe found that LSQ achieved a higher top-1 accuracy than all previous reported approaches for 2-, 3and 4-bit networks with the architectures considered here. For nearly all cases, LSQ also achieved the best-to-date top-5 accuracy on these networks, and best-to-date accuracy on 8-bit versions of these networks. In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest. \n\nIt is interesting to note that when comparing a full precision to a 2-bit precision model, top-1 accuracy drops only 2.9 for ResNet-18, but 14.0 for SqueezeNext-23-2x. One interpretation of this is that the SqueezeNext architecture was designed to maximize performance using as few parameters as possible, which may have placed it at a design point extremely sensitive to reductions in precision.",
            "score": 0.5395377158807793,
            "section_title": "COMPARISON WITH OTHER APPROACHES",
            "char_start_offset": 12270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1774
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "247362691",
            "title": "An Empirical Study of Low Precision Quantization for TinyML",
            "text": "Embedding AI directly on edge devices is becoming a key to revolutionize the internet of things (IoT), where billions of tiny devices are leveraged to gain great productivity and efficiency in areas including consumer, medical, automotive and industrial. As a result, tiny machine learning (tinyML) has emerged over the past few years that aims to deploy machine learning algorithms, especially neural networks, on embedded AI processors such as micro-controllers or embedded NPUs (e.g. Qualcomm's eNPU [23] and ARM's Mi-croNPU [1]) with very low power consumption in the level of a few milli-watts. By performing inferences on the devices, tinyML brings the advantage of high energy efficiency, fast responsiveness, high privacy and strong autonomy of edge devices. However, the embedded AI processors are usually highly resource-constrained with limited memory and computation capability. A large body of literature has focused on addressing these issues by making neural networks more efficient, including efficient neural architecture search [4,18,29], neural networks and system co-design [16,17] and model compression such as pruning [10], knowledge distillation [11] and quantization [10,14]. \n\nNeural network quantization is one of the most important model compression techniques. It compresses models by using low-bit precision representation for weight and activation tensors, instead of the 32-bit (or 16-bit for half-precision) precision that is commonly used during model training. The memory consumption and computation of a low precision model are greatly reduced. And quantization compresses a model without changing its architecture. This is particularly useful when the model architecture is already optimally designed for a specific inference system or device. Besides, quantization can be applied along with other model compression techniques, such as pruning and knowledge distillation. \n\nNeural networks have been shown to be quite robust to quantization with 8-bit precision without sacrificing the accuracy [20]. Quantization with lower precision can further reduce the memory consumption and computation. And ultra-low precision (1 or 2-bit) operations can often be computed efficiently with bit-wise arithmetic and thus achieving signification computation acceleration [28].",
            "score": 0.5391201893137267,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "201666112"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "240070553"
                },
                {
                    "start": 1098,
                    "end": 1101,
                    "matchedPaperCorpusId": "220646526"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "2134321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43505859375
        },
        {
            "corpus_id": "273821224",
            "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
            "text": "It is very common to train neural networks using 16-bit floatingpoint formats, such as fp16 or bfloat16, which are supported by most DL accelerators. After training, neural networks can be deployed for inference using even lower-precision formats, including floating-point, fixed-point, and integer representations. Low-precision formats confer several performance advantages. Firstly, many processors are equipped with higher-throughput mathematical pipelines for low-bit formats, thereby accelerating computation-intensive tasks like convolutions and matrix multiplications. Secondly, reduced word sizes reduce memory bandwidth constraints, resulting in improved performance for bandwidth-limited computations. Third, smaller word sizes decrease memory size requirements, which enhances cache utilization and positively impacts various aspects of memory system performance. Utilizing the int8 quantization, this work achieves the ability to sustain model accuracy within 1% of the baseline floating-point networks. This is particularly noteworthy for networks that are typically difficult to quantize, such as MobileNets and BERT-large. Moreover, Vector quantization used to compress the deep convolutional networks [4] is a good work supplements this research filed. In general, a CNN that works well object classification contains eight layers and a huge number of parameters, and it is widely known that the parameters are heavily over-parameterized. The goal of this work is to compress these parameters while maintain the high accuracy. This research mainly focus vector quantization methods for the compression of densely connected layers. It involves parameter binarization, scalar quantization through k-means clustering, and structured quantization employing product quantization or residual quantization, all of which lead to significant improvements in performance. \n\nLanguage model is a branch of machine learning that is designed to understand and generate natural language. Technically, it understands the context of the prompts and generate the missing part with coherent and contextually appropriate language. Language models can be classified into four major types: Statistical Language Models (SLM), Neural Language Models (NLM), Pre-trained Language Models (PLM), and Large Language Models (LLM). Each of these models represents a distinct approach to natural language processing, with varying techniques and capabilities for handling linguistic data [5]- [8].",
            "score": 0.5388563581311948,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2317
                },
                {
                    "start": 2318,
                    "end": 2481
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "4858508",
            "title": "Pieces of Eight: 8-bit Neural Machine Translation",
            "text": "Neural machine translation (NMT) (Bahdanau et al., 2014;Sutskever et al., 2014) has recently achieved remarkable performance improving fluency and adequacy over phrase-based machine translation and is being deployed in commercial settings (Koehn and Knowles, 2017). However, this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see section 3). \n\nNMT models are generally trained using 32-bit floating point values. At training time, multiple sentences can be processed in parallel leveraging graphical processing units (GPUs) to good advantage since the data is processed in batches. This is also true for decoding for non-interactive applications such as bulk document translation. \n\nWhy is fast execution on CPUs important? First, CPUs are cheaper than GPUs. Fast CPU computation will reduce commercial deployment costs. Second, for low-latency applications such as speech-to-speech translation (Neubig et al., 2017a), it is important to translate individual sentences quickly enough so that users can have an application experience that responds seamlessly. Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (Abdelfattah et al., 2016). In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks (Neubig et al., 2017b)). \n\nOthers have successfully used low precision approximations to neural net models. Vanhoucke et al. (2011) explored 8-bit quantization for feedforward neural nets for speech recognition. Devlin (2017) explored 16-bit quantization for machine translation. In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values. Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster. In addition, implementation is straightforward and we can use the models as is without altering training.",
            "score": 0.5385983850993332,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 79,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 239,
                    "end": 264,
                    "matchedPaperCorpusId": "8822680"
                },
                {
                    "start": 1577,
                    "end": 1600,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 1681,
                    "end": 1694,
                    "matchedPaperCorpusId": "29245285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39111328125
        },
        {
            "corpus_id": "244156385",
            "title": "Evaluation of Deep Neural Network Compression Methods for Edge Devices Using Weighted Score-Based Ranking Scheme",
            "text": "Bit reduction techniques have been around for quite a while [11,12]. These techniques aim to reduce the size of the model without a significant loss in the model performance. In practice, this is somewhat difficult to achieve due to the loss of information when approximating the 32-bit full precision weights and activations to a fixed point integer representation [13,14]. Quantization can be implemented using (16,8, or 4 bits); however, there can be extreme cases where 2 bits or 1 bit are used to represent the weights and/or activations. These are referred to as binarization and ternarization. Binary networks encode the weights and activations with 1 bit (-1, 1), in contrast to ternary, which uses 2 bits (-1, 0, 1) [15]. \n\nThe works of [11,12,16,17] showed the possibility of training deep networks with low bit-width weights. The weights and activations of the networks were discretized and a drastic reduction in model size with an insignificant drop in accuracy was achieved in [14]. An adaptive quantization framework that achieved a 20-40% higher compression rate, in contrast to non-adaptive quantization methods that relied on a uniform quantizer, was proposed in [18]. A vector-based quantization method that reduces the reconstruction in the network output was introduced in [19]. The work of [18] also showed that the network layers contribute differently to the model prediction result; therefore, it is logical not to use uniform bit-width quantization. \n\nQuantization techniques have shown promising results for large model compression. This breakthrough has caused different industries developing on-device/edge-based artificial intelligence solutions to adopt the methods. It is worth noting that the lower the bit-width used in quantization, the higher the compression rate and the model sensitivity to accuracy degradation.",
            "score": 0.5371596825241421,
            "section_title": "Bit Reduction",
            "char_start_offset": 8498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "212420024"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "212420024"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44384765625
        },
        {
            "corpus_id": "204402449",
            "title": "Bit Efficient Quantization for Deep Neural Networks",
            "text": "Significant progress has been recently made on algorithms, networks and models to enable low-power edge applications. For example, advances in network architecture search [1], parameter quantization [2], and pruning [3] have afforded models to below 100kB. However, there is still active research to understand the relationships among low bit-precision, data representation, and neural model architecture. \n\nIt is well understood in the deep learning community that to capture a wide spectrum of low, mid and high-level features for deep semantic understanding of complex patterns, DNNs with many layers, nodes, and with high local and global connectivity are needed. The success of recent DNNs (e.g. ResNet, Yolo, MobileNet) in speech, vision, and natural language, comes in part from the ability to train much larger models on much larger dataset [4][5][6]. One fundamental challenge for quantization is to find best assignment of bits for DNN parameter values that have a degree of non-linearity that increases exponentially as dataset size increases. \n\nThrough quantization, parameters for DNN can be converted from 32-bit floating point (FP32) towards 16-bit or 8-bit models, with minimal loss of accuracy. There are even techniques that can quantized down to binary (single bit precision). The main benefit of moving to lower bit-precision is the higher power efficiency and smaller storage needed. Moreover, DNN inference can be processed using more efficient and parallel processor hardware. And as such, quantization makes it possible to run DNN workloads that support object and voice recognition, navigation, and medical image analysis on smartphones and other edge devices. \n\nThrough training, the DNN model is learning a representation that support tasks such as classification. The converged DNN parameter values (e.g. its range and distribution) represent the relationship between filter types and class distribution. Therefore, in order to maintain algorithmic performance after quantization, it is desirable to maintain a \"similar\" range and distribution of learned DNN representations in the quantized parameter values. \n\nIn this paper, we present a comparison of data-free quantization schemes (i.e.",
            "score": 0.536660066536452,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "1856462"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "351666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "52197199",
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
            "text": "To harness the power of deep convolutional networks in embedded and large-scale application domains requires energy-efficient implementation, leading to great interest in low-precision networks suitable for deployment with lowprecision hardware accelerators. Consequently there have been a flurry of methods for quantizing both the weights and activations of these networks (Jacob et al. 2017;Courbariaux, Bengio, and David 2015;Polino, Pascanu, and Alistarh 2018;Xu et al. 2018;Baskin et al. 2018;Mishra et al. 2017;Choi et al. 2018). \n\n\u2022 We demonstrate 8-bit scores on 34,50,and 152, exceeding the full-precision scores after just one epoch of finetuning. \n\n\u2022 We present the first evidence of 4 bit, fully integer networks which match the accuracy of the original fullprecision networks on the ImageNet benchmark. \n\n\u2022 We present empirical evidence for gradient noise that is introduced by weight quantization. This gradient noise increases with decreasing precision and may account for the difficulty in fine-tuning low-precision networks. \n\n\u2022 We demonstrate that reducing noise in the training process through the use of larger batches provides further accuracy improvements. \n\n\u2022 We find direct empirical support that, as with 8-bit quantization, near optimal 4-bit quantized solutions exist close to high-precision solutions, making training from scratch unnecessary. (Polino, Pascanu, and Alistarh 2018), UNIQ (Baskin et al. 2018), IOA (Jacob et al. 2017), Joint Training (Jung et al. 2018). Since we found that only one epoch was necessary to train any of the models we tried with 8-bit quantization, we were able to quickly test new models to include in this table. However, training 4-bit networks, especially the larger models, takes much longer, and we therefore were not able to test as wide a variety of 4-bit networks, which we leave for future work.",
            "score": 0.5365630996582257,
            "section_title": "Introduction Problem Statement",
            "char_start_offset": 33,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 429,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 429,
                    "end": 464,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 517,
                    "end": 533,
                    "matchedPaperCorpusId": "21721698"
                },
                {
                    "start": 1372,
                    "end": 1408,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "253410123",
            "title": "SCA: Search-Based Computing Hardware Architecture with Precision Scalable and Computation Reconfigurable Scheme",
            "text": "Typically, the inference and training of neural network algorithmsemploy floatingpoint precision to ensure a higher level of accuracy. However, high precision computation requires large computing logic resources and more cycles for hardware implementation. To reduce computation and memory resources, an increasing number of designs based on model quantization have been proposed which quantize the model to 8 bits or even lower to 4 bits [35][36][37]. For example, ref. [35] proposed a parametric clipping activation technology PACT, which used the activation clipping parameter \u03b1 optimized during training to find the right quantization scale, with negligible accuracy loss (only 0.3% degradation in ResNet50). \n\nAs shown in Figure 2, through a series of experiments, neural network models were quantized to 4 bits with almost no accuracy loss, where the quantized model greatly reduced the hardware overhead and relieved the bandwidth pressure.",
            "score": 0.5355371832278429,
            "section_title": "Neural Network under Quantization",
            "char_start_offset": 7979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "221103745",
            "title": "Leveraging Automated Mixed-Low-Precision Quantization for tiny edge microcontrollers",
            "text": "Recent hand-crafted model design for image classification task has been driven from efficiency rather than accuracy metrics only. MobileNetV1 [11], MobileNetV2 [19] or ShuffleNet [22] are relevant examples of deep networks that trade-off classification errors with respect to model size and execution latency. More recent Neural Architecture Search (NAS) methodologies automate the model design process, employing Reinforcement Learning (RL) [20] or backpropagation, i.e. \n\nDiffentiable NAS [16]. This work is complementary to this class of studies: besides model compression, low-bitwidth quantization reduces the computational and memory bandwidth requirements concerning the full-precision model, e.g. up to 4\u00d7 smaller and faster in case of 8-bit quantization. Efficient models, such as MobileNetV1, have been turned into an 8-bit integer-only representation with an almost negligible accuracy loss through a quantization-aware retraining process [13], which is currently state-of-the-art for resource-constrained devices. However, 8-bit quantization is not sufficient to fit the tiny memory budgets of MCUs. On the other side, going below 8 bits on both weights and activation values demonstrated to be challenging [23]. Krishnamoorthi et al. [14] employed per-channel quantization rather than per-tensor to recover accuracy degradation when lowering the precision to 4 bits. In the case of extreme single-bit quantization, the accuracy gap concerning full-precision networks is still high, preventing their usage on complex decision problems [12,17]. All these works feature homogeneous quantization, which is sub-optimal if targeting the compression for resource-constrained devices. Motivated by the low on-chip memory budgets of low-cost MCUs systems, the SpArSe framework [9] exploits pruning to fit the RAM and ROM memory budgets for, respectively, intermediate activations values and weights parameters. Rusci et al. [18] addressed the same deployment problem on more complex Imagenet classification models using low-bitwidth mixed-precision quantization.",
            "score": 0.5352103025184414,
            "section_title": "Related Work",
            "char_start_offset": 5437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "51891697"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1551,
                    "end": 1554,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.387451171875
        },
        {
            "corpus_id": "275104652",
            "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
            "text": "In this experiment, we assess the effect of quantization on model precision, specifically focusing on 8-bit, 6-bit, and 4-bit quantization across the model configurations explored in Experiment 1. The analysis is conducted across three datasets (DS1, DS2, and DS3), with DS1 selected as the representative example due to similar trends observed across all datasets. \n\nFigure 11 illustrates the Test MSE distribution of quantized models with various configurations across different quantization bitwidths. The results highlight that bitwidth is the dominant factor influencing the performance of quantized models. Models quantized at 4-bit exhibit significantly higher Test MSE values and a broader distribution than those quantized at 6-bit or 8-bit, indicating a substantial loss introduced by lower bitwidths. As bitwidth increases, the Test MSE distribution narrows, particularly for 8-bit models, which achieve performance levels close to the FP32 benchmark. The neuron count significantly impacts the Test MSE under quantization. Models with a larger neuron count (e.g., 120 neurons) generally demonstrate better performance, with lower median Test MSE values and more centralized distributions. This improvement is particularly evident under 6-bit and 8-bit quantization, showing that larger models are better equipped to absorb quantization noise. However, under 4-bit quantization, the relationship becomes less predictable. While configurations with higher neuron count occasionally achieve better median precision, their overall distribution widens, indicating less stability and a reduced ability to absorb errors introduced by low bitwidth quantization consistently. \n\nThe layer count has a more mixed effect on performance. In deeper models (e.g., six or seven layers), the Test MSE distribution under 4-bit quantization becomes narrower, with lower medians than shallower models (e.g., four or five layers). This suggests that deeper models are better equipped to handle the errors introduced by quantization, leveraging their additional complexity to absorb and mitigate quantization noise. In contrast, shallower models exhibit broader and less consistent Test MSE distributions under lower bitwidths, underscoring their vulnerability to quantization-induced errors.",
            "score": 0.5351835601043213,
            "section_title": "Experiments 2: Quantized Models Analysis",
            "char_start_offset": 36949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2282
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "249431747",
            "title": "8-bit Numerical Formats for Deep Neural Networks",
            "text": "The ongoing trend to use larger deep learning model architectures not only has consequences on the time and cost to train a model, but it also has a significant environmental impact (Strubell et al., 2019). It is critical for the research community to study the implementation of more efficient training schemes to counter the increase in energy consumption with growing model size. \n\nThe results of this study confirm that low precision numerical formats can be a key component of large machine learning models that provide state of the art accuracy while reducing their environmental impact. In particular, by using 8-bit floating point arithmetic the energy efficiency can be increased by up to 4\u00d7 with respect to float-16 arithmetic and up to 16\u00d7 with respect to float-32 arithmetic. \n\nA DYNAMIC RANGE AND SIGNAL-TO-NOISE RATIO OF FIXED-POINT QUANTIZATION For a scaled fixed-point representation with 1 sign bit, n integer bits and quantization step q, the dynamic range is D = 20 log 10 [ 2 n ] \u2248 6.02 n dB. \n\n(1) \n\nThe roundoff error of a uniform quantizer with quantization step q can be modelled by additive noise with uniform distribution over the interval ( \u2212q/2, q/2 ). If the input signal x satisfies Widrow's quantization theorems (i.e., if the input signal characteristic function \u03a6 x (u) \u2248 0 for |u| > 2\u03c0/q) (Widrow et al., 1996), the statistical model of the quantization error \u03bd of uniform quantization corresponds to an independent noise source with zero mean and variance E{\u03bd 2 } = q 2 /12 (Widrow et al., 1996;Widrow and Koll\u00e1r, 2008). When quantizing a standard normal distributed random variable using a fixed-point quantizer with n bits of precision and a quantization interval q, E{\u03bd 2 } can be broken down into a clipping noise part that occurs when values that exceed the dynamic range of the quantizer get clipped and the rounding noise part that occurs within the dynamic range: \n\nwhere f x (x) is the standard normal PDF and m = 2 n \u2212 1.",
            "score": 0.5348107189285685,
            "section_title": "BROADER IMPACT",
            "char_start_offset": 32501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 1323,
                    "end": 1344,
                    "matchedPaperCorpusId": "1619817"
                },
                {
                    "start": 1509,
                    "end": 1530,
                    "matchedPaperCorpusId": "1619817"
                },
                {
                    "start": 1530,
                    "end": 1554,
                    "matchedPaperCorpusId": "60838055"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3486328125
        },
        {
            "corpus_id": "276106922",
            "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
            "text": "The results of our experiments, as summarized in Table 1 and Table 4, highlight the efficacy of QLESS in balancing storage efficiency and downstream model performance across various bit-width configurations. Our analysis focuses on the trade-offs between performance and storage, the generalizability of QLESS across models, and the surprising robustness of extreme quantization. \n\nOverall Performance. Our experiments demonstrate that QLESS achieves competitive performance across all evaluated models and benchmarks while significantly reducing storage requirements compared to LESS. On most benchmarks, QLESS results in comparable or better performance than LESS and the random baselines. Most notably, even with 1-bit quantization, Llama 3.1 8B for example achieves an average score of 65.93, exceeding the random 5% baseline of 64.71, demonstrating the robustness of our approach under extreme quantization. \n\nPerformance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation. \n\nRobustness Across Benchmarks. Across TyDiQA, MMLU, and BBH, QLESS at 8-bit precision frequently matches LESS, and 4-bit or 2-bit configurations remain within a tolerable performance gap. Although BBH can be more sensitive to quantization, 1-bit QLESS still performs competitively, confirming its resilience. These results underscore that even extreme compression maintains sufficient gradient information for robust data selection. \n\nInsights into 1-Bit Quantization. A particularly striking outcome is the effectiveness of 1-bit QLESS, which often attains performance near that of higher-bit variants. This finding challenges the assumption that low-precision representations inevitably undermine gradient-based methods.",
            "score": 0.5347619309258194,
            "section_title": "Main Results",
            "char_start_offset": 13569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "261531260",
            "title": "A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking",
            "text": "In this section, we compare the estimation performances of general-purpose quantization strategies, which have been introduced in Section 3.4 and formalized in Section 2.4. As commonly reported in the reference papers, the different strategies are compared to the same architecture structure; precisely, we report the results obtained over ViT, DeiT, and Swin architectures, respectively, in Table 5, Table 6, and Table 7. \n\nDifferently from previous studies, these tables report the bit-width (#Bit) in which the models are compressed for both weights (W) and activation functions (A), as well as the size (Size) of the model after the quantization. This choice is due to the fundamental effect of the Q efficient strategy, which tries to preserve the estimation performances of originally trained models with a lower data precision, i.e., compressing their data from 32-bit to 8/6/4-bit. As a result, after the Q methodology, the number of trainable parameters (#Pram.) will remain constant, but the data precision of each neuron, and hence the total size of the model, will be reduced. The first performed analysis regards the optimal bitwidth in order to generate efficient and accurate models. Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size.",
            "score": 0.5347225025726516,
            "section_title": "Results of Quantization strategies",
            "char_start_offset": 67069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1800
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "269930213",
            "title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression",
            "text": "Comparison with Other Quantization Methods.The results on LAMBADA are shown in Table 2. Compared with FP16, all quantization methods reduce the sizes of the KV cache significantly due to low bit-precisions.Overall, we observe that De-coQuant achieves better average scores than other methods.We note that RTN sometimes gives better results (LLaMA-13B), but this performance is not stable, and in other cases, it is not good.We suspect that it is related to the distribution of outliers in the model, an observation that is very similar to (Dettmers et al., 2022), which mentions that there is a clear difference in the distribution of outliers for large models.When comparing different quantization settings, we find that 4-bit quantization often exhibits close performance to 16-bit performance while 2-bit models get much worse.Interestingly, even in 2-bit quantization, DecoQuant still has a significant advantage over other methods, an observation that opens up the possibility of a 2-bit KV cache in the future, an exploration we leave to be completed in subsequent work.\n\nEvaluation on Long-text Tasks.We evaluate DecoQuant's in-context learning capabilities using OPT models on five distinct datasets.For each dataset, we conduct experiments with varying numbers of demonstrations to investigate the impact of KV cache quantization on the contextual length.The summarized results are presented in Table 3.Our findings indicate that a larger number of demonstrations often results in performance improvements, as evidenced by the performance comparison, e.g., 72.8 compared to 66.8 for FP16.This observation underscores the effectiveness of augmenting the contextual information.However, when comparing the performance of RTN and De-coQuant, we observe that, on average, RTN lags  behind DecoQuant.An interesting aspect of this comparison is that RTN's performance is comparable to DecoQuant's in the case of shorter contexts (2-shot), but it notably deteriorates for longer contexts (10-shot).This outcome reinforces the efficacy of our approach, which effectively compresses the prompt while preserving critical information.",
            "score": 0.5342129320635511,
            "section_title": "Main Results",
            "char_start_offset": 17980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 43,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 1076
                },
                {
                    "start": 1078,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 2000
                },
                {
                    "start": 2000,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 562,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466796875
        },
        {
            "corpus_id": "273822014",
            "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
            "text": "We evaluate using the highly-popular Llama 3.1 model series (Dubey et al., 2024). To assess quantization trade-offs, we conduct experiments on the instruction-tuned versions of all available sizes (8B, 70B, and 405B). For each, we examine the three main formats with kernel support in vLLM: W8A8-FP, W8A8-INT, and W4A16-INT. W8A8-FP quantizes all linear operators in transformer blocks to an 8-bit floating-point format, using round-to-nearest quantization. Weights follow a symmetric per-output-channel scheme, while activations are dynamically quantized per token. This requires no calibration data and remains computationally efficient, even for large-scale models. W8A8-INT reduces weights and activations to 8-bit integers, applying symmetric per-outputchannel GPTQ quantization for weights and dynamic per-token quantization for activations. While this scheme performs well for 8B and 405B models, it causes noticeable accuracy drops at 70B. To mitigate this, we apply SmoothQuant, shifting some activation complexity onto weights, which are easier to quantize. For calibration, random tokens suffice at 8B, but larger models require higher-quality calibration data, for which we use Lee et al. (2023). W4A16-INT quantizes weights to 4-bit integers while keeping activations at 16-bit precision. Weights are compressed using GPTQ with MSEoptimal clipping, applied in 128-element groups. Unlike higher-bit formats, random token calibration degrades accuracy, so we rely on OpenPlatypus data for calibration. INT4 Quantization Algorithms. We focus on two inference-efficient techniques: AWQ and GPTQ, evaluating them on Leaderboard V1/V2, Arena-Hard, HumanEval, and MBPP. Results (Table 1) show near-identical performance on academic benchmarks, with AWQ leading by just 0.23 and 0.35 points on a 0-100 scale.",
            "score": 0.5340186044756146,
            "section_title": "Models, Formats, and Algorithms",
            "char_start_offset": 11806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 1190,
                    "end": 1207,
                    "matchedPaperCorpusId": "272694046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "257312686",
            "title": "A privacy protection approach in edge-computing based on maximized dnn partition strategy with energy saving",
            "text": "Quantization can bring benefits like reducing model size, accelerating inference and saving energy, but it costs model accuracy [22]. In general, 8-bit quantization only leads to insignificant accuracy loss, but lower precision quantization need finetune to help model recovering from quantization loss. Thus during the search stage, we first finetune the model for one epoch on training set, then evaluate its quantization loss. We think this one turn finetune is necessary for model evaluation because the serve loss introduced by very-low bit quantization will make post training quantization models' accuracy too low to assess. One turn finetune will help a great deal for accuracy recovery without introducing too much cost like fully finetuning, and we notice that this method has been adopt in many NAS-like works [12]. Once the model is quantized and finetuned for one epoch on training set, we evaluate the quantization loss through model accuracy loss directly shown as Eq. 10, where accur origin denotes the model top1 classification accuracy, accur quant denotes the quantized model accuracy after one epoch",
            "score": 0.5334102927503633,
            "section_title": "Quantization loss evaluation",
            "char_start_offset": 26410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1119
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448974609375
        },
        {
            "corpus_id": "249431747",
            "title": "8-bit Numerical Formats for Deep Neural Networks",
            "text": "The performance of 8-bit floating-point formats has also been assessed for Natural Language Processing (NLP) applications. The more recent advanced NLP models are based on the Transformer (Vaswani et al., 2017) and in particular its attention concept (Bahdanau et al., 2014). Based on the model published by Kuchaiev et al. (2018), a version of the Transformer base model has been implemented with quantized activations, weights, and gradients in all fully connected layers and matrix multiplications. The inputs to the layer normalization and softmax layers are left unquantized. The model performance for language translation has been evaluated on an English-German translation task using the WMT14 dataset. For each experiment, the model has been trained for 400,000 iterations with a batch size of 128 sentence pairs, using Adam optimization (Kingma and Ba, 2014) with a base learning rate \u03b7 = 2/ \u221a 512, optimizer parameters \u03b2 1 = 0.9, \u03b2 2 = 0.997 and = 10 \u22129 , and a learning rate schedule with linear increase of the learning rate during 8,000 warm-up steps and learning rate decay proportional to 1/ \u221a step + 1 afterwards (Kuchaiev et al., 2018). To reproduce the performance of Transformer models reported in the literature (Vaswani et al., 2017), in Section 4 we also show the performance results of both the float-32 and float-8 models trained for 100,000 iterations with a batch size of 1024 sentence pairs. Similar to Vaswani et al. (2017) for these runs we used checkpoint-averaging to obtain the final weights used for testing. Figure 5(a) suggests a small degradation in the model accuracy when quantizing the activations of the Transformer using the 1.5.2 format. This is an effect of the limited SNR of the 1.5.2 format, since the 1.4.3 format with its higher SNR yields an accuracy on par with the float-32 baseline for a range of different exponent biases (Figure 5(b)). The latter format is therefore preferred for quantizing the Transformer activations.",
            "score": 0.533106923737567,
            "section_title": "RESULTS FOR LANGUAGE PROCESSING",
            "char_start_offset": 20641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1974
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "221103745",
            "title": "Leveraging Automated Mixed-Low-Precision Quantization for tiny edge microcontrollers",
            "text": "Top1 % (Full Precision) Fig. 1. The histogram (left axis) shows the required model compression of popular state-of-the-art CNNs for fitting in 2MB. The red line (right axis) displays the Top1% on Imagenet task for the same models [3]. \n\na tightly bound on-chip memory budget (mostly for cost reasons), i.e. typically not more than a few MB of internal flash storage and 1MB of RAM. Such a memory bottleneck stands as the major limitation for bringing the state-of-theart inference Deep Learning (DL) models on these devices [1]. For instance, the MobileNetV1 [11] model features up to 4.24 M parameters, resulting in a 16 MB of weight storage (32-bit floating point, i.e. FP32, format), which is much higher than the typical size of on-chip memories. \n\nFigure 1 plots the required compression ratio to be applied to several Imagenet classification models [3] for fitting into a memory budget of 2 MB. To reach the goal, a >100\u00d7 compression factor is required for highly accurate models, e.g. InceptionV4 and ResNet-152, while optimized topologies, e.g. Mobilenets, demand a \u223c10\u00d7 compression. Typically, quantization is used to shrink a DL model at the cost of an accuracy penalty with respect to the full-precision counterpart. Recent works demonstrated that 8-bit quantization applies almost losslessly but leads only to a 4\u00d7 compression over full-precision models (FP32). Therefore, to meet such requirement, a sub-byte quantization, i.e. using less than 8-bit, must be applied at the cost of a (potential) non-negligible accuracy loss [5,7]. To reduce the accuracy degradation when applying aggressive quantization, Mixed-Low-Precision quantization techniques have been introduced [18,21]. Differently from homogeneous quantization, which relies on a network-wise Q-bit compression, a mixed-precision scheme defines an individual bitwidth for every weights and activation tensors of a deep model [18], namely the quantization policy.",
            "score": 0.5330653968967922,
            "section_title": "Required Compression",
            "char_start_offset": 497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 1688,
                    "end": 1691,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "207870482",
            "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers",
            "text": "Using reduced precision for deep learning has been an active research topic. Prior efforts explore floatingpoints(e.g., 8-bit and 16-bit) for training [34,22] and maintain accuracy on a spectrum of deep learning models and datasets. However, as floating-point is more resourceintensive than fixed-point, the deployments always rely on quantization techniques. \n\nA branch of work explores the fixed-point for forward prorogation(FPROP) [16,17,6,33,35,35,37]. The weights and activations are quantified to 1-8 bits. However, the backward-pass, including gradient propagation (BPROP), and weight gradient computation (WTGRAD) still require float32. \n\nThere are recent attempts quantifying weight and activation on different layers with different bit-widths. For the inference of a trained network, there are some techniques that heuristically search the space of quantization bit-width combinations [35,33,37]. However, these inference techniques only need to consider single iteration, whose search space is much smaller than training. Hence, they are unsuitable for training. For training, some differentiable quantization methods [4,30,37] learn the quantization parameters (e.g., step size, dynamic range and bit-width) with gradient descent. However, the quantization parameters for backward propagation are hard to learn using differentiable    methods. [26] quantifies the backward propagation. Different from their method, which assigns layer-wise bit-width before training, our approach dynamically changes the bitwidth during training and we evaluate on widely used networks. \n\nResearchers have shown that 16-bit is sufficient for back propagation in most vision training tasks [7]. However, further quantization to 8-bit results in severe degradation [38,38,36,7,1]. WAGE [36] claims that first and last layers require higher precision. TBP [1] shows weight gradient computation (WTGRAD) needs more bits than gradient back propagation (BPROP). \n\nOur approach is different from others in three aspects. First, fixed-point is used in both forward-pass and backward-pass for training.",
            "score": 0.5330614076898849,
            "section_title": "Related Works",
            "char_start_offset": 4755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 444,
                    "end": 447,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1130,
                    "end": 1133,
                    "matchedPaperCorpusId": "57301568"
                },
                {
                    "start": 1766,
                    "end": 1769,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1771,
                    "end": 1773,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1849,
                    "end": 1852,
                    "matchedPaperCorpusId": "44071489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "276575417",
            "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
            "text": "In this section we discuss obtained critical insights into the effects of quantization bit-range, methods, safety benchmarks, and model architectures on the safety and trustworthiness of language models. Below, we summarize the key implications: \n\n6.1 Quantization Bit-Range \u2022 Int4 Precision: At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral. \n\n\u2022 Int2 Precision: Reducing precision to 2 bits introduces more pronounced performance declines, particularly for LLaMA. Vector quantization methods like AQLM demonstrate greater stability compared to scalar methods like QUIP#, which struggles with maintaining safety and factual accuracy.",
            "score": 0.5330188078774735,
            "section_title": "Discussion",
            "char_start_offset": 19046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 899
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "49356451",
            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
            "text": "We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. \nQuantization-aware training can provide further improvements, reducing the gap to floating point to 1% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.",
            "score": 0.5329473343539133,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "1193239",
            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
            "text": "We now empirically validate our approach on data-parallel GPU training of deep neural networks. \n\nOn ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]. QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. We note that we did not tune bucket size, number of bits used, number of epochs or learning rate for this experiment. \n\nOn AN4 using LSTMs, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. It is able to converge 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by less than 10%). \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if not trained further. However, increasing precision to 4-bit or 8-bit recovers accuracy. This finding suggests that modern architectures for vision tasks, such as ResNet or Inception, which are almost entirely convolutional, may benefit less from quantization than recurrent deep networks such as LSTMs.",
            "score": 0.5329459748696229,
            "section_title": "E Experiments",
            "char_start_offset": 51010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1839
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "229339862",
            "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead",
            "text": "In the following, we will provide an overview of hardware-friendly quantization methods, distinguishing between linear and non-linear methods (see Figure 45). Linear Quantization: It is characterized by evenly-spaced quantization intervals, as shown in Figure 45.a. An example of linear quantization is the above-discussed fixed-point coding, which has been widely studied and applied to NNs because its hardware implementation is well known. Moreover, most CPUs support fixed-point arithmetic on 8, 16, or 32 bits. Given the strong diffusion that the quantization of NNs is having, the Nvidia Tesla GPU supports 8-bit fixed-point operations, and so do the Tensor Processing Units (TPUs) [122] used in Google datacenters. \n\nIt has been demonstrated by several works that both the weights and activations can be quantized to 8-bit dynamic fixed-point numbers for inference without significantly affecting the accuracy [173] [169]. The Ristretto framework [173] identifies the quantization parameters (bitwidth and scale factor) by running a statistical analysis on the weights and activations. The weights are furtherly fine-tuned with a re-training step. With the Ristretto framework, complex models such as AlexNet [40], SqueezeNet [174] or GoogleNet [43] are inferred on 8 bits with less than 1% accuracy loss. In [170], an NN for speech recognition is implemented on 8-bit fixed-point numbers exploiting the Intel SSSE3 instruction set for SIMD execution. A speed-up of 7.6x is achieved compared to the floating-point baseline. \n\nGiven the great diversity between the various layers of an NN, it may be useful to use a different precision across the model, i.e., a variable bitwidth depending on the layer. Works in [117] and [175] show that the bitwidth can be set depending on the position in the model, making a per-layer optimization of the number of bits of weights and activations.",
            "score": 0.5324146676025476,
            "section_title": "F. FULL PRECISION VS QUANTIZED IMPLEMENTATIONS",
            "char_start_offset": 71793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 693,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 917,
                    "end": 922,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 923,
                    "end": 928,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 954,
                    "end": 959,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 1233,
                    "end": 1238,
                    "matchedPaperCorpusId": "14136028"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 1316,
                    "end": 1321,
                    "matchedPaperCorpusId": "15196840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "263138528",
            "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
            "text": "When paired with the modern OPTQ quantizer (Frantar et al., 2023), ModuLoRA enables finetuning 3-bit LLMs for the first time, often outperforming methods based on less sophisticated 4-bit and 8-bit quantization. When paired with the state-of-the-art QuIP# quantizer Chee et al. (2023); Tseng et al. (2023), ModuLoRA enables finetuning 2-bit LLMs for the first time, matching methods' performance on less sophisticated 4-bit and 8-bit quantization method. Across tasks in classification, natural language inference, and instruction following, our low-precision models achieve competitive performance using significantly less memory than existing approaches. On a popular summarization benchmark, we attain a new state-of-theart ROUGE score using a quantized LLaMA-65B model. We open-source all our low-precision models, including the first 3-bit family of Alpaca models that feature strong instruction-following performance at multiple model sizes. Our findings reveal that high performance can be achieved using smaller quantized LLMs than previously thought. \n\nContributions. In summary, this paper makes the following contributions: (1) we propose ModuLoRA, a memory-efficient finetuning method that operates over low-precision weights obtained via a user-specified black-box quantization module; (2) we release LLMTools, a user-friendly Python library that features an implementation of ModuLoRA and that enables users to easily finetune the largest LLMs on consumer GPUs; (3) we provide empirical evidence that high performance on downstream tasks can be achieved with a smaller LLM than previously thought.",
            "score": 0.5323192389704204,
            "section_title": "Introduction",
            "char_start_offset": 1542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1611
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 65,
                    "matchedPaperCorpusId": "259298689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "231861390",
            "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
            "text": "We conduct experiments on a variety of modern deep learning architectures, including ResNet (He et al., 2016) with normal convolution, MobileNetV2 (Sandler et al., 2018) with depthwise separa- ble convolution and RegNet (Radosavovic et al., 2020) with group convolution. Last but not least important, we also investigate the neural architecture searched (NAS) models, MNasNet (Tan et al., 2019). In Table 2, we only quantize weights into low-bit integers and keep activations full precision. We compare with strong baselines including Bias Correction, optimal MSE, AdaRound, AdaQuant, and Bit-split. Note that the first and the last layer are kept with 8-bit. While most of the existing methods have good performances in 4-bit quantization, they cannot successfully quantize the model into 2-bit. Our method consistently achieves the lowest accuracy degradation for ResNets (within 5%) and other compact models. We further quantize activations into 4-bit to make the quantized model run on integer-arithmetic hardware platforms. We find that 4-bit activation quantization can have a huge impact on RegNet and MobileNet. Nonetheless, our methods produce higher performance than other state-of-the-arts. To be noted, BRECQ is the first to promote the 2W4A accuracy of PTQ to a usable level while all other existing methods crashed. In this section, we compare our algorithm (post-training quantization) with some quantization-aware training methods, including PACT (Choi et al., 2018), DSQ (Gong et al., 2019), LSQ (Esser et al., 2020), and a mixed precision technique HAQ (Wang et al., 2019). Table 4 shows that although BRECQ is a PTQ method with limited available data, it can achieve comparable accuracy results with existing quantization-aware training models. In addition, our method can surpass them in 4bit MobileNetV2 while using less than one training GPU hours. Our method also has comparable accuracy with HAQ, which is a training-based mixed precision method.",
            "score": 0.5319170119570851,
            "section_title": "IMAGENET",
            "char_start_offset": 20022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 147,
                    "end": 169,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 220,
                    "end": 246,
                    "matchedPaperCorpusId": "214714446"
                },
                {
                    "start": 1571,
                    "end": 1590,
                    "matchedPaperCorpusId": "210848001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414794921875
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "is the added engineering complexity introduced by quantization, e.g. some quantization techniques add hyperparameters for clipping bounds, which would require tuning as well. This motivates us to focus on approaches that offer clear benefits while minimizing the amount of added complexity.\n\nIn this work, using ResNet [13] as an example, we seek to understand how different quantization precisions affect the compute cost-accuracy tradeoff curves, and find a simple strategy to compress models at different compute cost and quality requirements.\n\nAfter running experiments on ResNet using different precisions and numbers of parameters, we determined that 4-bit and 8-bit models strongly Pareto-dominate bfloat16 models, and mostly-4-bit models outperform 8-bit models. We present our results using two compute cost models (linear and quadratic), based on different assumptions about the compute speedups when the number of bits are reduced. Both cost models will be defined and justified in Section 4. While no hardware with the quadratic cost model is available yet to our knowledge, our results provide strong motivation for the development of such hardware.",
            "score": 0.5317361181319763,
            "section_title": "Introduction",
            "char_start_offset": 1964,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "258990120",
            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
            "text": "Since quantized models are not available in the case of the N-Caltech101 dataset, the models reported in this work are compared with other full-precision models in the literature. While the accuracy of our full-precision model is highest, our binarized model has slightly lower accuracy than the full-precision model in [31]. However, there is a substantial difference in memory size between our binarized model and other full-precision models while having comparable accuracy. B. Accuracy -bit-precision trade-off Fig. 2 illustrates the final test accuracy obtained for a particular combination of bit-precision value and dataset. The blue bar corresponds to the non-quantized, full-precision model and serves as a baseline for quantized models to be compared. Overall, the proposed quantization method provides similar performance for different classification tasks. Quantization has more impact on the SNN performance for more complex tasks, such as CIFAR10-DVS and N-Caltech101.\n\nQuantization of the SNN model for the CIFAR10-DVS dataset yields the highest degradation in accuracy compared to other datasets. While the accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively). Hence, for this dataset, the model can be quantized down to 1-bit with an accuracy degradation of around 1%.\n\nFor the N-Caltech101 dataset, on the other hand, the binarized SNN model has a more",
            "score": 0.5315229985658378,
            "section_title": "4) N-Caltech101:",
            "char_start_offset": 19203,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "251648053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "273811224",
            "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
            "text": "Setups. The language-only models we selected include Llama2-7B and 13B (Touvron et al., 2023b), Falcon-7B (Penedo et al., 2023), and Mistral-7B-v0.1 (Jiang et al., 2023). These models were quantized using the first sample of the RedPajama dataset. To evaluate Perplexity (PPL) and Accuracy (Acc.), we utilize the WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2020) validation sets. All experiments were conducted on an NVIDIA A100 (80G) GPU. Baselines. Our primary baselines include AWQ (Lin et al., 2024) and GPTQ (Frantar et al., 2022a), while we benchmark our approach against the stateof-the-arts SPQR (Dettmers et al., 2023). To ensure that the average bit-width of the model remains below 4 bits-surpassing GPTQ and AWQ in compression-we quantize the non-outlier weights at 4-bit and 3-bit precision. These configurations are denoted GWQ-O for 4-bit quantization, GWQ-R for 3-bit quantization.",
            "score": 0.5310227954404275,
            "section_title": "Experiments Setting",
            "char_start_offset": 12024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 908
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 373,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 496,
                    "end": 514,
                    "matchedPaperCorpusId": "258999941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27197265625
        },
        {
            "corpus_id": "220936526",
            "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands",
            "text": "Use of deeper and wider convolutional neural networks (CNNs) has led to impressive predictive performance in many machine learning applications, such as image classification, object detection, semantic segmentation, etc. However, the large model size and associated computational inefficiency of these deep neural networks often make it impossible to run many realtime machine learning tasks on resource-constrained mobile and embedded devices, such as smartphones, AR/VR devices, etc. One particularly effective approach has been the use of model quantization to enable this size and computation compression of CNN models. Quantization of model parameters to sub-byte values (i.e. numerical precision of \u2264 8 bits), especially to 4-bits has shown minimal loss in predictive performance across a range of representative networks and datasets in recent works. As a result, some heavily quantized machine learning models may use kernel weights which have fewer bits than the corresponding activations which they are to be multiplied with. For example, there is an increasing interest in using 4-bit weights and 8-bit activations, which means that matrix multiplications between 4-bit weights and 8-bit activations are likely to become a fundamental kernel of many important workloads including neural networks and machine learning, although such multiplications may also be useful for other purposes. This is evident by the increasing interest and successful development of a large number of novel machine learning and linear algebra techniques [1], [2], [10], [11], [18] to preserve the predictive performance of deep neural networks with 4-bit weights and 8-bit activations in recent years. \n\nHowever, in 4-bit-weight networks, the weights are encoded by 4 bits, while the activation matrices are represented by more bits (e.g., 8 bits in this example, although other examples could have larger activations). This creates a read width imbalance between 4-bit weights, 8-bit activations and outputs (accumulators) compared to previous technology. Ideally, we would like to sustain matched vector width of read and write operands while exploiting 4-bit weights for the best performance. In other words, we would like to utilize the full bandwidth of read and write ports while exploiting 4-bit weights for the best performance.",
            "score": 0.5300936497116204,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 1542,
                    "end": 1545,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 1552,
                    "end": 1556,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "198903430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "237421361",
            "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference",
            "text": "DNN algorithmic resilience allows for parameters to be represented in lower bit precision without accuracy loss. Fixed-point or integer quantization techniques, commonly adopted in CNN models, suffer from limited range and may be inadequate for NLP models, whose weights can be more than an order of magnitude larger [72]. This phenomenon is owed to layer normalization [7], which is commonly adopted in NLP models and has invariance properties that do not reparameterize the network -unlike batch normalization [30], which produces a weight normalization side effect in CNNs. \n\nIn this work, we employ floating-point based quantization, which provides 2\u00d7 higher dynamic range compared to integer datatypes [32]. Both weights and activations are quantized across ALBERT layers to 8-bit precision. \n\nWe also performed a search on the optimal exponent bit width to satisfy the dynamic range requirements of the ALBERT model. Setting the floating-point exponent space to 4 bits within the 8-bit word size, with the exponent being scaled at a per-layer granularity, provided the best accuracy performance across NLP tasks.",
            "score": 0.5299537346744314,
            "section_title": "Floating-Point Quantization",
            "char_start_offset": 15903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "203593200"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "8236317"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "5808102"
                },
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "53220634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "276902966",
            "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
            "text": "In this section, we report and discuss the results of our study to address the research questions formulated in Section 3. When reporting the results of the statistical tests, we adopt the notation in Table 3 for the significance level. 4.1 How does low-bit quantization affect the model's code generation ability? \n\nFig. 1 provides a handy graphical representation of the memory saving (xaxis) versus the relative performance loss in terms of pass@1 score (y-axis) when comparing models quantized at different bit precisions (8, 4, 3, and 2 bits per parameter) and the baseline models using fp16 precision. The blue lines represent CodeLlama 7B while the orange lines DeepSeek-Coder 7B. The left charts relate to the Python benchmarks (from MultiPL-E and McEval) while the right one to the Java benchmarks. Table 4 reports the detailed numbers on top of which Fig. 1 has been built: For each LLM subject of RQ 1 (i.e., CodeLlama and DeepSeek-Coder 7B) we show, both for the baseline fp16 precision model as well as for all its quantized versions, (i) their memory footprint in terms of GB, (ii) the pass@1 score they achieved on both the Python and the Java benchmarks; and (iii) the results of the statistical tests (i.e., adjusted p-value and OR), in which we compare each quantized model against the baseline. For example, looking at CodeLlama 7B, the fp16-precision baseline requires 13.48GB of memory to achieve a 29.8% of pass@1 score on the Python MultiPL-E benchmark, while its 4-bit quantized version only uses 4.00GB of memory with a pass@1 of 29.1%. This difference in performance is not statistically significant. When mapping this back to Fig. 1 (a) (blue line in the left chart), we can indeed see that the 4-bit quantized model, when compared to the fp16-precision baseline, allows achieving a 70% of memory reduction (i.e., 13.48\u22124.00 13.48",
            "score": 0.5295607187647628,
            "section_title": "Study Results",
            "char_start_offset": 36487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "272915293",
            "title": "QAttn: Efficient GPU Kernels for mixed-precision Vision Transformers",
            "text": "Recent advancements in Foundation Models (FM) [3], both in Natural Language Processing (NLP) [38,42,45] and Computer Vision (CV) [17,27,40], have extended the predictive performance of deep learning models. Nevertheless, these advances come at a cost in terms of computational requirements and memory resources. Currently, the base-line reference for FM is a transformer architecture enhanced with an attention mechanism [48]. Initially designed for NLP, transformers have been adapted for CV, resulting in the development of Vision Transformers (ViT) [17]. ViTs are encoder-only models that are typically self-supervised, pre-trained on a large amount of data, and later adapted for downstream tasks such as image classification, object detection, or instance segmentation. Similar to large language models [45], ViTs come in different sizes, depending on the number of layers and, as a result, parameters, which vary from millions to 22 billion [12]. As a result, the largest model requires a dedicated accelerator with sufficient memory to process the data. The large size of ViTs makes them appropriate candidates for compression methods such as quantization, but outliers in intermediate activations pose a challenge [4,11]. Quantization, a compression technique that reduces the number of bits, converts computation and data from \"continuous\" (floating point) to discrete (integer). Integer 8-bit (INT8) inference is faster and more energyefficient than its floating-point counterparts, but the limited range in which we can represent values makes it susceptible to quantization errors during computation that may affect the final accuracy of the deep learning model [20,30]. \n\nModern hardware includes dedicated units that support efficient matrix multiplication in 8-bit and 16-bit floating point (FP8 and {FP16; BFloat16}, resp.) as well as INT8 [6,28,46]. To take full advantage of these computing units, users must use a specialized hardware runtime application programming interface (API) such as CUDA [31], which requires significant programming skills to take full advantage of the graphics proccessing unit's (GPU) capabilities.",
            "score": 0.5293919989838008,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "265033915",
            "title": "M4BRAM: Mixed-Precision Matrix-Matrix Multiplication in FPGA Block RAMs",
            "text": "Accuracy, performance vs. activation quantization: Since the computation latency of M4BRAM scales linearly with the activation precision, we first evaluate the model accuracy and performance by sweeping the activation precision from 4 to 8 bits while keeping the weight precision at 8 bits. We choose the GX650 FPGA in this experiment. For Hetero-DLA, we consider three M4BRAM configurations -M4BRAM-S with double-pumped BPE (DP-M4S), and M4BRAM-L with synchronous (SY-M4L) or double-pumped BPE (DP-M4L). \n\nFig. 9 presents the Top-1 model accuracy vs. performance trade-off for various DNNs. For each DNN, we observe a drop in speedup when the activation becomes 5 bits, which is due to a 2\u00d7 increase in the DSP-packing factor as described in Section III-A. However, 5-bit activation causes a non-negligible accuracy loss of > 0.5% for all three evaluated DNNs, with a maximum of 0.94% for ResNet-18. This highlights that solely relying on DSP-packing cannot provide an optimal accuracy vs. performance trade-off for mixed-precision DNNs. \n\nWhen the activation precision is 6-bit, the three M4BRAM configurations deliver an average speedup of 2.16\u00d7 across all evaluated DNNs, while incurring an accuracy loss of < 0.5%. Specifically, the average speedups achieved by DP-  When considering cases where the activation precision changes from 8 to 6 bits, the performance of SY-M4L linearly increases, while the double-pumping feature of DP-M4S leads to an incremental speedup for every 2-bit reduction in the activation precision. Interestingly, DP-M4L shows nearly identical speedup. We attribute this to the much higher MAC throughput of DP-M4L compared to DSP, which causes the tile latency to be bottlenecked by the latter.",
            "score": 0.5293068895834148,
            "section_title": "D. Mixed-Precision Accuracy vs. Performance Trade-off",
            "char_start_offset": 27778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "273661854",
            "title": "Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments",
            "text": "This approach takes advantage of the observation that many tokens, especially in common phrases or predictable contexts, can be accurately predicted by simpler models, thus offloading a significant portion of the computational burden. \n\nRecent advancements in speculative decoding have further refined the technique by improving speculative model (Kwon et al., 2023;Cai et al., 2024) and better feature uncertainty (Li et al., 2024). In this paper, we focus on speculative decoding methods through the vLLM framework (Kwon et al., 2023). \n\nQuantization. Quantization is widely-used in deep learning to reduce the computational and memory demands of large models by lowering the precision of the model's parameters and activations. Instead of using 32-bit or 16-bit floating-point numbers (fp32 or fp16), quantization converts these values to lower-precision formats such as 8-bit integers (int8) or even smaller, which drastically reduces the memory footprint and computational cost (Kwon et al., 2023). The primary motivation for quantization is that many neural network operations can still achieve competitive performance using lower precision, without significant degradation in model accuracy. This has made quantization an attractive method for deploying large models like LLMs and image generation systems in resource-constrained environments such as mobile devices or edge computing platforms (Dettmers et al., 2022;Rajbhandari et al., 2020). \n\nIn this paper, we explore the tradeoff in quantization between model efficiency and accuracy.",
            "score": 0.5291986818512092,
            "section_title": "LANGUAGE MODELS",
            "char_start_offset": 11010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1546
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 517,
                    "end": 536,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 983,
                    "end": 1002,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 1401,
                    "end": 1424,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1424,
                    "end": 1449,
                    "matchedPaperCorpusId": "203736482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462158203125
        },
        {
            "corpus_id": "273962606",
            "title": "Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques",
            "text": "The emergence of Large Language Models (LLMs) has revolutionized the field of natural language processing (NLP), enabling significant advancements in tasks such as machine translation, sentiment analysis, question answering, and conversational agents. Models like GPT-3, with 175 billion parameters, and PaLM, boasting 540 billion parameters, have demonstrated unprecedented capabilities in understanding and generating human-like text. These models leverage vast amounts of data and intricate architectures to achieve high performance, often surpassing previous benchmarks and setting new standards in the industry. However, the impressive capabilities of LLMs come at a substantial computational and financial cost. Training such models requires extensive computational resources, including powerful GPUs or TPUs, vast memory, and significant energy consumption. Moreover, the inference phase-where the model is deployed to perform tasks-demands considerable computational power and memory bandwidth, which can limit the feasibility of deploying LLMs on devices with constrained resources [1], such as mobile phones, Internet of Things (IoT) devices, and edge computing platforms. This limitation poses a significant barrier to the widespread adoption and accessibility of LLMs, particularly in applications where low latency and high efficiency are critical. \n\nQuantization in neural networks offers a promising solution to these challenges by reducing the precision of model parameters and activations. Typically, neural network weights and activations are represented using 32-bit floating-point (FP32) numbers, which provide high precision but consume substantial memory and computational resources. Quantization techniques convert these high-precision values to lower-bit representations, such as 8-bit integers (INT8), 4-bit integers (INT4), or even binary representations [2]. This reduction not only decreases the memory footprint of the model but also accelerates computations by leveraging hardware optimizations designed for lower precision arithmetic. In the context of LLMs, which often consist of billions of parameters, quantization becomes particularly beneficial. By reducing the model size, quantization facilitates the deployment of LLMs on devices with limited computational capabilities and power budgets. Additionally, lower-precision computations can significantly speed up inference times, enabling real-time applications and reducing operational costs. \n\nDespite these advantages, quantization presents challenges, including potential degradation in model accuracy and the complexity of implementing effective quantization strategies.",
            "score": 0.5284114429991941,
            "section_title": "I. INTRODUCTION A. Background and Motivation",
            "char_start_offset": 47,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2479
                },
                {
                    "start": 2482,
                    "end": 2661
                }
            ],
            "ref_mentions": [
                {
                    "start": 1091,
                    "end": 1094,
                    "matchedPaperCorpusId": "272362488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "263908852",
            "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
            "text": "Models and datasets. We apply QLLM to quantize the LLaMA-1 (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) families. To evaluate the performance of the quantized LLM, we report the zero-shot accuracy on various benchmarks, including PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), and WinoGrande (Sakaguchi et al., 2021). Additionally, we evaluate the perplexity, a key indicator of a model's generative performance that correlates significantly with zero-shot outcomes, on WikiText2 (Merity et al., 2017), PTB (Marcus et al., 1993) and C4 (Raffel et al., 2020). \n\nQuantization settings. In alignment with prior research (Dettmers et al., 2022;Shao et al., 2023), we use per-channel weight quantization and per-token activation quantization. Following (Shao et al., 2023;Liu et al., 2023), we quantize all weights and intermediate activations, with the exception of the Softmax output probability, which is maintained at full precision. Following Om-niQuant (Shao et al., 2023), we focus on 4-and 6-bit weights and activations quantization. Additionally, we also explore 4-bit weights and 8-bit activations quantization, aiming for hardware-friendly configurations while maintaining high performance. We exclude 8-bit quantization as SmoothQuant (Xiao et al., 2023) is able to achieve lossless performance. \n\nCompared methods.",
            "score": 0.5281032863274757,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 23741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 269,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 307,
                    "end": 329,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 346,
                    "end": 370,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 534,
                    "end": 555,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 561,
                    "end": 582,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 590,
                    "end": 611,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 671,
                    "end": 694,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1296,
                    "end": 1315,
                    "matchedPaperCorpusId": "253708271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "275104652",
            "title": "Configurable Multi-Layer Perceptron-Based Soft Sensors on Embedded Field Programmable Gate Arrays: Targeting Diverse Deployment Goals in Fluid Flow Estimation",
            "text": "In contrast, shallower models exhibit broader and less consistent Test MSE distributions under lower bitwidths, underscoring their vulnerability to quantization-induced errors. Interestingly, this contrasts with the findings from Experiment 1, where increasing the number of layers showed diminishing returns in precision. Here, deeper models demonstrate a distinct advantage in maintaining performance robustness, particularly under 4-bit quantization. \n\nWe then identified the best-precise model under each configuration for subsequent experiments on the FPGA platform. Figure 12 shows that 8-bit quantized models exhibit exceptional performance. For instance, the seven-layer, 120-neuron configuration achieves a Test MSE of 57.59 and a Test MAPE of 1.59%, demonstrating its suitability for precisioncritical applications. Even smaller configurations, such as the 4-layer, 10-neuron model, maintain strong performance with a Test MSE of 63.95 and a Test MAPE of 1.82%, further highlighting the robustness of 8-bit quantization across diverse model sizes. Compared to their FP32 counterparts, 8-bit models show mixed results of percentage difference in MSE, as evidenced by Figure 13. Among the 16 configurations, five exhibit higher MSE values under 8-bit quantization, with deviations ranging from 0.28% to 6.69%, while the remaining 11 configurations achieve reduced MSE values, with improvements ranging from 0.32% to 11.04%. Notably, no consistent pattern emerges regarding the sensitivity of model configurations to 8-bit quantization. While smaller models might be expected to exhibit greater vulnerability to quantization noise due to their lower representational capacity, this trend is not universally observed. For example, the four-layer, 10-neuron model achieves better Test MSE performance under 8-bit quantization (\u221210.58%), while the six-layer, 120-neuron model shows a slight increase (+1.84%). This lack of a clear correlation suggests that other factors, such as the interplay between layer count, neuron count, and the inherent noise resilience of the dataset, may play a significant role in determining the quantization impact.",
            "score": 0.5275246365234991,
            "section_title": "Experiments 2: Quantized Models Analysis",
            "char_start_offset": 39055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "251928917",
            "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization",
            "text": "Finally, we observe that adding the float has the least impact on the quantization errors, whose role is replaced by other primitive types. \n\nAccuracy Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT. We also observe that our proposed flint data type is important for the accuracies of both vision and NLP tasks. Meanwhile, the PoT type is more important for Transformer-based models on NLP tasks than vision tasks.",
            "score": 0.5270610020594271,
            "section_title": "B. Quantization Accuracy",
            "char_start_offset": 48751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "268889951",
            "title": "DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization",
            "text": "In this section, we present the results of the proposed quantization experiments on two widely adopted models, ResNet50 and MobileNetV2.The quantization process is applied to the ImageNet dataset, employing precision levels of 8-bit, 6-bit, 4-bit, 3-bit, and 2-bit using our proposed method.Throughout all our experiments, we maintain a consistent 8-bit precision for activation functions.Additionally, we employ batch normalization fold techniques [18] to enhance DNN accuracy before quantization.Pre-trained models are utilized with the PyTorchVision 0.13.1 framework.Subsequently, we illustrate the trade-off between accuracy and memory size in the following section.Finally, we delve into benchmarking BOPs across varying precisions to provide a comprehensive assessment.",
            "score": 0.5269363397434336,
            "section_title": "III. EXPERIMENTAL RESULTS",
            "char_start_offset": 12159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "250334271",
            "title": "Attention Round for Post-Training Quantization",
            "text": "In recent years, deep neural networks have developed rapidly and achieved remarkable results in many domains, such as computer vision, autonomous driving, natural language processing and speech recognition. However, the implementation of deep neural networks often requires huge computational resources and expensive computational costs, severely limiting their application to resource-limited devices. At present, there are three main ways to solve this problem: model compression, lightweight model architecture design and model quantification. Model compression [1,2,3,4,5,6,7,8] trims the redundant parameters in the trained model to reduce model size and alleviate computational burden. Lightweight model architecture design generally uses neural architecture search [9,10,11,12,13] to directly design a small network structure. Model quantification [14,15,16,17,18,19,20,21,22,23] refers to mapping the model weights and activation values from 32 bit floating points to the fixed point number of lower bits, thus reducing the inference time and power consumption of model and realizing the acceleration. The current popular model quantification algorithms can be divided into two categories: quantization aware training algorithms and post-training quantization algorithms. Among them, the quantization aware training algorithm [24,25,26,27,28] generally require sufficient training on the complete training data set to obtain good quantitative results, which requires a lot of computing resources and a long training time. Adequate training data acquisition will also be a great challenge, which is severely limited in practical application. Compare to the quantization aware training algorithms, post-training quantization algorithms [29,30,31] has higher training efficiency and lower requirements for training data completeness. This quantization algorithm generally requires only a small amount of training data, and can basically complete the quantization process in a few hours. Although the posttraining quantization algorithms has the characteristics of high training efficiency, the quantization model will suffer from serious performance degradation when the quantization precision is low. For example, DFQ [29] quantify the ResNet18 to 8-bit, the quantified model accuracy is 69.7%, but when quantified to 4-bit, the accuracy is only 39%, suffering a large accuracy loss.",
            "score": 0.5263821788925499,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "231918483",
            "title": "Confounding Tradeoffs for Neural Network Quantization",
            "text": "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 5: The set of allowed bitwidths for mixedprecision quantization affects the ability of mixed precision to increase accuracy. We test the impact on performance of using any integer bitwidth versus the subset {2, 4, 8}. We implement a restricted version of the post-training quantization method that rounds the continuously parameterized bitwidth to the closest of the three discrete values {2, 4, 8} during training. As shown in Figure 6, restricting the set of bitwidths used by the mixed-precision method does not affect accuracy at high average bitwidths, while at lower bitwidths the degradation is large. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations. \n\nObservation 7: Post-training quantization methods that use labeled data can perform equally well with pseudolabels.",
            "score": 0.5261423521391426,
            "section_title": "Results",
            "char_start_offset": 19337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "221506623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "220074239",
            "title": "Quantized Deep Residual Convolutional Neural Network for Image-Based Dietary Assessment",
            "text": "Deep neural network usually stores its activations and parameters into 32-bit floating point representations to benefit from high precision training during backpropagation [53]. A fullfledged deep learning model requires the high-performance machine to timely return the predicted result. Modern computer is capable to handle 32-bit floating calculation within a reasonable period. However, it is not efficient for a smartphone with limited computational power to run a full precision model. To improve user experience, it requires a smaller size of model to deliver real-time feedback at low latency speed [54]. \n\nPost-training quantization helps to reduce memory usage and lower down the computational requirement [55]. This enables the proposed DRCNN model possible to be executed on any platform e.g. a mobile phone locally even without a network connection to the backend server. Performance deterioration is one of the most critical drawbacks for a quantized model [56], hence we must carefully evaluate a systematic quantization approach to deliver a power-efficiency model with negligible accuracy loss. There are few common techniques to perform quantization on a neural network. These techniques involve the reduction of 32-bit floating numbers into a lower bit representation. Two different strategies can be applied in this quantization process, namely static or dynamic quantizer [57]. In this paper, 16-bits floating points, 8-bits fixed point, and dynamic range quantization will be examined to evaluate the resource-efficient against potential performance degradation. As shown in (11), the parameters in the proposed DRCNN model is quantized into 8-bit or 16-bit representations with the respective minimum and maximum real value in the range. The integer quantized weights q are computed from the floating weights r, in respect of bit representation b and quantized factor z: \n\nDynamic range quantization is a new technique proposed by Google Tensorflow to enable on-the-fly quantization and dequantization [58]. The model is quantized into 8 bits representations to lower down the memory requirements. During processing, the supported kernels are dequantized into floating values whenever available. This reduces the required space of a model while trying to retain as many details as possible.",
            "score": 0.5260997228909929,
            "section_title": "D. POST TRAINING QUANTIZATION",
            "char_start_offset": 21351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "52966467"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 971,
                    "end": 975,
                    "matchedPaperCorpusId": "164546651"
                },
                {
                    "start": 1393,
                    "end": 1397,
                    "matchedPaperCorpusId": "197525143"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "206770267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2288818359375
        },
        {
            "corpus_id": "276902966",
            "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
            "text": "In RQ 2 , we analyze whether the calibration dataset used at quantization time plays a major role in the code generation capabilities of the quantized LLMs. Specifically, we compare the pass@1 scores achieved in the previous experiment, i.e., using the \"Random\" calibration dataset, with those obtained with the \"Mixed\" and \"Code\" datasets. Table 6 reports the results of the three treatments. \n\nIn Table 6 the statistical tests compare each model quantized with the \"Mixed\" or with the \"Code\" dataset to its same version quantized using the \"Random\" dataset (e.g., CodeLlama 8-bit with Mixed dataset vs CodeLlama 8-bit with Random dataset). \n\nFor 8-bit and 4-bit models, we do not observe a statistically significant difference in pass@1 score between models quantized with different calibration datasets. This holds both for CodeLlama and DeepSeek-Coder and in both languages (Python and Java). This finding reveals that code models are robust to the calibration dataset provided at quantization time for target precisions greater or equal to 4 bits per parameter. The only exceptions are the CodeLlama 4-bit model on the McEval Java benchmark and the DeepSeek-Coder 4-bit model on the McEval Python benchmark, which are compared with a \"Random\" quantization that already performs better than the fp16precision baseline. The 4-bit CodeLlama model, instead, shows a statistically significant improvement on the McEval Python benchmark after the \"Mixed\" calibration, thus recovering from the performance gap with the baseline observed in Section 4.1. On the other hand, we notice that 3-bit and 2-bit precision models are more sensible to the samples provided in the calibration dataset. Indeed, both models show a statistically significant improvement in performance when calibration datasets feature code samples, suggesting their need to better approximate the model weights at extreme quantization levels. Thanks to code-related calibration datasets, a 3-bit quantization might be considered an option in very resource-constrained hardware devices.",
            "score": 0.5258215699065862,
            "section_title": "Which impact does the calibration dataset have on model performance?",
            "char_start_offset": 41957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "263620300",
            "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
            "text": "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores. Table 6 in Appendix shows the score differences by quantization for both MoE and dense models on 10 different language pairs translations.",
            "score": 0.5255848758268349,
            "section_title": "Robustness comparison between MoE and dense models",
            "char_start_offset": 5122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "221266154",
            "title": "One Weight Bitwidth to Rule Them All",
            "text": "If bit ordering is indeed consistent across model sizes, these results suggest that the optimal bitwidth for MobileNetV2 is 4 bit and it is 1 bit for ResNet50. However, throughout our analysis, we have not considered mixed-precision, which makes it unclear if the so-called optimal bitwidth (4 bit for MobileNetV2 and 1 bit for ResNet-50) is still optimal when compared to mixed-precision quantization. \n\nAs a result, we further compare with mixed-precision quantization that uses reinforcement learning to find the layer-wise bitwidth [29]. Specifically, we follow [29] and use a reinforcement learning approach to search for the lowest bitwidths without accuracy degradation (compared to the 8 bits fixed point models). To compare the searched model with other alternatives, we use widthmultipliers on top of the searched network match the model size of the 8 bit quantized model. We consider networks of three sizes, i.e., the size of 1\u00d7, 0.5\u00d7 and 0.25\u00d7 8-bit fixed point models. As shown in Table 3, we find that a single bitwidth (selected via Table 2) outperforms both 8 bit quantization and mixed-precision quantization by a significant margin for both networks considered. This results suggest that searching for the bitwidth without accuracy degradation is indeed a sub-optimal strategy and can be improved by incorporating channel counts into the search space and reformulate the optimization problem as maximizing accuracy under storage constraints. Moreover, our results also imply that when the number of channels are allowed to be altered, a single weight bitwidth throughout the network shows great potential for model compression, which has the potential of greatly reducing the software and hardware optimization costs for quantized ConvNets.",
            "score": 0.5250336909612753,
            "section_title": "Remarks and scaling up to ImageNet",
            "char_start_offset": 26045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1759
                }
            ],
            "ref_mentions": [
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "229923538",
            "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
            "text": "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This  is similar to the findings in Figure 1.",
            "score": 0.5245855250236388,
            "section_title": "C.1 Performance Drop by Binarization",
            "char_start_offset": 29617,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "256697391",
            "title": "Data Quality-Aware Mixed-Precision Quantization via Hybrid Reinforcement Learning",
            "text": "M Odel quantization has attracted increasing attention recently as an effective solution that improves the computation and memory efficiency for deep neural networks (DNN), thus enabling the deployment of deep models on resource-constrained edge devices [2,3,4,15,19,37,38,39]. Specifically, the lower description bit-width of DNN can bring about significant benefits in terms of inference acceleration and model compression rate, for example, Int8/Int4 quantization and binary neural networks [6]. However, it inevitably causes a greater accuracy drop than higher description bitwidth models due to unavoidably more information loss. \n\nTo maximally excavate the description redundancy and learn the compressed model for a given training domain, mixedprecision quantization [2,3,7,8,9,10,20] has been proposed to explore a better balance between task performance and model description length. It allows network components, e.g., layers [14] or kernels [20], to be quantized with lower bit-widths instead of full precision bit-widths if they have less contribution to model accuracy, wherein, more sensitive components are quantized with relatively higher bit-widths to avoid large accuracy degradation and vice versa. \n\nDespite the more balanced compression power, the mixed bit-widths can bring about unique challenges for the quantization setting exploration and model training. Concretely, the search space for determining a sufficiently good mixedprecision quantization setting can grow exponentially with increasing number of layers and bit-width choices. For example, when a ResNet-152 is quantized layer-wisely with 4 possible bit-widths, the search space can easily reach 4 152 times. Worse still, the inherently discrete and non-differentiable bitwidth decision-making (or sampling process) makes it hard to learn the bit-width policy jointly with quantization training. To deal with the above challenges, existing state-of-the-art (SOTA) works usually decouple the non-derivable bit-width sampling process from the end-to-end supervised training, i.e., consisting of two mutually independent precision decisionmaking stage and quantization training stage.",
            "score": 0.5237545513385462,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "211252843"
                },
                {
                    "start": 257,
                    "end": 259,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 259,
                    "end": 261,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "14395129"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "21721698"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "236423513"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "4708078"
                },
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "215943626"
                },
                {
                    "start": 774,
                    "end": 777,
                    "matchedPaperCorpusId": "211252843"
                },
                {
                    "start": 777,
                    "end": 779,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 779,
                    "end": 781,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 781,
                    "end": 783,
                    "matchedPaperCorpusId": "265038491"
                },
                {
                    "start": 783,
                    "end": 785,
                    "matchedPaperCorpusId": "235703520"
                },
                {
                    "start": 785,
                    "end": 788,
                    "matchedPaperCorpusId": "220363587"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "211011309"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "211011309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "273822014",
            "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
            "text": "A significant body of work has explored the accuracy trade-offs under different quantization schemes (Yao et al., 2023;Liu et al., 2023b;Huang et al., 2024;Gong et al., 2024b;Li et al., 2024a;Gong et al., 2024a). However, much of this research relies primarily on academic benchmarks, which do not fully reflect real-world deployment scenarios. Additionally, the lack of hyperparameter tuning in some studies leads to misleading conclusions about accuracy, as we demonstrate in our experiments. We challenge the claim that 8-bit integer activation quantization causes substantial accuracy degradation (Li et al., 2024a;Lee et al., 2024b), providing vast evidence to the contrary. The closest work to ours is by Lee et al. (2024b), which, like most prior studies, focuses on quantization accuracy, but overlooks key factors. First, while the authors claim to analyze models up to 405B parameters, they omit open-ended benchmarks at this scale and fail to report full-precision baselines even for academic tasks. Without these references, the impact of quantization remains unclear. To address this, we enable efficient multinode evaluations for the 405B model, conducting a comprehensive accuracy analysis in both academic and real-world settings. Second, Lee et al. (2024b) asserts that AWQ outperforms GPTQ in a 4-bit weight-only quantization setup. We correct this claim, and attribute it to suboptimal hyperparameter choices. Our comparative analysis (Table 1 and Appendix A.2) shows that while both methods perform similarly on academic benchmarks, GPTQ exhibits notable gains over AWQ in realworld tasks, particularly coding. Third, we refute the conclusion that W8A8-INT is significantly inferior to W8A8-FP and W4A16-INT. With proper tuning, W8A8-INT achieves competitive accuracy, with only minor losses.",
            "score": 0.5236096173100933,
            "section_title": "Related Work",
            "char_start_offset": 6670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 156,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 156,
                    "end": 175,
                    "matchedPaperCorpusId": "268357772"
                },
                {
                    "start": 192,
                    "end": 211,
                    "matchedPaperCorpusId": "268357772"
                },
                {
                    "start": 619,
                    "end": 637,
                    "matchedPaperCorpusId": "272694046"
                },
                {
                    "start": 711,
                    "end": 729,
                    "matchedPaperCorpusId": "272694046"
                },
                {
                    "start": 1255,
                    "end": 1273,
                    "matchedPaperCorpusId": "272694046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "259982476",
            "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats",
            "text": "As Natural Language Processing (NLP) evolves, Large Language Models (LLMs) like Codex [9] and Chat-GPT [22] have become essential, transforming our interaction with technology and daily communication. However, their complexity and computational intensity present deployment challenges [23,8,26], particularly in resource-limited settings. One solution is quantization, which represents data in lower-precision formats such as 8-bit integers or floating-point numbers, reducing memory needs and potentially enhancing inference latency through better GEMM computation throughput on compatible GPUs. Post-Training Quantization (PTQ), which directly reduces the precision of a fully trained model's parameters, is often preferred for LLMs due to its simplicity and lower computational overhead. 1 Recent studies indicate that PTQ on 8-bit integer (INT8) weight-only quantization does not compromise the quality of LLMs [34,3,33,29], and only a minor accuracy drop is observed with INT4 weight quantization when advanced algorithm such as GPTQ applied [7,35,12,15]. \n\nThe exploration of activation quantization, in addition to weight-only quantization, has also gained interest. This approach expedites inference times by taking advantage of unified precision leading to more efficient execution on hardware. The primary challenge in implementing activation quantization lies in the trade-off between efficiency and performance. As evidenced in studies such as ZeroQuants [34,35], SmoothQuant [33] and others, reducing the precision of activation from FP16 to INT8 inevitably results in a decrease in model quality. This degradation is partially due to the presence of extreme values or outliers in the activation of LLMs [5,33,15,12], which is partly attributed to the pretraining effect [31]. In the presence of outliers, uniform quantization like INT8 or INT4, fail to accurately represent the main body of the data as they become skewed towards the outlier. This issue stems from the inherent assumption in these techniques of a uniform data distribution [30], an assumption that might not correspond to the actual data points distribution.",
            "score": 0.5235932700540298,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 919,
                    "end": 921,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "263237367",
            "title": "Differentiable Neural Architecture, Mixed Precision and Accelerator Co-Search",
            "text": "the goal is to automatically search for precision/bit-width of every weight and activation matrix, parameters of neural architecture: kernel size (k) and Number of filters/channels (C) and hardware accelerator: array height and array width. \n\nproven successful on many tasks such as Image Classification [11], Object Detection [16], etc., where searched networks surpassed the handcrafted models. \n\nNeural Network model reduction techniques, such as Pruning and Quantization, are crucial for optimized inference on various hardware platforms. The low precision inference is an effective approach to boost DNN performance on many hardware architectures. Quantization [17], [18], a wellknown and widely used method, converts high-precision floating point weight and/or activation parameters, such as Floating Point 32 (FP32), to low-precision values like Integer 8 (Int8) Integer 4 (Int4). Uniform Quantization assigns same precision or bit-width to all weight and activation matrices across all layers in a network. Nonetheless, quantizing the model with ultra-low precision, such as 1 or 2 bits, can result in significant accuracy degradation [17]. \n\nMixed Precision Quantization [19], [20] is a mechanism to allow different bit-width to different layers in a model to overcome the challenges faced by Uniform Quantization. Generally, mixed precision assignment enables the model to capture the layer information better by setting different weight and activation matrices to different range of precisions. However, this method's main problem is determining each layer's precision in the network so that the model attains acceptable accuracy while minimizing the model size. The algorithmic advancement of NAS facilitated the method to apply in other applications in Deep Learning which are combinatorial in nature. Therefore, Mixed Precision Quantization can be translated into a search problem, thereby utilizing similar NAS principles. We propose a Fast Hardware-aware Mixed Precision Search based on a search space pruning and tensor-sharing methodology. \n\nThe bit-width and the layer dimensions (kernel and channel sizes) of a neural network are often correlated, thus allowing us to co-design the hyperparameters together. The optimal combination of neural architecture and precision of each layer contributes to the model accuracy and efficiency on the hardware.",
            "score": 0.5228094587323586,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "184487125"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "231699188"
                },
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "52197199",
            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
            "text": "Fine-tuning matches or exceeds the accuracy of the initial high-precision network and outperforms other methods FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al. 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks. \n\nFAQ trained 4-bit network accuracy is sensitive to several hyperparameters (Table 2). In summary, longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and activation calibration all contribute to improved accuracy when training the 4-bit ResNet-18 network, while the exact learning rate decay schedule contributed the least. We elaborate on some of these results subsequently.",
            "score": 0.5227735788850375,
            "section_title": "Experiments",
            "char_start_offset": 12749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 843,
                    "matchedPaperCorpusId": "195347490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "272694046",
            "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
            "text": "Further dataset details are provided in Appendix A (Table 3). \n\nTo compare our results with ongoing community efforts, we synchronize our benchmarks with Huggingface OpenLLM Leaderboard-v1 (covering April 2023 to June 2024) and Open-LLM Leaderboard-v2 (launched on June 26, 2024). However, both versions currently provide only limited data on quantized models, highlighting the need for our comprehensive evaluation. Our key findings are as follows: \n\n\u2022 Quantized LLMs generally perform better than smaller models on most benchmarks and maintain their advantage across different architectures, showing significant improvements in both large and small language models. However, they still struggle with instruction-following (IFEval) and detecting hallucinations (TruthfulQA). \u2022 FP8 is the most reliable method for all model sizes and tasks, especially for LLMs with 405B parameters, where SmoothQuant encounters problems. AWQ usually performs better than GPTQ in weight-only quantization, and hardware support makes FP8 even more advantageous. \u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits. While model size is the main factor affecting quantization difficulty, differences in LLM architecture within the same parameter size can also affect accuracy. Nevertheless, AWQ consistently outperforms GPTQ across different tasks and model types. \u2022 Difficult tasks do not always have the biggest accuracy drops when quantized. The impact depends on the model design and the quantization method used, causing some hard tasks to remain stable while some easy tasks see bigger decreases. Overall, quantization highlights a model's existing weaknesses, especially in commonsense, logical, or mathematical reasoning. \u2022 Quantization greatly reduces performance in Coding and STEM tasks, although it sometimes improves reasoning accuracy. Additionally, GPT4-based evaluators can sometimes incorrectly judge wrong answers as correct.",
            "score": 0.5226000069836455,
            "section_title": "Introduction",
            "char_start_offset": 3512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 64,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2056
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "251467937",
            "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization",
            "text": "1) Quantization Schemes: Model quantization has been intensively explored for deep neural networks (DNNs) such as CNNs and recurrent neural networks (RNNs). \n\nThere are schemes using uniform quantization intervals including binary [5], [6] ternary [17], and low-bit-width fixed-point [7], [8]. Although binary and ternary quantization significantly reduce operations and simplify hardware implementation, it introduces large accuracy loss. The fixedpoint quantization scheme, on the other hand, applies modest and flexible quantization rates to preserve accuracy close to that of 32-bit floating-point models. For instance, 4-bit fixedpoint introduces zero or negligible accuracy loss. Fixed-point quantization scheme was implemented with different methods and algorithms, such as DoReFa-Net [7] and PACT [8]. \n\nThere are also schemes using non-uniform quantization intervals such as power-of-two (PoT) [9] and additive PoT [10], by which multiplications can be replaced with bit shifting operations. Furthermore, PoT presents higher precision around the mean, and therefore better fits the Gaussian distribution of DNN weights [18]. But it exhibits rigid resolution issue that results in moderate accuracy loss, which cannot be mitigated even with higher bit-width. To overcome it, additive PoT was proposed by using a sum of multiple PoT numbers. \n\n2) Transformer Quantization: Quantization has also been applied to transformers, in particular, bidirectional encoder representations from transformers (BERTs) [1]. Specifically, [13] finetuned BERT through 8-bit quantization-aware training. The later TernaryBERT [14] proposed to use an approximation-based and loss-aware ternarization for BERT, and distillation to further reduce accuracy drop caused by lower capacity. BinaryBERT [15] suggested that it is difficult to train a binary BERT directly due to its complex loss landscape and proposed a ternary weight splitting strategy to derive binary BERT with performance as the ternary one. All the aforementioned work targeted BERT in NLP tasks, not covering ViT in computer vision tasks.",
            "score": 0.5220992268199491,
            "section_title": "B. DNN Model Quantization",
            "char_start_offset": 4839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "52909828"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "39895556"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "matchedPaperCorpusId": "221970445"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "229923538"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "204575957",
            "title": "AI Benchmark: All About Deep Learning on Smartphones in 2019",
            "text": "Advantages: The model is first converted from a 16-bit floating point type to int-8 format. This reduces its size and RAM consumption by a factor of 4 and potentially speeds up its execution by 2-3 times. Since integer computations consume less energy on many platforms, this also makes the inference more power efficient, which is critical in the case of smartphones and other portable electronics. \n\nDisadvantages: Reducing the bit-width of the network weights (from 16 to 8 bits) leads to accuracy loss: in some cases, the converted model might show only a small performance degradation, while for some other tasks the resulting accuracy will be close to zero. Although a number of research papers dealing with network quantization were presented by Qualcomm [49,54] and Google [34,37], all showing decent accuracy results for many image classification models, there is no general recipe for quantizing arbitrary deep learning architectures. Thus, quantization is still more of a research topic, without working solutions for many AIrelated tasks (e.g., image-to-image mapping or various NLP problems). Besides that, many quantization approaches require the model to be retrained from scratch, preventing the developers from using available pre-trained models provided together with all major research papers.",
            "score": 0.5217054321844026,
            "section_title": "Quantized Inference",
            "char_start_offset": 32163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 785,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378662109375
        },
        {
            "corpus_id": "195767218",
            "title": "Compression of Acoustic Event Detection Models With Quantized Distillation",
            "text": "In our case, value quantization induces additional 0.0016G FLOPs, which is negligible compared to the 0.32G FLOPs of running full-precision LSTM. The exact speedup ratio by quantization depends implementation which are harware specific and will not be discussed here. In addition, we find that the gap between our quantized training scheme and PM grows as model scales down. Similarly we consider relative degradation from full-precision model. Both at 8-bit setting, difference of relative degradation between two quantization schemes increases from 1.1% to 8.5% in EER when number of hidden units is reduced from 256 to 64. Similar finding holds for 4-bit setting as well. Thus to an originally small model, quantization in training is important to prevent the accuracy drop when it is quantized.",
            "score": 0.521438742151913,
            "section_title": "Effect of knowledge distillation",
            "char_start_offset": 15201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41796875
        },
        {
            "corpus_id": "267740313",
            "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
            "text": "[cs.CL] 16 Feb 2024 quantization, and how to effectively learn low-bit representations during training. \n\nIn this work, we present BitDistiller, a novel framework that synergizes QAT with Knowledge Distillation (KD) to significantly boost the performance of sub-4-bit quantized LLMs. To minimize quantization error, BitDistiller employs a tailored asymmetric quantization and clipping strategy to maintain the capabilities of the full-precision model as much as possible, particularly at ultra-low-bit levels. For efficient and effective low-bit representation learning, BitDistiller leverages a simple yet effective self-distillation approach, wherein the full-precision model acts as its own teacher to refine the low-bit student model. Notably, BitDistiller innovates with a Confidence-Aware Kullback-Leibler divergence (CAKLD) objective that optimizes knowledge transferring efficacy, enabling faster convergence and enhanced model performance. \n\nOur empirical evaluations, conducted on a diverse suite of general language understanding and complex reasoning tasks including mathematics and coding, demonstrate that BitDistiller significantly outperforms existing PTQ and QAT methods in the realm of sub-4-bit quantization. As illustrated in Figure 1, BitDistiller achieves the most favorable scaling law in both 3-bit and 2-bit configurations on the code reasoning benchmark. Moreover, BitDistiller is demonstrated to be more costeffective, requiring less training data and fewer training resources, thereby marking a significant advancement toward deploying robust Large Language Models on resource-constrained devices. \n\n2 Background and Related Work 2.1 Weight Quantization for LLMs PTQ and QAT PTQ is directly applied to pretrained models without additional training. PTQ for LLMs typically employs techniques that either adjust quantization error (Frantar et al., 2022;Chee et al., 2023) or prioritize salient weights (Dettmers et al., 2023b;Lin et al., 2023;Kim et al., 2023b). However, the lack of retraining with PTQ may cause notable decreases in model performance at extremely low precisions.",
            "score": 0.5208538618240821,
            "section_title": "Introduction",
            "char_start_offset": 1947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1625
                },
                {
                    "start": 1628,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "257427488",
            "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
            "text": "The training hyperparamters, such as learning rates, are picked following standard benchmarks and open implementaitons (Liu et al., 2019;Vaswani et al., 2017). We summarize them in Table 3 for repeatability. We use the Adam optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.98 for both training and finetuning models. The learning rate schedule is Inverse Square Root for training models, and Polynomial Decay for finetuning models. Dropout with rates of P IW SLT = 0.3 and P W M T = 0.2, label smoothing with value \u03f5 = 0.1 are applied to train models. DSQ precision configurations are decided through experimentation on the IWSLT dataset as discussed in section 6 Limitations. Table 4 shows a collection of tuning runs we had, we found that heavily quantized models still work at the start of training stage, and [16,4,4,16] quantized BFP model works as well as less aggressive ones. This indicates that DSQ should start with heavily aggressive precision setup (we pick [2, 2, 2, 16] for IWSLT14 DE-EN), and jump to [16,4,4,16] when needed during training process. C The effect of q 3 \n\nThe gradient output (dx l ) plays an important role in the performance of fixed-point quantization. Notice in table Table 5, gradient output quantized to 8 bits leads to training failure for fixed-point quantization. \n\nIn order to focus on the idea of stashing, we apply 16 bits quantization of gradient output for all our stashing precision setups.",
            "score": 0.5206037610477807,
            "section_title": "B Hyperparameters",
            "char_start_offset": 12931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1421
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "256442993",
            "title": "Efficient Binary Weight Convolutional Network Accelerator for Speech Recognition",
            "text": "The dataset used in this work is the ST CMD released by an AI company Surfing Technology, which contains more than 100,000 speech files and approximately 100 h of speech data. The data content is mainly based on the usual online chat and intelligent speech control sentences. There were 855 speakers, including half male voice and half female voice, suitable for various scenarios. We experimented with various quantisations with different bit widths for the network weight and activation function. The model PER comparison is presented in Table 2, where W is the weight bit width, A is the bit width output by the activation function, and PER is the evaluation index of the speech model. The lower the PER, the better the network performance. Note: W = weight fixed-point bit width, A = activation fixed-point bit width. \n\nIn Table 2, we can see that the difference in PER between customized VGG-16 at the data precision of Float 32-bit and 1W/Float 32-bit is insignificant. However, the data precision of 1W/Float 32-bit increases the PER by only 0.27% with an 18.03 M reduction in convolution parameters. 16-bit and 8-bit quantization of multibit-weighted network activation functions have little accuracy loss. For the binary weight network, the PER increases by 1.77% when the activation function is quantized to 16 bits, but the PER is not very different from that of the activation function 8-bit quantization. In addition, we found that when the activation function is fixed to 4-bits in the experiment, the recognition result cannot be predicted, and the data bit width in the hardware is generally 2 n . Hence, the accelerator selects 1W/8A to quantify the neural network and conduct a performance comparison in the subsequent hardware implementation. LUT, Flip Flop (FF), and DSP are essential resources in FPGA. ZYNQ 7035 implements common 8W/8A multiplication in vivado2018.2 and consumes 72LUT or 1DSP of hardware resources. However, our designed 1W/8A multiplication consumes only 7LUT, which greatly reduces hardware resources.",
            "score": 0.5203182625966775,
            "section_title": "Accuracy of Network",
            "char_start_offset": 18713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2043
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "1193239",
            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
            "text": "Also, QSGD with 2-bit and 64 bucket size has gap 1.73% for top-1, and 1.18% for top-1. \n\nSimilarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version. \n\nOn CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%. We observe a similar result on MNIST, where 2-bit QSGD with buckets equal to the size of hidden layers improves accuracy by 0.5%. These results are consistent with recent work [27] noting benefits of added noise in training deep networks. Linear models on e.g. MNIST do not show such improvements. Across all our experiments, 8-bit gradients with 512 bucket size have been sufficient to recover or improve upon the full precision target accuracy. \n\nOn AN4 using an LSTM, 2-bit QSGD has similar convergence rate and the same accuracy as 32bit. On a two-GPU system, it is able to converge around 3\u00d7 faster to the target accuracy with respect to full precision, thanks to reduced communication overheads. The 4-bit variant has the same convergence and accuracy, but is slightly slower than 2-bit (by about 10%). \n\nOne issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy.",
            "score": 0.520006435511394,
            "section_title": "Experiments",
            "char_start_offset": 32510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1853
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "278326880",
            "title": "An Empirical Study of Qwen3 Quantization",
            "text": "Developed by Alibaba Group, the Qwen series [1; 17] has rapidly advanced as a competitive open-source family of autoregressive large language models (LLMs) based on the Transformer architecture [15]. With its initial release in 2023, Qwen demonstrated exceptional scalability, with even its 7B parameter model rivaling larger proprietary models like GPT-3.5 in certain benchmarks. The recently launched Qwen3 1 , available in configurations from 0.6B to 235B parameters, further elevates performance through refined pre-training on diverse, high-quality corpora. This positions the Qwen family among the most capable open-source LLMs, adaptable to diverse deployment scenarios. \n\nDespite its strengths, practical deployment of Qwen3 faces challenges due to high computational and memory demands. Low-bit quantization [16; 5; 9; 6] has emerged as a critical technique to mitigate these issues, enabling efficient inference on resource-constrained devices. However, quantization often introduces performance degradation. Qwen3's state-of-the-art capabilities present a timely opportunity to reassess quantization techniques, uncovering new insights into their efficacy and limitations for cutting-edge models. \n\nIn this empirical study, we systematically evaluate Qwen3's robustness under quantization across Post-Training Quantization (PTQ) methods. We test 5 classic methods, including Round-To-Nearest (RTN), GPTQ [5], AWQ [10], SmoothQuant [16] and BiLLM [8] for PTQ, spanning bit-widths Evaluation protocol. For a comprehensive PTQ evaluation, we measure perplexity (PPL) on WikiText2 [12] and a 256-sample subset of C4 [13] with a sequence length of 2048. Zero-shot accuracy is evaluated across six established reasoning benchmarks: PIQA [2], Winogrande [14], ARC-Easy and ARC-Challenge [4], HellaSwag [19], and BoolQ [3]. Few-shot capability is further examined using 5-shot MMLU [7].",
            "score": 0.5196542863128955,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1742,
                    "end": 1745,
                    "matchedPaperCorpusId": "208290939"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "53568871",
            "title": "Protection of Superconducting Industrial Machinery Using RNN-Based Anomaly Detection for Implementation in Smart Sensor",
            "text": "Table 4 shows results of coefficients quantization for the neural models from Tables 2 and 3 using several different methods. For all methods, the quantization above the ten bits yields results nearly identical to original. A significant drop in accuracy is observed below 8 bits of representation. It may be noted (Table 3) that for lower number of bits the performance oscillates between \u22480.7 and \u22480.3 which means classifying all the features as one category.",
            "score": 0.5194441316545422,
            "section_title": "Coefficients Quantization",
            "char_start_offset": 27830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "267312437",
            "title": "HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference",
            "text": "Quantization is one of the most impactful ways to decrease the computational time and energy consumption of neural networks. In neural network quantization, the weights and activation tensors are stored in lower bit precision than the 16 or 32-bit precision they are usually trained in. When moving from 32 to 8 bits, the memory overhead of storing tensors decreases by a factor of 4 while the computational cost for matrix multiplication reduces quadratically by a factor of 16. Neural networks have been shown to be robust to quantization [13,40], meaning they can be quantized to lower bit-widths with a relatively small impact on the network's accuracy. Neural network quantization is an essential step in the model efficiency pipeline for any practical use-case of deep learning [2,28,36,51].",
            "score": 0.5192359212895472,
            "section_title": "A. Network quantization",
            "char_start_offset": 27787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 797
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 790,
                    "end": 793,
                    "matchedPaperCorpusId": "244715141"
                },
                {
                    "start": 793,
                    "end": 796,
                    "matchedPaperCorpusId": "232223166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "270620336",
            "title": "Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox",
            "text": "For each downstream task, we utilize the training set from different datasets as the calibration set for the quantization process and test on the corresponding I.I.D and OOD test sets.In our experiments, we employ LLaMA2-7B (54) as the target for quantization and selected four PTQ methods: GPTQ (14), AWQ (31), SpQR (10), and SmoothQuant (62).Given that there is not much difference in performance between excessively high bits and full precision, and too low a bit has already lost basic performance in these tasks, we quantize the model weights to 3-4 bits with SmoothQuant quantizing the activations to 8 bits.We test two forms: 0-shot and few-shot.\n\nWe present the results in Tab. 2. We evaluate four downstream tasks in BOSS: EQA, SA, NLI, and TD.Each downstream task consists of four datasets, with each dataset tested using four datasets as calibration set.The following conclusions can be observed:\n\nFor datasets with poor performance or even close to zero, few-shot learning significantly improves the performance.For the EQA task with both 4-bit and 3-bit quantization and the SA 0.00 0.00 0.00 0.00 3/8 0.00 0.00 0.00 0.00 3/8 0.00 0.00 0.00 -3/8 0.00 0.00 0.00 0.00 task with 4-bit quantization, where satisfactory performance can be achieved, there is a relatively slight improvement with few-shot learning compared to 0-shot.However, for the SA task with 3-bit quantization, the NLI task with both 4-bit and 3-bit quantization and the TD task with 4-bit and 3-bit quantization with poor performance, few-shot learning shows a qualitative leap compared to 0-shot.Especially on some datasets where the 0-shot performance is nearly zero, few-shot learning achieves accuracy ranging from 80% to 90%.",
            "score": 0.5189700500830027,
            "section_title": "S2: Generalization Assessment of Quantized LLMs with Domain Shifts",
            "char_start_offset": 11173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 653
                },
                {
                    "start": 655,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "253708271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "276903421",
            "title": "Emergent Abilities in Large Language Models: A Survey",
            "text": "Liu et al. [53] investigate a critical but often overlooked factor affecting the emergent abilities of LLMs: i.e., quantization. As LLMs grow in size, their memory and computational requirements become increasingly demanding, prompting the need for quantization techniques that reduce precision and optimize efficiency. However, a key question remains: Does quantization compromise the emergent abilities that make these models so powerful? Their study systematically examines this trade-off, particularly in the context of in-context learning, chain-of-thought reasoning, and instruction following -three hallmarks of advanced LLM capabilities. \n\nHow Quantization Affects Performance. Quantization is a widely used technique that compresses neural networks by reducing the number of bits used to represent each parameter. While this drastically lowers memory usage and inference costs, it often comes at the expense of model performance. Liu et al. [53] conduct extensive empirical evaluations on a range of LLaMA models, spanning 7B to 65B parameters, across multiple quantization levels, including 2-bit, 4-bit, 8-bit, and 16-bit precision. Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels. This suggests that there exists a critical threshold beyond which the model struggles to maintain the structured reasoning and learning dynamics that enable emergent behaviors. \n\nA particularly insightful aspect of the study is its componentwise analysis, which reveals that not all parts of the model are equally affected by quantization. The feed-forward networks within transformer blocks are found to be especially crucial for retaining performance. When these layers undergo aggressive quantization, the model experiences significant losses in reasoning ability and generalization. This highlights the fact that while weight compression is beneficial, indiscriminate quantization of all components can be detrimental. Can Emergent Abilities Be Preserved?",
            "score": 0.5182505185436381,
            "section_title": "D. The Impact of Quantization on Emergent Abilities",
            "char_start_offset": 31198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2384
                },
                {
                    "start": 2385,
                    "end": 2421
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "268230676",
            "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
            "text": "For precisions lower than 8 (e.g., 3 and 4), we use weight-only kernels provided by GPTQ [14] following serving system setup of HuggingFace TGI [13], OpenLLM [28]. \n\nHowever, Existing LLM quantization works uniformly quantize all model layers to the same bit by default (e.g., 3, 4, or 8 [6,14]) which leads to underutilized memory on high-calibre GPUs or OOM problems on low-calibre GPUs in a heterogeneous cluster. This is because different types of GPUs are not allowed to choose their most suitable quantization precision to match their capacities. Opporunity 2: Adaptive Quantization for Better Accuracy and Speed. We advocate adaptive quantization by choosing potentially different bits for model layers on different GPUs, to better utilize the available memory, as well as to improve model quality and computation speed as compared to uniform quantization. We illustrate the benefits of adaptive quantization as follows: 1. Adaptive quantization can lead to better model accuracy. We run BLOOM-3b1 [31] and OPT-1.3b [40] with different precision setups on A100 and evaluate the perplexity [19], on three text datasets [23,24,29]. We also measure the model accuracy on popular zero-shot question-answering benchmarks LAMBADA [26], ARC [5] and PIQA [2];We use calibration data from the C4 dataset to determine quantization statistics. In Fig. 4, the 'mixed4-8' case denotes that we uniformly randomly assign 4 or 8 bits to each model layer, while 'mixed3-4' is to uniformly randomly assign bitwidth 3 or 4  to each layer. Mixed-precision quantization leads to better model performance than uniformly using the lower bit. \n\n2. Adaptive quantization speeds up inference. Fig. 5 shows how quantization performs with different device types and input shapes. The latency is measured on a single layer of OPT-30b with prompt length 512.",
            "score": 0.518011456757894,
            "section_title": "Quantization in LLM",
            "char_start_offset": 12918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 166,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1625
                },
                {
                    "start": 1628,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "252796"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "2381275"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "208290939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73046875
        },
        {
            "corpus_id": "53217792",
            "title": "Bi-Real Net: Binarizing Deep Network Towards Real-Network Performance",
            "text": "Parameter quantization. The study in [18] demonstrates that real-valued deep neural networks such as AlexNet [17], GoogLeNet [33] and VGG-16 [29] only encounter marginal accuracy degradation when quantizing 32-bit param-eters to 8-bit. In Incremental Network Quantization [41], Zhou et al. quantize the parameter incrementally and show that it is even possible to further reduce the weight precision to 2-5 bits with slightly higher accuracy on the Ima-geNet dataset than a full-precision network. Based on that, Zhou et al. further propose explicit loss-error-aware quantization [42], which obtains comparable accuracy as the real-valued network with very low-bit parameter values. In BinaryConnect, Courbariaux et al. more radically employ 1-bit precision weights (1 and -1) while maintaining sufficiently high accuracy on the MNIST, CIFAR10 and SVHN datasets. Ho et al. utilize a proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss with regard to the binarized weights [10]. \n\nQuantizing weights properly can achieve considerable memory savings with little accuracy degradation. However, acceleration via weight quantization alone would be limited due to the real-valued activations (i.e., the input to convolutional layers). Several recent studies have been conducted to explore new network structures and/or training techniques for quantizing both weights and activations while minimizing accuracy degradation. Successful attempts include DoReFa-Net [44] and QNN [13], which explore neural networks trained with 1-bit weights and 2-bit activations, and the accuracy drops by 6.1% and 4.9%, respectively, on the ImageNet dataset compared to the real-valued AlexNet. Recently, Zhuang et al. [46] propose to jointly train a full-precision model alongside the low-precision one, which leads to no performance decrease in a 4-bit precision network compared with its full-precision counterpart.",
            "score": 0.5177228820914777,
            "section_title": "Related Work",
            "char_start_offset": 11610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "53872663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "249953775",
            "title": "Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Switchboard Corpus",
            "text": "State-of-the-art automatic speech recognition (ASR) systems represented by both the traditional hybrid HMM-DNN architectures [1,2,3,4,5,6,7,8,9] and the recently emerging all neural end-to-end (E2E) modelling paradigm [10,11,12,13,14,15] are becoming increasingly complex and expensive for practical applications. For example, the best E2E encoder-decoder ASR system presented in [16] contained up to 280.1 million model parameters. This not only lead to a large increase in their computational cost when operating on the cloud, but also creates difficulty when deploying them on edge devices to enhance privacy and reduce latency. \n\nAn ultimate goal for many data and resource intensive deep learning based AI applications, including ASR systems, is to derive \"lossless\" model compression approaches that allow high performance and low-footprint speech recognition systems to be constructed while incurring minimum performance degradation. To this end, one efficient solution is to use low-bit deep neural network (DNN) quantization techniques [17,18,19,20], which has drawn increasing interest in the machine learning and speech technology community in recent years. By replacing floating point weights with low precision values, the resulting quantization methods can significantly reduce the model size and inference time without modifying the model architectures. \n\nTraditional DNN quantization approaches [21,22,23] are predominantly based on uniform precision, where a manually defined identical bit-width is applied to all weight parameters. This fails to account for the varying performance sensitivity at different parts of the system to quantization errors. \n\nThis paper presents the development of a high performance and ultra-compact 4-bit quantized LF-MMI trained factored time delay neural networks (TDNNs) based ASR system on the 300-hr Switchboard corpus. A key feature of the overall system design is to account for the fine-grained, varying performance sensitivity at different model components to compression and quantization errors. This allows the trade-off between model compression ratio and accuracy performance target. The overall system development contained two stages.",
            "score": 0.5174049421440684,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 130,
                    "matchedPaperCorpusId": "10042024"
                },
                {
                    "start": 132,
                    "end": 134,
                    "matchedPaperCorpusId": "206741496"
                },
                {
                    "start": 134,
                    "end": 136,
                    "matchedPaperCorpusId": "15027653"
                },
                {
                    "start": 140,
                    "end": 142,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "231775923"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "221387784"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "190651327"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "216526299"
                },
                {
                    "start": 1418,
                    "end": 1421,
                    "matchedPaperCorpusId": "237347204"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "201645363",
            "title": "Efficient Deep Neural Networks",
            "text": "In the first experiment, we focus on quantizing ResNet20, ResNet56, and ResNet110 [50] on CIFAR10 [79] dataset. We start by focusing on reducing model size, since smaller models require less storage and communication cost, which is important for mobile and embedded devices. We only perform quantization on weights and use full-precision activations. We conduct mixed-precision search at the block level -all layers in one block use the same precision. Following the convention, we do not quantize the first or the last layer. We construct a super net whose macro architecture is exactly the same as our target network. For each block, we can choose a precision from {0, 1, 2, 3, 4, 8, 32}. If the precision is 0, we simply skip this block so the input and output are identical. If the precision is 32, we use the full-precision floating point weights. For all other precisions with k-bit, we quantize weights to k-bit fixed-point numbers.\n\nOur experiment result is summarized in   the one with the highest test accuracy (top) and the one with the highest compression rate (bottom). We compare our method with [183], where they use 2-bit (ternary) weights for all the layers of the network, except the first convolution and the last fully connect layer. From the table, we have the following observations: 1) All of our most accurate models out-perform their full-precision counterparts by up to 0.37% while still achieves 11.6 -12.5X model size reduction. 2) Our most efficient models can achieve 16.6 -20.3X model size compression with accuracy drop less than 0.39%. 3) Compared with [183], our model achieves up to 1.59% better accuracy. This is partially due to our improved training recipe as our full-precision model's accuracy is also higher. But it still demonstrates that our models with searched mixed-precision assignment can very well preserve the accuracy. Table 8.2 compares the precision assignment for the most accurate and the most efficient models for ResNet20. Note that for the most efficient model, it directly skips the 3rd block in group-1. In Fig. 8",
            "score": 0.5171324403420837,
            "section_title": "Mixed-precision quantization experiments CIFAR10 experiments",
            "char_start_offset": 208233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "265506648",
            "title": "Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural Networks",
            "text": "As deep learning continues to advance, its application across various domains has necessitated the adaptation and continual improvement of existing models. Traditionally, most neural network models are trained using 32-bit precision, which has been a standard due to its balance between computational efficiency and model accuracy [9]. This precision level has enabled the development of robust and highly accurate models across a range of applications. However, as new challenges arise, such as changes in data distribution, the emergence of new classes in classification tasks, or the need for model compression for deployment in resource-constrained environments, the necessity for additional training becomes increasingly apparent [5,10]. \n\nIn response to these evolving needs, this study proposes a novel method of continuing the training of pre-existing 32-bit models using 16-bit precision. This approach enhances training efficiency and speed by utilizing the lesser-explored 16-bit precision, a technique that has been gaining recognition for its effectiveness in deep learning. We focus specifically on the ResNet architecture [4], a model renowned for its performance in a variety of deep learning tasks, to demonstrate the practicality and advantages of our proposed method in real-world scenarios. \n\nThe necessity of this research is further underscored by the evolving demands of deep learning applications in dynamic environments. As models are increasingly deployed on edge devices and mobile platforms, the ability to efficiently update and refine pre-trained models becomes crucial.",
            "score": 0.516535390814188,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1600
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "362467"
                },
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39404296875
        },
        {
            "corpus_id": "219792555",
            "title": "Efficient Execution of Quantized Deep Learning Models: A Compiler Approach",
            "text": "The effectiveness of deep learning in image processing and natural language processing tasks has led to the development of a growing number of applications that run deep learning models on a wide range of systems, from cloudscale clusters to resource-limited edge-devices [1,2,3,4]. The widespread use of deep learning frameworks such as Tensorflow [5], PyTorch [6] and MXNet [7] drives the applications of deep learning. These frameworks enable model developers to quickly build, train, and deploy models on many hardware platforms. The frameworks provide a set of operators, where each operator represents a mathematical computation, e.g., convolution2D (referred to as conv2d), ReLU (rectified linear unit), batch normalization etc. These operators are typically converted to machine code using a hardware-specific library, e.g., Intel DNNL [8] and Nvidia CuDNN [9]. \n\nIn general, deep learning models require substantial compute and memory resources [3,4], which can burden even powerful servers leave alone low-power edge devices. Researchers have implemented various techniques -algorithms, software, hardware -to reduce the compute and memory burden [4,10,11] of deep learning models to simplify their development and deployment [12,13]. \n\nAmong these techniques, quantization is a promising and well-studied approach. Quantization represents floating point 32-bit (fp32) numbers, which are used frequently in the deep learning models, with integer 8-bits (int8) [11,14,15], reducing the memory footprint by a factor of four. The most widelyused form of integer quantization is uniform quantization [11], where an fp32 tensor (A f p32 ) is represented with a quantized int8 tensor (Q A ) along with quantization attributes -scale (scale A ) and zero point (zp A ) as shown below \n\nQuantization enables a model to consume fewer compute and memory resources while keeping its accuracy close to that of the unquantized model (where the inputs and parameters are represented in fp32).",
            "score": 0.5162076763673354,
            "section_title": "I. INTRODUCTION A. Background",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 279,
                    "matchedPaperCorpusId": "3541031"
                },
                {
                    "start": 279,
                    "end": 281,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "12330432"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "3541031"
                },
                {
                    "start": 957,
                    "end": 959,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 1157,
                    "end": 1160,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 1160,
                    "end": 1163,
                    "matchedPaperCorpusId": "140309863"
                },
                {
                    "start": 1163,
                    "end": 1166,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "52241381"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "195798852"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1474,
                    "end": 1477,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1477,
                    "end": 1480,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 1606,
                    "end": 1610,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3701171875
        },
        {
            "corpus_id": "270227790",
            "title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers",
            "text": "2.2.1 Quantization Scheme.To compress model size and improve inference speed, model quantization has been widely explored for deep neural networks (DNNs).Existing quantization research can be categorized according to quantization schemes, such as binary [8,36], ternary [13], and low-bit-width fixed-point [7,7,68,68] quantize models with the same interval between each quantization level.Although binary and ternary quantization reduce operations and simplify hardware implementation to the extreme, they introduce large accuracy loss due to insufficient bit-width.For example, based on reports from the above works, accuracy typically degrades by > 5% under binary quantization and 2 \u2212 3% for ternary quantization.To overcome the large accuracy loss coming from insufficient bit-width, the fixed-point quantization is proposed, applying moderate and adjustable quantization bit-width, to maintain accuracy.This quantization scheme was implemented with different methods and algorithms, such as DoReFa-Net [68] and PACT [7].\n\nFinally, there are also non-linear quantization schemes, such as power-of-two (PoT) [17] and additive PoT [19].They replace the multiplication with shifting operations where the distribution of quantization levels becomes unbalanced, having higher precision around the mean and less precision at the two sides.\n\n2.2.2 Mixed-Precision/Scheme Quantization.To explore more quantization potential while preserving the model accuracy, Besides the single scheme quantization, some works [9,39,45,49,54] explore inter-layer mixed-precision quantization by assigning different precisions to layers.For example, HAQ [49] determines the bit-width of each layer by an agent trained with reinforcement learning.DNAS [54] used NAS to search layer-wise bit-width.Furthermore, [29] explored intra-layer mixed quantization to enable different precisions or schemes within each layer.Based on them, hardware designs [5,40] leveraged the intra-layer mixedprecision/mixed-scheme to enable uniformity within each layer, guaranteeing inference acceleration.",
            "score": 0.5160839609766086,
            "section_title": "Non-Transformer DNN Model Quantization",
            "char_start_offset": 9462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 26,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1337
                },
                {
                    "start": 1339,
                    "end": 1381
                },
                {
                    "start": 1381,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "52909828"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "210848001"
                },
                {
                    "start": 1508,
                    "end": 1511,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 1517,
                    "end": 1520,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "211011309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "266374655",
            "title": "A Performance Evaluation of a Quantized Large Language Model on Various Smartphones",
            "text": "In the LLM domain, quantization can be defined as the process of reducing the bit-width of model parameters. LLMs typically store weight parameters as 32-bit floating points. Quantization attempts to maintain these with lower resolution with minimal quality loss. In the literature, LLM-specific quantization has been extensively studied for various resolutions, such as 8-bit and 4-bit integers [11,12,13]. Additionally, there exist distinct methodologies in the literature either focusing on the execution of quantization during the training phase, or quantizing the parameters of a trained model [8]. Depending on the quantization method employed, an LLM's memory footprint and processing requirements can be significantly reduced. However, it is essential to note that quantization introduces a precision loss, which affects inference performance. The evaluation of inference degradation due to quantization is a research area we wish to elaborate on in future work. Nonetheless, we would like to highlight that our preliminary benchmarks have yielded promising results. For our study, owing to accessibility and ease of integration, we have opted to conduct our tests using a quantized model.",
            "score": 0.515461050401643,
            "section_title": "Quantization",
            "char_start_offset": 4742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "231855747",
            "title": "VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference",
            "text": "Deep neural networks (DNNs) continue to achieve groundbreaking accuracy on a range of tasks, including image classification, object detection, machine translation, and natural language processing (NLP) (LeCun et al., 2015). In parallel, hardware designers have been racing to achieve the best performance per watt for running DNN inference on devices ranging from the edge to the datacenter (Sze et al., 2020). While most DNN models are trained in singleprecision floating-point, they can be deployed for inference in lower-precision formats such as half-precision floatingpoint, fixed-point, and low-bitwidth integer depending on the target device and application specifications. Quantizing DNN models to lower precisions allows us to accelerate compute-bound operations such as convolution on highthroughput low-cost math units, conserve memory bandwidth for memory-bound computations, and reduce storage requirements in on-chip buffers and caches. For example, NVIDIA's Ampere Graphics Processing Unit (GPU) architecture supports INT8 and INT4 data types for these purposes (NVIDIA Corporation, 2020). \n\nOne way to quantize a DNN model is through quantizationaware training (QAT). QAT either trains the model from scratch or fine-tunes the trained full-precision model, with quantization operations included in the model. Alternatively, post-training quantization (PTQ) directly quantizes the val-ues of the full-precision model before and during inference without any retraining (Wu et al., 2020). Often, PTQ is more desirable because it does not require access to the complete set of possibly confidential training data, eliminates lengthy fine-tuning, requires little hyperparameter tuning, and provides a turnkey solution for quantizing any DNN. However, PTQ usually results in more accuracy loss than QAT because of the lack of training with quantizers in the loop. With both QAT and PTQ, accuracy loss from quantization varies by precision, model, and quantization algorithm. \n\nQuantization scales high-precision values of a particular range to lower-precision values of a different range.",
            "score": 0.5153295264974405,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 222,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 391,
                    "end": 409,
                    "matchedPaperCorpusId": "211266813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "Technique Performance Cost Scalability GPTQ [80] Achieves high accuracy with post-training quantization to 3 or 4 bits per parameter. \n\nReduces the bit width of the weights (down to 3 or 4 bits), significantly reducing the model size. \n\nAllows inference of large GPT models on a single GPU due to its compressed size. FPTQ [81] Achieves SOTA performance on popular LLMs (BLOOM, LLaMA) with 4-bit weights and 8-bit activations (W4A8). \n\nUtilizes a 4-bit weight quantization strategy, reduces the model size compared to full precision models. \n\nEnables deployment of LLMs on resource-constrained devices by achieving high-performance W4A8 quantization (low memory footprint) without sacrificing accuracy. Norm Tweaking [56] Achieves high accuracy for large language models (GLM-130B, OPT-66B) even at 2-bit quantization. \n\nEnables effective quantization down to even 2 bits, significantly reduces the model size compared to full precision. \n\nAllows for deploying LLMs on devices with limited memory or computational power. FineQuant [83] Up to 3.65\u00d7 higher throughput on LLM inference with minimal accuracy loss for large models (OPT-175B). \n\nFocuses on weight-only quantization, reduces model size efficiently, potentially enabling deployment on less powerful hardware. \n\nEnables deployment of massive LLMs (like OPT-175B) on resource-constrained environments by achieving efficient weight-only quantization with high throughput and minimal accuracy loss. PETALS [62] Achieves faster inference for large language models, with an optimal setup inferring a 176 billion parameter model in 5.5 seconds. \n\n8-bit compression reduces resource requirements. \n\nScales by distributing computations across a network, enabling it to handle even larger models or more inference requests simultaneously. QuantEase [84] Up to 15% better accuracy (perplexity, zero-shot) than GPTQ. Sub-3-bit quantization with minimal accuracy loss. \n\nEnables effective quantization down to 3-bit or even lower precisions, significantly reducing the model size. \n\nQuantizes large models (Falcon-180B) on 1 GPU in 3 hours.",
            "score": 0.5150169582776634,
            "section_title": "TABLE 7. Comparative analysis between different strategies [B]",
            "char_start_offset": 142379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4794921875
        },
        {
            "corpus_id": "234310903",
            "title": "Design and Deployment of an Image Polarity Detector with Visual Attention",
            "text": "Low-level programming languages can improve the efficiency in the implementation of deep neural networks, but at the same time their adoption can increase a product's Time-To-Market significantly. Thus, designers often rely on high-level programming languages that simplify optimization by means of dedicated libraries and tools, and make large use of high-precision data representations. \n\nPost-training quantization is a common approach to deploy the resulting deep-learning models on resourceconstrained devices after training. This solution consists in converting the representation of the network parameters from single-precision floating-point (FP32) to half-precision floating point (FP16), or even 8-bit integer (INT8), coding. FP32 complies with the IEEE floating-point standard and does not prove critical from an accuracy point of view. Many devices, however, inherently support FP16 in their instruction set. In addition to halving memory occupation, this also yields a considerable speedup in inference operations. For example, high-end microcontrollers typically support the Single Instruction Multiple Data (SIMD) paradigm with the FP16 format. \n\nThe INT8 format further reduces memory occupation, and makes fixed-point operations faster, to the detriment of dynamic range and resolution. To minimize the consequent loss in accuracy, a pair of factors modulate integer postquantization as follows: \n\nwhere bias is an integer quantity that sets the center value of represented numbers, and scale is a floating-point 32-bit value that re-calibrates the dynamic range. \n\nAn INT8 representation calls for some steps for floatingpoint re-calibration. These steps avoid that rounding issues cause data losses when data propagate through the architecture. Even in the presence of scaling and biasing, 8-bit quantization can affect a predictor accuracy significantly. At the same time, most commercial tools do not provide software libraries supporting optimization for 8/16 bit data; hence, the model parameters are often converted to a floating-point representation. As a result, the adoption of INT8 ultimately does not reduce the run-time memory occupation.",
            "score": 0.5148387543615889,
            "section_title": "Data Representation",
            "char_start_offset": 19007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2310791015625
        },
        {
            "corpus_id": "276117030",
            "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
            "text": "The key contributions of this study are as follows: \n\n\u2022 We present a comprehensive study on the intertwined effects of QAT budget allocation and the specific choices of quantization functions across 8 models (125M to 3B) and 5 quantization strategies. Our study highlights the unique characteristics and challenges of binary, ternary, and 2/3/4-bit quantization, offering actionable insights and best practices for achieving optimal accuracy-efficiency trade-offs. \n\n\u2022 We introduce ParetoQ, the first systematic, apples-toapples comparison of quantization functions at extreme lowbit settings. Each point in the Pareto chart outperforms prior methods optimized for specific bit widths. Specifically, the 1.58-bit ParetoQ LLaMA-3 8B model reduces the performance gap to full precision by relatively 37.8% compared to the 1-bit Era's LLaMA-3 8B model (Ma et al., 2024), while using only 30% of the training tokens. \n\n\u2022 Our research highlights the potential of 2-bit quantization as a prospective alternative to the traditional 4-bit approach, offering improved accuracy-size trade-off, as underlined in Figure 1. Preliminary speed benchmarks also demonstrate  where BFP + BQAT = Btrain, we explore optimal allocation between full-precision pretraining (BFP) and QAT fine-tuning (BQAT). \"0.0\" represents QAT from scratch, while \"1.0\" indicates full-precision pretraining followed by PTQ. Results on MobileLLM-125M show peak accuracy with \u223c90% of the budget for full-precision pretraining and \u223c10% for QAT fine-tuning. \n\npromising efficiency gains with 2-bit quantization. Nevertheless, widespread adoption will require community-wide efforts, such as INT2 support in NVIDIA tensor cores, to unlock the full benefits of 2-bit quantization.",
            "score": 0.5144837850415722,
            "section_title": "Introduction",
            "char_start_offset": 3796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 54,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1735
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407958984375
        },
        {
            "corpus_id": "10817450",
            "title": "Training Quantized Nets: A Deeper Understanding",
            "text": "The arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in inference performance [5][6][7][8][9]. The most extreme scenario of quantization is binarization, in which only 1-bit (two states) is available for weight representation [10,3,1,4,11,12]. \n\nPrevious work on obtaining a quantized neural network (NN) can be divided into two categories: quantizing pre-trained models with or without retraining [7,13,6,14,15], and training a quantized model from scratch [5,3,4,1,16]. We focus on approaches that belong to the second category, as they can be used for both training and inference under constrained resources. \n\nFor training quantized NNs from scratch, many authors suggest maintaining a high-precision copy of the weights while feeding quantized weights into backprop [3,11,4,16], which results in good empirical performance. There are limitations in using such methods on low-power devices, however, where floating-point arithmetic is not available or not desirable. Another widely used solution using only low-precision weights is stochastic rounding [17,5]. Experiments show that networks using 16-bit fixed-point representations with stochastic rounding can deliver results nearly identical to 32-bit floating-point computations [5], while lowering the precision down to 3-bit fixed-point often results in a significant performance degradation [18]. Bayesian learning has also been applied to train binary networks [19,20]. A more comprehensive review can be found in [4].",
            "score": 0.5141320032383621,
            "section_title": "Background and Related Work",
            "char_start_offset": 2721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "16104422"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "2595906"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "16104422"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "13282575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "251279961",
            "title": "PalQuant: Accelerating High-precision Networks on Low-precision Accelerators",
            "text": "Recently various model compression techniques have been proposed to deploy deep neural networks on resource-constrained edge devices. Among these, fixedpoint quantization [16,30,31] that converts full-precision floating-point operation to low-bit integer counterpart has become the de facto method due to its hardware efficiency. \n\nAt present, most of the commercial CNN accelerators are designed for highprecision (such as INT16/INT8) arithmetic. One important reason for this is because the accuracy of a quantized network is hard to retain as the quantization bit-width narrows. Yet low-precision accelerators lead to orders of magnitude decrease in chip-area and energy consumption compared to the high-precision hardware [10]. This motivates plenty of researchers to study how to improve the accuracy of quantized networks on low-precision accelerators [31,15,28,22,13]. While these methods focus on designing low-precision quantization algorithms, the accuracy of quantized networks may be limited by the low computation precision of accelerators. Different from the above methods, we try to answer the question: Is it possible to deploy high-precision networks on the existing well-designed low-precision accelerator to capture both model accuracy and inference efficiency? To achieve this, a naive solution is decomposing a high-precision network at the bit level and conducting inference on low-precision hardware in a nibble iteration manner [1]. For example, to run an 8-bit network on a 2-bit accelerator, we can split each 8-bit operand into four 2-bit operands so that the original 8-bit multiplication can be carried out in 4 \u00d7 4 = 16 steps using 2-bit multipliers with proper shifting. Although the 2-bit operation is much cheaper than the 8-bit counterpart in terms of chip area and power consumption, there is no gain in inference latency since the total amount of bit operations is unchanged. \n\nIn this paper, we investigate the opportunity of fast and accurate inference of high-precision CNN models on low-precision hardware from the algorithm side. We propose PArallel Low-precision Quantization (PalQuant), a hardwarefriendly, simple yet effective quantization method for efficient CNN acceleration.",
            "score": 0.513819960345955,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "207233273"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 858,
                    "end": 862,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "235761857"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "53719799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "232428330",
            "title": "Integer-Only Zero-Shot Quantization for Efficient Speech Recognition",
            "text": "For accuracy evaluation, we use the NeMo [27] implementation of the pre-trained QuartzNet-15x5, JasperDR-10x5, and Conformer-Large for the full-precision baseline models. The synthetic data is initialized with the uniform distribution from [\u22120.3, 0.3], and is trained using the Adam optimizer [28] with batch size 8, \u03b2 1 = 0.9, \u03b2 2 = 0.999, and learning rate of {0.03, 0.04, 0.05, 0.06} for {200, 250} iterations. We generate 20 such batches and use them for calibration. Figure 3 contains some examples of the generated synthetic data. For calibration, we use the min/max values for clipping. We quantize the baseline models with W8A8 (i.e., 8-bit weights and activations) and W6A8, and do not finetune the models after quantization. All the reported numbers are averaged over 4 different runs (i.e., synthetic data generation and calibration). \n\nBoth the baseline and quantized models are evaluated on the widely used LibriSpeech [24] benchmark. Table 1 compares the WER of the baseline and quantized models with different bit-width settings on dev-clean/other and testclean/other datasets, where hyperparameters are tuned on the dev sets. We also include the model size and BOPs (bitoperations) of each model for comparison. BOP is the total number of bit operations and is a hardware-agnostic proxy to model complexity [29]. For QuartzNet, the W8A8 setting results in a modest WER degradation of 0.23% and 0.29% on the test-clean/other datasets with 4\u00d7 reduction of the model size as compared to the full-precision baseline. The W6A8 setting further reduces the model size to \u223c5\u00d7 of the baseline only within 0.50% and 0.97% WER degradation on the test sets.",
            "score": 0.5133952349190298,
            "section_title": "Accuracy Evaluation",
            "char_start_offset": 10765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1661
                }
            ],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "2191379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "276106922",
            "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
            "text": "To counteract this, we explore absmean-based quantization, which shifts the quantization scale to emphasize the mean absolute value rather than the maximum. By pushing values away from zero and closer to the outer bins, absmean-based quantization yields a denser distribution of nonzero entries, thereby preserving more effective gradient directions for similarity calculations. Interestingly, 1-bit quantization does not exhibit the extreme sparsity problem because its representation inherently omits a zero bin. Figure 3 provides visualizations showing how absmax-based quantization leads to severe zero bin occupancy in low-bit regimes and how absmean-based quantization alleviates this issue at the cost of reducing the fidelity of gradient information, especially in 8-bit and 4-bit precision. \n\nTable 3 compares absmean and absmax quantization for LLaMA2-7B under various bit-widths. At higher precisions (e.g., 16-bit), absmax slightly outperforms absmean on Ty-DiQA and BBH, resulting in a higher overall average (48.59 vs. 47.80). However, as bit-width decreases to 4-bit and 2-bit, absmean consistently yields better or on-par results across all benchmarks, notably improving TyDiQA scores and maintaining competitive performance on MMLU and BBH. This reversal suggests that absmean's tendency to push values away from zero is more robust when the quantization granularity is coarse, mitigating the zero-bin effect that degrades similarity computation. Empirical results on selected data percentage. Our main experiments follow LESS (Xia et al., 2025) in selecting only 5% of data for fair comparisons, although there has yet been empirical results on whether or not this is the optimal percentage. We experiment on selecting 0.1%, 0.5%, 1%, 2%, and 10% in addition to 5% of data, selected using QLESS specifically at 16-bit model precision and 1-bit gradient store quantization using Llama 2 7B and Qwen 2.5 7B on the 3 benchmarks.",
            "score": 0.5130646052421431,
            "section_title": "Ablation Studies and Analysis",
            "char_start_offset": 18099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 1544,
                    "end": 1562,
                    "matchedPaperCorpusId": "267522839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "277452419",
            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
            "text": "Fig. 4: GGUF Progressive quantization. Different network architectures exhibit varying sensitivities to quantization. Certain layers, such as initial and output layers, are highly sensitive to quantization errors and require higher precision representation, whereas some intermediate layers, such as the Feedforward layers in Transformers, are more robust to low-precision quantization. For example, studies have shown that Attention layers in Transformers are particularly sensitive to precision degradation during quantization, while Feedforward layers maintain acceptable performance even after INT8 quantization. Similarly, in vision models, shallow convolutional kernels are more sensitive to quantization errors than deeper feature mappings [44].Further investigations from ZeroQuant-V2 reveal that activation quantization is generally more sensitive than weight quantization, especially in larger models. For models exceeding 10B parameters (e.g., OPT-66B [45] and BLOOM-176B [46]), activation quantization leads to more significant accuracy degradation compared to smaller models [47]. This phenomenon highlights the need for careful handling of activations during quantization in large-scale language models. \n\nOur experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5). This advantage amplifies at extreme compression (70%+), where quantization preserves functional coherence while pruning induces hemorrhage.",
            "score": 0.5128573861718937,
            "section_title": "2-bits 4-bits 16-bits",
            "char_start_offset": 30817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "249395624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "220936526",
            "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands",
            "text": "In recent years, numerous research efforts have been devoted to quantizing neural network architectures to sub-byte values while preserving the accuracy of full-precision model [2]- [6], [10], [11], [13], [17]- [21]. Furthermore, several approaches were proposed on developing compressed neural networks through the use of weight pruning [7], tensor decomposition [8], [14]- [16], compact network architecture design, etc. Learning quantization for numerical precision of 4-bits has been shown to be effective in recent works [1], [2], [10], [11], [18], in turn creating demand for efficient execution of matrix multiplication kernel between 4-bit weights and 8-bit activations on existing CPUs and DNN hardware accelerators. However, the mismatch between read bandwidth of 4-bit weights, 8-bit activations, and write bandwidth of accumulators poses major obstacles in implementing such an instruction for matrix multiplication hardware in CPUs and DNN hardware accelerators. The use of existing instructions (that execute MAC operations between symmetric bit-width operands) to perform such matrix multiplication between asymmetric bit-width operands will not be able to fully exploit the benefit of 4-bit weight quantization. On the other hand, failure to match the vector width of weights, activations, and accumulators by a matrix multiply instruction will either under-utilize expensive CPU resources (e.g., register file port bandwidth, etc.) or require significant increase in the DNN hardware accelerator resident SRAM resources (e.g., size of accumulator buffers, etc.) to realize any throughput benefit from 4-bit quantization. None of the recent works on 4bit model quantization reports performance benefit on either existing CPUs or hardware accelerators.",
            "score": 0.5125443691698427,
            "section_title": "II. BACKGROUND AND RELATED WORK",
            "char_start_offset": 3990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1767
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "11244259"
                },
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "70055866"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "30684700"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "53761197"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "17864746"
                },
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "186206998"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "203836150"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "52920066"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "198903430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "266421411",
            "title": "Deployment of Machine Learning Algorithms on Resource-Constrained Hardware Platforms for Prosthetics",
            "text": "With the success of deep neural networks and their everincreasing sizes, the quantization of neural networks has emerged as a fundamental technique to reduce model size and memory footprints. Remarkable progress in this area has led to quantized neural networks achieving similar levels of accuracy as their full-precision counterparts [14], [15], [17]. In neural network quantization, three key components can be targeted: weights, activations, and gradients [40]. In this study, our focus is on quantizing weights and activations. \n\nWhile various approaches have explored 4-bit [41], [42], binary [43], [44], [45], [46], [47], and adaptive [48], [49] quantization, our work centers on 8-bit quantization, which is widely supported by most microcontrollers (MCUs). The quantization of parameters offers the following advantages:  [50], [51], or done without re-training, a process that is often referred to as Post-Training Quantization [42], [52], [53]. In this work, we use the Quantization-Aware Training [40], [54] since it has proven to achieve higher accuracy value [41].",
            "score": 0.512116407633654,
            "section_title": "B. NETWORK QUANTIZATION",
            "char_start_offset": 6669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1078
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "1875476"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "195601375"
                },
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "102352789"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "11244259"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "67732874"
                },
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "221136343"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "226203265"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "213004198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392333984375
        },
        {
            "corpus_id": "246548397",
            "title": "An Integrated Approach to Produce Robust Deep Neural Network Models with High Efficiency",
            "text": "When we quantize DNNs with precision higher than binary, such as ternery and 4-bit quantization, zero is in quantization levels. In fact, we find that a large proportion of weights will be quantized to zero. This suggests that a ternary or 4-bit quantized model can be further simplified via channel pruning. However, such simplification requires structure sparsity of DNN architecture. In our study, we use the algorithm 1 as before with the projection replaced by ternary and 4-bit respectively. As shown in  5) shows that a small factor of natural loss, \u03b1 = 1 and \u03b2 in (8), can push the sparsity to be more structured. Meanwhile, the deepness of models also has an impact on the structure of sparsity. The deeper the more structured the sparsity is. We see in Table 5 that, under the same settings, the structure of sparsity increases as the model becomes deeper. Figure 1 shows the difference between a unstructured sparsity of a ternary ResNet20 with adversarial training and a much more structured sparsity of ResNet56 with natural training. The trade-off function not only improves the natural accuracy of models with merely minor harm to robustness but also structures the sparsity of high-precision quantization, so further simplification of models can be done through channel pruning.",
            "score": 0.511616112786775,
            "section_title": "Sparse neural network delivered by high-precision quantization",
            "char_start_offset": 14140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1294
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "258999233",
            "title": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models",
            "text": "For all the tasks, we adopt AdamW (Loshchilov and Hutter, 2018) as the optimizer and search batch size in {16, 32}. For full-parameter finetuning the learning rate for PreQuant is searched within {1e-5, 2e-5, 3e-5, 4e-5} for BERT base , RoBERTa base , and RoBERTa large and {1e-4, 2e-4, 3e-4} for T5 Encoder. For PreQuant, the learning rate is searched within {1e-4, 3e-4, 5e-4, 7e-4, 9e-4}. We set the dropout rate to 0.1 and weight decay to 0.01. For all tasks, the model is trained for 10 epochs at maximum and the best performance on the validation set is reported. Experiments are conducted upon the Huggingface Transformers library (Wolf et al., 2020). \n\nA During investigation, we find quantization is more challenging in small datasets. We further explore the effect of data size on quantization and finetuning. To this end, we randomly sample MNLI training set to {2k, 4k, 6k, 8k} examples and finetune T5 Encoder on them. As seen in Table 6, smaller data size leads to larger performance gap between the full-precision model and the quantized one. \n\nA.3 Visualization of Quantization Error As shown in Fig. 5, when the number of bits for weight is 8, the performance of all quantization methods is close. However, when the bit-width decreases to 4, performance disparities between various approaches start to become apparent. PTQ fails to predict reasonable answers on 4-bit quantization, indicating that the quantization error is too strong to be minimized with a modest amount of calibration data. QAT and PreQuant still remain an acceptable performance for 4-bit quantization.",
            "score": 0.511414110373679,
            "section_title": "A.1 Training Details",
            "char_start_offset": 24168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 63,
                    "matchedPaperCorpusId": "53592270"
                },
                {
                    "start": 638,
                    "end": 657,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "268032411",
            "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
            "text": "We have presented a comprehensive evaluation of quantization strategies for LLMs, demonstrating the trade-offs between model efficiency and performance degradation across various benchmarks. By employing a structured evaluation framework that assesses models in terms of knowledge & capacity, alignment, and efficiency, we aim to offer valuable insights into the scalability and practical application of quantized LLMs. Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower. Moreover, the results suggest that perplexity can be a reliable performance indicator for quantized LLMs on various evaluation benchmarks. SpQR effectively quantizes LLMs to an extreme level of 2 bits by isolating outlier weights and maintaining high precision during computation. When memory constraints exist and inference speed is a secondary concern, LLMs quantized to lower bit precision with a larger parameter scale can be preferred over smaller models. Additionally, we highlight the need for engineering effort and hardware support to efficiently deploy quantized LLMs in real-world scenarios.",
            "score": 0.5110896201316686,
            "section_title": "Conclusion",
            "char_start_offset": 19055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "1757803",
            "title": "The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning",
            "text": "Recently there has been significant interest in training machine-learning models at low precision: by reducing precision, one can reduce computation and communication by one order of magnitude. We examine training at reduced precision, both from a theoretical and practical perspective, and ask: is it possible to train models at end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude speedups? We present a framework called ZipML to answer these questions. For linear models, the answer is yes. We develop a simple framework based on one simple but novel strategy called double sampling. Our framework is able to execute training at low precision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of applications, and show that it enables an FPGA prototype that is up to 6.5x faster than an implementation using full 32-bit precision. We further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When applied to linear models together with double sampling, we save up to another 1.7x in data movement compared with uniform quantization. When training deep networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally, we extend our framework through approximation to non-linear models, such as SVM. We show that, although using low-precision data induces bias, we can appropriately bound and control the bias. We find in practice 8-bit precision is often sufficient to converge to the correct solution. Interestingly, however, in practice we notice that our framework does not always outperform the naive rounding approach. We discuss this negative result in detail.",
            "score": 0.5108087483153654,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473388671875
        },
        {
            "corpus_id": "250048704",
            "title": "QReg: On Regularization Effects of Quantization",
            "text": "Figure 2 illustrates part of the experiments that we ran to test our hypothesis. Figure 2 shows results of nine different test configuration in nine squares. Each square consists of 20 cells, which as we discussed in Section 4.1.2, corresponds to the test accuracy difference between full precision model and quantized models when different augmentation is used. \n\nFrom left to right, each column shows the quantization levels (2-bit, 4-bit and 8-bit) that was used for our tests. From top to bottom, we have tested our hypothesis on Resnet20 on cifar10, Resnet18 on cifar100 and YOLOv5n on VOC dataset respectively. As it can be seen (i.e. the overall cell colors in the right column squares are green), regardless of model, dataset, augmentation or quantization technique, 8-bit quantized models generally have better generalization compared to full precision models. As we discussed in 2.3, we believe that this performance improvement corresponds to the regularization effect of quantizing the models. As we predicted, this regularization effect is correlated with the quantization level. Moving from left to right columns, the overall color codes for each cell changes from red to green. Which indicates that quantized models are generalizing better compared to their full precision counterparts as we use higher precision quantized models (from 2-bit to 8-bit). Regardless of the model, dataset, data augmentation or quantization technique, the generalization difference shrinks and even get worse as we use less precision for quantization. Another way to evaluate how quantization is helping with better generalization, is to compare the error of quantized and full precision models. Table 1 shows the performance improvement relative to model error. We averaged the accuracy of each square in Figure 2. We then, calculated the performance improvement relative to error using the following formula: \n\nWhere f val and qval are the average accuracy of full precision and quantized model over 19 different augmentation setups.",
            "score": 0.5106102336194183,
            "section_title": "RESULTS",
            "char_start_offset": 13149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1905
                },
                {
                    "start": 1908,
                    "end": 2030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "208631617",
            "title": "Non-Volatile Memory Array Based Quantization- and Noise-Resilient LSTM Neural Networks",
            "text": "So, the NVM weight array size used for this task is 356 \u00d7 1024 (m = 100, n = 256 in Fig. 3). After 8000 training iterations, the training perplexity and accuracy are measured. As shown in Table I, using 2-bit weight and 2-bit ADC/DAC is sufficient to produce result within 5% degradation compared to the floating point baseline case. Comparing with the result from the Penn Treebank, a lower bit-precision requirement on the weight and ADC/DAC is needed for the simpler character prediction task. To conclude from both NLP tasks, a 4-bit weight along with 4bit ADC/DAC can ensure almost-zero degradation for LSTM network performance. Such bit-width requirements for training also ensure the inference performance as our training approach is quantization-aware. Quantization-aware model-based training ensures that the forward pass matches precision for both training and inference.",
            "score": 0.510580414099489,
            "section_title": "B. Natural Language Processing (NLP)",
            "char_start_offset": 17041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 881
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "232380042",
            "title": "A Practical Survey on Faster and Lighter Transformers",
            "text": "Compression Speed-up Mean Relative Performance BERT BASE [24] 1.0\u00d7 1.0\u00d7 100% DistilBERT [99] 1.7\u00d7 1.6\u00d7 97% MiniBERT [120] 6.0\u00d7 2.6 \u2212 4.3\u00d7 97 \u2212 99% TinyBERT [54] 7.5\u00d7 9.4\u00d7 97% \n\nMixed-Precision [80]: Modern GPUs and TPUs perform at least twice as many half-precision (16 bits) float operations as single-precision (32 bits) ones. A popular approach to accelerate training and reduce memory consumption is storing and computing the weights, activations, and gradients in half-precision. A master copy of the weights is stored in single-precision for numerical stability and minimal performance loss. Thanks to NVIDIA's Automatic Mixed-Precision included in some of the most popular deep learning libraries, namely TensorFlow, PyTorch, and MXNet, using mixed precision can be as simple as adding one line of code. Consequently, we highly recommend mixedprecision. Jacob et al. [51] improved over this approach by quantizing both weights and activations as 8-bit integers and biases as 32-bit integers, effectively allowing inference to be performed using integer-only arithmetic. Given a parameter matrix  ,  -bit quantization rounds each parameter to one of 2  codewords corresponding to bins evenly spaced by a scale factor  and shifted by a bias  computed as follows: \n\nEach parameter  , is quantized to its nearest codeword, and dequantized as: \n\nIn order to mitigate the performance loss associated with the low-precision approximation, Quantization Aware Training (QAT) [51] quantizes the parameters during training. Since quantization is not differentiable, gradients are approximated with a straight-through approximator [7]. Notably, Zafrir et al. [138] quantized all matrix product operations in BERT fully connected and embedding layers during training, reducing the memory footprint by 4\u00d7 while retaining 99% of the original accuracy on the GLUE [124] and SQuAD [94] tasks.",
            "score": 0.5104771079090495,
            "section_title": "Model",
            "char_start_offset": 29926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 116,
                    "end": 121,
                    "matchedPaperCorpusId": "202122780"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "3297437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3759765625
        },
        {
            "corpus_id": "237698061",
            "title": "Trigonometric Inference Providing Learning in Deep Neural Networks",
            "text": "Conventional deep neural networks are trained using 32-bit parameters. The training cost can be reduced by reducing the precision of the parameters. Extreme examples in this direction are binarized neural networks (BNNs) [11] and XNOR-Net [12]. By limiting the weights and activations to 1 and \u22121, not only is memory consumption reduced, a large number of multiplication operations are also avoided. A BNN is suitable for devices with a weak performance, such as mobile and embedded terminals, whereas such a sacrifice results in a loss of accuracy. Moreover, the convergence of the BNN models is prolonged. A large number of training iterations cause the burden of additional training. \n\nModels with comparatively higher precision have also been studied. It was verified that training with a lower precision does not significantly influence the accuracy of image classification. For example, Banner [13] showed that 8-bit neural networks can achieve a comparable accuracy to their full precision counterparts. In addition to image classification, efficient low-precision networks are also applicable to other tasks, such as language translation [14]. Recently, sparsity of the activations in DNNs is also utilized for quantization algorithm design. In [15], the zero value bits are leveraged to trim an 8 bits quantized value to lower n-bits resolution by picking the most significant n bits while skipping leading zero value bits.",
            "score": 0.5098681159545798,
            "section_title": "Training with Low Precision",
            "char_start_offset": 8278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1432
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "258048735",
            "title": "AutoQNN: An End-to-End Framework for Automatically Quantizing Neural Networks",
            "text": "The heavy computational burden immensely hinders the deployment of deep neural networks (DNNs) on resource-limited devices in real application scenarios. Quantization is a technique which compresses DNN weight and activation values from high-precision to lowprecision. The low-precision weights and activation occupy smaller memory bandwidth, registers, and computing units, thus significantly improving the computing performance. For example, AlexNet with 32-bit floating-point (FP32) can only achieve the performance of 1.93TFLOPS on RTX2080Ti since the bandwidth constraints. However, the low-precision AlexNet with 16-bit floating-point (FP16) can achieve the performance of 7.74TFLOPS on the same device because the required bandwidth is halved while the available computing units are doubled 1 . The FP16 AlexNet is four times faster than the FP32 one on RTX2080Ti GPU. Therefore, quantization can reduce the computing budgets in DNN inference phases and enable DNN model deployment on resource-limited devices. \n\nHowever, unreasonable quantizing strategies, such as binary [2] and ternary [3], tend to seriously affect DNN model accuracy [4,5] and lead to customer frustration. Lower bitwidth usually leads to higher computing performance but larger accuracy degradation. The quantization strategy selection dominates the computing performance and inference accuracy of models. In order to balance the computing performance and inference accuracy, many previous investigations have tried to select a unified bitwidth for all layers in DNNs [6,7] cautiously. However, many studies show that different layers of DNNs have different sensitivities [8,9] and computing budgets [10]. Using the same bitwidth for different layers is hard to obtain superior speed-up and accuracy. To strike a fine-grained balance between efficiency and accuracy, it is strongly demanded to explore desirable quantizing schemes [11] and reasonable mixed-precision policies [10,12] for various neural architectures. The brute force approaches are not feasible for this exploration since the search space grows exponentially with the number of layers [8]. Heuristic explorations work in some scenarios but greatly rely on the heavy workloads of domain experts [11].",
            "score": 0.5098655417793025,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 619,
                    "end": 621,
                    "matchedPaperCorpusId": "218614013"
                },
                {
                    "start": 1080,
                    "end": 1083,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1148,
                    "end": 1150,
                    "matchedPaperCorpusId": "2753399"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "218674432"
                },
                {
                    "start": 1550,
                    "end": 1552,
                    "matchedPaperCorpusId": "203605628"
                },
                {
                    "start": 1654,
                    "end": 1656,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 1910,
                    "end": 1914,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1955,
                    "end": 1959,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "277627966",
            "title": "Achieving binary weight and activation for LLMs using Post-Training Quantization",
            "text": "Although these works can achieve satisfactory 4 bit quantization, they struggle to further reduce the quantization bit-width, which implies that the simple RTN quantization method is not sufficient for very low-bit quantization with high performance. \n\nAnother line of works adopt vector quantiza-tion technique (Liu et al., 2024a;van Baalen et al., 2024). These approaches quantized the model weights via grouping the weights and assigning the weights in the same group to the same value. Those methods usually yield more accurate quantization models with smaller size, but cannot be speed-up the inference speed, as they have to dequantization the weight to recover float point format before computation. \n\nTo address these issues, we propose a quantization framework, that can simultaneously achieve small model size, fast computation, and highquality inferece outputs. It consists of three parts: 1-bit weight quantization with another bit for finegrain grouping, and 1 \u00d7 4 bits activation quantization using 4 times 1 bit channel to represent 4 bit quantization. By this way, we are able to compute the inner loop vector product in pure binary operations drastically boosting the inference speed as well as reduce the model size. The main contributions of this work are as follows: \n\n\u2022 We propose a W(1+1)A(1\u00d74) post-training quantization framework, with the boolean operations for the inner-loop multiplicationsummation operations. \n\n\u2022 We present an EM-based algorithm for searching boolean weight quantization with finegrouping, noticeably improved the performance compared with the usual RTN approach. \n\n\u2022 Our method outperforms existing PTQ methods for 2 or 1 bit-widths. On the Wikitext2 benchmark, our method achieves perplexities of 8.58 and 8.89 on LLaMA-7B and LLaMA2-7B, respectively, using only the W2A4 quantization setting. This significantly surpasses the existing state-of-the-art methods and is comparable to the performance of 5.68 and 5.47 achieved by the original full-precision models.",
            "score": 0.5097058593691229,
            "section_title": "Introduction",
            "char_start_offset": 1666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "261030579",
            "title": "ResQ: Residual Quantization for Video Perception",
            "text": "Efficiency metric and evaluation. As current deep learning frameworks (e.g. PyTorch) currently support 32, 16, and 8-bit kernels only, we follow the common practice in quantization [3,58] and rely on Bit Operations (BOPs) to measure the computational costs: BOPs are computed by re-weighting the multiply-accumulate (MAC) count of a model accounting for bit-widths of weights and activations. Since residual quantization utilizes different bit-width for the keyframe and the residual frames, we report precision in a unified notation: for instance, W8A8\u2192W8A4 indicates bit-width of 8-8, for weights-activation of keyframes, and bit-width of 8-4 for weight-activation of residual frames. \n\nFollowing [21,46,14], we split each video into sequences of length T , whose first frame is treated as keyframes. BOPs and accuracy metrics are averaged over all frames in a sequence. For the Dynamic-ResQ experiments, the policy function is considered in the BOP count. \n\nQuantization methods. We conduct our experiments with both PTQ and QAT, as explained in Sec. 2. For PTQ, we use c = 64 calibration samples and r = 20 line search points for selecting the activation quantization range. For QAT, we initialize the scale factor s of the quantizer via PTQ, and propagate gradients through the rounding operation in Eq. ( 1) by means of a straight-through estimator [4]. We either apply the same scaling factor s for the whole tensor, i.e. tensor quantization, or have separate s for each channel of the weight tensor, i.e. channel quantization. Unless specified otherwise, we utilize tensor quantization. For the dynamic policy, we fix \u03c4 = 0.0003 for all experiments.",
            "score": 0.509618759706354,
            "section_title": "Experiments",
            "char_start_offset": 15830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1657
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "13996487"
                },
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "218628799"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "215238650"
                },
                {
                    "start": 703,
                    "end": 706,
                    "matchedPaperCorpusId": "247315646"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "233388124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2374267578125
        },
        {
            "corpus_id": "266624782",
            "title": "Research on Efficient CNN Acceleration Through Mixed Precision Quantization: A Comprehensive Methodology",
            "text": "The maximum operating frequency is 200MHz, and the supported data bit-widths are 8bit and 16-bit. \n\nAdditionally, quantization [13] can be used to reduce model sizes and hardware resource consumption, such as replacing original 32-bit floating-point operations with lower precision fixed-point numbers like 8-bit or 16-bit. Jacob et al. [15] proposed an integer quantization method, which uniformly quantizes both weight and activation to 8-bit. Furthermore, there are ultra low-bit quantization methods, such as ternary quantization [16] which quantifies weights into {-w, 0, +w}, and even binary quantization neural networks [17], which quantize weights and activation values to 1 or -1. \n\nHowever, using a unified quantization bit-width in ultra low-bit-width situations would significantly affect CNN performance. A highly effective solution to this problem is through mixed precision quantization [20]. It allows each layer www.ijacsa.thesai.org of the CNN model to have different quantization bit-widths. which can greatly preserve the performance. Lin et al. [21] proposed an analytical solution to address the fixed-point quantization problem. It seeks an optimal bit-width allocation strategy across network layers by optimizing the Signal-to-Quantization-Noise Ratio (SQNR). Wang et al. [22] designed a Hardware Aware Quantization (HAQ) algorithm that incorporates inference speed information evaluated by a hardware simulator into the training process, which utilized reinforcement learning to automatically determine quantization strategies. It reduces latency by 1.4-1.95 times and energy consumption by 1.9 times. However, the current methods face challenges in their applicability to FPGA platforms or in terms of high time and space complexity when searching for mixed precision strategies. \n\nIn conclusion, if we have an efficient mixed precision search algorithm and can apply the strategies obtained by this algorithm to FPGA platforms; it will be greatly significant for the application of deep learning on AIoT devices. Therefore, we propose a method for implementing a mixed precision CNN model on FPGA, co-designing from software and hardware aspects.",
            "score": 0.5095255696425587,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "51813470"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "238416197"
                },
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "102350477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1943359375
        },
        {
            "corpus_id": "67750088",
            "title": "Low-bit Quantization of Neural Networks for Efficient Inference",
            "text": "Recent machine learning methods use increasingly large deep neural networks to achieve state of the art results in various tasks. The gains in performance come at the cost of a substantial increase in computation and storage requirements. This makes real-time implementations on limited resources hardware a challenging task. One popular approach to address this challenge is to perform low-bit precision computations via neural network quantization. However, aggressive quantization generally entails a severe penalty in terms of accuracy, and often requires retraining of the network, or resorting to higher bit precision quantization. In this paper, we formalize the linear quantization task as a Minimum Mean Squared Error (MMSE) problem for both weights and activations, allowing low-bit precision inference without the need for full network retraining. We propose the analysis and the optimization of constrained MSE problems for performant hardware aware quantization. The proposed approach allows 4 bits integer (INT4) quantization for deployment of pretrained models on limited hardware resources. Multiple experiments on various network architectures show that the suggested method yields state of the art results with minimal loss of tasks accuracy.",
            "score": 0.5093484242340965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "198318067",
            "title": "Thread: Towards fine-grained precision reconfiguration in variable-precision neural network accelerator",
            "text": "Such architectures could selfadapt to different problem complexities and accuracy requirements, and work with neural models with different precisions, thereby always offering on-demand network capacity and increasing system energy efficiency in different scenarios. In theory, a larger precision tuning range and a finer tuning granularity mean that the accelerator could have sufficient operating options to choose and optimize when it faces a given deep learning problem. \n\nAccording to the neural network training results by many other researchers' studies, we can find that different neural network models on different datasets have different bit widths to achieve a good accuracy, as shown in Fig. 1. From these observations and other potential possibilities, we can conclude that different networks for different tasks need different bit widths for both activations and weights to achieve the goal of high efficiency. The existing variableprecision accelerators [24,25,26,27] cannot fit in all the possible bit widths. They have to work in a higher-precision mode like 8-bit operations to replace 5-bit, 6-bit or others, which is a waste of computing and power resources. Thus, if accelerators can have a wide precision tuning range and a finer tuning granularity, we can always select the lowest possible bit width while meeting the accuracy and resource requirements such that the performance and energy efficiency can be both maximized. Therefore, it is beneficial to design a flexible neural network accelerator supporting continuously variable bit widths for both activations and weights. \n\nTable I shows the state-of-the-art accelerators that support variable precisions. A-P represents the activation precision and W-P represents the weight precision. BRein [24] simply supports binary and ternary weights. Stripes [25] and UNPU [26] support one kind of variability for only activations or weights, but the other uses a fixed precision. Bit-Fusion [27] supports both variabilities but its bit width must be a power of two. It is a long jump from 4 to 8 bits for Bit-Fusion and it is even worse from 8 to 16 bits when using 9 bits, 10 bits or so is sufficient. To better adapt to different precision requirements of neural network models, this work presents a finer-grained variable-precision DNN accelerator named Thread.",
            "score": 0.5078392142117412,
            "section_title": "Introduction",
            "char_start_offset": 1795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "29661460"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "3784424"
                },
                {
                    "start": 975,
                    "end": 978,
                    "matchedPaperCorpusId": "3861747"
                },
                {
                    "start": 978,
                    "end": 981,
                    "matchedPaperCorpusId": "21681898"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "29661460"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "3784424"
                },
                {
                    "start": 1842,
                    "end": 1846,
                    "matchedPaperCorpusId": "3861747"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "21681898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193603515625
        },
        {
            "corpus_id": "261531646",
            "title": "Memory Efficient Optimizers with 4-bit States",
            "text": "Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.",
            "score": 0.507635665266441,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "3323727",
            "title": "Model compression via distillation and quantization",
            "text": "In terms of size, this model is more than 2\u00d7 smaller than ResNet18 (but has higher accuracy), and is 4\u00d7 smaller than ResNet34, and about 1.5\u00d7 faster on inference, as it has fewer layers. This is state-of-the-art for 4bit models with 18 layers; to our knowledge, no such model has been able to surpass the accuracy of ResNet18. \n\nWe re-iterated this experiment using a 4-bit quantized 2xResNet34 student transferring from a ResNet50 full-precision teacher. We obtain a 4-bit quantized student of almost the same accuracy, which is 50% shallower and has a 2.5\u00d7 smaller size. One key question we are interested in is whether distillation loss is a consistently better metric when quantizing, compared to standard loss. We tested this for CIFAR-10, comparing the performance of quantized training with respect to each loss. At 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss. This strongly suggests that distillation loss is superior when quantizing. For details, see Section A.4.1 in the Appendix. \n\nImpact of Heuristics on Differentiable Quantization. We also performed an in-depth study of how the various heuristics impact accuracy. We found that, for differentiable quantization, redistributing bits according to the gradient norm of the layers is absolutely essential for good accuracy; quantiles and distillation loss also seem to provide an improvement, albeit smaller. Due to space constraints, we defer the results and their discussion to Section A.4.2 of the Appendix. \n\nInference Speed.",
            "score": 0.5073581656222779,
            "section_title": "LARGER DATASETS",
            "char_start_offset": 26005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 1917
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.880859375
        },
        {
            "corpus_id": "210942717",
            "title": "Compact Recurrent Neural Networks for Acoustic Event Detection on Low-Energy Low-Complexity Platforms",
            "text": "Feed-forward propagation through a neural network requires vector/matrix/tensor multiplication and convolution. Therefore, all the core features employed in signal processing can be used for neural network computations. For this reason, ARM developed the CMSIS-NN framework for neural network propagation on top of DSP libraries [21]. The CMSIS-NN library maximizes performance and energy efficiency of common deep learning kernels on top of Cortex-M series cores. \n\nLike for DSP, truncation of floating point numbers to 8 or 16-bit fixed point numbers improves the execution time and reduces the memory footprint. According to [21], 8-bit quantization achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency in image classification with CIFAR10 dataset. On the other hand, since quantization implies loss of precision, we could expect a direct impact on the final prediction performance. However, the authors of [44] experimented different kinds of quantization in image classification, achieving a 20% drop in model size without significant loss of accuracy. In the following, we describe the quantization process from a 32-bit floating-point to an 8-bit fixed-point representation. From now on, we will assume that floating point numbers has infinite precision and we will use the nomenclature typically used for DSP. \n\nQuantization describes a real number with finite resolution. When a uniform quantization is applied, three parameters are used to define the fixed point representation: bit-width, stepsize (resolution) and dynamic range [44]. These parameters are correlated by the following expression: \n\nwhere bitwidth \u2212 1 accounts for the bit used to represent the sign. Stepsize is the minimum step between two fixed point numbers and will be always chosen as a power of two for convenience with binary representation. An equivalent formulation of Eq. 8 can be obtained by considering the number of bits used for the decimal and integer parts of numbers: \n\nwhere, integer is the number of bits used to represent the integer and decimal is the number of bits used for the decimal part. +1 in the last equation accounts for the sign bit.",
            "score": 0.5073573070331882,
            "section_title": "A. Quantization",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "649645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34423828125
        },
        {
            "corpus_id": "75136732",
            "title": "Secure Evaluation of Quantized Neural Networks",
            "text": "Deep learning models are at the core of many real-world tasks like computer vision, natural language processing and speech recognition. However, in spite of their high accuracy for many such tasks, their usage on embedded devices like mobile phones, which have tight resource constrains, becomes restricted by the large amount of storage required to store the model and the high amount of energy consumption when carrying the computations that are typically done over floatingpoint numbers. To this end, researchers in the machine learning community have developed techniques that allow weights to be represented by low-width integers instead of the usual 32-bit floating-point numbers, and quantization is recognized to be the most effective such technique when the storage/accuracy ratio is taken into account. \n\nQuantization allows the representation of the weights and activations to be as low as 8 bits, or even 1 bit in some cases [11,53]. 3 This is a long-standing research area, with initial works already dating back to the 1990s [23,4,61,48], and this extensive research body have enabled modern quantized neural networks to have essentially the same accuracy as their full-precision counterparts [12,68,26,30,52], even with large CNN architectures like AlexNet [43], VGGNet [59], GoogleNet [60] and ResNet [31].",
            "score": 0.5072143217400784,
            "section_title": "II. DEEP LEARNING AND QUANTIZATION",
            "char_start_offset": 21524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1322
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "62160723"
                },
                {
                    "start": 1043,
                    "end": 1045,
                    "matchedPaperCorpusId": "7418039"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "matchedPaperCorpusId": "13085796"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "904909"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "29200607"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 1301,
                    "end": 1305,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        },
        {
            "corpus_id": "274305769",
            "title": "Generative AI on the Edge: Architecture and Performance Evaluation",
            "text": "The models studied in this research did not encounter outof-memory (OOM) errors or hardware restarts during cluster deployment, demonstrating robustness and stability in handling large workloads. While quantization effectively reduces memory usage and accelerates inference, it often sacrifices accuracy, especially at lower precision levels (e.g., 4 bits). \n\nTo evaluate accuracy, we use the Winogrande benchmark dataset 15 , designed for common-sense reasoning, and the LM-Evaluation Harness [16] framework with its inference server to serve our quantized models. The dataset includes Natural Language Inference (NLI) tasks, where models select the answer with the highest log likelihood, which is then compared to the correct label. Model accuracy on Winogrande benchmark varies significantly, with InternLM leading at 0.8, followed by Gemma at 0.7, and Llama3 at 0.69. Mistral, Llama2, and Phi all share an accuracy of 0.68. Yi and Zephyr score lower at 0.49 and 0.46, respectively. These differences largely stem from model architecture and size [5]. It is important to note that we evaluate the non-fine-tuned variants of the models, which typically serve as a proxy for the accuracy degradation in downstream models [5]. We selected pre-trained models rather than fine-tuned versions due to the alignment in fine-tuning processes, which often leads to performance enhancements that are not directly comparable to the base models.",
            "score": 0.5071789972251938,
            "section_title": "C. Deployment Stability and Accuracy",
            "char_start_offset": 16969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 357
                },
                {
                    "start": 360,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86474609375
        },
        {
            "corpus_id": "220363587",
            "title": "FracBits: Mixed Precision Quantization via Fractional Bit-Widths",
            "text": "full precision (FP) models, we also add the top-1 accuracy of FP models reported in corresponding papers and report the relative ac-curacy drop for each method. Here we also include results of directly applying our method on PACT scheme, and denote this as FracBits-PACT. Comparing absolute accuracy, FracBits-PACT achieves comparable performance as stateof-the-art mixed precision methods. Note DNAS uses several tricks in training to boost performance, thus its result is not directly comparable to others. Comparing relative accuracy drop, our method achieves least performance drop on 3-bit ResNet18. Enhanced by SAT quantization method, FracBits-SAT further improves over SAT baseline and achieves only 0.8% accuracy drop on 3-bit ResNet18 and even a 0.4% performance gain on 4-bit ResNet18. Note DQ and our method are one-shot differentiable method which only need one pass of training to obtain the final model, and are much more efficient than the other mixed quantization approaches (DNAS, AutoQ, US).\n\nTo have a more intuitive understanding of the learned bitwidth structure from our algorithm, we plot the bit-widths from different layers for 3-bit MobileNet V2 and ResNet18, as shown in Fig. 2. We find that models for mixed quantization contrained on computational cost generally uses more bit-width on the late stage of the network, potentially due to the larger computation cost of early layers than later layers. Also, in MobileNet V2, depth-wise convolutions result in more bit-width than point-wise convolutions due to their low computation cost.\n\nFor model size constrained quantization, we show comparison with previous methods Deep Compression (Han, Mao, and Dally 2015), HAQ and uniform quantization approach SAT in Table 4. Our FracBits-SAT outperforms mixed precision methods HAQ and strong uniform quantization baseline SAT on all experimented bit-widths consistently. Note that FracBits has an over 3% absolute gain on top-1 accuracy over SAT on 2-bit MobileNet V1/V2. On the challenging 3-bit setting where quantized models already achieve similar performance as full precision ones, FracB",
            "score": 0.5071120563139115,
            "section_title": "Quantization with Layer-wise Precision",
            "char_start_offset": 25347,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3994140625
        },
        {
            "corpus_id": "247318473",
            "title": "Neural Network Training on In-Memory-Computing Hardware With Radix-4 Gradients",
            "text": "Quantized training methods can be split into two broad categories: quantization-aware training and quantized training. In quantization-aware training only the forward MVM associated with inferencing is quantized while the training-specific MVMs are kept at a higher precision such as 32-bit floating point (FP32). This creates a more efficient model during inferencing. Binarized neural networks and wide-reduced precision networks represent two prominent examples associated with quantization-aware training, through which similar testing accuracies as FP32 models has been achieved while reducing activations and weights to as low as 1-bit resolution [19], [20]. \n\nIn quantized training, one or both of the training-specific MVMs are also quantized, enabling efficiency benefits during training. Early work showed that using stochastic rounding to prevent rounding bias could enable 16-bit fixed-precision values for all three training MVMs [12]. Further work in integer-based training was able to show that 8-bit integer activations, weights, and gradients could be used for the forward and backward MVMs while the weight-update MVM was kept at FP32 [13]. [21] demonstrates a quantized training method which uses a tertiary quantization for the weights, and an 8-bit fixed point integer representation for the activations, gradients, and errors. To maximize the energy efficiency of the training approach, this method minimizes the number of floating-point operations by replacing batch normalization with a simple scaling approach and uses stochastic gradient descent without momentum. \n\nRecently, new methods using reduced-bit floating-point and radix formats for the gradients have achieved significant success thanks to extended dynamic range. A method using 8-bit floating-point operands for the forward, backward, and weight-update MVMs demonstrated no loss in precision on a variety of Deep Neural Networks (DNNs) [22]. An even more aggressive approach, which is central to the work in this paper, extends DNN training to 4-bit values for all three training MVMs. In [18] radix-4 4-bit floating-point gradients are used along with 4-bit integer activations and weights.",
            "score": 0.5066448307394918,
            "section_title": "II. RELATED WORK A. Quantized Training Methods",
            "char_start_offset": 3533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "3603886"
                },
                {
                    "start": 1924,
                    "end": 1928,
                    "matchedPaperCorpusId": "53977760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4677734375
        },
        {
            "corpus_id": "268031848",
            "title": "Adaptive Quantization with Mixed-Precision Based on Low-Cost Proxy",
            "text": "The large scale and complexity of deep learning models often hinder their deployment, especially when considering advanced models with high computational demands. A promising approach to address this is through neural network quantization [1][2][3][4][5][6][7][8][9][10][11], which seeks to minimize computational time and energy cost while preserving network precision. Mixed-precision quantization (MPQ) is presented to optimize the scale of the deep learning model, the computational demand, and the accuracy by assigning different bit widths to the layers. \n\nRecent studies have presented methods such as HAQ [12] and DNAS [13] to address the MPQ challenge. However, the expansive search space makes it infeasible to evaluate every mixed-precision combination for optimal performance. HAWQv2 [14] determines quantization sensitivity by assessing the mean of the eigenvalues of the Hessian matrix. It *Corresponding author then applies a Pareto frontier strategy for bit-width decisionmaking to avoid manual selection. But given the immense search space, HAWQv2 may not always identify the optimal configuration. To improve this, we integrate hardware-aware constraints into our optimal bit-width search and explore the impact of various quantization hyperparameters. \n\nHardware-aware ensures the model is optimized for specific hardware, boosting the performance. For example, HAWQv3 [15] provides an Integer Linear Programming (ILP) solution to generate mixed-precision configurations under various constraints like model size, bit operations (Bops), and latency. We use ILP as our hardware constraint for MPQ, integrating it to sensitivity measurements to refine the bit-width selection. \n\nOur proposed solution, LCPAQ, combines a hardwareaware module, an adaptive mixed-precision quantization, and a low-cost proxy neural architecture search module. Our main contributions include: \n\n(1) We present a method computing the Hessian matrix's trace to determine layer sensitivity and employ a Pareto frontier strategy for adaptive bit-width selection in mixedprecision quantization.",
            "score": 0.5066135317996323,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "9897483"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "216036085"
                },
                {
                    "start": 251,
                    "end": 254,
                    "matchedPaperCorpusId": "221266154"
                },
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "231850887"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "219117593"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "50778049"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "258107900"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "265038491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3349609375
        },
        {
            "corpus_id": "208138922",
            "title": "Any-Precision Deep Neural Networks",
            "text": "While state-of-the-art deep learning models can achieve very high accuracy on various benchmarks, runtime cost is another crucial factor to consider in practice. In general, the capacity of a deep learning model is positively correlated with its complexity. As a result, accurate models mostly run slower, consume more power, and have larger memory footprint as well as model size. In practice, it is inevitable to balance efficiency and accuracy to get a good trade-off when deploying any deep learning models.\n\nTo alleviate this issue, a number of approaches have been proposed to address it from different perspectives. We observe active researches (Liu et al. 2018a;Cai, Zhu, and Han 2018;Chen et al. 2019)   deep neural network architectures to support practical usage (Howard et al. 2019;Yu and Huang 2019;Tan and Le 2019). People also consider to adaptively modify general deep learning model inference to dynamically determine the execution during the feed-forward pass to save some computation at the cost of potential accuracy drop (Figurnov et al. 2017;Teerapittayanon, McDanel, and Kung 2016;Wu et al. 2018;Veit and Belongie 2018). Besides these explorations, another important line of research proposes a low-level solution to use less bits to represent deep learning model and its runtime data to achieve largely reduced runtime cost. It has been shown in various literatures that full-precision is over-abundant in many applications that we can use 8-bit or even 4-bit models without obvious performance degradation.\n\nSome previous works went further in this direction. For example, BNN, XNOR-Net, and others (Courbariaux et al. 2016;Rastegari et al. 2016;Zhou et al. 2016) are proposed to use as low as 1-bit for both the weights and acti-vations of the deep neural networks to reduce power-usage, memory-footprint, running time, and model size. However, ultra low-precision models always observe obvious accuracy drop (Courbariaux et al. 2016). While many methods have been proposed to improve accuracy of the lowprecision models, so far we see no silver bullet. Stepping back from uniformly ultra-low precision models, mixedprecision models have been proposed to serve as a",
            "score": 0.5065168552516305,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 652,
                    "end": 670,
                    "matchedPaperCorpusId": "40430109"
                },
                {
                    "start": 1042,
                    "end": 1064,
                    "matchedPaperCorpusId": "10569278"
                },
                {
                    "start": 1064,
                    "end": 1104,
                    "matchedPaperCorpusId": "2916466"
                },
                {
                    "start": 1104,
                    "end": 1119,
                    "matchedPaperCorpusId": "13739860"
                },
                {
                    "start": 1119,
                    "end": 1142,
                    "matchedPaperCorpusId": "50780153"
                },
                {
                    "start": 1649,
                    "end": 1671,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "259732219",
            "title": "Quantized Feature Distillation for Network Quantization",
            "text": "precision's top-1 accuracy under 3and 4-bit quantization (4-bit ResNet-34 with 74.7% top-1 even surpasses its full precision counterpart by 1.3%). Meanwhile, the accuracy of MobileNetV2 is relatively more difficult to recover under low bit, possibly due to its large channel variance, as pointed out by Nagel et al. (2021). But our QFD is still better than other methods. CUB200 with ViT. We also quantize vision transformers on the image classification benchmark CUB200 (Wah et al. 2011), which contains 200 categories of birds, with 5,994 and 5,794 images for training and testing, respectively. Specifically, we quantize the linear layer in multilayer-perceptron (MLP) and multi-head-attention (MHA) to 3-or 4-bit, using different structures of ViT (Dosovitskiy et al. 2021) and Deit (Touvron et al. 2021), including ViT Small, ViT Base, Deit Tiny, Deit Small and Deit Base. We also list the accuracy of the teacher network with quantized features (a preprocessing step of our QFD method). As Table 5 shows, although quantizing only the feature brings a slight accuracy drop to the original FP model, the improvement of QFD method over Baseline is significant and consistent. But, there is still a gap between 4-bit and the FP models. Quantizing transformer still remains a challenging task.",
            "score": 0.5061933034271344,
            "section_title": "Classification Results",
            "char_start_offset": 16989,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 777,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 787,
                    "end": 808,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66796875
        },
        {
            "corpus_id": "234337594",
            "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit",
            "text": "While the primary focus of our work is to evaluate the impact of quantization on Pareto curves, we want to provide evidence for the competitiveness of our quantization method compared to prior work. Table 2 shows the top-1 accuracy of a ResNet50 model quantized using various methods, differing by goals and constraints, which we did our best to take into account as much as possible. As can be seen, our quantization method achieves or beats the results found in prior work on ResNet-50 quantization. E.g. our 4-bit model (with first and last layers in 8-bit) achieves a higher accuracy compared to results in the PACT paper [6], as well as the XILINX paper [33].\n\nOne may notice that even though all the results in Table 2 are obtained on ResNet50, they differ significantly in Top-1 accuracy, potentially due to differences in ResNet versions and hyperparameter choices. Therefore, we focus on quantization loss, i.e. the difference between Top-1 of unquantized and quantized model, as the main metric to evaluate quality of the quantization algorithms. In our case, both our 4-bit model (with first and last layers in 8-bit) and our fully 8-bit model outperform the bfloat16 baseline model, highlighting a regularizing effect of quantization. This regularizing effect can be clearly seen in the last three rows of Table 2. Differences in the generalization gap show that the unquantized model is overfitting significantly more to the training data than the quantized models do. We would like 3089 to point out that other works may differ slightly due to quantization of ops other than Conv2D or MatMul, BatchNorm folding, etc.. We did not attempt to catch all the differences.",
            "score": 0.5061901136192006,
            "section_title": "Comparison to Prior Work",
            "char_start_offset": 17765,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "251928917",
            "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization",
            "text": "Since ANT uses multiple primitive data types (flint/int/float/PoT), we first study the contribution of each primitive for improving the quantization accuracy. \n\nPrimitive Combination We study six combinations of the four primitive data types. Int is the combination with only a single data type. Two combinations int-PoT (IP) and float-int-PoT (FIP) excludes flint and hence only exploit the inter-tensor adaptivity. Correspondingly, we evaluate these two combinations that add flint and exploit the intra-and inter-tensor adaptivity. They are int-PoT-flint (IP-F) and float-int-PoT-flint (FIP-F). ANT4-8 uses the mixed-precision of 4-bit int-based ANT (i.e., IP-F) and 8-bit int for accuracy comparison. All types use 4-bit quantization except ANT4-8. For the quantization metrics, we use the MSE and model accuracy loss against the original FP32 model. Fig. 10 plots the quantization MSE of these combinations on eight DNN models. Fig. 11 and Fig. 12 demonstrate their accuracy loss compared to original highprecision models before and after fine-tuning, respectively. \n\nQuantization MSE For quantizing each tensor in DNN models, we employ the ANT algorithm described in in Sec. IV-C to choose the primitive data type with minimum MSE. From the results in Fig. 10, we find that adding more primitive data types generally lets us decrease the accuracy loss owing to quantization errors. In specific, adding the PoT type is critical for Transformer-based models on NLP datasets (MNLI, CoLA, and SST2), since they have large activation values. The benefit of the PoT type is smaller for the vision tasks including ViT. Adding the flint type is important for both vision and NLP tasks. Finally, we observe that adding the float has the least impact on the quantization errors, whose role is replaced by other primitive types.",
            "score": 0.505951521892148,
            "section_title": "B. Quantization Accuracy",
            "char_start_offset": 47067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1823
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "31312287",
            "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks",
            "text": "In 2011, Vanhoucke et al. first showed that inference and training of deep neural networks is feasible with values of certain tensors quantized to a low-precision fixed point format [8]. More recently, an increasing number of studies demonstrated low-precision inference with substantially reduced computation. These studies involve, usually in a model-dependent manner, quantization of specific tensors into low-precision fixed point formats. These include quantization of weights and/or activations to 8-bit [8,9,10,11], down to 4-bit, 2-bit [12,13] or ternary [10], and ultimately all binary [7,14,5,6]. Weights trained at full precision are commonly converted from floating point values, and bit-widths of component tensors are either pre-determined based on the characteristics of the model, or optimized per layer [11]. Low-precision inference has already made its way into production hardware such as Google's tensor processing unit (TPU) [15]. \n\nOn the other hand, reasonable successes in low-precision training have been obtained with binarized [13,16,17,5] or ternarized weights [18], or binarized gradients in the case of stochastic gradient descent [19], while accumulation of activations and gradients is usually at higher precision. Motivated by the non-uniform distribution of weights and activations, Miyashita et al. [20] used a logarithmic quantizer to quantize the parameters and gradients to 6 bits without significant loss in performance. XNOR-nets focused on speeding up neural network computations by parametrizing the activations and weights as rank-1 products of binary tensors and higher precision scalar values [7]. This enables the use of kernels composed of XNOR and bit-count operations to perform highly efficient convolutions. However, additional high-precision multipliers are still needed to perform the scaling after each convolution which limits its performance.",
            "score": 0.5058138646148271,
            "section_title": "Related Work",
            "char_start_offset": 2394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 595,
                    "end": 598,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 820,
                    "end": 824,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1058,
                    "end": 1061,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "16104422"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "2189412"
                },
                {
                    "start": 1638,
                    "end": 1641,
                    "matchedPaperCorpusId": "14925907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "261243764",
            "title": "An Efficient FPGA-Based Accelerator for Swin Transformer",
            "text": "In [22], the authors employed techniques named POT quantization for multi-scale ViT quantization. They demonstrated the feasibility of using 8-bit and even 4-bit quantization in ViT models. In [30] the authors proposed a high-speed reconfigurable architecture that utilizes 16-bit fixed-point representation for running all Softmax and GELU operations. In the work [28], a precision-adjustable architecture for the Softmax function was developed, with all inputs and outputs represented in 16-bit format, achieving both efficiency and adjustability. Furthermore, [27] even explored the use of 8bit quantization for Softmax function computations, achieving minimal precision loss while working with attention mechanisms in deep neural networks. \n\nThe proposed accelerator in this study adopts a fullquantized method, eliminating a significant number of intermediate quantized and de-quantized operations. This approach also extends to quantizing biases, leading to an inevitable loss of a certain degree of flexibility. However, throughout this process, there is no involvement of float32 computations. This design choice enhances hardware efficiency, particularly for hardware like FPGA that may not be as well-suited to handling float operations. In order to maintain accuracy, the matrix multiplication can be quantized to 16-bit fixed point without any noticeable loss in precision. \n\nHence, to ensure both accuracy and efficiency, the proposed accelerator in this paper utilizes 16-bit fixed-point computation for linear functions and 16-bit fixed-point calculations for Softmax and Gelu nonlinear functions.",
            "score": 0.5053249829857609,
            "section_title": "C. Quantification Methods",
            "char_start_offset": 38976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1612
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "251467937"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "257544791"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "226674012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "237453537",
            "title": "Fine-grained Data Distribution Alignment for Post-Training Quantization",
            "text": "Recent years have witnessed the rising of deep neural networks (DNNs) in computer vision. Nevertheless, the increasing model size barricades the deployment of DNNs on resource-limited platforms such as mobile phones, embedding devices, etc. To overcome this dilemma, varieties of methods [9,17] are explored to reduce the complexity of DNNs. Network quantization, which represents full-precision DNNs in a low-precision format, emerges as a promising direction [1,17,20]. By tuning the quantized DNNs using a small calibration dataset, post-training quantization, a sub-topic of low-precision quantization, has received increasing popularity from both academia and industries. Recent studies manifest that a post-training quantized model in high precision, such as 8-bit, can reach performance on par with its full-precision counterpart [17,1]. However, performance drops severely if being quantized to lower precision such as 4-bit [19]. For example, as reported in LAPQ [26], quantizing ResNet-18 [11] to 8-bit can well retain the accuracy of the full-precision network (around 71.5%), but only 60.3% top-1 accuracy can be observed when quantized to 4-bit. To alleviate this problem, many studies are explored to enhance the low-bit performance. The mainstream can be outlined into two folds. The first group designs sophisticated quantization methods, such as linear combination of multiple low-bit vectors [21], weight region separation [6], mixed-precision quantization [21], partial quantization [18], etc. The second group reformulates the rounding function or loss constraint from an analytical perspective. For example, Nagel et al. [25] derived an adaptive rounding by modeling rounding problem as quadratic constrained binary optimization. By the second-order analysis on rebuilding intermediate outputs, Li et al. [19] showed that the best output reconstruction lies in a block unit. \n\nThough great efforts have been made, improvements of these studies are still limited.",
            "score": 0.5050795129019865,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "221970417"
                },
                {
                    "start": 841,
                    "end": 843,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "231861390"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1410,
                    "end": 1414,
                    "matchedPaperCorpusId": "211252843"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "213004198"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "211252843"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "235719197"
                },
                {
                    "start": 1642,
                    "end": 1646,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 1826,
                    "end": 1830,
                    "matchedPaperCorpusId": "231861390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "231918377",
            "title": "Dynamic Precision Analog Computing for Neural Networks",
            "text": "Neural networks are able to perform accurate inference at low bit precision in digital hardware. Empirical research has demonstrated that neural network accuracy degrades minimally when quantizing to 4 to 8-bit fixed point integer representations, despite the networks being trained using 32bit floating point numbers [10]- [12]. \n\nA common method for quantizing floating point values to low precision is affine quantization [10]. In affine quantization, floating point inputs x (l) (or weights) in the range \n\nmax ] are mapped to fixed point integers of B bits from 0 to 2 B\u22121 by scaling, translating, and rounding the inputs. Mathematically, this is \n\nThe average precision required by commonly deployed neural networks can be lowered by using mixed precision. Different layers of neural networks are tolerant to different degrees of precision, and uniformly quantizing all layers of a neural network to the same bit precision leads to accuracy degradation [14]. There are many approaches for determining the bit precision of each layer, which requires searching an exponentially large space in the number of layers [12]- [18]. The method most similar to the one presented in this paper learns the bitwidth of each layer via gradient descent [26]. Other works perform post-training mixed precision quantization by making theoretical assumptions about signal-to-quantization noise ratio (SQNR) [27].",
            "score": 0.5050155517598576,
            "section_title": "B. Low Precision Neural Networks",
            "char_start_offset": 7398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "204803866"
                },
                {
                    "start": 1395,
                    "end": 1399,
                    "matchedPaperCorpusId": "649645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5625
        },
        {
            "corpus_id": "252611827",
            "title": "A Method of Deep Learning Model Optimization for Image Classification on Edge Device",
            "text": "In [2], a model quantization was a widely used technique to compress and accelerate the inference stage of deep learning. Recent hardware accelerators for deep learning have begun to support mixed precision (1-8 bits) to further improve computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space, trading off between accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. \n\nA quantization technique compresses the network weights by reducing the number of bits, and then the network weight becomes smaller from 32 bits. Therefore, the quantization method limits the dynamic range and the expression accuracy of bits but also has the advantage of reducing the overall network weight size as much as the number of quantized bits. \n\nTo analyze the quantization transformation mathematically, let us define the former FP32-bit tensor as x f and the quantized INT8 tensor as x q . The basic transformation method of quantization then becomes: \n\nAs shown in Figure 1, the distribution of the dynamic range of FP32 is in [\u22123.4 \u00d7 10 \u221238 , 3.4 \u00d7 10 38 ] based on IEEE 754 standard [32] and the dynamic range of INT8 can express 255 equally spaced numbers. In other words, in order to map numbers from FP32 to INT8, the Clip function is used to discard some outlier numbers outside \u2212r and r in the dynamic range of FP32. In addition, s is used for spacing adjustment. Therefore, the conversion from FP32 to INT8 results in some latency due to the operations of Equation ( 1), such as Clip, Round and Scale. On the other hand, the dynamic range of FP16 becomes [\u221265504, 65504] with the half size of FP32 based on IEEE 754 standard [32].",
            "score": 0.5047866532278968,
            "section_title": "Quantization Technique",
            "char_start_offset": 8227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21240234375
        },
        {
            "corpus_id": "240353756",
            "title": "RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions",
            "text": "With tremendous success of the deep learning or deep neural networks (DNNs), there exist urgent needs for deployments of inference models onto edge-computing plat-* Equal contribution. forms/devices. As a major type of model compression technique, the DNN quantization becomes an essential method to reduce the computation, memory, and storage requirements in on-device inference, especially for platforms with capability of customized architecture design, such as FPGA devices and ASIC chips. Generally speaking, DNN quantization learns DNN models in low bit-width representation with accuracy performance close to that of the full-precision models, while accelerating inference speed. \n\nVarious quantization schemes have been investigated including binary [5,6,26,22], ternary [20,15,39], Fixed-point (Fixed) [38,4,12,16,3,11], Power-of-Two (PoT) [24,37,19,36], Additive Power-of-Two (APoT) [21], etc. Those schemes have diverse accuracy and hardware performance. Binary and ternary significantly reduce computation by eliminating multiplication operations, but experience relatively large accuracy loss (> 2% in general). Low bit-width Fixed quantization has better accuracy The proposed DNN quantization framework with row-wise mixed schemes and multiple precisions, which assigns quantization scheme and precision to filters of the weight tensor (or rows of the weight matrix). It features (i) layer-wise uniformality to fulfill the requirement of practical hardware implementation, (ii) row-wise flexibility for mixed schemes and multiple precisions, (iii) hardware-informative selection of candidate schemes and precisions (bit-widths) for significantly reducing the algorithm search space, and (iv) superior accuracy performance among the state-of-the-arts. \n\nperformance. For example, 4-bit Fixed can achieve negligible accuracy loss comparing with its 32-bit floating-point counterpart, although it still needs multiplication operations during inference computation. Different from binary, ternary, and Fixed, PoT is a nonlinear quantization scheme, where with quantization levels as power-of-two numbers, multiplications can be replaced with bit shifting operations, thereby reducing computation to speedup inference.",
            "score": 0.5047641138661031,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 758,
                    "end": 761,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "52909828"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "199577815"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 823,
                    "end": 825,
                    "matchedPaperCorpusId": "203605628"
                },
                {
                    "start": 853,
                    "end": 856,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "210848001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375732421875
        },
        {
            "corpus_id": "10562854",
            "title": "Ternary Neural Networks with Fine-Grained Quantization",
            "text": "Deep learning inference using low-precision weights and activations is a well-researched topic. Many researchers have experimented with custom data representations to perform deep learning tasks and have shown significant benefits over the general purpose floating point representation. [20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware. With this they show up to 4\u00d7 improvement over an aggressively tuned floating point implementation. [5] have done a comprehensive study on the effect of low precision fixed point computation for deep learning and have successfully trained smaller networks using 16-bit fixed point on specialized hardware. This suggests that fixed point representations are better suited for low(er) precision deep learning. \n\nThere have also been recent efforts exploring 8-bit floating point representation [4], however such schemes have the additional overhead of reduced precision since the exponent is replicated for each value. Whereas with fixed point representation, using a single shared exponent improves capacity for precision. Typically for deep neural networks, with reduced bit-widths it is desired to preserve the numerical precision, since the loss in range can be augmented by the dynamic scaling of the shared exponent. \n\nCommonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14,16,9,2,8,25,6]. This can be prohibitive in applications which rely on using previously trained models. Such use cases are typical in many edge device deployments. To address such cases, FGQ is developed with motivation to be able to achieve state-of-art accuracies without any training and hence enabling direct use of pre-trained models. This requirement results in the quantization scheme being quite complex but making it more widely applicable and making it also easily usable for the former casewith training from scratch.",
            "score": 0.5044034049881978,
            "section_title": "Related Work",
            "char_start_offset": 5277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "123114772"
                },
                {
                    "start": 664,
                    "end": 667,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1689,
                    "end": 1691,
                    "matchedPaperCorpusId": "2238772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "250048704",
            "title": "QReg: On Regularization Effects of Quantization",
            "text": "Deep neural network (DNN) quantization has been an active research topic in the past few years. Quantization is the task of approximating high precision (floating point) weights and activations of a model with lower bit resolution counterparts (usually 1 to 8 bit integers). Quantization can be done after (post training quantization) or during training (quantization aware training). Although both methods can compress the model and reduce overall execution time, at the cost of model accuracy, post training quantization usually takes less time to quantize a model. On the other hand, quantization aware training usually requires a full training session and with a moderate quantization level (int8) can produce quantized models with little to no accuracy loss compared to their full precision counterparts. In fact, as we will discuss in Section 2, there are many existing works Objectiveness Loss Value Epoch Number Validation objective loss of 4-bit quantized , 8-bit quantized and full precision model for the YOLOv5n model on the VOC dataset. This figure illustrates that unlike the quantized model, the full precision model exhibits heavy overfitting. We believe that the quantized model has better generalization performance due to its regularization effect. \n\nclaiming that quantization aware training produced accuracy improvements compared to some of the best full precision models. In these studies, the authors usually attribute such accuracy gains to the regularization effect of quantization. This has motivated us to explore how quantization affects training. In this work, we first hypothesize the regularization effect of quantization. Moreover, we hypothesize that this regularization effect is correlated with the quantization level. To confirm our hypothesis, we analytically explore how quantization noise propagates in a network at training time, and how the quantization level is correlated with the amount of regularization. We then empirically explore the regularization effect of quantization by applying different quantization aware training methods on different models and datasets with different quantization levels. In both analytical and empirical studies, we confirm our initial hypothesis.",
            "score": 0.5042140477623003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2224
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "211011030",
            "title": "SQWA: Stochastic Quantized Weight Averaging For Improving The Generalization Capability Of Low-Precision Deep Neural Networks",
            "text": "tuning it to obtain the final 2-bit QDNN. The results are reported in Table 1.\n\nThe performance of the averaged model is 69.7%, and we obtained 60.8% as the direct quantization results. After the fine-tuning, the accuracy improved to 69.4%, which is significantly better than those of the captured models. We conducted additional experiments to investigate the effect of the number of models on averaging. As discussed in Section 3, the number of captured models is related to the precision of the averaged model. More specifically, the averaged model using three 2-bit ternary models becomes a 3-bit QDNN. Thus, we employed 3, 7, 15, and 31 models such that the precision of the averaged model was 3, 4, 5, and 6 bits, respectively, and fine-tuned each model. The results are reported in Table 2. Adopting three models afforded a top-1 accuracy of 69.2%, which is 0.2% worse than the seven models. When 15 models were employed, the top-1 accuracy was similar to that of the seven models but the top-5 accuracy was 0.2% higher. Using 31 models did not improve the performance.\n\nWe compare our results with those of previous studies in Table 3. Our result outperformed those of previous studies including the 2-bit 4-level (LQ-NET [26] and WNQ [7]) and ternary (TWN [3], TTQ [23], INQ [24], ADMM [25], QNet [27], QIL [13], and Apprentice [5]). More specifically, we achieved a top-1 accuracy of 69.4%. Only the QNet result is comparable with our result, although it is 0.3% lower. This result is significant because QNet employs non-linear quantizer while we adopt uniform quantization.  [20]. The Mo-bileNetV2 [30] is adopted as the backbone network. We downloaded the full-precision pretrained model 1 . We only used the original training images for the SQWA training. The maximum and minimum values of the cyclic learning rate scheduling were 1e-4 and 1e-7, respectively. The total number of training epoch",
            "score": 0.5037587194407144,
            "section_title": "Image classification on ImageNet",
            "char_start_offset": 12487,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 1265,
                    "end": 1268,
                    "matchedPaperCorpusId": "13556195"
                },
                {
                    "start": 1274,
                    "end": 1278,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "198903430"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "53719799"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "matchedPaperCorpusId": "3643430"
                },
                {
                    "start": 1587,
                    "end": 1591,
                    "matchedPaperCorpusId": "207252270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42041015625
        },
        {
            "corpus_id": "219447096",
            "title": "Moving Deep Learning to the Edge",
            "text": "Different simplifications and optimizations were proposed to reduce the implementation complexity of deep learning models. The objective of the simplifications is to reduce the memory footprint and the computational complexity to run a network inference. There are two main classes of optimizations: (1) data quantization and (2) data reduction. Some optimizations reduce the complexity of the model with some accuracy reduction. Others can reduce the memory and computation footprints without accuracy degradation. All methods have been considered on resource-limited end devices, but only a few are considered on edge nodes, typically those that do not affect the accuracy. \n\nData quantization methods reduce the complexity of arithmetic operators and the number of bits (bitwidth) to represent parameters and activations. The complexity of hardware implementations of arithmetic operations depends on the type of data [112]. Operators for floating-point arithmetic are more complex than for fixed-point or integer arithmetic. The number of bits used to represent data also determines the complexity of the operators. Custom floating-point representations with 16 bits [113] and eight bits [114] considerably reduce the complexity of operators and achieve similar accuracies of networks implemented with single-precision floating-points. \n\nIn [115], the authors used 8 bit fixed-point data representations for parameters and activations. They concluded that the model achieved an accuracy close to that obtained with the same model using 32 bit floating-points. The same conclusion was found in [116][117][118]. All works showed that parameters and activations could be represented in fixed-points and with fewer bits with negligible accuracy reduction. Less than 8 bit quantizations were proposed in [119,120]. \n\nPrevious works considered a fixed quantization for all layers, but customized representations for different layers reduced further the complexity without incurring an accuracy reduction [121][122][123]. Studies with this hybrid quantization concluded that the first and last layers were the most sensitive to the size of data among all layers. Furthermore, different sizes could be adopted for weights and activations. In [122], the authors concluded that activations were more sensitive to data size reduction. \n\nData quantization can be taken to the limit with Binary Neural Networks (BNN).",
            "score": 0.5035848580324531,
            "section_title": "Hardware-Oriented Deep Neural Network Optimizations",
            "char_start_offset": 43117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2327
                },
                {
                    "start": 2330,
                    "end": 2408
                }
            ],
            "ref_mentions": [
                {
                    "start": 1345,
                    "end": 1350,
                    "matchedPaperCorpusId": "9455864"
                },
                {
                    "start": 1597,
                    "end": 1602,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1602,
                    "end": 1607,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 1607,
                    "end": 1612,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "202773563"
                },
                {
                    "start": 2002,
                    "end": 2007,
                    "matchedPaperCorpusId": "207233292"
                },
                {
                    "start": 2007,
                    "end": 2012,
                    "matchedPaperCorpusId": "51974633"
                },
                {
                    "start": 2012,
                    "end": 2017,
                    "matchedPaperCorpusId": "207959781"
                },
                {
                    "start": 2238,
                    "end": 2243,
                    "matchedPaperCorpusId": "51974633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "233476471",
            "title": "Stealthy Backdoors as Compression Artifacts",
            "text": "Quantization based compression techniques work by reducing the numerical precision of the model weights and activations to save memory and make use of less expensive arithmetical instructions to reduce inference cost. Early methods clustered the weights of a deep learning model and represented the model weights using the centroid of the corresponding cluster [36]- [38]. A simpler but more effective compression method than the clustering technique directly uses 16-bit precision values to represent model weights of FP32 precision and reduces the model size by half without incurring significant accuracy drop [39]. Recently, it has been shown that lowering the precision to 8-bit is also feasible [6], [7]. Weights and activations of the model are converted from their 32-bit precision values to 8-bit precision values, and computations are mainly performed using 8-bit integer arithmetic. The 8-bit quantized model can achieve 2-3\u00d7 speedup (when the inference runs on CPUs) compared to the original floating point model, without sacrificing much model accuracy [18]. We focus our attacks on 8-bit quantization since 8-bit quantization achieves the best speed-up among all these quantization methods and it is now supported by all major mobile deep learning frameworks. \n\nIn the 8-bit quantization, the weights and activations (the inputs of each layer) of a model are converted into 8-bit precision using an affine function: \n\nwhere Q takes an FP32 (full-precision) tensor X as input and uses a scale factor s and an 8-bit integer z as parameters. \n\nThe parameters s and z depend on the data distribution of the FP32 tensor to be quantized, and determining these parameters is a critical part of model quantization. Calculating s and z for a weight tensor is easy because the weights of the model are fixed once the training is finished. Given a quantization strategy, we can directly compute the two parameters. For example, we can directly set z as 0 and s as max(X)\u2212min(X) 2 8   . For the activations, though, the values are only available when the model is executed. There are two methods used to compute the s and z for an activation tensor.",
            "score": 0.5030860511034221,
            "section_title": "A. Model Quantization",
            "char_start_offset": 6257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "231740594",
            "title": "Understanding Cache Boundness of ML Operators on ARM Processors",
            "text": "The bit serial approach does not scale according to reduced data size and moreover, it does not seem to be bound by cache bandwidth, at least not in regard to the cache-bound model discussed in this work. Also, as in our opinion quantization shall not be limited to certain data types, such as the defacto industry standard of 8-bit integer, any quantization option has to be flexible for bestmost trading among accuracy and performance. \n\nIn particular, this work makes the following contributions: 1) Measure the computational peak performance and memory bandwidths for caches and RAM of selected ARM processors. 2) Benchmarking of convolutional and dense operations on embedded ARM processors, including auto-generated code and comparison with openBLAS. 3) Detailed performance analysis to understand the disparity between sustained and theoretical peak performance, resulting in a cache-bound model. 4) Benchmarking 8-bit and bit-serial results, and discussing observed behavior based on the cache-bound model. \n\nII. RELATED WORK Static inference libraries, like ARM's Compute Library 2 or other specialized, hand-tuned, ultra-low-precision operators [11,12] achieve impressive performance, but make it difficult to combine multiple compression techniques if not provided by the library. However, the combination of such techniques, especially quantization and pruning, can be very effective [13]. \n\nIn contrast, deep learning compilers like TVM, Tensorflow's XLA [14] and Lift [15] close the gap between high-level machine learning frameworks and deployment on hardware in a flexible way. Their independence from static libraries enables straightforward research of new ML methods, the usage of non-standard operators, and allows to use compression techniques and to combine them with little effort. \n\nAuto-tuning is one of the most important features of TVM. With an automated end-to-end feedback loop, executing runtime measurements, and a domain-specific machine-learningbased cost model, optimal parameters can be found [6]. One step further, it is possible to exploit AutoTVM and use it in the decision process of hardware design [16]. \n\nWhile import and execution of previously quantized 8-bit models is supported using a specialized QNN dialect [17],",
            "score": 0.5028746066477582,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4314,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2145
                },
                {
                    "start": 2148,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "221070245"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "46772205"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "7490888"
                },
                {
                    "start": 2029,
                    "end": 2032,
                    "matchedPaperCorpusId": "29160233"
                },
                {
                    "start": 2140,
                    "end": 2144,
                    "matchedPaperCorpusId": "216080460"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216064453125
        },
        {
            "corpus_id": "207870482",
            "title": "Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers",
            "text": "While deep neural networks have become state-of-theart techniques for a wide range of machine learning applications, such as image recognition [14], object detection [21], machine translation [32,8], the computation costs of deep neural networks are continuously increasing, which greatly hampers the development and deployment of deep neural networks. For example, 10,000 GPU hours are used to perform neural architecture search on ImageNet [2]. Quantization is a promising technique to reduce the computation cost of neural network training, which can replace high-cost floating-point numbers (e.g., float32) with low-cost fixedpoint numbers (e.g., int8/int16). Recently, both the software society [6,12,16,19,27,35] and the hardware society [11,24,23,31] have carried out extensive researches about quantization of deep neural network for inference tasks. \n\nThough various investigations have demonstrated that deep learning inference can be accurately performed with low bit-width fixed-point numbers through quantization, the quantified training remains an open challenge. Some ex-isting approaches quantify the backward-pass to low-bit (e.g., int8) but incur significant accuracy drop, for examples, 3\u02dc7% loss for AlexNet [38,36]. [7] uses int16 for both forward-pass and backward-pass to ensure accuracy. However, there is no guarantee that unified int16 precision works for all the tasks and networks. \n\nMost previous investigations on quantified training use unified precision (i.e., bit-width) for all network layers. Intuitively, using mixed precisions for different layers will promote the network performance. However, it is hard to find the most appropriate precisions for so many layers in so many training iterations. Considering a widely used ResNet50 model, with 4 candidate quantization bit-widths (e.g., 8, 16, 24, 32 for weights, activations and activation gradients), the size of quantization precision combination search space for 450,000 training iterations can achieve 4 3 * 50 * 450,000 .",
            "score": 0.5027440271306294,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "2141740"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 196,
                    "end": 198,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 709,
                    "end": 712,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "3603886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493408203125
        },
        {
            "corpus_id": "199453139",
            "title": "Tuning Algorithms and Generators for Efficient Edge Inference",
            "text": "Closely tied to the algorithmic optimization, the appropriate number of representations for both training and inference have been extensively studied. Considering the fine tuning of the gradients and its calculation throughout the training phase, higher levels of precision are usually required to preserve the accuracy levels. On the other hand, in the inference mode, the required precision for achieving the accuracy of computation is greatly relaxed. Several models of networks are proposed with reduced precision ranging from 8 bits down to ternary [19] and binary representation [20]. Moreover, the type of quantization applied has a large impact on preserving the accuracy of the system. In particular, non-uniform quantization [15] tends to incur no loss of accuracy for the networks operating down to 4-bit precision and even with a lower number of bits, as shown at IBM research that achieved highly accurate deep neural network models with 2 bits [21]. This is, in particular, the case for the reduced precision across all the network layers. More aggressive quantization levels are possible when considering the dynamic optimization of layers during training. Accelerators as well have already provided support for reduced precision with Google TPU1 [22] operating at 8-bits and Nvidia's NVLDA [14] open source accelerator providing support down to 4 bits. In our work, we combine both the quantization and structured pruning iteratively during the training phase to ensure the satisfactory performance of the compressed network. Considering the hardware/software design paradigm we are proposing at the early stage, our framework is able to accommodate the integration of any quantization algorithm. Moreover, with the parameter tuning feature, different instances could be streamed out depending on the application requirements.",
            "score": 0.5025667759538555,
            "section_title": "Data Representation",
            "char_start_offset": 9777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 735,
                    "end": 739,
                    "matchedPaperCorpusId": "52037947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "251564521",
            "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "text": "This loss of precision is reflected in C4 evaluation perplexity (Section 3) as well as zeroshot accuracy as soon as these outlier features emerge, as shown in Figure 1. Figure 1: OPT model mean zeroshot accuracy for WinoGrande, HellaSwag, PIQA, and LAMBADA datasets. Shown is the 16-bit baseline, the most precise previous 8-bit quantization method as a baseline, and our new 8-bit quantization method, LLM.int8(). We can see once systematic outliers occur at a scale of 6.7B parameters, regular quantization methods fail, while LLM.int8() maintains 16-bit accuracy. \n\nWe show that with the first part of our method, vector-wise quantization, it is possible to retain performance at scales up to 2.7B parameters. \n\nFor vector-wise quantization, matrix multiplication can be seen as a sequence of independent inner products of row and column vectors. As such, we can use a separate quantization normalization constant for each inner product to improve quantization precision. We can recover the output of the matrix multiplication by denormalizing by the outer product of column and row normalization constants before we perform the next operation. \n\nTo scale beyond 6.7B parameters without performance degradation, it is critical to understand the emergence of extreme outliers in the feature dimensions of the hidden states during inference. To this end, we provide a new descriptive analysis which shows that large features with magnitudes up to 20x larger than in other dimensions first appear in about 25% of all transformer layers and then gradually spread to other layers as we scale transformers to 6B parameters. At around 6.7B parameters, a phase shift occurs, and all transformer layers and 75% of all sequence dimensions are affected by extreme magnitude features. These outliers are highly systematic: at the 6.7B scale, 150,000 outliers occur per sequence, but they are concentrated in only 6 feature dimensions across the entire transformer.",
            "score": 0.5024071709968901,
            "section_title": "Introduction",
            "char_start_offset": 1839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4951171875
        },
        {
            "corpus_id": "231906389",
            "title": "Quantization-Guided Training for Compact TinyML Models",
            "text": "Table 1 presents the results of a number of QGT experiments for various architectures. Note that the main purpose of these experiments is to QGT's utility on small and large architectures in the context of a range of tasks. We used the asymmetric quantizer in all of the experiments. We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models. In the experiments, the quantizer was applied, both, in per-tensor and per-channel (for convolutional and depth-wise convolutional layers) fashions. We do not apply QGT to biases and the trainable parameters ( and  -see [16]) of the batch normalization layers. The reported top-1 accuracies were computed based on the dequantized weights with activations kept at four-byte floating point. Since the asymmetric quantization requires retaining slope and intercept, the sizes of per-channel-quantized models are slightly larger than their per-tensor counterparts quantized at the same bit-widths. \n\nThe results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy. We note that per-channel quantization provides higher accuracy than its per-tensor counterparts at the same bit-precision targets. At 2-bit precision target, our early results are not conclusive as the solution space might require a more comprehensive search, or the model architecture might reach its capacity for the task / dataset. \n\nTo see why QGT, in spite of the simplicity of its formulation is so effective, it is instructive to compare the histograms of a model parameters trained under QGT against those of its base floating-point model. Fig. 4 presents several histograms (from the 27 convolutional kernels plus the final dense layer) of the MobileNetV1 model trained on the ten-class ImageNette task.",
            "score": 0.5023636650151021,
            "section_title": "Benchmark Experiments",
            "char_start_offset": 16545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2056
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "215862562",
            "title": "Fully Quantized Transformer for Machine Translation",
            "text": "for post-quantization, BLEU was computed on the test set using the checkpoint which scored the highest accuracy on the validation set. Towards the end of training, we ran one validation epoch for every 100 training steps. Baselines and FullyQT 8-bit results were averaged over 5 trials. Standard deviation of the BLEU scores did not seem higher for any method and ranged between 0.09 and 0.51. Training with quantization was about twice as slow as with the baselines. As for post-training quantization, the BLEU score was computed on the test set using the best validation performance out of 20 trials. The default approach's nan in the EN-FR task is due to numerical instability. By quantizing every operation, zeros in the LayerNorm's denominator are more frequent.   Results on additional translation datasets can be found in Table 3. All models were trained following the same setup as WMT14 EN-FR and WMT14 EN-DE. Vocabulary size is set to 32k for all models. Since there is no test set for WMT14 ES-EN, we used the validation set as a test set and omitted computing any validation epochs during training.\n\nLooking at all conducted experiments, including section 5.3, translation quality of the 8-bit Ful-lyQT models seems to be on par with full-precision. Most of the time, the highest BLEU was scored by the quantized model. For example in the case of WMT14 EN-DE, the maximum BLEU FullyQT base 8-bit obtained was 26.98, while the baseline's highest was 26.64. As for the big models, the max FullyQT scored was 27.95, whereas the baseline's was 27.43. We looked at training and validation curves to see if quantization had any effect, but saw no discernible difference.\n\nAll models use full-precision biases, s and x min . This amounts to 11.61 Mb in the base models and 23.15 Mb in the big models. In the case of 8-bit, these represent less than 2.35% of the total size. Without bucketing, this would amount to 2.18 Mb and 4.35 Mb",
            "score": 0.5021518968442433,
            "section_title": "Full Quantization",
            "char_start_offset": 14955,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310791015625
        },
        {
            "corpus_id": "269214209",
            "title": "QGen: On the Ability to Generalize in Quantization Aware Training",
            "text": "The exceptional growth of technology involving deep learning has made it one of the most promising technologies for applications such as computer vision, natural language processing, and speech recognition.The ongoing advances in these models consistently enhance their capabilities, yet their improving performances often come at the price of growing complexity and an increased number of parameters.The increasing complexity of these models poses a challenge to the deployment in production systems due to higher operating costs, greater memory requirements, and longer response times.Quantization Courbariaux et al. (2015); Hubara et al. (2016); Polino et al. (2018); Zhou et al. (2017); Jacob et al. (2018); Krishnamoorthi (2018) is one of the prominent techniques that have been developed to reduce the model sizes and/or reduce latency.Model quantization represents full precision model weights and/or activations using fewer bits, resulting in models with lower memory usage, energy consumption, and faster inference.Quantization has gained significant attention in academia and industry.Especially with the emergence of the transformer Vaswani et al. (2017) model, quantization has become a standard technique to reduce memory and computation requirements.\n\nThe impact on accuracy and the benefits of quantization, such as memory footprint and latency, is well studied Courbariaux et al. (2015); Li et al. (2017); Gholami et al. (2021).These studies are mainly driven by the fact that modern hardware is faster and more energy efficient in low-precision (byte, sub-byte) arithmetic compared to the floating point counterpart.Despite its numerous benefits, quantization may adversely impact accuracy.\n\nHence, substantial research efforts on quantization revolve around addressing the accuracy degradation resulting from lower bit representation.This involves analyzing the model's convergence qualities for various numerical precisions and studying their impacts on gradients and network updates Li et al. (2017); Hou et al. (2019).\n\nIn this work, we delve into the generalization properties of quantized neural networks.This key aspect has received limited attention despite its significant implications for the performance of models on unseen data.",
            "score": 0.501449310704865,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1264
                },
                {
                    "start": 1266,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1707
                },
                {
                    "start": 1709,
                    "end": 1852
                },
                {
                    "start": 1852,
                    "end": 2039
                },
                {
                    "start": 2041,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 625,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 627,
                    "end": 647,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 649,
                    "end": 669,
                    "matchedPaperCorpusId": "3323727"
                },
                {
                    "start": 691,
                    "end": 710,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1377,
                    "end": 1402,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1404,
                    "end": 1420,
                    "matchedPaperCorpusId": "10817450"
                },
                {
                    "start": 2003,
                    "end": 2019,
                    "matchedPaperCorpusId": "10817450"
                },
                {
                    "start": 2021,
                    "end": 2038,
                    "matchedPaperCorpusId": "108296452"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "267320710",
            "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis",
            "text": "The development of data-driven science, which involves analyzing large-scale observational data using models with a vast number of parameters, has led to a significant increase in computational costs. A prime example is the large-scale neural networks used in artificial intelligence technologies. For instance, GPT (Generative Pre-trained Transformer)-3 [BMR + 20] possesses tens of billions of parameters, requiring substantial computational resources for storage and updates in computer memory. Moreover, with the growing need to implement these models in small-edge devices, it's crucial to minimize their computational resource requirements. Practical applications, such as implementing neural networks in smartphones and in-vehicle sensors, are examples of this trend [LGW + 21]. Reducing computational costs is essential to achieve accurate and sustainable inference on these devices. \n\nQuantization is one of the most common techniques for compressing large models (for a survey, see [GKD + 22]). Quantization involves rounding the parameters or activation values of a model to discrete values with lower bit precision. Specifically, it converts original values stored with higher bits, e.g., 32 bits, to lower bits, such as 4 or 8 bits. This operation not only reduces the amount of memory required to store parameters but also decreases the computational resources needed for operations like matrix products. In practice, it is empirically known that quantization minimally impacts the prediction accuracy of neural networks, thereby enhancing the utility of this technology (see [BHHS18] for an example). Additionally, quantization can be integrated with other model compression techniques, such as distillation and pruning [LGW + 21]. Its significance is anticipated to grow as models, including new versions of GPT, continue to increase in size. \n\nAn ongoing challenge in quantization is the selection of hyperparameters, including the number of bits and the quantization range. While neural networks exhibit some robustness to quantization, a significant reduction in the number of bits can make them susceptible to outliers and data shifts. Consequently, it is crucial to choose appropriate hyperparameters that balance accuracy with computational load. Nevertheless, the comprehension of quantization remains an evolving issue. The characteristics of optimal hyperparameters and their very existence are still not fully understood.",
            "score": 0.5011667856411731,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2343
                },
                {
                    "start": 2344,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 1590,
                    "end": 1598,
                    "matchedPaperCorpusId": "44071489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "221103917",
            "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance",
            "text": "see that 4-bit quantization, which has quantization SNR (signal to noise ratio) 6.02 \u00d7 4 \u2248 24 dB, led to output jitter of 0.957% which means SNR 20 log 10 (1/.00957) \u2248 40 dB. A 40 dB SNR means that the neuron was extremely robust to weight quantization.\n\nDeep SNN Robustness: To evaluate the robustness of deep SNNs, we experimented with both weight quantization and noise perturbation. For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;Rastegari et al. 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;Zhang et al. 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over.\n\nTo evaluate noise perturbation, we added random noise to the trained weights. Experiment results are shown in Fig. 3, together with the results of weight quantization in Table 4 (expressed in SNR). We find that 24dB quantization noise (4-bit quantization) reduced ImageNet classification accuracy to 65.2%. Noise at 24dB SNR reduced ImageNet clas-   sification accuracy to 65.43",
            "score": 0.5008697440498938,
            "section_title": "Robustness",
            "char_start_offset": 27448,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1275,
                    "end": 1293,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73291015625
        },
        {
            "corpus_id": "251788140",
            "title": "Human Activity Recognition on Microcontrollers with Quantized and Adaptive Deep Neural Networks",
            "text": "Similar considerations apply to the results on UCI HAPT, the second most complex of our benchmarks (Figure 8b). 4-bit networks are again on the global Pareto curve for architectures with intermediate size, whereas mixed-precision CNNs achieve the best trade-off for high accuracy values (> 80%), obtaining them with far less parameters than any fixed-precision solution. Precisely, our best mixed-precision CNN obtains an accuracy of 85.63%, i.e., +1.3% with respect to the best 8-bit quantized network, with a memory reduction of 77%. At the other end of the Pareto curve (accuracy < 60%) full 1-bit quantization (binarization) also configures as an interesting alternative, generating extremely small yet still accurate models (considering that this dataset has 12 classes). \n\nThe WISDM dataset shows again a similar trend (Figure 8c), despite supporting a lower number of classes than the previous two and using yet another classification metric. In this case, however, all quantization bit-widths above 2-bit achieve very similar trade-offs. Mixed-precision CNNs are able to reduce the memory occupation by up 66% with respect to 8-bit networks for the same F1-score. \n\nWALK is the only dataset that exhibits a different trend (Figure 8d), with BNNs occupying most of the global Pareto-curve. This is mainly due to the simpler binary classification task, which calls for more compact models (see the different range of the x axis). A fixed-precision 4-bit network reaches the highest balanced accuracy of 95.74%, while saving 91% of the memory when compared to the most accurate 8-bit network. Figure 9 shows the confusion matrix of the most accurate model for each dataset as a heat map, confirming that the networks found during the proposed search, despite their small size and low-precision quantization, are able to effectively differentiate among classes, handling correctly the imbalance of the considered HAR datasets. \n\nOverall, these results show that using sub-byte quantization, both at fixed-and mixed-precision, permits significant memory savings for HAR tasks with no classification score drop.",
            "score": 0.5007168755675371,
            "section_title": "Memory Occupation",
            "char_start_offset": 50818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2113
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "173990430",
            "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model",
            "text": "The Transformer model using self-attention mechanism has recently achieved the state of the art accuracy in language translation (Vaswani et al., 2017). Compared to its predecessors, this sequence transduction model \n\nThe growing presence of machine language translation services and tools (Microsoft, 2018), (Google, 2018), (AWS, 2018) and (LingoTek, 2019) to name a few, clearly shows that machine translation inference is an important workload. Quantization is a technique to improve the performance of inference workloads by using lower precision data types (8-bit, 4-bit or 2-bit integers) in place of 32-bit floating point. The latest Intel R Xeon R Cascade Lake processors include specialized vectorized neural network instructions (VNNI) to expedite quantized inference by fusing 64 8-bit multiply and add (FMA) operations into a single instruction (Fomenko, 2018). This means that the vectorized FMAs can be completed in fewer clock cycles than previous generation Intel R Xeon R processors. As a result, 8-bit matrix multiplications (MatMuls) or quantized MatMuls execute faster on these platforms. This motivated us to explore the impact of VNNI on the performance of Transformer model inference. To the best of our knowledge, the Transformer model has not been quantized before. However, the impact of quantized MatMuls on the overall performance of Transformer inference was not known before this work as speedup between INT8 and FP32 MatMuls depend on the shape and size of the matrices involved. \n\nAdditionally, we want to minimize the drop in translation accuracy which can result due to the usage of reduced precision data types. In this work, our contributions include the following: \n\n1. Quantized a trained FP32 Transformer model to INT8 to achieve < 0.5 drop in state-of-the-art (SOTA) BLEU score. \n\n2. Improve inference performance by: The rest of the paper is organized as follows. In section 2, we describe prior work on quantization techniques for deep learning models. In section 3, we provide a brief description of the Transformer translation model.",
            "score": 0.5004916582898553,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "259282673",
            "title": "A Metric Driven Approach to Mixed Precision Training",
            "text": "The wide success of Deep neural networks has led to continued increases in model sizes and the computing resources needed to train them. Further the introduction of Large Language Models has dramatically increased this demand for training and serving. Such a massive demand for system resources outperforms Moore's Law and hardware capabilities by a wide margin. Several model efficiency techniques have been proposed to mitigate this unprecedented demand [5], [11], including the use of reduced precision operations. Quantization -the process of reducing the number of bits used to represent a number, can improve the performance of deep learning models by reducing the amount of memory and computational power required. \n\nDeep learning training today includes a wide range of data types. Common floating point types include IEEE single precision, FP32 mode for single precision, IEEE half precision [8], and bfloat16 [1], [7]. More recently 8 bit types have been introduced in deep learning accelerators with trade-offs between the exponent and the mantissa bits to accommodate the needs of different operations within a model. In addition to floating bit representations integer hardware has also been introduced and has two key advantages -(1) Area and energy efficient hardware units (2) Fewer sources of introduced error within the accumulation hardware unit. A given neural network accelerator may provide a few different numerical data types depending on its applicability to operations within the model structure. While the choice of data types provides flexibility in training, it is often a complex search for the right set of numerics for a given model. At bit widths of 16 bits and lower a careful quantization application is required, without which model quality suffers. \n\nWe make the following contributions \n\n\u2022 Develop a metric driven methodology to guide the use of different low precision numeric formats. \u2022 Demonstrate the methodology predicts training quality using different mixed precisions for the BERT model.",
            "score": 0.5000234060481228,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 901,
                    "end": 904,
                    "matchedPaperCorpusId": "3297437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "265066991",
            "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization",
            "text": "Second, we observe that underflow, where smallmagnitude values round to zero, severely impacts W4A8 quantization in LLMs because the quantization error associated with values rounding to zero constitutes a significant portion of the output error. While underflow is a well-known issue in reduced-precision formats for deep neural networks (DNNs) (Sun et al., 2019(Sun et al., , 2020;;Chmiel et al., 2022;Jin et al., 2022), previous PTQ research in LLMs mainly focuses on outliers, neglecting underflow. We discover that standard INT4 representation discards crucial small-magnitude weights when multiplied with activations. As existing data formats like integer, floating-point, or logarithmic formats are inadequate for this underflow issue, we introduce dINT, a new integer format with denormal representation. dINT merges the uniform coverage of integers with the denormal of floatingpoints, effectively mitigating underflow and improving accuracy. We also propose a MAC unit supporting dINT to ensure hardware efficiency. \n\nWe evaluate AQAS, SLAC, and dINT on OPT and LLaMA, focusing on language modeling, zeroshot reasoning, and 5-shot in-context learning. The results show that integrating these methods for W4A8 PTQ significantly improves task accuracies for both OPT and LLaMA across a diverse set of benchmarks (Wikitext, Common Sense Question Answering (CSQA), and Massive Multitask Language Understanding (MMLU)) and the model sizes ranging from 125M to 65B parameters.",
            "score": 0.4997238574257612,
            "section_title": "Introduction",
            "char_start_offset": 3443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 363,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 404,
                    "end": 421,
                    "matchedPaperCorpusId": "246705922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "227127479",
            "title": "HAWQV3: Dyadic Neural Network Quantization",
            "text": "Quantization is one of the key techniques used to make Neural Networks (NNs) faster and more energy efficient. However, current low precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing NNs. To address this, we present HAWQV3, a novel dyadic quantization framework. The contributions of HAWQV3 are the following. (i) The entire inference process consists of only integer multiplication, addition, and bit shifting in INT4/8 mixed precision, without any floating point operations/casting or even integer division. (ii) We pose the mixed-precision quantization as an integer linear programming problem, where the bit precision setting is computed to minimize model perturbation, while observing application specific constraints on memory footprint, latency, and BOPS. (iii) To verify our approach, we develop the first open source 4-bit mixed-precision quantization in TVM, and we directly deploy the quantized models to T4 GPUs using only the Turing Tensor Cores. We observe an average speed up of $1.45\\times$ for uniform 4-bit, as compared to uniform 8-bit, precision for ResNet50. (iv) We extensively test the proposed dyadic quantization approach on multiple different NNs, including ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For instance, we achieve an accuracy of $78.50\\%$ with dyadic INT8 quantization, which is more than $4\\%$ higher than prior integer-only work for InceptionV3. Furthermore, we show that mixed-precision INT4/8 quantization can be used to achieve higher speed ups, as compared to INT8 inference, with minimal impact on accuracy. For example, for ResNet50 we can reduce INT8 latency by $23\\%$ with mixed precision and still achieve $76.73\\%$ accuracy.",
            "score": 0.4992443128967714,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "235485236",
            "title": "Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration",
            "text": "In particular, it can also combine with other strategies (e.g., pruning) to achieve more efficient performance. \n\nModel quantization methods mainly include low-bit quantization approaches (e.g., BNN [17], XNOR-Net [11], TWN [18], and TBN [19]) and M -bit (e.g., M = 4 or M = 8) quantization approaches (e.g., DoReFa-Net [20], INQ [21], ABC-Net [12], and LQ-Nets [22]). Those low-bit quantization methods can achieve extreme model compression, computational acceleration and resource saving. For example, XNOR-Net [17] uses bitwise operations (i.e., xnor and bitcount) to replace full-precision matrix multiplication, achieving 58\u00d7 speedups and 32\u00d7 memory saving in CPU [11]. As discussed in [23], FP-BNN attains a higher acceleration ratio on FPGA, which can speed up to about 705\u00d7 in the peak condition compared with CPU, and is 70\u00d7 faster than GPU. However, most models were proposed for a fixed precision, and cannot be extended to other precision models. The representation capability of low-bit parameters is insufficient for many practical applications, especially for large-scale image classification (e.g., ImageNet) and regression tasks. Therefore, the lowbit quantization methods suffer from significant performance degradation. In contrast, M -bit quantization networks have better representation capability than low-bit quantization ones, and thus can be applied to more complex real-world applications. Some scholars use M -bit quantization to improve both accuracy and compression ratio of their networks, but seldom consider their computational acceleration [16], [21], [24]. Both ABC-Net [12] and LQ-Nets [22] used the linear combination of multiple binary parameters constrained to {\u22121, +1} to approximate full-precision weights and activations. Therefore, the complex full-precision matrix multiplication can be decomposed into some simpler operations.",
            "score": 0.4985879723129343,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "13556195"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "52961271"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "12130431"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "10533533"
                },
                {
                    "start": 1621,
                    "end": 1625,
                    "matchedPaperCorpusId": "50784025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "274116726",
            "title": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference",
            "text": "Reduced-precision data formats are vital for enhancing scalability and computational efficiency in deep learning accelerators, conserving area and energy in direct proportion to bit-width reduction (Horowitz, 2014). This scaling enables higher floating-point operations per second (FLOPS) with lower power usage, thereby increasing accelerator throughput. For instance, NVIDIA's Tensor Cores have progressed from FP16 in Volta (Nvidia, 2017) to FP8 in Hopper (Andersch et al., 2022) and FP4 in Blackwell (Nvidia, 2024), boosting computational speeds from 112 tera to 20 peta FLOPS. Similar advancements by other computing platform companies in scaling precision from 16-bit to 4-bit are crucial for managing the growing complexity of LLMs (AMD, 2024;AzureAI, 2024). \n\nReduced-precision computation takes advantage of neural networks' resilience to small numerical errors, though aggressive scaling often results in accuracy loss. To address this, various algorithmic enhancements have been developed. \n\nEarly methods introduced formats like BFloat16 (Burgess et al., 2019) and DLFloat (Agrawal et al., 2019) to optimize the exponent and mantissa trade-off. More recent research has advanced 8-bit (Wang et al., 2018;Sun et al., 2019) and 4-bit (Sun et al., 2020) formats, although FP4 has shown noticeable accuracy degradation. To enhance  low-precision LLM inference, the microscaling (MX) format (Rouhani et al., 2023a;Darvish Rouhani et al., 2023;Rouhani et al., 2023b) has been developed from Block Floating Point (BFP) (Drumond et al., 2018;Darvish Rouhani et al., 2020) by incorporating a shared scale across a block of reduced-precision elements, thus mitigating quantization error due to limited dynamic range.",
            "score": 0.49804083335016075,
            "section_title": "Bit-Precision Scaling for Accelerators",
            "char_start_offset": 6259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 1050,
                    "end": 1072,
                    "matchedPaperCorpusId": "204819410"
                },
                {
                    "start": 1085,
                    "end": 1107,
                    "matchedPaperCorpusId": "204816525"
                },
                {
                    "start": 1197,
                    "end": 1216,
                    "matchedPaperCorpusId": "53977760"
                },
                {
                    "start": 1216,
                    "end": 1233,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 1244,
                    "end": 1262,
                    "matchedPaperCorpusId": "202779157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1827392578125
        },
        {
            "corpus_id": "207769430",
            "title": "Post-Training 4-bit Quantization on Embedding Tables",
            "text": "Post-training quantization is simple to use and convenient for rapid deployment. Recent studies have shown post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]. \n\nSeveral state-of-the-art post-training quantization techniques that rely on the clipping have been proposed to mitigate accuracy degradation. Shin et al. [24] and Sung et al. [25] approximated the inputs as a histogram and adopt a clipping threshold that minimizes the 2 norm of the quantization error. Migacz et al. [19] proposed an iterative approach to search for the clipping threshold based on Kullback-Leibler Divergence measure for quantizing activations. Later, Banner et al. [3] proposed ACIQ, an analytic solution that computes the optimal clip threshold by assuming the input values are sampled from a Gaussian or Laplacian distribution. Although these approaches are demonstrated to reduce the accuracy drops to some extent, the problem of post-training 4-bit quantization without accuracy drop is still unsolved yet. Empirically, we also observe that the above-mentioned approaches can result in significant accuracy drops when applied to embedding table quantization. \n\nIn this paper, we explore a variety of post-training 4-bit quantization methods on embedding tables and propose novel quantization approaches that can reduce model size while incurring negligible accuracy degradation. Quantization on embedding tables is usually applied to row vectors (row-wise quantization) to reduce quantization error. Throughout the paper, quantization is applied to row vectors unless noted differently. We notice that the prior post-training quantization approaches approximate the inputs to quantize using either a histogram or some distributions.",
            "score": 0.4980271217964394,
            "section_title": "Introduction",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "7583752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "235651596",
            "title": "Adaptive Binary-Ternary Quantization",
            "text": "Deep Neural Network (DNN) models have achieved tremendous attraction due to their success on a wide variety of tasks, including computer vision, automatic speech recognition, natural language processing, and reinforcement learning [3]. More specifically, in computer vision DNN have led to a series of breakthrough for image classification [6], [16], [17], and object detection [14], [11], [15]. DNN models are computationally intensive and require large memory to store the model parameters. Computation and storage resource requirement becomes the main obstacle to deploy such models in many edge devices due to lack of memory, computation power, energy, etc. This motivated the researchers to develop compression techniques to reduce the cost for such models.\n\nRecently, several techniques have been introduced in the literature to solve the storage and computational limitations of the edge devices. Among them, quantization methods focus on representing the weights of a neural network in lower precision than the usual 32-bits float representation, saving on the memory footprint of the model. Binary quantization [1], [4], [13], [19], [9], [2] represent weights with 1 bit precision and ternary quantization [10], [8], [20] with 2 bits precision. While the latter frameworks lead to significant memory reduction compared to their full precision counterpart, they are constrained to quantize the model with 1 bit or 2 bits, on demand. We relax this constraint, and present Smart Quantization (SQ) that allows adapting layers to 1 bit and 2 bits while training the network. Consequently, this approach automatically quantizes weights into binary or ternary depending upon a trainable control parameter. We show that this approach leads to mixed bit precision models that beats ternary networks both in terms of accuracy and memory consumption. Here we only focus on quantizing layers because it is more feasible to implement layer-wise quantization at inference time after training. However, this method can be also adapted for mixed precision training of sub-network, block, filter, or weight.",
            "score": 0.497995111425937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "6453539"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "10533533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451171875
        },
        {
            "corpus_id": "259937594",
            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
            "text": "However, they mainly analyze the general performance of quantized LLMs (e.g., language modeling), lacking a deep investigation into LLM's abilities on complex tasks. \n\nIn this work, we focus on examining the per-1 There is still no consensus on the existence of emergent abilities, due to the lack of continuity in evaluation metrics and model sizes in the empirical study (Wei et al., 2022). It is also known that small models can possess some emergent abilities with special adaptation. Despite that, we still use this term to emphasize the superior performance of LLMs. \n\nformance of quantized LLMs on solving complex tasks, to explore the impact of quantization on the emergent abilities of LLMs. As demonstrated in previous studies (Wei et al., 2022), there exists a strong dependency between emergent abilities and parameter scale. It is curious whether the emergent abilities would vanish under the setting of low-bit precision though the model size remains to be the original scale. In addition, it is also important to explore the factors (e.g., the model structure) that potentially affect the emergent abilities. Furthermore, we are also interested in the potential approaches to enhance the performance of the low-bit models. \n\nSpecially, we aim to answer the following two questions: (1) Do emergent abilities exist in quantized large language models? If so, what level of performance it can achieve? (2) How to enhance the performance of low-bit models? To answer the two key questions, we assess three key abilities, namely in-context cearning (ICL), chain-of-thought reasoning (CoT), and Instruction-Following ability (IF), on a collection of LLaMA models (Touvron et al., 2023) which are widely used as the backbone models. We conduct extensive empirical experiments, aiming to gain a better understanding of the model performance of quantized LLMs. \n\nFor the first question, we evaluate the LLaMA models at four sizes (i.e., 7B, 13B, 30B, and 65B), examining their performance across a range of precision levels: 2-bit, 4-bit, 8-bit, and 16-bit.",
            "score": 0.4979515879977951,
            "section_title": "Introduction",
            "char_start_offset": 1843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2063
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487548828125
        },
        {
            "corpus_id": "274514776",
            "title": "CPTQuant - A Novel Mixed Precision Post-Training Quantization Techniques for Large Language Models",
            "text": "In another approach, Gpt3.int8() (Dettmers et al., 2022) combines INT8 and FP16 to address activation outliers. Though this method controls data range, it can introduce latency overheads and possibly making less efficient than using FP16 alone. To address activation outliers, the outlier suppression technique (Wei et al., 2022) uses non-scaling LayerNorm and token-wise clipping. These methods are effective for smaller models such as BERT (Devlin et al., 2018) and BART (Lewis et al., 2019) but struggle to maintain accuracy in larger LLM configurations. \n\nResearchers have begun exploring cost-effective techniques for larger LLM models to facilitate efficient inference. SmoothQuant (Xiao et al., 2023) enables 8-bit quantization for both weights and activations and significantly reduces memory usage and computational demands. The activationaware weight quantization (AWQ) (Lin et al., 2023) method selectively protects salient weights based on activation observation. Half precision (FP16) optimizes the performance of neural networks by using 16-bit floating point precision, significantly reducing memory usage and speeding up computation compared to full precision (FP32). Additionally, LUT-GEMM (Park et al., 2022) introduces efficient GPU kernels tailored for specific binary-coding-based quantization. Though several post-training quantization schemes are available in the literature, mixed-precision post-training quantization methodologies are relatively rare. Our proposed approach utilizes mixed-precision posttraining quantization and demonstrates more sophisticated and precise strategies to quantize largelanguage models. Specifically, CPTQuant achieves more than double the compression compared to previous techniques while maintaining a similar level of accuracy.",
            "score": 0.49786875903129935,
            "section_title": "Related Works",
            "char_start_offset": 7240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 329,
                    "matchedPaperCorpusId": "252545187"
                },
                {
                    "start": 688,
                    "end": 707,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 880,
                    "end": 898,
                    "matchedPaperCorpusId": "253708271"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445556640625
        },
        {
            "corpus_id": "274815509",
            "title": "Federated and edge learning for large language models",
            "text": "Presently, a prevalent strategy in research to tailor LLMs for edge computing involves compressing the model [196]. This compression is achieved through various methods, with model pruning, model quantization, and knowledge distillation (KD) being the most commonly utilized techniques. \n\n\u2022 Model quantization. This process aims to reduce the size of Language Models (LLMs) by modifying how model weights are stored [197]. Typically, deep neural network weights are stored as 32-bit floating-point numbers. Quantization, as discussed in literature [198], involves using tensors with fewer bits, commonly reducing them to 16 bits, 8 bits, and 4 bits. Model quantization is further categorized into weight-only quantization and weightactivated quantization based on the quantization scope [159,199]. This distinction is essential because the activation function is more sensitive to quantization. However, retaining softmax without quantization or maintaining higher accuracy might introduce additional latency [200]. The quantization process is further detailed based on execution steps, with two main approaches: post-training quantization (PTQ) and quantization-aware training (QAT) [201]. PTQ involves converting a pre-trained model into a quantized version without additional training, making it faster and more cost-effective. In contrast, QAT employs an extended training process that simulates quantization effects, ensuring the model's adaptability to reduced precision without compromising performance. \n\nThe studies conducted by [105,202] demonstrate that Transformer models like BERT and GPT-3 can significantly enhance memory efficiency and speed by reducing the precision of weights and activations to INT8. Another innovative approach, presented in [163], introduces Sparse Quantized Representation (SpQR), which encodes and decodes weights during quantization, resulting in a 15% speedup for running LLMs. To further enhance the flexibility of quantization, [150] introduces the Quantization-Aware Low-Rank Adaptive (QA-LoRA) technique. This method involves quantizing LLM weights to INT4 during fine-tuning, incorporating both LLM and auxiliary weights into the quantized model without compromising accuracy.",
            "score": 0.49690893966263994,
            "section_title": "Model compression: enabling edge computing",
            "char_start_offset": 44841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 421,
                    "matchedPaperCorpusId": "252545187"
                },
                {
                    "start": 548,
                    "end": 553,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 1009,
                    "end": 1014,
                    "matchedPaperCorpusId": "261530032"
                },
                {
                    "start": 1538,
                    "end": 1543,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458251953125
        },
        {
            "corpus_id": "271723297",
            "title": "A Comprehensive Approach Toward Wheat Leaf Disease Identification Leveraging Transformer Models and Federated Learning",
            "text": "The key innovations consist of Flexible Dual Binarization (FDB), which divides 2-bit quantized weights into two binary sets to enable efficient operations without sacrificing accuracy, and Deviation-Aware Distillation (DAD), which modifies the distillation loss to prioritize ambiguous samples. The experiments conducted on LLaMA-1 and LLaMA-2 models provide evidence that DB-LLM surpasses existing quantization approaches in terms of perplexity and accuracy, resulting in a 20% decrease in computing expenditure. \n\nQin et al. [17] presented IR-QLoRA, an innovative approach to enhance the precision of quantized large language models (LLMs) by LoRA finetuning. The system employs two primary methodologies: Statistics-based Information Calibration Quantization, which effectively preserves the original information in quantized parameters, and Finetuning-based Information Elastic Connection, which enables versatile representation of information. The experiments demonstrate that IR-QLoRA greatly enhances the accuracy of the LLaMA and LLaMA2 models when using 2-4 bit-widths. This improvement in accuracy is achieved with only a slight increase in time consumption, showcasing the efficiency and versatility of IR-QLoRA. \n\nQin et al. in [18] introduced BiBench, a benchmark designed to examine network binarization holistically. It mainly evaluates the prerequisites for viable binarization, sets evaluation metrics, and assesses milestone binarization algorithms. Key findings include the crucial importance of binarized operators on performance and deployability, large accuracy variations across jobs and architectures, and promising efficiency on edge devices despite hardware limits. \n\nHuang et al. in [19] introduced BiLLM, a revolutionary 1-bit post-training quantization approach for LLMs. BiLLM discovered salient weights and minimized compression loss by binary residual approximation, while accurately binarizing non-salient weights using an optimal splitting search. BiLLM produced high-accuracy inference (e.g., 8.41 perplexity on LLaMA2-70B) with 1.08-bit weights, greatly surpassing state-of-the-art quantization approaches.",
            "score": 0.49674735380930635,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 12446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356201171875
        },
        {
            "corpus_id": "249712097",
            "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
            "text": "Table 1 shows the WER results at different stages of the quantization. At beam width 16, the fusion process improves the non-quantized model WER by 2.0% while almost doubling the model size. As discussed in section 4.1, LM fusion also increases inference runtime dramatically. \n\nINT4 quantization of the RNN-T alone (following the scheme shown in fig. 1 but without LMEXT and LMSRC) shows a small WER degradation across datasets (0.6% on average for Hub5 2000) with a compression factor of 7.2\u00d7 (vs. FP32) and a 3.4\u00d7 acceleration (vs. FP16). Although implementation of the Density Ratio LM Fusion approach shows a remarkable WER improvement on the INT4 quantized RNN-T, it is apparent that it also calls for quantization of LMEXT and LMSRC to retain the gains in model size and acceleration. \n\nWe initialize the INT8 and INT4 LMEXT with pre-trained FP32 weights, which we found helpful to minimize WER degradation. The 4-bit LMEXT quantized as per Fig. 1, achieves comparable WER to the FP32 counterpart across benchmarks. To further boost compression and acceleration, we use an equivalent strategy to quantize the LMSRC at 4 bits, with no change in performance. With both LMs fully quantized at 4 bits, we obtain a 1.7% WER improvement on Hub5 2000 compared to the model without LM fusion, while benefiting from a close-to-ideal 7.6\u00d7 compression ratio of the full model and a 3.4\u00d7 acceleration. \n\nIn the beam width comparison, the INT4 model with LM fusion achieves at beam width 16 better WER than the full precision model without LMs at any beam widths, while providing a substantial reduction in model size and comparable RTF (as discussed in sec. 4.1). \n\nMore aggressive quantization of the RNN-T weights (INT2) shows increased but limited WER degradation in the model without LMs, \u22641% across benchmarks compared to INT4 weights.",
            "score": 0.49644702192558404,
            "section_title": "Accuracy and compression",
            "char_start_offset": 16827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "278326880",
            "title": "An Empirical Study of Qwen3 Quantization",
            "text": "The newly released Qwen3 series has emerged as one of the most capable open-source LLM families, garnering substantial attention from both academia and industry. In this study, we present the first systematic evaluation of Qwen3's robustness under various low-bit quantization schemes, with particular focus on post-training quantization methods. Our investigation seeks to establish practical boundaries for deploying Qwen3 in resource-constrained scenarios through comprehensive quantization analysis. \n\nOur experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios. \n\nThese findings underscore two critical implications: (1) current quantization techniques require further innovation to better preserve Qwen3's advanced capabilities, and (2) the trade-offs between model compression and performance retention need careful reconsideration for state-of-the-art LLMs. We believe our empirical analysis provides valuable guidance for future research directions in LLM quantization, particularly in developing methods that can maintain high accuracy at ultra-low  bit-widths. As the field progresses, we anticipate these insights will contribute to more efficient deployment of powerful models like Qwen3, ultimately advancing the practical applications of large language models while reducing their computational overhead. \n\nFuture Work. We plan to evaluate more advanced forms of quantization methods, such as channel reordering-based approaches [18] and rotation-based quantization strategies [11], to assess Qwen3's performance under these techniques, particularly regarding their impact on activation quantization. Table 1: 2 to 8-bits per-channel PTQ results of Qwen3-Base Models. We report ppl on Wikitext2 and c4, 0-shot reasoning tasks and 5-shot mmlu performance.",
            "score": 0.4963973602685909,
            "section_title": "Conclusion",
            "char_start_offset": 4881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "102350477",
            "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision",
            "text": "In many real-time machine learning applications (such as robotics, autonomous driving, and mobile VR/AR), deep neural networks is strictly constrained by the latency, energy, and model size. In order to improve the hardware efficiency, many researchers have proposed to quantize the weights and activations to low precision [8,18,34]. \n\nConventional quantization methods use the same number of bits for all layers [2,14], but as different layers have different redundancy and behave differently on the hardware (computation bounded or memory bounded), it is necessary to use mixed precision for different layers (as shown in Figure 1). This flexibility was originally not supported by chip vendors until recently the hardware manufacturers started to implement this feature: Apple released the A12 Bionic chip that supports mixed precision for the neural network inference [6]; NVIDIA recently introduced the Turing GPU architecture that supports 1-bit, 4-bit, 8-bit and 16-bit arithmetic operations [21]; Imagination launched a flexible neural network IP that supports per-layer bitwidth adjustment for  1: Inference latency of MobileNet-V1 [12] on three hardware architectures under different quantization policies. \n\nThe quantization policy that is optimized for one hardware is not optimal for the other. This suggests we need a specialized quantization solution for different hardware architectures. (HW1: BitFusion [25], HW2: BISMO [26] edge accelerator, HW3: BISMO cloud accelerator, batch = 16). \n\nboth weights and activations [13]. Besides industry, recently academia also works on the bit-level flexible hardware design: BISMO [26] proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion [25] supports multiplications of 2, 4, 8 and 16 bits in a spatial manner. However, a very missing part is how to determine the bitwidth of both weights and activations for each layer on different hardware accelerators. This is a vast design space: with M different neural network models, each with N layers, on H different hardware platforms, there are in total O(H \u00d7 M \u00d7 8 2N ) * possible solutions.",
            "score": 0.49635409875612785,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "38486148"
                },
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "224893"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "21681898"
                },
                {
                    "start": 1438,
                    "end": 1442,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "21681898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28662109375
        },
        {
            "corpus_id": "247187847",
            "title": "Multi-task Learning Approach for Modulation and Wireless Signal Classification for 5G and Beyond: Edge Deployment via Model Compression",
            "text": "As one of the key motivations of this paper is to enable and demonstrate the design of lightweight neural network architecture for support on resource-constrained edge devices, we now discuss the prospects of model compression. The challenging part of the MTL design process is the careful hyper-parameter tuning as discussed in section IV to maintain the classification accuracy across multiple tasks. In this section, we emphasize the significance of model compression and present it as an important step to consider in the deep learning pipeline for resource-limited edge platforms. Fig. 22 shows the proposed deep learning pipeline whereby the model is trained, validated, tested, and also undergoes a necessary step model compression and refining to arrive at a well balanced compressed model. Although model compression techniques have been explored and practiced heavily for computer vision and NLP, their application in the wireless  Deep learning models, in general, consume substantial compute and memory resources [62], which can exhaust even powerful servers let alone resource-constrained edge devices. \n\nSeveral algorithms, software, and hardware have been proposed and implemented to lighten the imposed burden on resources [62], [63]. Among which, quantization is a promising approach. Quantization is a reduced precision strategy whereby the weights and activations of the trained neural network is shrunk from higher to lower bit precision. The standard number format in deep learning is floating point 32 (FP32), specified in IEEE754 [64]. The FP32 representation is referred to as single precision. Similarly, the format which use half the bits of FP32 is called half precision (floating point 16). tf.lite.TFLiteConverter.from_keras_ model(QatModel). 4) Specify the optimization policy for the converter class as tf.lite.Optimize.DEFAULT which would quantize the weights to 8-bits precision. Finally, the convert function will generate the quantized model. We state here that the FP32 model which in itself was handcrafted to be lighter in architecture had a model size of 2.97 MB. The INT8 quantization yielded a 11.8\u00d7 smaller model, i.e., of size 251.6 kB.",
            "score": 0.4961963632626222,
            "section_title": "VII. COMPRESSED MODEL -QUANTIZED NEURAL NETWORK",
            "char_start_offset": 42769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "3541031"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "3541031"
                },
                {
                    "start": 1245,
                    "end": 1249,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "61507968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "259075884",
            "title": "Binary and Ternary Natural Language Generation",
            "text": "s. In order to achieve this, we combine and unify best practices for weight and activation quantization and present a frame-work that uses gradient-matching quantization for weights and elastic quantization for activations. We apply our method to natural language generation tasks and, for the first time, demonstrate low-bit generative transformers of competitive accuracy. Our ternary (weight and activation) model lags a full-precision BART (Lewis et al., 2020) model by only 4 points in ROUGE on the XSUM summarization dataset. In contrast, our model with ternary weights and 8-bit activations comes within 1 point and even outperforms comparable state-of-the-art models with 8-bit weights. We also demonstrate a fully binary (weights and activations) model. While not as competitive, it is able to achieve a highly non-trivial ROUGE-1 score of 31.7.\n\nOur results also extend to machine translation models. On the WMT16 En-Ro benchmark, we quantize an mBART model to extend the ternaryweight 8-bit activation SoTA by 1.2 points while demonstrating fully ternary and fully binary translation models for the first time.\n\nWe summarize our contributions as follows: \u2022 We propose a novel combination of statisticsbased weight quantization with learning-based activation quantization, which enables stably training transformer encoder-decoder models to converge in the fully ternary/binary settings, which was not previously possible.\n\n\u2022 We significantly improve the state-of-the-art text generation models in the 8-bit activation and ternary/binary weight settings while setting the first non-trivial baselines for the fully ternary and fully binary settings.",
            "score": 0.4960547051444022,
            "section_title": "Introduction",
            "char_start_offset": 2137,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 464,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "250334271",
            "title": "Attention Round for Post-Training Quantization",
            "text": "In this paper, different deep learning model architectures were selected for quantification experiments, including Resnet [44]MobilenetV2 [45], Regnetx [46] and also with the Mnasnet [47]. Ordinary convolution operators are included in Resnet, deep separable convolutions in MobilenetV2 and group convolutions in Regnetx. Besides, quantitative experiments on architectures obtained through neural architecture search. First, only the weights were quantified, and the experimental results are shown in Table 1. When the weights were quantified to 6 bit, the quantified model accuracy was comparable to that of the floating-point model. As the width of the quantization bit decreases, the quantization model accuracy gradually decreases. Then, by comparing the method with the most popular post-training quantization algorithms, it is seen that the method achieves better performance than these algorithms when quantifying bit widths of 4bit and 3bit. For example, when the bit width is 4 bit, the restnet18 is quantified with a quantization accuracy of 70.72, and the resnet50 with a quantization accuracy of 76.44. Excellent performance is also achieved on different bit widths and models. \n\nTo validate the performance of our quantification algorithm, we quantified both the model weights and the activation values, and the experimental results are presented in Table 2. When the weights and activation values were simultaneously quantified up to 6 bit, the performance of the quantization model obtained using the proposed method was even better than that quantified up to 8 bit using the DFQ method. When the weight and activation value of the model are quantified to 4 bit, the quantification accuracy of resnet18 model is 69.65 and the resnet50 model is 74.89. When the weight and activation value of the model are quantified to 3 bit, the quantification accuracy of resnet18 model is 68.55 and the resnet50 model is 73.86. As can be seen from Tables 1 and 2, the algorithms proposed here all achieved better performance than the other algorithms by using different quantification accuracy for different models.",
            "score": 0.49592723581705167,
            "section_title": "Results on ImageNet",
            "char_start_offset": 21038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2116
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "258587983",
            "title": "Post-training Model Quantization Using GANs for Synthetic Data Generation",
            "text": "Prompted by the ever-increasing size of deep neural networks, techniques for reducing their computational costs have been thoroughly studied for efficient learning. Early attempts focused on reducing the number of network parameters by grouping the weights [6], replacing costly operations such as fully connected layers [36], or by pruning connections between layers [16,26,27]. Network quantization has also been studied, with early work representing weights and activations using only a single bit, introducing the Binarized Neural Networks (BNNs) [9,21]. While this representation achieved a substantial reduction in computational costs, it also led to accuracy degradation on more complex models and datasets. In Gupta et al. [13], the authors demonstrated that using 16-bit fixed-point representation with stochastic rounding when training a CNN leads to negligible degradation in the classification accuracy. In Banner et al. [2], the precision was further reduced to an 8-bit representation while they quantized both weights and activations in all layers. Their proposed 8-bit training did not affect the models' accuracy when trained on a largescale dataset. In Zhang et al. [39], the authors proposed a quantization method by training quantizers from data. In Han et al. [14], they attempted to compress deep neural networks by combining pruning, trained quantization, and Huffman coding. Although their method could significantly compress the size of deep neural networks, the pruning process can be time-consuming and difficult to optimize. \n\nPost-training quantization has also been the focus of research. In Lin et al. [28], the authors proposed an SQNRbased optimization approach to convert a pre-trained floating point deep convolutional model to its fixed-point equivalent. In Banner et al. [3], 4-bit quantization was proposed using the ACIQ method to optimize the clipping value, while in Choukroun et al. [7], they minimized the quantization MSE for both weights and activations. More recent studies suggested quantization by splitting outlier channels [40], using adaptive weight rounding [30], or bias correction [11].",
            "score": 0.4951293218803481,
            "section_title": "Related Work",
            "char_start_offset": 3016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "543597"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "61815367"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "2547043"
                },
                {
                    "start": 1633,
                    "end": 1637,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1808,
                    "end": 1811,
                    "matchedPaperCorpusId": "59292009"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "67750088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "202565587",
            "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT",
            "text": "Efforts in this area could be broadly categorized as follows: (i) new architectures that are compact by design [11,13]; (ii) automated neural architecture search (NAS) with reward function set as latency or model size [32,35]; (iii) pruning based methods to reduce model size of existing architectures [16,18]; (iv) knowledge distillation from a large model to help train a more compact model [1,10]; (v) hardware and architecture co-design [9]; and (vi) inference quantization [8,40]. \n\nHere we solely focus on quantization [4,6,8,14,17,24,40,42]. One of the challenges here is that ultra low precision quantization can lead to significant accuracy degradation. Mixed precision quantization [32,36,43] and multi-stage quantization [41] have been proposed to solve/alleviate this problem. However, the challenge with mixed-precision quantization is that the search space is exponentially large. For instance, if we have three precision options for a specific layer (2, 4 or 8-bits), then the total search space of each fine-tuned BERT model [7] becomes 3 12 \u2248 5.3\u00d7 10 5 different precision settings. Recently, [8] proposed a second-order sensitivity based method to address this issue and achieved state-of-the-art results on computer vision tasks. Part of our paper builds upon this prior work and extends the results to include other variations of second order information instead of just the mean value of the Hessian spectrum. \n\nCompressed NLP model. Notable examples for NLP compression work are LSTM and GRU-based models for machine translation and language model [33,37]. From the recent introduction of Tranformer models, we have observed a significant increase in NLP model size. This is due to the incorporation of very large fully connected layers and attention matrices in Transformers [7,19,22,30,38]. Model compression is crucial for deploying these models in resource constrained environments. Pilot works addressing this are [3,21].",
            "score": 0.4946004810273537,
            "section_title": "body",
            "char_start_offset": 3634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "54461508"
                },
                {
                    "start": 302,
                    "end": 306,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "4390700"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 528,
                    "end": 530,
                    "matchedPaperCorpusId": "1518846"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 538,
                    "end": 541,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 699,
                    "end": 702,
                    "matchedPaperCorpusId": "19227870"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "54056451"
                },
                {
                    "start": 1798,
                    "end": 1801,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1807,
                    "end": 1810,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "278602295",
            "title": "An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits",
            "text": "Large language models (LLMs) have revolutionized NLP but remain costly to deploy due to their size. Quantization [1] reduces memory and compute needs. Post-training quantization (PTQ) is fast but can harm accuracy, whereas quantization-aware training (QAT) recovers more performance at the cost of additional training [1,2]. Ternary quantization (2 bits) further cuts model size but is challenging to train. Building on earlier \"trained ternary quantization\" (TTQ) methods [6], BitNet [5] showed that a carefully pre-normalized Transformer can achieve a \"1.58-bit LLM\" using RMSNorm [7], bias removal, scaled SwiGLU, and straight-through estimator (STE) [3,8]. An extra RMSNorm before each quantized linear is especially crucial [5]. Though knowledge distillation (KD) [4,9,10] often helps low-bit models emulate full-precision teachers, recent findings suggest that direct fine-tuning with proper normalization can suffice [11]. Here, we show that adding RMSNorm layers at each linear input-plus a gradual quantization schedule-stably fine-tunes ternary LLMs, surpassing more complex KD setups.",
            "score": 0.49430423727981543,
            "section_title": "Introduction and Related Work",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 772,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 774,
                    "end": 777,
                    "matchedPaperCorpusId": "274749235"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435302734375
        },
        {
            "corpus_id": "270391795",
            "title": "QuantMoE-Bench: Examining Post-Training Quantization for Mixture-of-Experts",
            "text": "Post-training quantization is a family of model compression techniques that converts pre-trained model weights from highprecision formats (e.g., FP32) to lower-precision representations without model retraining or task-specific tuning. Recent works, such as GPTQ [8], which adapts quantization intervals based on the Hessian information, and SmoothQuant [18], which jointly quantizes the model weight and activation by offline migrating the activation outliers, have demonstrated the effectiveness of posttraining quantization for dense LLMs toward 4 bits compression. This advantage is particularly desired for MoE-based LLMs given their high training cost due to vast parameter sizes and prevailing deployment for various tasks [14,19]. \n\nHowever, our experiments show that directly deploy those techniques for MoE models leads to subpar performance. Existing methods employ a uniform bit-width across all components of the MoE model. This one-size-fits-all approach fails to account for the inherent sparse structure of the MoE architecture. For example, the sparse expert activations in MoE models exhibit distinct statistical properties from dense activations, suggesting adaptive bit allocation among experts' quantization. This yields the primary research question to be explored: \n\n(RQ) Do different MoE structures require varying numbers of bits for effective quantization? \n\nOur investigations reveal that different components in MoE require varying bit allocations, as shown in Figure 1. For example, shared experts and the first few MoE layers demand higher precision for effective quantization. Moreover, these findings naturally motivate two key questions: (1) How to identify the layers that are more sensitive to quantization; (2) How to systematically determine the importance of each MoE layer for bit allocation. To address these questions, we introduce novel data-driven techniques for optimizing bit allocation in MoE quantization, including the outlier-aware linear layer scorer that captures weight magnitude variations, and the MoE block importance predictor that leverages block-level activations patterns. Our key contributions are listed: \n\n(1) We establish the first benchmark for Mixture-of-Experts posttraining quantization, i.e., QuantMoE-Bench.",
            "score": 0.4942566422549347,
            "section_title": "Introduction",
            "char_start_offset": 1960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2165
                },
                {
                    "start": 2168,
                    "end": 2276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "244391498",
            "title": "Integer-Only CNNs with 4 Bit Weights and Bit-Shift Quantization Scales at Full-Precision Accuracy",
            "text": "Quantization of neural networks has been one of the most popular techniques to compress models for embedded (IoT) hardware platforms with highly constrained latency, storage, memory-bandwidth, and energy specifications. Limiting the number of bits per weight and activation has been the main focus in the literature. To avoid major degradation of accuracy, common quantization methods introduce additional scale factors to adapt the quantized values to the diverse data ranges, present in full-precision (floating-point) neural networks. These scales are usually kept in high precision, requiring the target compute engine to support a few high-precision multiplications, which is not desirable due to the larger hardware cost. Little effort has yet been invested in trying to avoid high-precision multipliers altogether, especially in combination with 4 bit weights. This work proposes a new quantization scheme, based on power-of-two quantization scales, that works on-par compared to uniform per-channel quantization with full-precision 32 bit quantization scales when using only 4 bit weights. This is done through the addition of a low-precision lookup-table that translates stored 4 bit weights into nonuniformly distributed 8 bit weights for internal computation. All our quantized ImageNet CNNs achieved or even exceeded the Top-1 accuracy of their full-precision counterparts, with ResNet18 exceeding its full-precision model by 0.35%. Our MobileNetV2 model achieved state-of-the-art performance with only a slight drop in accuracy of 0.51%.",
            "score": 0.494240933441667,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "269983763",
            "title": "AdpQ: A Zero-shot Calibration Free Adaptive Post Training Quantization Method for LLMs",
            "text": "In the field of low-precision deep learning, three existing notable categories are (i) low-precision or quantized training, (ii) quantization-aware training (QAT), and (iii) post-training quantization (PTQ).While our proposed method can be applied to both low-precision training (e.g.[7][8][9][10][11]) and QAT (e.g.[12][13][14] ), the primary focus of this section is to review the research around PTQ of LLMs which is found to be more challenging in the literature.\n\nHistorically, PTQ methods were common for computer vision models with small number of parameters, some notable methods are AdaRound [15], OBQ [16], AdaQuant [17], and BRECQ [18].However, these methods were found to be either compute-intensive or inaccurate for large language models.\n\nLLM.int8() [19] and ZeroQuant [20] are among the first PTQ techniques that were designed for LLMs.LLM.int8() separates the outlier activations and keeps them in floating-point number format while quantizing weights and non-outlier activations to 8-bit integers.LLM.int8() separates the outlier activations based on their magnitude.On the other hand, ZeroQuant uses a fine-grained hardware-friendly quantization scheme as well as layer-by-layer knowledge distillation for quantizing both weight and activations.However, both LLM.int8() and ZeroQuant are not efficient for quantizing LLMs to extreme low-percision number formats such as 3-bit integers.OPTQ [3] is a PTQ algorithm for LLMs that can quantize weights to 3-or 4-bit integers.OPTQ adapted a calibration algorithm inspired by [21] that minimizes the \u2113 2 loss of the quantized layer output with the original output.SpQR [4] uses OPTQ algorithm while separating the salient weights and keeping them in FP16 format and further uses double quantization to reduce the memory.",
            "score": 0.49420689658309996,
            "section_title": "Related Works",
            "char_start_offset": 5182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 284
                },
                {
                    "start": 284,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 467
                },
                {
                    "start": 469,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 752
                },
                {
                    "start": 754,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1627
                },
                {
                    "start": 1627,
                    "end": 1783
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "44071489"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "219629751"
                },
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "209515558"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "231855779"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "250644135"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "216056295"
                },
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "251765570"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "235825979"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "231861390"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "259298689"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "7057040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2386474609375
        },
        {
            "corpus_id": "258833451",
            "title": "Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models",
            "text": "Integer vs. Floating Point Formats. In computing systems, numbers are primarily represented as integers or floating points. The main difference between them is their value distribution. Integer format uniformly distributes values across its range, while the floating point format has a non-uniform distribution due to its exponent (E) and mantissa (M) design. To highlight the distribution differences between FP and INT, Figure 1 visualizes a small range of values around zero in INT8 and FP8-E5M2. As for hardware efficiency, FP operations generally incur higher costs than INT operations. However, for low-bit inference where the bit-width drops to 8/4 bits, their hardware efficiency is not well-established. Therefore, this work also benchmarks the hardware cost of INT and FP operations at various bit widths, as will be shown in Section III-A. LLM Post-training Quantization (PTQ) Methods. PTQ can be directly applied to pre-trained models without additional training. There are two PTQ methods for LLMs: 1) Quantization both weight (W) and activation (A) tensors (e.g. W8A8, needs 8-bit MAC hardware during inference); 2) W-only quantization (e.g. W4A16, uses 16-bit MAC during inference, explained in Section IV-C). Current SOTA methods for WA quantization include LLM.int8() [7] and SmoothQuant [8]. LLM.int8() employs mixed precision (INT8+FP16) to quantize individual tensors in LLMs, while SmoothQuant enhances LLM quantization accuracy by offline shifting the quantization difficulty from activations to weights. For W-only PTQ, the SOTA methods include GPTQ [9] and AWQ [14]. GPTQ leverages second-order information to offset the error of quantized weights, whereas AWQ pre-scales prominent weights before quantization.",
            "score": 0.4939488967498066,
            "section_title": "II. BACKGROUND AND RELATED WORKS",
            "char_start_offset": 3660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 1285,
                    "end": 1288,
                    "matchedPaperCorpusId": "251564521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "237940329",
            "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization",
            "text": ", 2018;Zhang et al., 2018;Choukroun et al., 2019;Dong et al., 2019;Esser et al., 2019;Nagel et al., 2019Nagel et al., , 2020, there is relatively little work done on quantizing NLP models (Wang et al., 2018b;, and specifically on transformer models.\n\nUnderstanding the challenges of transformer quantization and designing a robust and easy-touse quantization pipeline for them constitute the primary goal of this paper. The contributions of our work include:\n\n\u2022 We show that standard 8-bit post-training quantization techniques lead to a significant performance degradation for transformer encoder models.\n\n\u2022 We conduct a systematic study to identify the underlying reason that precludes efficient transformer quantization. We find that the main bottleneck is a considerable mismatch between the different dynamic ranges of activation tensors in the residual connections. Further analysis shows that these activation tensors contain structured outliers that facilitate specific attention patterns in deeper encoder layers, such as attending to the special [SEP] token. We highlight that this issue is inherent to many architectures and pre-training objectives.\n\n\u2022 Based on these findings, we propose a set of solutions with different trade-offs to overcome the dynamic range problem, including techniques based on post-training, mixed precision, and quantization-aware training. In particular, we introduce a new per-embeddinggroup quantization scheme, which solves the activation quantization issue without a significant compute overhead or increase in complexity.\n\n\u2022 Finally, we show that weights and embeddings in BERT-like models can be quantized to ultralow (2-4) bits, reducing the memory footprint by more than 8\u00d7 with a minimal accuracy loss.\n\nWe evaluate our proposed solutions on eight different NLP tasks from the well-known GLUE benchmark. Our techniques set a new state-ofthe-art of post-training quantization and per-tensor quantization-aware training for the BERT model. To the best of our knowledge, this is the first work for the BERT-like transformer quantization with a strong focus on post-training quantization. The presented method is not exclusive to BERT and is easily applicable to other pre-",
            "score": 0.4938982380233813,
            "section_title": "Introduction",
            "char_start_offset": 1802,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 7,
                    "end": 26,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 49,
                    "end": 67,
                    "matchedPaperCorpusId": "148571720"
                },
                {
                    "start": 188,
                    "end": 208,
                    "matchedPaperCorpusId": "54056451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "272881356",
            "title": "Accumulator-Aware Post-Training Quantization",
            "text": "Modern deep learning models have scaled to use billions of parameters, requiring billions (or even trillions) of multiply-accumulate (MAC) operations during inference. Their enormous size presents a major obstacle to their deployment as their compute and memory requirements during inference often exceed the budgets of real-world systems. As a result, model compression has emerged as an active area in deep learning research, with quantization being among the most prevalent techniques studied in literature [1,2,3] and applied in practice [4,5,6]. \n\nQuantization techniques commonly reduce the inference costs of a deep learning model by restricting the precision of its weights and activations. Although substituting the standard 32-bit floating-point operands for low-precision counterparts can drastically reduce the cost of multiplications, this only accounts for part of the core MAC operation; the resulting products are often still accumulated using 32-bit additions. Recent studies have demonstrated that also restricting the precision of the accumulator can yield significant benefits [7,8,9,10,11]. For example, de Bruin et al. [8] and Xie et al. [9] both show that 16-bit integer accumulation on ARM processors can yield nearly a 2\u00d7 throughput increase over 32-bit, and Ni et al. [7] show that further reducing to 8-bit accumulation on custom ASICs can improve energy efficiency by over 4\u00d7. However, exploiting such an optimization is non-trivial in practice as reducing the width of the accumulator exponentially increases the risk of numerical overflow, which is known to introduce arithmetic errors that significantly degrade model accuracy [7,12]. \n\nTo address this, recent work has proposed an accumulator-aware quantization paradigm that entirely eliminates the risk of numerical overflow via strict learning constraints informed by theoretical guarantees [12]. The resulting scope of investigations has been limited to the quantization-aware training (QAT) setting in which models are trained from scratch or fine-tuned from checkpoints with quantization in the loop [12,13].",
            "score": 0.49328748680974377,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 517,
                    "matchedPaperCorpusId": "232352683"
                },
                {
                    "start": 542,
                    "end": 545,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 545,
                    "end": 547,
                    "matchedPaperCorpusId": "3273340"
                },
                {
                    "start": 1100,
                    "end": 1102,
                    "matchedPaperCorpusId": "201895129"
                },
                {
                    "start": 1102,
                    "end": 1104,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "268142399"
                },
                {
                    "start": 1141,
                    "end": 1144,
                    "matchedPaperCorpusId": "201895129"
                },
                {
                    "start": 1160,
                    "end": 1163,
                    "matchedPaperCorpusId": "220484674"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "261214350"
                },
                {
                    "start": 1876,
                    "end": 1880,
                    "matchedPaperCorpusId": "261214350"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "235434086",
            "title": "MQBench: Towards Reproducible and Deployable Model Quantization Benchmark",
            "text": "In 1990, Hammerstrom [65] already designed specialized VLSI hardware for 8-bit and 16-bit training. More recently, specialized hardware like BISMO [66] and BitFusion [67] are designed to handle mixed-precision inference. On general hardware, NVIDIA Volta Tensor Cores [68] can support FP16 & FP32 mixed training and achieve at least 2x speed-up. As for low-bit inference, there are several hardware libraries: such as NVIDIA's TensorRT [22], Qualcomm's SNPE [24], and so on. \n\nHardware providers also build some framework for model quantization (or other compression techniques). For example, the Neural Network Compression Framework (NNCF) [69] developed by Intel supports quantization and pruning. However, their implementation can only be shared on OpenVINO. AIMET, developed by Qualcomm, also just focuses on their own hardware. To our best knowledge, no existing framework can handle multi-platform deployment of quantization neural networks with diverse advanced algorithms. \n\nBenchmarks. In many sub-fields of deep learning or computer vision, a thorough analysis and evaluation is necessary [70]. In neural architecture search (NAS) [71], the search space is notoriously large and the search process requires tremendous computation resources, making researchers hard to reproduce results of prior works. For this reason, NAS-Bench-101 [19] and -201 [48] are proposed to solve the reproducibility issues and make a fair evaluation for NAS research. Recently, Hardware-NAS-Bench [72] was proposed to benchmark the inference latency and energy on several hardware devices. Despite the progress of benchmarks in NAS and other areas, model quantization lacks such standard to foster the reproducibility and deployability.",
            "score": 0.4932749643257002,
            "section_title": "D Related Work",
            "char_start_offset": 44698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "206915906"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "49421250"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "1660289"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "67856022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16357421875
        },
        {
            "corpus_id": "231627504",
            "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization",
            "text": "To further study the performance of our method on different NLP tasks, we conduct KDLSQ-BERT by setting KDLSQ-BERT in ultra-low bit quantization. The specific experimental results are presented in Table 6 and Table 7, respectively. As shown in the tables, it demonstrates that for both the GLUE benchmark and the SQuAD, our quantization method can obtain the same accuracy as the full-precision base-line model even if doing the ultra-low bit quantization. Especially, it should be noted that compared with the quantization configuration of \"4-4-2\" and \"2-2-2\", setting \"2-2-4\" can get almost the same accuracy performance as the full-precision base-line model. On the one hand, it proves that activation quantization is more sensitive to affect the accuracy performance than the weight quantization. On the other hand, these empirical results tell us that it 2-bit is enough for weight quantization when using our method to do quantization training. Therefore, even if conduct-",
            "score": 0.49326180249416685,
            "section_title": "EXPERIMENTAL ANALYSIS ON LOW BIT QUANTIZATION",
            "char_start_offset": 29415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 978
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "227119136",
            "title": "Empirical Evaluation of Deep Learning Model Compression Techniques on the WaveNet Vocoder",
            "text": "Quantized models also reduce c i as the operations are now performed at a numerical format in which the operations are less computationally expensive. This approach has been applied to a wide variety of models including BERT (Wu et al. 2020), ResNet and GNMT (Zafrir et al. 2019), and sees adoption in widely recognised machine learning benchmarks (Reddi et al. 2019). \n\nThe quantization process includes one or both of: \n\n1. Reducing the number of bits of the datatype. e.g. use 8 bits instead of 32 bits. \n\n2. Using a less expensive format. e.g. use integer instead of floating-point. \n\nA simple scheme is to perform all multiplications in the FP16 data format as this is already widely supported on a variety of hardware devices. The results are accumulated in either FP16 or FP32; this distinction matters for the range of representable values and for what precision any activation functions are later performed in. We represent these choices as FP16.16 and FP16.32 to represent using FP16 for multiplies and either FP16 or FP32 for accumulations respectively. \n\nQuantizing to integers is another popular choice for quantization. When quantizing from floating point to integer, it is necessary to use a quantization scheme in which there is no quantization error for the value 0 as these parameters will have an outsized impact on model performance (Jacob et al. 2018) especially when quantizing sparse matrices. Running inference at INTX (most often INT8) is widely used for deployment including for models from the domains of Machine Translation (Wu et al. 2016), Automatic Speech Recognition (He et al. 2019), Computer Vision (Wu et al. 2018) and NLP embeddings (Zafrir et al. 2019). \n\nHowever, formats other than INTX and the basic FP16 are becoming more widely used as hardware vendors accommodate them. For example, BrainFloat16 is supported by Google TPUs2 and Intel engineers have demonstrated a 1.5\u00d7 speedup using this format for both Parallel-WaveNet and FeatherWave on Xeon CPUs3 .",
            "score": 0.4930627676675257,
            "section_title": "Quantization",
            "char_start_offset": 7187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 1353,
                    "end": 1371,
                    "matchedPaperCorpusId": "39867659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "260682506",
            "title": "FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search",
            "text": "Low-Precision Floating Point: Low-precision floating point is being discussed as the next generation format for DNN training and inference. [6]. Companies, including AMD, Intel, NVIDIA, and Qualcomm, have recently agreed to adopt 8-bit floating-point (FP8) in future deep learning systems. Within these formats, recent studies generally focus on two variants: E5M2 and E4M3, where E represents the number of exponent bits and M is the number of mantissa bits. For example, HFP8 suggests using E4M3 for the forward pass and E5M2 for backpropagation [7]. Building upon these uniform precision works [7,8,9,10,11], FLIQS proposes an automated approach for finding Quantization Search: Prior work has explored mixed-precision integer quantization searches, as shown in Figure 1(b). For instance, HAQ [12] and ReLeQ [13] both perform PTQ quantization searches that utilize RL to allocate bitwidths based on the model accuracy and cost estimates. In addition, the HAWQ series of works further develops these PTQ searches, using the Hessian spectrum to determine layer sensitivities and constrained ILP formulations to find optimal bitwidth configurations [14,15,16]. However, being PTQ-based, these methods cannot take advantage of the higher accuracy and more accurate feedback provided by quantization-aware training (QAT) during the search. \n\nOther efforts perform quantization search during training, often using neural architecture search (NAS) with super-networks or differentiable NAS [17,18,19,13,20]. For instance, MPQ uses an adaptive one-shot method that trains models using multiple bitwidths and automatically freezes the bitwidths of specific layers during training to improve the model convergence across bitwidths [21]. In addition, EDMIPS creates branches for each bitwidth, forms a linear combination of them, and then alternates training the layer weights and the branch weights [22]. These differentiable searches often have simpler formulations since the layer and branch weights are unified and trained together with gradient descent.",
            "score": 0.4928811045985727,
            "section_title": "Related Work",
            "char_start_offset": 3875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "202779157"
                },
                {
                    "start": 602,
                    "end": 604,
                    "matchedPaperCorpusId": "251710272"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "251649202"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "102350477"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "195347626"
                },
                {
                    "start": 1153,
                    "end": 1156,
                    "matchedPaperCorpusId": "207852310"
                },
                {
                    "start": 1490,
                    "end": 1493,
                    "matchedPaperCorpusId": "237100483"
                },
                {
                    "start": 1493,
                    "end": 1496,
                    "matchedPaperCorpusId": "234778162"
                },
                {
                    "start": 1496,
                    "end": 1499,
                    "matchedPaperCorpusId": "195347626"
                },
                {
                    "start": 1892,
                    "end": 1896,
                    "matchedPaperCorpusId": "215745195"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "263605754",
            "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
            "text": "With the recent open-source releases of language models like BLOOM, Vicuna, LLaMa, OPT, etc., quantization has emerged as a widely embraced technique to alleviate the storage and computational overhead of deep learning models. Recent research endeavors have harnessed quantization to compress LLMs and they can be classified into the two mentioned approaches: Quantization-Aware Training (QAT), and Post-Training Quantization (PTQ). In QAT, the quantization objective is embedded into the LLM training process, enabling them to adapt to low-precision representations and handle precision loss caused by quantization. LLM-QAT (Liu et al., 2023c) proposes a data-free distillation method that leverages generations produced by the pre-trained model, preserving the original output distribution and allows quantizing LLaMa models independent of its training data. PEQA (Kim et al., 2023) operates through a dual-stage process: initially, the parameter matrix of each fully-connected layer undergoes quantization into a matrix of low-bit integers and a scalar vector; subsequently, fine-tuning occurs on the scalar vector for each downstream task. QLoRA (Dettmers et al., 2023a) proposes an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). PTQ involves quantizing the parameters of LLMs after the completion of the LLM's training phase. GPTQ (Frantar et al., 2022) proposes a novel layer-wise quantization technique based on approximate second-order information resulting a bitwidth reduction to 3 or 4 bits per weight, with minimal accuracy loss compared to the uncompressed version. AWQ (Lin et al., 2023a) based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error, employs an activation-aware approach by considering the significance of weight channels corresponding to larger activation magnitudes.",
            "score": 0.4927216501180524,
            "section_title": "A.1.2 QUANTIZATION IN LARGE LANGUAGE MODELS",
            "char_start_offset": 28277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373779296875
        },
        {
            "corpus_id": "201895129",
            "title": "Quantization of deep neural networks for accumulator-constrained processors",
            "text": "Vanhoucke et al. [11] linearly normalizes weights and (sigmoid) activations of every layer in a speedrecognition NN to 8-bit by analysing the range of weights and activations.A similar approach is implemented in several deep learning frameworks such as Tensorflow [12] and Caffe-Ristretto [13].Lin, Talathi, and Annapureddy [10] propose an analytical model to quickly convert pre-trained models to fixed-point.The advantage of this model is that it does not require exhaustive layer-wise optimization, but rather determines the bit width of every layer based on the approximate parameter and data range distribution.\n\nAn exhaustive layer-wise optimization is a straight-forward approach for NN quantization.This procedure generally consists of testing many possible quantization solutions for every layer in the network [3,14].To reduce the number of solutions to consider, several heuristics were developed.Gysel et al. [13] propose an iterative quantization procedure where weights are quantized first, and activations are quantized second.A similar two-step approach is described by other related works [15].Shan et al. [16] considers some target platform characteristics, and shows that the accumulator bit width can be reduced to 16-bit without a significant penalty in classification accuracy for the LeNet5 benchmark.\n\nTo regain some of the lost accuracy during quantization, the quantized model is generally finetuned (retrained) [2,14,10,3].The primary difficulty in fixed-point retraining lies in the update step of the Stochastic Gradient Descent (SGD) algorithm, which iteratively updates all weights by a fraction of its output loss contribution.With insufficient weight precision, the update steps get quantized to zero, which prohibits learning.A common training and finetuning procedure [4] is to perform the forward and gradient computation in reduced precision, while the weights are updated in high-precision [14,2].After training these high-precision weights are permanently quantized, which allows for efficient model deployment.",
            "score": 0.49249395120755934,
            "section_title": "Related Works",
            "char_start_offset": 5545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 616
                },
                {
                    "start": 618,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1324
                },
                {
                    "start": 1326,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1659
                },
                {
                    "start": 1659,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1935
                },
                {
                    "start": 1935,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "15196840"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "206628725"
                },
                {
                    "start": 823,
                    "end": 826,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 921,
                    "end": 925,
                    "matchedPaperCorpusId": "51610353"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "51868101"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "15178138"
                },
                {
                    "start": 1444,
                    "end": 1447,
                    "matchedPaperCorpusId": "649645"
                },
                {
                    "start": 1447,
                    "end": 1449,
                    "matchedPaperCorpusId": "206628725"
                },
                {
                    "start": 1928,
                    "end": 1932,
                    "matchedPaperCorpusId": "15178138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.300048828125
        },
        {
            "corpus_id": "265043878",
            "title": "MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks",
            "text": "Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization. Memory Saving. In this section, we first present the memory footprint reduction with MINT with a batch size of 1. As shown in Fig. 4 (Left), our W2U2 models on average give us more than 93% reduction in the total memory footprint. However, to improve inference speed, prior SNN works [2], [3], [26] commonly use mini-batch sizes larger than 1. In such cases, membrane potential quantization is critical. As illustrated in Fig. 4 (Right), the membrane potential overhead becomes sizeable with increasing batch size. We find that for a MINT quantized VGG-16 model on TinyImageNet with a batch size of 16, the compression of weight from 32-bit to 4-bit alone reduces the total memory footprint by 15%. Whereas, quantizing the membrane potential to 4 bits can further reduce 72.4% of the total memory footprint. We can expect that a larger batch size (> 64) will amplify the benefits from membrane potential quantization. Energy Saving. We study the inference energy difference between MINT and the vanilla UQ method on our proposed accelerator design in Sec. V. We normalize the results with the energy cost of a 16-bit integer multiply-accumulate operation. \n\nAs illustrated in Fig. 5, the computation energy of the UQ model hardly reduces with decreasing operand size, due to the power-hungry full-precision multipliers for scaling factors.",
            "score": 0.49196784780356134,
            "section_title": "B. Experimental Results",
            "char_start_offset": 17313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "52283296"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "211258776"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "222134047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "237416732",
            "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]",
            "text": "The digital software accuracy of this network is shown in Table 2. When simulating the analog accuracy of this network, these scaling steps are processed digitally between in situ MVMs. \n\nFig. 19 compares the error sensitivity of the 4-bit QAT model with the floating-point ResNet50-v1.5 model, whose weights are quantized to 8 bits after training. For a fair comparison, an 8-bit ADC is included for both cases; in the 4-bit model, this higher-resolution ADC helps minimize errors prior to the 4-bit quantization step, which is performed digitally. Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity. \n\nNotably, for the 8-bit network, the sensitivity to both types of error with an 8-bit ADC remains largely unchanged from the case with no ADC quantization, shown in Fig. 9(b) and Fig. 10(b). As shown in Section 6, a calibrated 8-bit ADC on its own induces very little accuracy loss for this network.",
            "score": 0.49150313743210206,
            "section_title": "SUPPRESSING ERROR PROPAGATION",
            "char_start_offset": 55501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2046
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "231918483",
            "title": "Confounding Tradeoffs for Neural Network Quantization",
            "text": "To address this problem, we propose that novel quantization research discuss these tradeoffs through \"quantization cards,\" described in Section 3. Quantization cards are a structure for reporting design decisions, analogous to datasheets for hardware, and more recently, datasheets for datasets [15] or model cards for machine learning models [28]. Such a structure is important for reporting quantization design decisions because unlike other hyperparameters, these design decisions affect the attainable inference acceleration. We propose quantization cards instead of establishing a benchmark because the set of tradeoffs is rapidly growing, such as using integer-only operations excluding division [40] or generating synthetic data [7,38]. \n\nWe expect quantization cards to be useful for two groups of people. First, researchers of quantization techniques will be able to more accurately compare across methods and replicate prior results. Second, engineers deploying quantized models will be able to assess quantization cards, determine whether the tradeoffs made by the work are applicable for their hardware and application setting, and adopt or combine strategies accordingly. Without quantization cards, these choices are often scattered throughout different parts of the text or even overlooked, making it challenging for users to determine how the method can be used. \n\nWe evaluate the impacts of these tradeoffs on quantized model accuracy in Section 4 for uniform quantization and post-training mixed-precision quantization. These experiments demonstrate that changes in confounding tradeoffs can affect accuracy at low bit precision by multiple percentage points, emphasizing the need for quantization cards and unified reporting of design decisions.",
            "score": 0.4912983246823721,
            "section_title": "Introduction",
            "char_start_offset": 1822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1764
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "52946140"
                },
                {
                    "start": 736,
                    "end": 739,
                    "matchedPaperCorpusId": "209531713"
                },
                {
                    "start": 739,
                    "end": 742,
                    "matchedPaperCorpusId": "212633494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "paperId": "a00b379c5ee6ebe400e5c88e0331066efa87be1b",
            "corpusId": 249526829,
            "title": "A 16-nm SoC for Noise-Robust Speech and NLP Edge AI Inference With Bayesian Sound Source Separation and Attention-Based DNNs",
            "venue": "IEEE Journal of Solid-State Circuits",
            "year": 2023,
            "referenceCount": 59,
            "citationCount": 20,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://discovery.ucl.ac.uk/10150658/1/A_16-nm_SoC_for_Noise-Robust_Speech.pdf",
                "status": "GREEN",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSSC.2022.3179303?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSSC.2022.3179303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1387249067",
                    "name": "Thierry Tambe"
                },
                {
                    "authorId": "38866509",
                    "name": "En-Yu Yang"
                },
                {
                    "authorId": "2054461",
                    "name": "Glenn G. Ko"
                },
                {
                    "authorId": "148385111",
                    "name": "Yuji Chai"
                },
                {
                    "authorId": "2029486869",
                    "name": "Coleman Hooper"
                },
                {
                    "authorId": "2026851",
                    "name": "M. Donato"
                },
                {
                    "authorId": "3313708",
                    "name": "P. Whatmough"
                },
                {
                    "authorId": "2531268",
                    "name": "Alexander M. Rush"
                },
                {
                    "authorId": "1896817",
                    "name": "D. Brooks"
                },
                {
                    "authorId": "2255803",
                    "name": "Gu-Yeon Wei"
                }
            ],
            "abstract": "The proliferation of personal artificial intelligence (AI) -assistant technologies with speech-based conversational AI interfaces is driving the exponential growth in the consumer Internet of Things (IoT) market. As these technologies are being applied to keyword spotting (KWS), automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS) applications, it is of paramount importance that they provide uncompromising performance for context learning in long sequences, which is a key benefit of the attention mechanism, and that they work seamlessly in polyphonic environments. In this work, we present a 25-mm2 system-on-chip (SoC) in 16-nm FinFET technology, codenamed SM6, which executes end-to-end speech-enhancing attention-based ASR and NLP workloads. The SoC includes: 1) FlexASR, a highly reconfigurable NLP inference processor optimized for whole-model acceleration of bidirectional attention-based sequence-to-sequence (seq2seq) deep neural networks (DNNs); 2) a Markov random field source separation engine (MSSE), a probabilistic graphical model accelerator for unsupervised inference via Gibbs sampling, used for sound source separation; 3) a dual-core Arm Cortex A53 CPU cluster, which provides on-demand single Instruction/multiple data (SIMD) fast fourier transform (FFT) processing and performs various application logic (e.g., expectation\u2013maximization (EM) algorithm and 8-bit floating-point (FP8) quantization); and 4) an always-ON M0 subsystem for audio detection and power management. Measurement results demonstrate the efficiency ranges of 2.6\u20137.8 TFLOPs/W and 4.33\u201317.6 Gsamples/s/W for FlexASR and MSSE, respectively; MSSE denoising performance allowing 6 $\\times $ smaller ASR model to be stored on-chip with negligible accuracy loss; and 2.24-mJ energy consumption while achieving real-time throughput, end-to-end, and per-frame ASR latencies of 18 ms.",
            "corpus_id": "249526829",
            "text": "The proliferation of personal artificial intelligence (AI) -assistant technologies with speech-based conversational AI interfaces is driving the exponential growth in the consumer Internet of Things (IoT) market. As these technologies are being applied to keyword spotting (KWS), automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS) applications, it is of paramount importance that they provide uncompromising performance for context learning in long sequences, which is a key benefit of the attention mechanism, and that they work seamlessly in polyphonic environments. In this work, we present a 25-mm2 system-on-chip (SoC) in 16-nm FinFET technology, codenamed SM6, which executes end-to-end speech-enhancing attention-based ASR and NLP workloads. The SoC includes: 1) FlexASR, a highly reconfigurable NLP inference processor optimized for whole-model acceleration of bidirectional attention-based sequence-to-sequence (seq2seq) deep neural networks (DNNs); 2) a Markov random field source separation engine (MSSE), a probabilistic graphical model accelerator for unsupervised inference via Gibbs sampling, used for sound source separation; 3) a dual-core Arm Cortex A53 CPU cluster, which provides on-demand single Instruction/multiple data (SIMD) fast fourier transform (FFT) processing and performs various application logic (e.g., expectation\u2013maximization (EM) algorithm and 8-bit floating-point (FP8) quantization); and 4) an always-ON M0 subsystem for audio detection and power management. Measurement results demonstrate the efficiency ranges of 2.6\u20137.8 TFLOPs/W and 4.33\u201317.6 Gsamples/s/W for FlexASR and MSSE, respectively; MSSE denoising performance allowing 6 $\\times $ smaller ASR model to be stored on-chip with negligible accuracy loss; and 2.24-mJ energy consumption while achieving real-time throughput, end-to-end, and per-frame ASR latencies of 18 ms.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11456298828125
        },
        {
            "paperId": "07e7b418a24891a7335075748c429cfb6af50884",
            "corpusId": 269457259,
            "title": "SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks",
            "venue": "Neuro Inspired Computational Elements Workshop",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.19668",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2132187927",
                    "name": "Sreyes P. Venkatesh"
                },
                {
                    "authorId": "2261707840",
                    "name": "Razvan Marinescu"
                },
                {
                    "authorId": "3444950",
                    "name": "J. Eshraghian"
                }
            ],
            "abstract": "Weight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an \u2018event-driven\u2019 approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch: https://github.com/jeshraghian/snntorch.",
            "corpus_id": "269457259",
            "text": "Weight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an \u2018event-driven\u2019 approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch: https://github.com/jeshraghian/snntorch.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.775390625
        },
        {
            "paperId": "d969a8171a4cd2e803015559d1a5a2b15a90861b",
            "corpusId": 250118516,
            "title": "Low-Precision Quantization Techniques for Hardware-Implementation-Friendly BERT Models",
            "venue": "IEEE International Symposium on Quality Electronic Design",
            "year": 2022,
            "referenceCount": 11,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISQED54688.2022.9806238?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISQED54688.2022.9806238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108083761",
                    "name": "Xinpei Zhang"
                },
                {
                    "authorId": "143714514",
                    "name": "Yi Ding"
                },
                {
                    "authorId": "31398042",
                    "name": "Mingfei Yu"
                },
                {
                    "authorId": "98303293",
                    "name": "Shin-ichi O'Uchi"
                },
                {
                    "authorId": "144146557",
                    "name": "M. Fujita"
                }
            ],
            "abstract": "Transformer-based deep learning models have been widely recognized as highly effective for NLP(natural language processing) tasks, among which BERT(Bidirectional Encoder Representations from Transformers) [1] is achieving outstanding performance on popular benchmarks. However, it is also noticed that the large amount of parameters, along with computational burden, are constraining its implementation on top of hardware platforms with limited computing and memory resources. In this work, having hardware implementation in mind, we introduce and evaluate 2 quantization techniques - clipping, and two piece-wise quantization, which have been implemented in convolutional neural network (CNN) models - to quantize this originally heavy model into the one requiring smaller number of bits. Our experimental results have revealed that, with the implementation of clipping and piece-wise quantization in an independent or joint manner, it is possible to maintain the accuracy of the BERT model after operating quantization with lower bit-width in activation and model quantized for smaller hardware implementation. Evaluations on four typical NLP tasks prove that, with 8-bit integer activations, even if the weights are quantized to only 4-bit integer, the loss of performance is relatively small. We show various quantization results for weights and activations, which indicate that from 4-bit to 8-bit quantization for both of weights and activations can be used with good accuracy with all weights in the model quantized.",
            "corpus_id": "250118516",
            "text": "Transformer-based deep learning models have been widely recognized as highly effective for NLP(natural language processing) tasks, among which BERT(Bidirectional Encoder Representations from Transformers) [1] is achieving outstanding performance on popular benchmarks. However, it is also noticed that the large amount of parameters, along with computational burden, are constraining its implementation on top of hardware platforms with limited computing and memory resources. In this work, having hardware implementation in mind, we introduce and evaluate 2 quantization techniques - clipping, and two piece-wise quantization, which have been implemented in convolutional neural network (CNN) models - to quantize this originally heavy model into the one requiring smaller number of bits. Our experimental results have revealed that, with the implementation of clipping and piece-wise quantization in an independent or joint manner, it is possible to maintain the accuracy of the BERT model after operating quantization with lower bit-width in activation and model quantized for smaller hardware implementation. Evaluations on four typical NLP tasks prove that, with 8-bit integer activations, even if the weights are quantized to only 4-bit integer, the loss of performance is relatively small. We show various quantization results for weights and activations, which indicate that from 4-bit to 8-bit quantization for both of weights and activations can be used with good accuracy with all weights in the model quantized.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.73291015625
        }
    ],
    "quotes": {
        "cost": 0.249966,
        "quotes": [
            {
                "idx": 0,
                "key": "[1193239 | Alistarh et al. | 2016 | Citations: 422]",
                "snippets": "On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]...Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version...On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%...One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E Experiments",
                        "pdf_hash": "",
                        "start": 98,
                        "end": 289,
                        "sentence_offsets": [
                            {
                                "start": 98,
                                "end": 214
                            },
                            {
                                "start": 215,
                                "end": 290
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]"
                    },
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 89,
                        "end": 497,
                        "sentence_offsets": [
                            {
                                "start": 89,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 353
                            },
                            {
                                "start": 354,
                                "end": 497
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version"
                    },
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 500,
                        "end": 701,
                        "sentence_offsets": [
                            {
                                "start": 500,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 701
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%"
                    },
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 1513,
                        "end": 1854,
                        "sentence_offsets": [
                            {
                                "start": 1513,
                                "end": 1601
                            },
                            {
                                "start": 1602,
                                "end": 1786
                            },
                            {
                                "start": 1787,
                                "end": 1853
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[207769430 | Guan et al. | 2019 | Citations: 34]",
                "snippets": "Post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 107,
                        "end": 383,
                        "sentence_offsets": [
                            {
                                "start": 81,
                                "end": 265
                            },
                            {
                                "start": 266,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 383
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[211677681 | Lam et al. | 2020 | Citations: 1]",
                "snippets": "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[8257350 | Xu et al. | 2018 | Citations: 116]": "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance."
                },
                "metadata": [
                    {
                        "section_title": "Benefits of Higher Precision Activations",
                        "pdf_hash": "",
                        "start": 866,
                        "end": 2093,
                        "sentence_offsets": [
                            {
                                "start": 866,
                                "end": 1141
                            },
                            {
                                "start": 1142,
                                "end": 1397
                            },
                            {
                                "start": 1398,
                                "end": 1649
                            },
                            {
                                "start": 1652,
                                "end": 1896
                            },
                            {
                                "start": 1897,
                                "end": 2093
                            }
                        ],
                        "ref_mentions": [
                            "8257350"
                        ],
                        "quote": "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[218788236 | Charan et al. | 2020 | Citations: 30]",
                "snippets": "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. DEVICE QUANTIZATION",
                        "pdf_hash": "",
                        "start": 543,
                        "end": 894,
                        "sentence_offsets": [
                            {
                                "start": 543,
                                "end": 894
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[218862856 | Kim et al. | 2020 | Citations: 7]",
                "snippets": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated...Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Method",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 777,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            },
                            {
                                "start": 133,
                                "end": 252
                            },
                            {
                                "start": 253,
                                "end": 367
                            },
                            {
                                "start": 368,
                                "end": 472
                            },
                            {
                                "start": 473,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "211724357",
                            "211724357"
                        ],
                        "quote": "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated"
                    },
                    {
                        "section_title": "Method",
                        "pdf_hash": "",
                        "start": 1362,
                        "end": 1486,
                        "sentence_offsets": [
                            {
                                "start": 1362,
                                "end": 1485
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[225583435 | Yee et al. | 2020 | Citations: 2]",
                "snippets": "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RESULTS AND DISCUSSION",
                        "pdf_hash": "",
                        "start": 352,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 352,
                                "end": 532
                            },
                            {
                                "start": 533,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 911
                            },
                            {
                                "start": 912,
                                "end": 1163
                            },
                            {
                                "start": 1164,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[229923538 | Bai et al. | 2020 | Citations: 227]",
                "snippets": "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C.1 Performance Drop by Binarization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 775,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[231906389 | Ghamari et al. | 2021 | Citations: 17]",
                "snippets": "We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models...The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Benchmark Experiments",
                        "pdf_hash": "",
                        "start": 284,
                        "end": 480,
                        "sentence_offsets": [
                            {
                                "start": 284,
                                "end": 481
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models"
                    },
                    {
                        "section_title": "Benchmark Experiments",
                        "pdf_hash": "",
                        "start": 1078,
                        "end": 1344,
                        "sentence_offsets": [
                            {
                                "start": 1078,
                                "end": 1228
                            },
                            {
                                "start": 1229,
                                "end": 1343
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[231918483 | Garg et al. | 2021 | Citations: 18]",
                "snippets": "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[244531228 | Wang et al. | 2021 | Citations: 3]",
                "snippets": "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model compression study",
                        "pdf_hash": "",
                        "start": 480,
                        "end": 1266,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[247880733 | Rezk et al. | 2022 | Citations: 1]",
                "snippets": "In this section, we apply post-training quantization on the four models visualized in Figure 1 with selected path skipping as explained in Section 4. We first quantize the models' weights and activations to 16 bit-fixed and 8-bit, 4-bit, and 2-bit integers. Then we select eight mixed-precision configurations named M1 to M8. In the mixed-precisions configuration, we mix between 8-bit, 4-bit, and 2-bit weights and 8-bit and 4-bit activations. We select the precisions of the first layer input computations separately as we know it is the most sensitive layer (Zhao et al., 2019). The rest of the recurrent layer M \u00d7 V multiplication operations have the same precision. \n\nFor each quantization configuration, we show the resulting testing error rate and the size of the model in Table 2. As observed in the table, all models maintain low error rates at 8-bit quantization. The error rate increases reasonably at 4-bit quantization, But, at 2-bit quantization all the models fail.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[59413897 | Zhao et al. | 2019 | Citations: 311]": "Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training. DNN weights and activations follow a bell-shaped distribution post-training, while practical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by clipping the outliers or using specialized hardware. In this work, we propose outlier channel splitting (OCS), which duplicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Experimental evaluation on ImageNet classification and language modeling shows that OCS can outperform state-of-the-art clipping techniques with only minor overhead."
                },
                "metadata": [
                    {
                        "section_title": "Post-Training Quantization of RNN Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 964,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 325
                            },
                            {
                                "start": 326,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 565
                            },
                            {
                                "start": 566,
                                "end": 654
                            },
                            {
                                "start": 657,
                                "end": 772
                            },
                            {
                                "start": 773,
                                "end": 857
                            },
                            {
                                "start": 858,
                                "end": 964
                            }
                        ],
                        "ref_mentions": [
                            "59413897"
                        ],
                        "quote": "In this section, we apply post-training quantization on the four models visualized in Figure 1 with selected path skipping as explained in Section 4. We first quantize the models' weights and activations to 16 bit-fixed and 8-bit, 4-bit, and 2-bit integers. Then we select eight mixed-precision configurations named M1 to M8. In the mixed-precisions configuration, we mix between 8-bit, 4-bit, and 2-bit weights and 8-bit and 4-bit activations. We select the precisions of the first layer input computations separately as we know it is the most sensitive layer (Zhao et al., 2019). The rest of the recurrent layer M \u00d7 V multiplication operations have the same precision. \n\nFor each quantization configuration, we show the resulting testing error rate and the size of the model in Table 2. As observed in the table, all models maintain low error rates at 8-bit quantization. The error rate increases reasonably at 4-bit quantization, But, at 2-bit quantization all the models fail."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[250048704 | Askarihemmat et al. | 2022 | Citations: 5]",
                "snippets": "Figure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RESULTS",
                        "pdf_hash": "",
                        "start": 1227,
                        "end": 1481,
                        "sentence_offsets": [
                            {
                                "start": 1227,
                                "end": 1362
                            },
                            {
                                "start": 1363,
                                "end": 1481
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Figure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[251928917 | Guo et al. | 2022 | Citations: 60]",
                "snippets": "Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Quantization Accuracy",
                        "pdf_hash": "",
                        "start": 151,
                        "end": 858,
                        "sentence_offsets": [
                            {
                                "start": 142,
                                "end": 386
                            },
                            {
                                "start": 387,
                                "end": 499
                            },
                            {
                                "start": 500,
                                "end": 580
                            },
                            {
                                "start": 581,
                                "end": 727
                            },
                            {
                                "start": 728,
                                "end": 858
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[253535294 | Arsalan et al. | 2022 | Citations: 5]",
                "snippets": "As expected the accuracy drops for high quantization i.e., 4 bits for all the models and drops with a higher percentage for our proposed model (model 3). This drop is due to the higher number of neurons used in the proposed model and hence the high impact of quantization. However, we believe higher accuracy can be achieved with quantization aware training which currently the nengoDL framework does not support. Increasing the bits for quantization increases the accuracy as expected and with 8-bit and 16-bit, the proposed model achieves 92.68% and 96.46% of accuracy with good precision and recall as indicated by f1-scores.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 151,
                        "end": 779,
                        "sentence_offsets": [
                            {
                                "start": 151,
                                "end": 304
                            },
                            {
                                "start": 305,
                                "end": 423
                            },
                            {
                                "start": 424,
                                "end": 564
                            },
                            {
                                "start": 565,
                                "end": 779
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As expected the accuracy drops for high quantization i.e., 4 bits for all the models and drops with a higher percentage for our proposed model (model 3). This drop is due to the higher number of neurons used in the proposed model and hence the high impact of quantization. However, we believe higher accuracy can be achieved with quantization aware training which currently the nengoDL framework does not support. Increasing the bits for quantization increases the accuracy as expected and with 8-bit and 16-bit, the proposed model achieves 92.68% and 96.46% of accuracy with good precision and recall as indicated by f1-scores."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258841328 | Dettmers et al. | 2023 | Citations: 2606]",
                "snippets": "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4-bit NormalFloat yields better performance than 4-bit Floating Point",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 454,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 124
                            },
                            {
                                "start": 125,
                                "end": 248
                            },
                            {
                                "start": 249,
                                "end": 454
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[258990120 | Shymyrbay et al. | 2023 | Citations: 7]",
                "snippets": "The accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "4) N-Caltech101:",
                        "pdf_hash": "",
                        "start": 1119,
                        "end": 1829,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[259937594 | Liu et al. | 2023 | Citations: 35]",
                "snippets": "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results and Analysis",
                        "pdf_hash": "",
                        "start": 84,
                        "end": 883,
                        "sentence_offsets": [
                            {
                                "start": 84,
                                "end": 182
                            },
                            {
                                "start": 183,
                                "end": 415
                            },
                            {
                                "start": 416,
                                "end": 570
                            },
                            {
                                "start": 571,
                                "end": 780
                            },
                            {
                                "start": 781,
                                "end": 883
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[261531260 | Papa et al. | 2023 | Citations: 44]",
                "snippets": "Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results of Quantization strategies",
                        "pdf_hash": "",
                        "start": 1199,
                        "end": 1800,
                        "sentence_offsets": [
                            {
                                "start": 1199,
                                "end": 1347
                            },
                            {
                                "start": 1348,
                                "end": 1548
                            },
                            {
                                "start": 1549,
                                "end": 1800
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[263620300 | Kim et al. | 2023 | Citations: 20]",
                "snippets": "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Robustness comparison between MoE and dense models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 956,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 231
                            },
                            {
                                "start": 232,
                                "end": 302
                            },
                            {
                                "start": 303,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 518
                            },
                            {
                                "start": 519,
                                "end": 620
                            },
                            {
                                "start": 621,
                                "end": 703
                            },
                            {
                                "start": 704,
                                "end": 877
                            },
                            {
                                "start": 878,
                                "end": 943
                            },
                            {
                                "start": 944,
                                "end": 956
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[263671632 | Schiemer et al. | 2023 | Citations: 2]",
                "snippets": "For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RESULTS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 681,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 152
                            },
                            {
                                "start": 153,
                                "end": 216
                            },
                            {
                                "start": 217,
                                "end": 296
                            },
                            {
                                "start": 297,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 681
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[265043878 | Yin et al. | 2023 | Citations: 13]",
                "snippets": "Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Experimental Results",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 576,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 9
                            },
                            {
                                "start": 10,
                                "end": 50
                            },
                            {
                                "start": 51,
                                "end": 104
                            },
                            {
                                "start": 105,
                                "end": 207
                            },
                            {
                                "start": 208,
                                "end": 382
                            },
                            {
                                "start": 383,
                                "end": 576
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[267897495 | Guan et al. | 2024 | Citations: 16]",
                "snippets": "Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B...The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Evaluation of Perplexity performance",
                        "pdf_hash": "",
                        "start": 636,
                        "end": 940,
                        "sentence_offsets": [
                            {
                                "start": 636,
                                "end": 767
                            },
                            {
                                "start": 768,
                                "end": 941
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B"
                    },
                    {
                        "section_title": "Evaluation of Perplexity performance",
                        "pdf_hash": "",
                        "start": 1242,
                        "end": 1546,
                        "sentence_offsets": [
                            {
                                "start": 1242,
                                "end": 1431
                            },
                            {
                                "start": 1432,
                                "end": 1545
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[268032411 | Jin et al. | 2024 | Citations: 37]",
                "snippets": "Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 420,
                        "end": 627,
                        "sentence_offsets": [
                            {
                                "start": 420,
                                "end": 627
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[270257725 | Yuan et al. | 2024 | Citations: 1]",
                "snippets": "Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients...As shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Results",
                        "pdf_hash": "",
                        "start": 492,
                        "end": 897,
                        "sentence_offsets": [
                            {
                                "start": 492,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 898
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients"
                    },
                    {
                        "section_title": "B. Results",
                        "pdf_hash": "",
                        "start": 943,
                        "end": 1671,
                        "sentence_offsets": [
                            {
                                "start": 899,
                                "end": 995
                            },
                            {
                                "start": 996,
                                "end": 1198
                            },
                            {
                                "start": 1201,
                                "end": 1496
                            },
                            {
                                "start": 1497,
                                "end": 1784
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[272694046 | Lee et al. | 2024 | Citations: 2]",
                "snippets": "\u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1044,
                        "end": 1229,
                        "sentence_offsets": [
                            {
                                "start": 1044,
                                "end": 1149
                            },
                            {
                                "start": 1150,
                                "end": 1229
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "\u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273228873 | Javed et al. | 2024 | Citations: 2]",
                "snippets": "We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[49744838 | Beery et al. | 2018 | Citations: 853]": "It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)",
                    "[6037691 | Li et al. | 2017 | Citations: 1453]": "The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research."
                },
                "metadata": [
                    {
                        "section_title": "PACS (ERM Baseline) Terra Incognito (ERM Baseline)",
                        "pdf_hash": "",
                        "start": 506,
                        "end": 1167,
                        "sentence_offsets": [
                            {
                                "start": 506,
                                "end": 675
                            },
                            {
                                "start": 676,
                                "end": 740
                            },
                            {
                                "start": 741,
                                "end": 900
                            },
                            {
                                "start": 901,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1167
                            }
                        ],
                        "ref_mentions": [
                            "6037691",
                            "49744838"
                        ],
                        "quote": "We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[274305769 | Nezami et al. | 2024 | Citations: 3]",
                "snippets": "While quantization effectively reduces memory usage and accelerates inference, it often sacrifices accuracy, especially at lower precision levels (e.g., 4 bits).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Deployment Stability and Accuracy",
                        "pdf_hash": "",
                        "start": 196,
                        "end": 357,
                        "sentence_offsets": [
                            {
                                "start": 196,
                                "end": 357
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While quantization effectively reduces memory usage and accelerates inference, it often sacrifices accuracy, especially at lower precision levels (e.g., 4 bits)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[275104652 | Ling et al. | 2024 | Citations: 2]",
                "snippets": "Models quantized at 4-bit exhibit significantly higher Test MSE values and a broader distribution than those quantized at 6-bit or 8-bit, indicating a substantial loss introduced by lower bitwidths. As bitwidth increases, the Test MSE distribution narrows, particularly for 8-bit models, which achieve performance levels close to the FP32 benchmark.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiments 2: Quantized Models Analysis",
                        "pdf_hash": "",
                        "start": 613,
                        "end": 962,
                        "sentence_offsets": [
                            {
                                "start": 613,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 962
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Models quantized at 4-bit exhibit significantly higher Test MSE values and a broader distribution than those quantized at 6-bit or 8-bit, indicating a substantial loss introduced by lower bitwidths. As bitwidth increases, the Test MSE distribution narrows, particularly for 8-bit models, which achieve performance levels close to the FP32 benchmark."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276106922 | Ananta et al. | 2025 | Citations: 0]",
                "snippets": "Performance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 915,
                        "end": 1661,
                        "sentence_offsets": [
                            {
                                "start": 915,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1118
                            },
                            {
                                "start": 1119,
                                "end": 1277
                            },
                            {
                                "start": 1278,
                                "end": 1446
                            },
                            {
                                "start": 1447,
                                "end": 1661
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Performance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276575417 | Kharinaev et al. | 2025 | Citations: 0]",
                "snippets": "At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 293,
                        "end": 608,
                        "sentence_offsets": [
                            {
                                "start": 248,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 501
                            },
                            {
                                "start": 502,
                                "end": 608
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[276902966 | Giagnorio et al. | 2025 | Citations: 1]",
                "snippets": "The empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276903421 | Berti et al. | 2025 | Citations: 6]",
                "snippets": "Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. The Impact of Quantization on Emergent Abilities",
                        "pdf_hash": "",
                        "start": 1144,
                        "end": 1661,
                        "sentence_offsets": [
                            {
                                "start": 1144,
                                "end": 1236
                            },
                            {
                                "start": 1237,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1661
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[277452419 | Ma et al. | 2025 | Citations: 1]",
                "snippets": "Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "2-bits 4-bits 16-bits",
                        "pdf_hash": "",
                        "start": 1220,
                        "end": 1834,
                        "sentence_offsets": [
                            {
                                "start": 1220,
                                "end": 1321
                            },
                            {
                                "start": 1322,
                                "end": 1519
                            },
                            {
                                "start": 1520,
                                "end": 1645
                            },
                            {
                                "start": 1646,
                                "end": 1834
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[278326880 | Zheng et al. | 2025 | Citations: 6]",
                "snippets": "Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 506,
                        "end": 1128,
                        "sentence_offsets": [
                            {
                                "start": 506,
                                "end": 753
                            },
                            {
                                "start": 754,
                                "end": 1005
                            },
                            {
                                "start": 1006,
                                "end": 1128
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[3323727 | Polino et al. | 2018 | Citations: 732]",
                "snippets": "Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[49356451 | Krishnamoorthi | 2018 | Citations: 1021]",
                "snippets": "Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Lower Precision Networks",
                        "pdf_hash": "",
                        "start": 1901,
                        "end": 2127,
                        "sentence_offsets": [
                            {
                                "start": 1785,
                                "end": 1987
                            },
                            {
                                "start": 1988,
                                "end": 2127
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[52197199 | McKinstry et al. | 2018 | Citations: 94]",
                "snippets": "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiments",
                        "pdf_hash": "",
                        "start": 112,
                        "end": 1224,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 533
                            },
                            {
                                "start": 534,
                                "end": 675
                            },
                            {
                                "start": 678,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1035
                            },
                            {
                                "start": 1036,
                                "end": 1224
                            }
                        ],
                        "ref_mentions": [
                            "195347490"
                        ],
                        "quote": "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[67788003 | Esser et al. | 2019 | Citations: 810]",
                "snippets": "In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "COMPARISON WITH OTHER APPROACHES",
                        "pdf_hash": "",
                        "start": 914,
                        "end": 1375,
                        "sentence_offsets": [
                            {
                                "start": 914,
                                "end": 1004
                            },
                            {
                                "start": 1005,
                                "end": 1375
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.036735,
        "cot": "I'll break down the user query about empirical differences in model accuracy between 4-bit and higher precision quantization for deep learning models on NLP benchmarks.\n\nFirst, I need to organize these quotes into meaningful dimensions that will help answer the query directly and comprehensively:\n\n1. \"Introduction/Background\" - This section will provide context about quantization in deep learning, explaining what it is and why it matters. This needs to be a synthesis paragraph to lay the foundation for understanding the technical comparisons that follow.\n\n2. \"Accuracy Comparison: 4-bit vs. 8-bit/16-bit Quantization\" - This is the core of the query, directly addressing the empirical differences in accuracy. This should be a synthesis paragraph to analyze and compare the performance differences.\n\n3. \"Performance Thresholds and Critical Bit Levels\" - Many quotes mention specific thresholds where performance significantly drops. This should be a synthesis paragraph to explain these patterns.\n\n4. \"Model Size and Architecture Effects\" - The quotes indicate different models respond differently to quantization. This should be a synthesis paragraph examining how model architecture influences quantization outcomes.\n\n5. \"Task-Specific Performance Variations\" - Several quotes highlight how different NLP tasks respond differently to quantization. This should be a synthesis paragraph to analyze these variations.\n\n6. \"Quantization Methods and Techniques\" - Many quotes discuss specific quantization techniques that affect the accuracy results. This should be a synthesis paragraph to explain how different approaches to quantization impact performance.\n\n7. \"Practical Recommendations\" - Based on the empirical evidence, what are the practical takeaways? This should be a synthesis paragraph to provide actionable insights.\n\nThese dimensions cover the core aspects of the query while organizing the information in a logical flow. I've chosen synthesis format for all sections because the query asks for an analysis of empirical differences, which requires integrating and comparing information rather than simply listing facts.",
        "plan": {
            "Introduction/Background (synthesis)": [],
            "Accuracy Comparison: 4-bit vs. 8-bit/16-bit Quantization (synthesis)": [
                0,
                1,
                2,
                3,
                4,
                5,
                7,
                8,
                9,
                10,
                11,
                12,
                13,
                14,
                15,
                16,
                17,
                19,
                20,
                21,
                22,
                23,
                24,
                26,
                27,
                28,
                29,
                30,
                31,
                32,
                34,
                36,
                37
            ],
            "Performance Thresholds and Critical Bit Levels (synthesis)": [
                0,
                2,
                3,
                4,
                6,
                10,
                16,
                18,
                19,
                22,
                32,
                33
            ],
            "Model Size and Architecture Effects (synthesis)": [
                0,
                2,
                3,
                5,
                7,
                8,
                17,
                18,
                24,
                33
            ],
            "Task-Specific Performance Variations (synthesis)": [
                2,
                6,
                15,
                16,
                18,
                25,
                31,
                33
            ],
            "Quantization Methods and Techniques (synthesis)": [
                2,
                4,
                8,
                12,
                14,
                20,
                21,
                29,
                34,
                35,
                36,
                37
            ],
            "Practical Recommendations (synthesis)": [
                5,
                7,
                9,
                23,
                28,
                30,
                32
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Quantization is a technique to reduce the precision of deep learning model weights from their typical 32-bit floating-point representation to lower bit formats like 4-bit, 8-bit, or 16-bit. This compression technique has become increasingly important as large language models grow in size, offering reduced memory footprint and faster inference at the potential cost of accuracy. (LLM Memory)",
                "text": "\nModel quantization refers to the process of converting the weights and activations of neural networks from high-precision formats (typically 32-bit floating point) to lower-precision representations. This technique has gained significant traction as deep learning models, particularly in NLP, have grown to billions of parameters, creating challenges for deployment in resource-constrained environments. The primary motivation behind quantization is to reduce model size, decrease memory bandwidth requirements, and accelerate inference speed while maintaining acceptable performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe standard precision for training deep learning models is 32-bit floating point (FP32), but this high precision is often unnecessary during inference. Research has shown that models can operate effectively with dramatically reduced precision. Common quantization levels include 16-bit (FP16 or INT16), 8-bit (INT8), and more recently, 4-bit (INT4) or even lower for specific applications. Each reduction in bit-width approximately halves the memory footprint and can significantly improve computational efficiency. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe tradeoff between quantization benefits and performance degradation is at the heart of empirical research in this area. As bit precision decreases, the quantization error increases, potentially affecting model accuracy. This is particularly relevant when comparing 4-bit quantization against higher precision alternatives like 8-bit and 16-bit, especially for complex NLP tasks that require nuanced language understanding. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent advancements in quantization techniques have pushed the boundaries of low-bit inference, with innovations in post-training quantization, quantization-aware training, and mixed-precision approaches that strategically assign different bit-widths to different parts of a model. These developments have made even 4-bit quantization viable for certain applications, though its performance relative to higher-precision alternatives varies across model architectures and tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Accuracy Comparison: 4-bit vs. 8-bit/16-bit Quantization",
                "tldr": "4-bit quantization typically causes more noticeable accuracy drops compared to 8-bit and 16-bit alternatives, with performance degradation becoming more pronounced in deeper networks. While 8-bit quantization often maintains performance close to full precision models (sometimes even improving it), 4-bit represents a critical threshold where significant trade-offs between model compression and accuracy begin to emerge. (16 sources)",
                "text": "\nQuantitative comparisons between different precision levels reveal consistent patterns across model architectures and tasks. For 8-bit quantization, most studies report minimal accuracy loss compared to full-precision counterparts, with some even showing slight improvements. Post-training quantization using 8-bit precision can achieve accuracy close to single-precision models across various DNN architectures <Paper corpusId=\"207769430\" paperTitle=\"(Guan et al., 2019)\" isShortName></Paper>. In some cases, 8-bit quantized models outperform their full-precision counterparts, demonstrating a regularization effect that helps prevent overfitting <Paper corpusId=\"250048704\" paperTitle=\"(Askarihemmat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>.\n\nWhen moving to 4-bit quantization, however, the impact on model accuracy becomes more significant. Post-training quantization at 4-bit usually incurs noticeable accuracy drops <Paper corpusId=\"207769430\" paperTitle=\"(Guan et al., 2019)\" isShortName></Paper>. The severity of this degradation varies based on model architecture and task complexity. For instance, in CNN architectures like VGG-16, accuracy can drop dramatically from 93.3% when trained in FP32 precision to approximately 20% with 4-bit quantization <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This degradation is generally more pronounced in deeper networks, suggesting that model complexity correlates with quantization sensitivity <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>.\n\nSeveral studies identify 4-bit precision as a critical threshold where performance begins to substantially deteriorate. For example, ImageNet experiments show reasonable results until 6-bits, but accuracy drops drastically at 4-bits <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. Similarly, when testing large language models (LLMs) on various benchmarks, models maintain strong performance at 4-bit precision but show significant degradation when quantized to 3 bits or lower <Paper corpusId=\"268032411\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>. This \"3-bit threshold\" appears consistently across research, with accuracy degradation transitioning from linear to exponential collapse when precision falls below this level <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nFor many LLMs, 4-bit quantization represents a reasonable compromise, often maintaining emergent abilities while offering substantial memory savings. Research on various LLM benchmarks shows that 4-bit and 8-bit quantization results are very similar to original 16-bit floating-point performance, while 2-bit quantization leads to near-random performance levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This pattern is consistently observed across reasoning and instruction-following tasks, confirming 4-bit as a viable minimal precision for maintaining LLM capabilities <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.\n\nInterestingly, quantization effects vary significantly by layer type and position within the network. Convolutional layers appear particularly sensitive to aggressive quantization, with 2-bit precision often leading to significant accuracy loss, while increasing precision to 4-bit or 8-bit recovers accuracy <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>. The first and last layers of networks are especially sensitive to quantization, with studies showing that keeping these layers at higher precision (8-bit) while quantizing the rest to 4-bit can improve accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>.\n\nAdvanced quantization techniques can significantly mitigate accuracy losses even at 4-bit precision. Quantized distillation outperforms other methods across various bit widths, achieving accuracy within 0.2% of the teacher model at 8 bits and showing relatively minor accuracy loss at 4-bit quantization <Paper corpusId=\"3323727\" paperTitle=\"(Polino et al., 2018)\" isShortName></Paper>. Similarly, FAQ (Frequency-Aware Quantization) trained 4-bit networks outperform comparable quantization methods and can match or exceed pretrained FP32 network accuracy, although they require significantly longer fine-tuning\u2014approximately 110 epochs\u2014compared to 8-bit networks <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n\nFor certain applications and architectures, there may be no accuracy advantage to increasing precision from 4-bit to 8-bit, especially when using advanced quantization techniques <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>. However, this is not universally true across all model types and tasks. The impact of quantization also depends on activation precision, with research showing that keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to 4-bit can maintain performance within 1% of full-precision models <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>.\n\nRecent advances in 4-bit quantization for large language models show particularly promising results. Techniques like APTQ (Adaptive Precision Task Quantization) can maintain performance comparable to full-precision models even at average bit rates as low as 3.5, outperforming other post-training quantization approaches at reduced precision <Paper corpusId=\"267897495\" paperTitle=\"(Guan et al., 2024)\" isShortName></Paper>. Similarly, specialized number formats like NF4 with double quantization can fully recover 16-bit performance in LoRA fine-tuning scenarios <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\nOverall, while 8-bit quantization typically maintains performance very close to full precision across most model architectures and tasks, 4-bit quantization represents a critical threshold where meaningful accuracy trade-offs begin to emerge. The viability of 4-bit quantization depends heavily on model size, task complexity, and the specific quantization technique employed.",
                "citations": [
                    {
                        "id": "(Guan et al., 2019)",
                        "snippets": [
                            "Post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]."
                        ],
                        "paper": {
                            "corpus_id": 207769430,
                            "title": "Post-Training 4-bit Quantization on Embedding Tables",
                            "authors": [
                                {
                                    "authorId": "2055337890",
                                    "name": "Hui Guan"
                                },
                                {
                                    "authorId": "143682293",
                                    "name": "Andrey Malevich"
                                },
                                {
                                    "authorId": "2791531",
                                    "name": "Jiyan Yang"
                                },
                                {
                                    "authorId": "1686843",
                                    "name": "Jongsoo Park"
                                },
                                {
                                    "authorId": "30891915",
                                    "name": "Hector Yuen"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Askarihemmat et al., 2022)",
                        "snippets": [
                            "Figure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model."
                        ],
                        "paper": {
                            "corpus_id": 250048704,
                            "title": "QReg: On Regularization Effects of Quantization",
                            "authors": [
                                {
                                    "authorId": "1610857529",
                                    "name": "Mohammadhossein Askarihemmat"
                                },
                                {
                                    "authorId": "7872299",
                                    "name": "Reyhane Askari Hemmat"
                                },
                                {
                                    "authorId": "2062267546",
                                    "name": "Alexander Hoffman"
                                },
                                {
                                    "authorId": "4481888",
                                    "name": "I. Lazarevich"
                                },
                                {
                                    "authorId": "1682819",
                                    "name": "Ehsan Saboori"
                                },
                                {
                                    "authorId": "3422889",
                                    "name": "Olivier Mastropietro"
                                },
                                {
                                    "authorId": "1756949",
                                    "name": "Y. Savaria"
                                },
                                {
                                    "authorId": "145719986",
                                    "name": "J. David"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Alistarh et al., 2016)",
                        "snippets": [
                            "On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]",
                            "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version",
                            "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%",
                            "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."
                        ],
                        "paper": {
                            "corpus_id": 1193239,
                            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
                            "authors": [
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "29916095",
                                    "name": "Demjan Grubic"
                                },
                                {
                                    "authorId": "2800851",
                                    "name": "Jerry Li"
                                },
                                {
                                    "authorId": "2870603",
                                    "name": "Ryota Tomioka"
                                },
                                {
                                    "authorId": "1782150",
                                    "name": "M. Vojnovi\u0107"
                                }
                            ],
                            "year": 2016,
                            "venue": "",
                            "n_citations": 422
                        },
                        "score": 0.80615234375
                    },
                    {
                        "id": "(Charan et al., 2020)",
                        "snippets": [
                            "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                        ],
                        "paper": {
                            "corpus_id": 218788236,
                            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                            "authors": [
                                {
                                    "authorId": "91781917",
                                    "name": "Gouranga Charan"
                                },
                                {
                                    "authorId": "2942785",
                                    "name": "Abinash Mohanty"
                                },
                                {
                                    "authorId": "3457252",
                                    "name": "Xiaocong Du"
                                },
                                {
                                    "authorId": "144828251",
                                    "name": "Gokul Krishnan"
                                },
                                {
                                    "authorId": "145553390",
                                    "name": "R. Joshi"
                                },
                                {
                                    "authorId": "1965873861",
                                    "name": "Yu Cao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
                            "n_citations": 30
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Kim et al., 2020)",
                        "snippets": [
                            "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated",
                            "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."
                        ],
                        "paper": {
                            "corpus_id": 218862856,
                            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                            "authors": [
                                {
                                    "authorId": "49476045",
                                    "name": "Jangho Kim"
                                },
                                {
                                    "authorId": "1713608836",
                                    "name": "Kiyoon Yoo"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.86865234375
                    },
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower."
                        ],
                        "paper": {
                            "corpus_id": 268032411,
                            "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2184143149",
                                    "name": "Renren Jin"
                                },
                                {
                                    "authorId": "2287758280",
                                    "name": "Jiangcun Du"
                                },
                                {
                                    "authorId": "1588102980",
                                    "name": "Wuwei Huang"
                                },
                                {
                                    "authorId": "2257333016",
                                    "name": "Wei Liu"
                                },
                                {
                                    "authorId": "2257013742",
                                    "name": "Jian Luan"
                                },
                                {
                                    "authorId": "2257388949",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "2263617513",
                                    "name": "Deyi Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 37
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."
                        ],
                        "paper": {
                            "corpus_id": 259937594,
                            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                            "authors": [
                                {
                                    "authorId": "2108129670",
                                    "name": "Peiyu Liu"
                                },
                                {
                                    "authorId": "2119618242",
                                    "name": "Zikang Liu"
                                },
                                {
                                    "authorId": "9136116",
                                    "name": "Ze-Feng Gao"
                                },
                                {
                                    "authorId": "2162036220",
                                    "name": "Dawei Gao"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "1696332",
                                    "name": "Bolin Ding"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 35
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Berti et al., 2025)",
                        "snippets": [
                            "Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."
                        ],
                        "paper": {
                            "corpus_id": 276903421,
                            "title": "Emergent Abilities in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2229196235",
                                    "name": "Leonardo Berti"
                                },
                                {
                                    "authorId": "2325903265",
                                    "name": "Flavio Giorgi"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Garg et al., 2021)",
                        "snippets": [
                            "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."
                        ],
                        "paper": {
                            "corpus_id": 231918483,
                            "title": "Confounding Tradeoffs for Neural Network Quantization",
                            "authors": [
                                {
                                    "authorId": "9273400",
                                    "name": "Sahaj Garg"
                                },
                                {
                                    "authorId": "153408223",
                                    "name": "Anirudh Jain"
                                },
                                {
                                    "authorId": "134183581",
                                    "name": "Joe Lou"
                                },
                                {
                                    "authorId": "2060968473",
                                    "name": "Mitchell Nahmias"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Polino et al., 2018)",
                        "snippets": [
                            "Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss."
                        ],
                        "paper": {
                            "corpus_id": 3323727,
                            "title": "Model compression via distillation and quantization",
                            "authors": [
                                {
                                    "authorId": "36060478",
                                    "name": "A. Polino"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 732
                        },
                        "score": 0.880859375
                    },
                    {
                        "id": "(McKinstry et al., 2018)",
                        "snippets": [
                            "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                        ],
                        "paper": {
                            "corpus_id": 52197199,
                            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                            "authors": [
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2248110488",
                                    "name": "John V. Arthur"
                                },
                                {
                                    "authorId": "3121907",
                                    "name": "Izzet B. Yildiz"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 94
                        },
                        "score": 0.87548828125
                    },
                    {
                        "id": "(Esser et al., 2019)",
                        "snippets": [
                            "In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest."
                        ],
                        "paper": {
                            "corpus_id": 67788003,
                            "title": "Learned Step Size Quantization",
                            "authors": [
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 810
                        },
                        "score": 0.783203125
                    },
                    {
                        "id": "(Lam et al., 2020)",
                        "snippets": [
                            "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 211677681,
                            "title": "Quantized Neural Network Inference with Precision Batching",
                            "authors": [
                                {
                                    "authorId": "2347284",
                                    "name": "Maximilian Lam"
                                },
                                {
                                    "authorId": "1515569180",
                                    "name": "Zachary Yedidia"
                                },
                                {
                                    "authorId": "103876904",
                                    "name": "Colby R. Banbury"
                                },
                                {
                                    "authorId": "1805668",
                                    "name": "V. Reddi"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Guan et al., 2024)",
                        "snippets": [
                            "Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B",
                            "The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy."
                        ],
                        "paper": {
                            "corpus_id": 267897495,
                            "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2120170158",
                                    "name": "Ziyi Guan"
                                },
                                {
                                    "authorId": "2301516332",
                                    "name": "Hantao Huang"
                                },
                                {
                                    "authorId": "2286850679",
                                    "name": "Yupeng Su"
                                },
                                {
                                    "authorId": "2351687700",
                                    "name": "Hong Huang"
                                },
                                {
                                    "authorId": "2287187433",
                                    "name": "Ngai Wong"
                                },
                                {
                                    "authorId": "2288350991",
                                    "name": "Hao Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Design Automation Conference",
                            "n_citations": 16
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0.79931640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Thresholds and Critical Bit Levels",
                "tldr": "Research consistently identifies a critical threshold at around 3-4 bits, below which model performance deteriorates dramatically across various architectures and tasks. While quantization to 4 bits generally maintains reasonable accuracy with modest degradation, reducing to 2-3 bits often results in catastrophic performance collapse approaching random chance levels. (12 sources)",
                "text": "\nEmpirical evidence across multiple studies demonstrates the existence of a critical bit-width threshold where model performance transitions from gradual to severe degradation. This threshold consistently emerges around the 3-4 bit level for both weights and activations. For instance, studies on large language models (LLMs) reveal that performance remains relatively stable down to 4-bit precision but drops precipitously when quantized to 3 bits or lower <Paper corpusId=\"268032411\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This pattern is described as a transition from \"linear to exponential collapse\" in accuracy when crossing this critical threshold <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nThe threshold effect is observable across diverse model architectures and tasks. For image classification on ImageNet, models maintain reasonable performance down to 6 bits, but accuracy \"drastically drops at 4-bits\" <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. Similarly, experiments with CIFAR-10 show that while 4-bit QSGD can converge to the same accuracy as full precision models, 2-bit quantization drops accuracy by approximately 1.22% <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>.\n\nThe severity of performance degradation at low bit-widths increases with network depth and complexity. For deep networks like VGG-16, quantizing to 4 bits can cause accuracy to plummet from 93.3% when trained in FP32 to approximately 20% <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This phenomenon suggests that more complex models with greater parameter interdependencies are particularly sensitive to extreme quantization.\n\nFor LLMs, the threshold effect is particularly evident across reasoning and instruction-following benchmarks. Comparative testing across multiple datasets shows that 4-bit and 8-bit quantization produces results very similar to original 16-bit floating-point performance, while 2-bit quantization leads to near-random performance levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This pattern holds consistently across diverse tasks including in-context learning, chain-of-thought reasoning, and instruction following.\n\nRecent research on Mixture-of-Experts (MoE) models demonstrates that architectural choices can influence quantization robustness. MoE models can maintain accuracy within -0.3 BLEU points down to 3-bit precision, while comparable dense models preserve accuracy only down to 4 bits and experience catastrophic degradation at lower precisions <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This suggests that certain architectures may extend the viability threshold to slightly lower bit-widths.\n\nThe activation precision plays a crucial role in determining this threshold. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower bits can significantly mitigate accuracy loss. For instance, models with 4-bit weights but higher precision activations can maintain performance within 1% of full-precision models across tasks ranging from MNIST classification to language modeling <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper> <Paper corpusId=\"8257350\" paperTitle=\"(Xu et al., 2018)\" isShortName></Paper>.\n\nThe threshold effect is also evident in specific computational components. When constraining input precision to 4 bits but limiting accumulator bit-width to the same level, accuracy drops to no better than random chance. However, increasing accumulator bit-width to 8 bits allows the model to maintain reasonable performance, with only marginal improvements from further increases to 12 or 16 bits <Paper corpusId=\"263671632\" paperTitle=\"(Schiemer et al., 2023)\" isShortName></Paper>.\n\nNewer models may exhibit different quantization sensitivity patterns due to their training techniques. Advanced pre-training approaches that reduce parameter redundancy, such as those employed in Qwen3, can make models more vulnerable to quantization-induced information loss, especially at 3 bits or lower <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>.\n\nThe binarization threshold (reducing to 1-bit precision) represents an even more dramatic cliff. While performance drops gradually from full precision to ternarization (3 values), there is a \"consistent sharp drop by binarization\" across various GLUE benchmark tasks with both 8-bit and 4-bit activation quantization <Paper corpusId=\"229923538\" paperTitle=\"(Bai et al., 2020)\" isShortName></Paper>. This suggests that the threshold effect becomes more pronounced as bit precision approaches its theoretical minimum.\n\nIn summary, empirical evidence consistently identifies 3-4 bits as a critical precision threshold across diverse neural network architectures and tasks. While 4-bit quantization generally preserves most model capabilities with modest accuracy degradation, reducing to 2-3 bits typically triggers catastrophic performance collapse, with results approaching random chance levels.",
                "citations": [
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower."
                        ],
                        "paper": {
                            "corpus_id": 268032411,
                            "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2184143149",
                                    "name": "Renren Jin"
                                },
                                {
                                    "authorId": "2287758280",
                                    "name": "Jiangcun Du"
                                },
                                {
                                    "authorId": "1588102980",
                                    "name": "Wuwei Huang"
                                },
                                {
                                    "authorId": "2257333016",
                                    "name": "Wei Liu"
                                },
                                {
                                    "authorId": "2257013742",
                                    "name": "Jian Luan"
                                },
                                {
                                    "authorId": "2257388949",
                                    "name": "Bin Wang"
                                },
                                {
                                    "authorId": "2263617513",
                                    "name": "Deyi Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 37
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Kim et al., 2020)",
                        "snippets": [
                            "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated",
                            "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."
                        ],
                        "paper": {
                            "corpus_id": 218862856,
                            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                            "authors": [
                                {
                                    "authorId": "49476045",
                                    "name": "Jangho Kim"
                                },
                                {
                                    "authorId": "1713608836",
                                    "name": "Kiyoon Yoo"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.86865234375
                    },
                    {
                        "id": "(Alistarh et al., 2016)",
                        "snippets": [
                            "On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]",
                            "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version",
                            "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%",
                            "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."
                        ],
                        "paper": {
                            "corpus_id": 1193239,
                            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
                            "authors": [
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "29916095",
                                    "name": "Demjan Grubic"
                                },
                                {
                                    "authorId": "2800851",
                                    "name": "Jerry Li"
                                },
                                {
                                    "authorId": "2870603",
                                    "name": "Ryota Tomioka"
                                },
                                {
                                    "authorId": "1782150",
                                    "name": "M. Vojnovi\u0107"
                                }
                            ],
                            "year": 2016,
                            "venue": "",
                            "n_citations": 422
                        },
                        "score": 0.80615234375
                    },
                    {
                        "id": "(Charan et al., 2020)",
                        "snippets": [
                            "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                        ],
                        "paper": {
                            "corpus_id": 218788236,
                            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                            "authors": [
                                {
                                    "authorId": "91781917",
                                    "name": "Gouranga Charan"
                                },
                                {
                                    "authorId": "2942785",
                                    "name": "Abinash Mohanty"
                                },
                                {
                                    "authorId": "3457252",
                                    "name": "Xiaocong Du"
                                },
                                {
                                    "authorId": "144828251",
                                    "name": "Gokul Krishnan"
                                },
                                {
                                    "authorId": "145553390",
                                    "name": "R. Joshi"
                                },
                                {
                                    "authorId": "1965873861",
                                    "name": "Yu Cao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
                            "n_citations": 30
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."
                        ],
                        "paper": {
                            "corpus_id": 259937594,
                            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                            "authors": [
                                {
                                    "authorId": "2108129670",
                                    "name": "Peiyu Liu"
                                },
                                {
                                    "authorId": "2119618242",
                                    "name": "Zikang Liu"
                                },
                                {
                                    "authorId": "9136116",
                                    "name": "Ze-Feng Gao"
                                },
                                {
                                    "authorId": "2162036220",
                                    "name": "Dawei Gao"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "1696332",
                                    "name": "Bolin Ding"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 35
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."
                        ],
                        "paper": {
                            "corpus_id": 263620300,
                            "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
                            "authors": [
                                {
                                    "authorId": "2152658577",
                                    "name": "Young Jin Kim"
                                },
                                {
                                    "authorId": "2191522437",
                                    "name": "Raffy Fahim"
                                },
                                {
                                    "authorId": "3032929",
                                    "name": "H. Awadalla"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.74951171875
                    },
                    {
                        "id": "(Lam et al., 2020)",
                        "snippets": [
                            "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 211677681,
                            "title": "Quantized Neural Network Inference with Precision Batching",
                            "authors": [
                                {
                                    "authorId": "2347284",
                                    "name": "Maximilian Lam"
                                },
                                {
                                    "authorId": "1515569180",
                                    "name": "Zachary Yedidia"
                                },
                                {
                                    "authorId": "103876904",
                                    "name": "Colby R. Banbury"
                                },
                                {
                                    "authorId": "1805668",
                                    "name": "V. Reddi"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Xu et al., 2018)",
                        "snippets": [
                            "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance."
                        ],
                        "paper": {
                            "corpus_id": 8257350,
                            "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2153078936",
                                    "name": "Chen Xu"
                                },
                                {
                                    "authorId": "32320704",
                                    "name": "Jianqiang Yao"
                                },
                                {
                                    "authorId": "33383055",
                                    "name": "Zhouchen Lin"
                                },
                                {
                                    "authorId": "10336865",
                                    "name": "Wenwu Ou"
                                },
                                {
                                    "authorId": "35667580",
                                    "name": "Yuanbin Cao"
                                },
                                {
                                    "authorId": "2108201369",
                                    "name": "Zhirong Wang"
                                },
                                {
                                    "authorId": "1687248",
                                    "name": "H. Zha"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 116
                        },
                        "score": 0
                    },
                    {
                        "id": "(Schiemer et al., 2023)",
                        "snippets": [
                            "For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits."
                        ],
                        "paper": {
                            "corpus_id": 263671632,
                            "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning",
                            "authors": [
                                {
                                    "authorId": "1644333695",
                                    "name": "Martin Schiemer"
                                },
                                {
                                    "authorId": "1592769267",
                                    "name": "Clemens J. S. Schaefer"
                                },
                                {
                                    "authorId": "2163400188",
                                    "name": "Jayden Parker Vap"
                                },
                                {
                                    "authorId": "51184096",
                                    "name": "M. Horeni"
                                },
                                {
                                    "authorId": "2256185768",
                                    "name": "Yu Emma Wang"
                                },
                                {
                                    "authorId": "2255995570",
                                    "name": "Juan Ye"
                                },
                                {
                                    "authorId": "2254256505",
                                    "name": "Siddharth Joshi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Zheng et al., 2025)",
                        "snippets": [
                            "Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."
                        ],
                        "paper": {
                            "corpus_id": 278326880,
                            "title": "An Empirical Study of Qwen3 Quantization",
                            "authors": [
                                {
                                    "authorId": "2283439767",
                                    "name": "Xingyu Zheng"
                                },
                                {
                                    "authorId": "2359206148",
                                    "name": "Yuye Li"
                                },
                                {
                                    "authorId": "2359612976",
                                    "name": "Haoran Chu"
                                },
                                {
                                    "authorId": "2359148500",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "2190791504",
                                    "name": "Xudong Ma"
                                },
                                {
                                    "authorId": "2283307736",
                                    "name": "Jie Luo"
                                },
                                {
                                    "authorId": "2261414086",
                                    "name": "Jinyang Guo"
                                },
                                {
                                    "authorId": "2329344125",
                                    "name": "Haotong Qin"
                                },
                                {
                                    "authorId": "2283140094",
                                    "name": "Michele Magno"
                                },
                                {
                                    "authorId": "2278053544",
                                    "name": "Xianglong Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.75634765625
                    },
                    {
                        "id": "(Bai et al., 2020)",
                        "snippets": [
                            "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1."
                        ],
                        "paper": {
                            "corpus_id": 229923538,
                            "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
                            "authors": [
                                {
                                    "authorId": "9583912",
                                    "name": "Haoli Bai"
                                },
                                {
                                    "authorId": "2155468228",
                                    "name": "Wei Zhang"
                                },
                                {
                                    "authorId": "48557122",
                                    "name": "Lu Hou"
                                },
                                {
                                    "authorId": "50812138",
                                    "name": "Lifeng Shang"
                                },
                                {
                                    "authorId": "2115757711",
                                    "name": "Jing Jin"
                                },
                                {
                                    "authorId": "145820291",
                                    "name": "Xin Jiang"
                                },
                                {
                                    "authorId": "30738758",
                                    "name": "Qun Liu"
                                },
                                {
                                    "authorId": "1785083",
                                    "name": "Michael R. Lyu"
                                },
                                {
                                    "authorId": "145310663",
                                    "name": "Irwin King"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 227
                        },
                        "score": 0.83349609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Model Size and Architecture Effects",
                "tldr": "Model size and architecture significantly influence quantization impacts, with larger models (70B+ parameters) generally maintaining better performance at 4-bit precision compared to smaller counterparts. Network depth, layer type, and specific architectural choices like residual connections and mixture-of-experts designs also substantially affect quantization robustness. (10 sources)",
                "text": "\nThe relationship between model size and quantization impact reveals a clear pattern: larger language models tend to be more resilient to aggressive quantization. For 70B parameter models, 4-bit quantization typically maintains good performance, while smaller LLMs experience significant accuracy drops at the same bit precision <Paper corpusId=\"272694046\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This size-dependent resilience suggests that parameter redundancy in larger models provides a buffer against quantization-induced information loss.\n\nNetwork depth emerges as a critical factor influencing quantization sensitivity. As network depth increases, the performance degradation from quantization becomes more severe. For instance, in VGG-16, accuracy plummets from 93.3% when trained in FP32 precision to approximately 20% with 4-bit quantization <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This pattern appears consistent across architectures, with deeper networks showing heightened sensitivity to precision reduction.\n\nThe type of layer being quantized significantly affects overall model performance. Convolutional layers prove particularly sensitive to aggressive quantization, with 2-bit precision often leading to accuracy loss. However, increasing precision to 4-bit or 8-bit for these layers can recover accuracy <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>. Moreover, the first and last layers of networks demonstrate special sensitivity to quantization, with research showing that keeping these layers at higher precision (8-bit) while quantizing the rest to 4-bit can improve accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>.\n\nVision transformers (ViT) exhibit distinct quantization characteristics based on their size. Smaller models like ViT-T and DeiT-Ti show dramatic performance drops at 4-bit precision, achieving only 17.6% and 47.9% top-1 accuracy respectively. However, these same architectures maintain adequate performance with 8/6-bit quantization, reaching up to 74.8% and 72.0% top-1 accuracy while still providing meaningful model compression <Paper corpusId=\"261531260\" paperTitle=\"(Papa et al., 2023)\" isShortName></Paper>.\n\nAdvanced architecture designs can significantly impact quantization robustness. Mixture-of-Experts (MoE) models demonstrate superior quantization resilience compared to dense counterparts. While MoE models maintain accuracy within -0.3 BLEU points down to 3-bit precision, comparable dense models preserve accuracy only down to 4 bits and experience catastrophic degradation at lower precisions <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This architectural advantage is particularly valuable for extreme quantization scenarios.\n\nNewer model generations may exhibit different quantization sensitivity patterns due to their training techniques. Advanced pre-training approaches that reduce parameter redundancy, such as those employed in Qwen3, can make models more vulnerable to quantization-induced information loss, especially at 3 bits or lower <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. This suggests that quantization robustness may sometimes trade off against other training objectives that optimize parameter efficiency.\n\nActivation precision plays a crucial role alongside weight quantization. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower bits significantly mitigates accuracy loss. For instance, models with 4-bit weights but higher precision activations can maintain performance within 1% of full-precision models <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This pattern holds across diverse tasks from image classification to language modeling, indicating a generalizable principle for quantization strategy design.\n\nFor compact architectures like SqueezeNet, layer-specific quantization impacts vary dramatically. When applying 4-bit fixed-point quantization to individual layers, accuracy drops range from modest 5% reductions (for 'fire9-expand3x3' layer) to severe 47.5% reductions (for 'fire6-expand3x3' layer). Interestingly, 8-bit and 16-bit quantization show no accuracy change regardless of which layer is quantized, highlighting the existence of architecture-specific precision thresholds <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\nSpecific architectural elements like residual and skip connections deserve special attention during quantization. Research demonstrates that methods achieving high accuracy with low-bit activations (e.g., 3-bit) often succeed partly because they exclude these connections from quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>. This finding underscores the importance of selectively applying quantization based on architectural components rather than uniformly across the model.\n\nDespite architecture-specific variations, post-training quantization consistently achieves significant model compression (7-8\u00d7) with only 1-3% accuracy drops at 4-bit precision across both small and large DNN architectures, regardless of task complexity <Paper corpusId=\"231906389\" paperTitle=\"(Ghamari et al., 2021)\" isShortName></Paper>. This demonstrates that while architecture influences quantization sensitivity, well-designed quantization strategies can effectively compress diverse model architectures with minimal performance impact.",
                "citations": [
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "\u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits."
                        ],
                        "paper": {
                            "corpus_id": 272694046,
                            "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",
                            "authors": [
                                {
                                    "authorId": "2154961369",
                                    "name": "Jemin Lee"
                                },
                                {
                                    "authorId": "1936868",
                                    "name": "Sihyeong Park"
                                },
                                {
                                    "authorId": "3460677",
                                    "name": "Jinse Kwon"
                                },
                                {
                                    "authorId": "2321710033",
                                    "name": "Jihun Oh"
                                },
                                {
                                    "authorId": "3156595",
                                    "name": "Yongin Kwon"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.775390625
                    },
                    {
                        "id": "(Charan et al., 2020)",
                        "snippets": [
                            "We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."
                        ],
                        "paper": {
                            "corpus_id": 218788236,
                            "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution",
                            "authors": [
                                {
                                    "authorId": "91781917",
                                    "name": "Gouranga Charan"
                                },
                                {
                                    "authorId": "2942785",
                                    "name": "Abinash Mohanty"
                                },
                                {
                                    "authorId": "3457252",
                                    "name": "Xiaocong Du"
                                },
                                {
                                    "authorId": "144828251",
                                    "name": "Gokul Krishnan"
                                },
                                {
                                    "authorId": "145553390",
                                    "name": "R. Joshi"
                                },
                                {
                                    "authorId": "1965873861",
                                    "name": "Yu Cao"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits",
                            "n_citations": 30
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Alistarh et al., 2016)",
                        "snippets": [
                            "On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]",
                            "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version",
                            "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%",
                            "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."
                        ],
                        "paper": {
                            "corpus_id": 1193239,
                            "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks",
                            "authors": [
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                },
                                {
                                    "authorId": "29916095",
                                    "name": "Demjan Grubic"
                                },
                                {
                                    "authorId": "2800851",
                                    "name": "Jerry Li"
                                },
                                {
                                    "authorId": "2870603",
                                    "name": "Ryota Tomioka"
                                },
                                {
                                    "authorId": "1782150",
                                    "name": "M. Vojnovi\u0107"
                                }
                            ],
                            "year": 2016,
                            "venue": "",
                            "n_citations": 422
                        },
                        "score": 0.80615234375
                    },
                    {
                        "id": "(Garg et al., 2021)",
                        "snippets": [
                            "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."
                        ],
                        "paper": {
                            "corpus_id": 231918483,
                            "title": "Confounding Tradeoffs for Neural Network Quantization",
                            "authors": [
                                {
                                    "authorId": "9273400",
                                    "name": "Sahaj Garg"
                                },
                                {
                                    "authorId": "153408223",
                                    "name": "Anirudh Jain"
                                },
                                {
                                    "authorId": "134183581",
                                    "name": "Joe Lou"
                                },
                                {
                                    "authorId": "2060968473",
                                    "name": "Mitchell Nahmias"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Papa et al., 2023)",
                        "snippets": [
                            "Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size."
                        ],
                        "paper": {
                            "corpus_id": 261531260,
                            "title": "A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking",
                            "authors": [
                                {
                                    "authorId": "2164141997",
                                    "name": "Lorenzo Papa"
                                },
                                {
                                    "authorId": "2047330818",
                                    "name": "Paolo Russo"
                                },
                                {
                                    "authorId": "2281890830",
                                    "name": "Irene Amerini"
                                },
                                {
                                    "authorId": "2237944863",
                                    "name": "Luping Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 44
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."
                        ],
                        "paper": {
                            "corpus_id": 263620300,
                            "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
                            "authors": [
                                {
                                    "authorId": "2152658577",
                                    "name": "Young Jin Kim"
                                },
                                {
                                    "authorId": "2191522437",
                                    "name": "Raffy Fahim"
                                },
                                {
                                    "authorId": "3032929",
                                    "name": "H. Awadalla"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.74951171875
                    },
                    {
                        "id": "(Zheng et al., 2025)",
                        "snippets": [
                            "Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."
                        ],
                        "paper": {
                            "corpus_id": 278326880,
                            "title": "An Empirical Study of Qwen3 Quantization",
                            "authors": [
                                {
                                    "authorId": "2283439767",
                                    "name": "Xingyu Zheng"
                                },
                                {
                                    "authorId": "2359206148",
                                    "name": "Yuye Li"
                                },
                                {
                                    "authorId": "2359612976",
                                    "name": "Haoran Chu"
                                },
                                {
                                    "authorId": "2359148500",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "2190791504",
                                    "name": "Xudong Ma"
                                },
                                {
                                    "authorId": "2283307736",
                                    "name": "Jie Luo"
                                },
                                {
                                    "authorId": "2261414086",
                                    "name": "Jinyang Guo"
                                },
                                {
                                    "authorId": "2329344125",
                                    "name": "Haotong Qin"
                                },
                                {
                                    "authorId": "2283140094",
                                    "name": "Michele Magno"
                                },
                                {
                                    "authorId": "2278053544",
                                    "name": "Xianglong Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.75634765625
                    },
                    {
                        "id": "(Lam et al., 2020)",
                        "snippets": [
                            "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 211677681,
                            "title": "Quantized Neural Network Inference with Precision Batching",
                            "authors": [
                                {
                                    "authorId": "2347284",
                                    "name": "Maximilian Lam"
                                },
                                {
                                    "authorId": "1515569180",
                                    "name": "Zachary Yedidia"
                                },
                                {
                                    "authorId": "103876904",
                                    "name": "Colby R. Banbury"
                                },
                                {
                                    "authorId": "1805668",
                                    "name": "V. Reddi"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Yee et al., 2020)",
                        "snippets": [
                            "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                        ],
                        "paper": {
                            "corpus_id": 225583435,
                            "title": "Face Recognition and Machine Learning at the Edge",
                            "authors": [
                                {
                                    "authorId": "2004954858",
                                    "name": "Joanne Ling Sin Yee"
                                },
                                {
                                    "authorId": "2412102",
                                    "name": "U. U. Sheikh"
                                },
                                {
                                    "authorId": "1951977",
                                    "name": "M. Mokji"
                                },
                                {
                                    "authorId": "2111988471",
                                    "name": "S. Rahman"
                                }
                            ],
                            "year": 2020,
                            "venue": "IOP Conference Series: Materials Science and Engineering",
                            "n_citations": 2
                        },
                        "score": 0.791015625
                    },
                    {
                        "id": "(Ghamari et al., 2021)",
                        "snippets": [
                            "We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models",
                            "The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy."
                        ],
                        "paper": {
                            "corpus_id": 231906389,
                            "title": "Quantization-Guided Training for Compact TinyML Models",
                            "authors": [
                                {
                                    "authorId": "103148446",
                                    "name": "Sedigh Ghamari"
                                },
                                {
                                    "authorId": "40408041",
                                    "name": "Koray Ozcan"
                                },
                                {
                                    "authorId": "40844014",
                                    "name": "Thu Dinh"
                                },
                                {
                                    "authorId": "38363587",
                                    "name": "A. Melnikov"
                                },
                                {
                                    "authorId": "2062260709",
                                    "name": "Juan Carvajal"
                                },
                                {
                                    "authorId": "39497207",
                                    "name": "Jan Ernst"
                                },
                                {
                                    "authorId": "1691331",
                                    "name": "S. Chai"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 17
                        },
                        "score": 0.7744140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Task-Specific Performance Variations",
                "tldr": "The impact of 4-bit quantization varies significantly across different NLP and machine learning tasks, with complex reasoning and few-shot learning tasks showing greater sensitivity to precision reduction compared to simpler classification tasks. Performance degradation patterns are task-dependent, with some applications maintaining accuracy within 1% of full-precision models at 4-bit, while others experience significant drops requiring 8-bit precision to maintain comparable results. (11 sources)",
                "text": "\nDifferent NLP and machine learning tasks exhibit varying levels of resilience to quantization, creating distinct performance profiles across bit-width levels. For natural language inference tasks, using full-precision activations allows quantization down to 1-bit with only minor accuracy degradation (from 78% to 76%), whereas quantizing both weights and activations to the same low precision leads to random-guess performance (33%) <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. Interestingly, for some language inference tasks, 8-bit quantized models actually outperform their full-precision counterparts, demonstrating quantization's potential regularization benefits <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper> <Paper corpusId=\"8257350\" paperTitle=\"(Xu et al., 2018)\" isShortName></Paper>.\n\nLanguage modeling tasks show distinct sensitivity patterns, with 4-bit weight quantization causing moderate perplexity increases when activations are also quantized, but maintaining performance within a few points of full precision when activations remain at higher precision <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This pattern holds across various model architectures and datasets, suggesting that preserving activation precision is particularly important for language modeling tasks.\n\nFor BERT models on GLUE benchmark tasks, a consistent pattern emerges regarding extreme quantization thresholds. While performance degrades gradually from full precision to ternarization (3 values), there is a \"consistent sharp drop by binarization\" across various GLUE tasks with both 8-bit and 4-bit activation quantization <Paper corpusId=\"229923538\" paperTitle=\"(Bai et al., 2020)\" isShortName></Paper>. This indicates that extremely low-bit quantization affects different language understanding tasks similarly, despite their varying complexity.\n\nThe impact of quantization on emergent abilities in large language models reveals task-specific patterns. Across multiple benchmarks including MMLU (multiple-choice reasoning), BBH (in-context learning), GSM8K (chain-of-thought reasoning), and general language modeling on WikiText, 4-bit and 8-bit quantization results remain very similar to original 16-bit floating-point performance. However, 2-bit quantization consistently produces near-random performance levels (around 0.25 accuracy in 4-choice classification tasks) <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This suggests that 4-bit represents a viable threshold for preserving complex emergent abilities in language models.\n\nRecent quantization studies on Qwen3 reveal that advanced models using sophisticated pre-training techniques may exhibit greater task-specific sensitivity. While maintaining competitive performance at 4-bit and above, Qwen3 shows more pronounced degradation than previous model generations when quantized to 3-bit or lower, particularly for complex reasoning tasks and few-shot learning scenarios <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. This suggests that models with less parameter redundancy due to advanced training may be more vulnerable to quantization-induced information loss in complex reasoning tasks.\n\nFor machine translation, comparison between Mixture-of-Experts (MoE) and dense models reveals significant task-specific resilience differences. MoE models maintain translation quality within -0.3 BLEU points down to 3-bit precision, while comparable dense models preserve accuracy only down to 4-bit and experience catastrophic degradation (-42.96 BLEU points) at 2-bit <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This indicates that architectural choices significantly influence quantization robustness for specific NLP tasks.\n\nVisual recognition tasks show varying quantization sensitivity based on dataset complexity. For the CIFAR10-DVS dataset, 8-bit quantization causes a modest 1.78% accuracy drop, while 4-bit models show more significant degradation (7.98%). In contrast, for the simpler DVS128 Gesture dataset, the impact is much smaller, with just 0.81% accuracy loss at 4-bit precision <Paper corpusId=\"258990120\" paperTitle=\"(Shymyrbay et al., 2023)\" isShortName></Paper>. This pattern suggests that dataset complexity significantly influences quantization's impact on performance.\n\nInterestingly, some domain generalization tasks actually benefit from moderate quantization. Experiments on PACS and TerraIncognita datasets show that 7-bit precision yields optimal out-of-domain generalization while maintaining in-domain accuracy, with 6-bit and 8-bit also showing improvements <Paper corpusId=\"273228873\" paperTitle=\"(Javed et al., 2024)\" isShortName></Paper> <Paper corpusId=\"49744838\" paperTitle=\"(Beery et al., 2018)\" isShortName></Paper> <Paper corpusId=\"6037691\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper>. This demonstrates that quantization can sometimes enhance generalization capabilities, particularly for tasks requiring robust feature representations.\n\nAcross the spectrum of emergent abilities in LLMs, a consistent pattern emerges: 8-bit and 16-bit quantization preserves most capabilities with minimal degradation, 4-bit represents a critical threshold maintaining most emergent abilities, while 2-bit quantization severely degrades performance to near-random levels <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>. This pattern holds consistently across reasoning and instruction-following tasks, providing practical guidance for task-specific quantization decisions.",
                "citations": [
                    {
                        "id": "(Lam et al., 2020)",
                        "snippets": [
                            "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 211677681,
                            "title": "Quantized Neural Network Inference with Precision Batching",
                            "authors": [
                                {
                                    "authorId": "2347284",
                                    "name": "Maximilian Lam"
                                },
                                {
                                    "authorId": "1515569180",
                                    "name": "Zachary Yedidia"
                                },
                                {
                                    "authorId": "103876904",
                                    "name": "Colby R. Banbury"
                                },
                                {
                                    "authorId": "1805668",
                                    "name": "V. Reddi"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Xu et al., 2018)",
                        "snippets": [
                            "Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance."
                        ],
                        "paper": {
                            "corpus_id": 8257350,
                            "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2153078936",
                                    "name": "Chen Xu"
                                },
                                {
                                    "authorId": "32320704",
                                    "name": "Jianqiang Yao"
                                },
                                {
                                    "authorId": "33383055",
                                    "name": "Zhouchen Lin"
                                },
                                {
                                    "authorId": "10336865",
                                    "name": "Wenwu Ou"
                                },
                                {
                                    "authorId": "35667580",
                                    "name": "Yuanbin Cao"
                                },
                                {
                                    "authorId": "2108201369",
                                    "name": "Zhirong Wang"
                                },
                                {
                                    "authorId": "1687248",
                                    "name": "H. Zha"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 116
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bai et al., 2020)",
                        "snippets": [
                            "Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1."
                        ],
                        "paper": {
                            "corpus_id": 229923538,
                            "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
                            "authors": [
                                {
                                    "authorId": "9583912",
                                    "name": "Haoli Bai"
                                },
                                {
                                    "authorId": "2155468228",
                                    "name": "Wei Zhang"
                                },
                                {
                                    "authorId": "48557122",
                                    "name": "Lu Hou"
                                },
                                {
                                    "authorId": "50812138",
                                    "name": "Lifeng Shang"
                                },
                                {
                                    "authorId": "2115757711",
                                    "name": "Jing Jin"
                                },
                                {
                                    "authorId": "145820291",
                                    "name": "Xin Jiang"
                                },
                                {
                                    "authorId": "30738758",
                                    "name": "Qun Liu"
                                },
                                {
                                    "authorId": "1785083",
                                    "name": "Michael R. Lyu"
                                },
                                {
                                    "authorId": "145310663",
                                    "name": "Irwin King"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 227
                        },
                        "score": 0.83349609375
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."
                        ],
                        "paper": {
                            "corpus_id": 259937594,
                            "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study",
                            "authors": [
                                {
                                    "authorId": "2108129670",
                                    "name": "Peiyu Liu"
                                },
                                {
                                    "authorId": "2119618242",
                                    "name": "Zikang Liu"
                                },
                                {
                                    "authorId": "9136116",
                                    "name": "Ze-Feng Gao"
                                },
                                {
                                    "authorId": "2162036220",
                                    "name": "Dawei Gao"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "1696332",
                                    "name": "Bolin Ding"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Language Resources and Evaluation",
                            "n_citations": 35
                        },
                        "score": 0.94970703125
                    },
                    {
                        "id": "(Zheng et al., 2025)",
                        "snippets": [
                            "Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."
                        ],
                        "paper": {
                            "corpus_id": 278326880,
                            "title": "An Empirical Study of Qwen3 Quantization",
                            "authors": [
                                {
                                    "authorId": "2283439767",
                                    "name": "Xingyu Zheng"
                                },
                                {
                                    "authorId": "2359206148",
                                    "name": "Yuye Li"
                                },
                                {
                                    "authorId": "2359612976",
                                    "name": "Haoran Chu"
                                },
                                {
                                    "authorId": "2359148500",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "2190791504",
                                    "name": "Xudong Ma"
                                },
                                {
                                    "authorId": "2283307736",
                                    "name": "Jie Luo"
                                },
                                {
                                    "authorId": "2261414086",
                                    "name": "Jinyang Guo"
                                },
                                {
                                    "authorId": "2329344125",
                                    "name": "Haotong Qin"
                                },
                                {
                                    "authorId": "2283140094",
                                    "name": "Michele Magno"
                                },
                                {
                                    "authorId": "2278053544",
                                    "name": "Xianglong Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.75634765625
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."
                        ],
                        "paper": {
                            "corpus_id": 263620300,
                            "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness",
                            "authors": [
                                {
                                    "authorId": "2152658577",
                                    "name": "Young Jin Kim"
                                },
                                {
                                    "authorId": "2191522437",
                                    "name": "Raffy Fahim"
                                },
                                {
                                    "authorId": "3032929",
                                    "name": "H. Awadalla"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.74951171875
                    },
                    {
                        "id": "(Shymyrbay et al., 2023)",
                        "snippets": [
                            "The accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively)."
                        ],
                        "paper": {
                            "corpus_id": 258990120,
                            "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function",
                            "authors": [
                                {
                                    "authorId": "81112415",
                                    "name": "Ayan Shymyrbay"
                                },
                                {
                                    "authorId": "11702681",
                                    "name": "M. Fouda"
                                },
                                {
                                    "authorId": "2868015",
                                    "name": "A. Eltawil"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 7
                        },
                        "score": 0.77685546875
                    },
                    {
                        "id": "(Javed et al., 2024)",
                        "snippets": [
                            "We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy."
                        ],
                        "paper": {
                            "corpus_id": 273228873,
                            "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2211424900",
                                    "name": "Saqib Javed"
                                },
                                {
                                    "authorId": "2310800861",
                                    "name": "Hieu Le"
                                },
                                {
                                    "authorId": "2243289145",
                                    "name": "Mathieu Salzmann"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Beery et al., 2018)",
                        "snippets": [
                            "It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)"
                        ],
                        "paper": {
                            "corpus_id": 49744838,
                            "title": "Recognition in Terra Incognita",
                            "authors": [
                                {
                                    "authorId": "2134791809",
                                    "name": "Sara Meghan Beery"
                                },
                                {
                                    "authorId": "2996914",
                                    "name": "Grant Van Horn"
                                },
                                {
                                    "authorId": "1690922",
                                    "name": "P. Perona"
                                }
                            ],
                            "year": 2018,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 853
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2017)",
                        "snippets": [
                            "The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research."
                        ],
                        "paper": {
                            "corpus_id": 6037691,
                            "title": "Deeper, Broader and Artier Domain Generalization",
                            "authors": [
                                {
                                    "authorId": "2108338672",
                                    "name": "Da Li"
                                },
                                {
                                    "authorId": "2653152",
                                    "name": "Yongxin Yang"
                                },
                                {
                                    "authorId": "1705408",
                                    "name": "Yi-Zhe Song"
                                },
                                {
                                    "authorId": "1697755",
                                    "name": "Timothy M. Hospedales"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 1453
                        },
                        "score": 0
                    },
                    {
                        "id": "(Berti et al., 2025)",
                        "snippets": [
                            "Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."
                        ],
                        "paper": {
                            "corpus_id": 276903421,
                            "title": "Emergent Abilities in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2229196235",
                                    "name": "Leonardo Berti"
                                },
                                {
                                    "authorId": "2325903265",
                                    "name": "Flavio Giorgi"
                                },
                                {
                                    "authorId": "1686448",
                                    "name": "Gjergji Kasneci"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.900390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Quantization Methods and Techniques",
                "tldr": "Various quantization techniques have emerged to mitigate accuracy loss at low bit precision, with post-training quantization (PTQ) offering simple deployment and quantization-aware training (QAT) providing superior accuracy. Advanced methods like APTQ, QLORA with specialized number formats, and mixed-precision approaches that maintain higher bit rates for sensitive layers can significantly improve 4-bit model performance compared to uniform quantization. (12 sources)",
                "text": "\nQuantization approaches broadly fall into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ applies quantization to pre-trained models without retraining, offering simplicity but potentially greater accuracy loss, while QAT incorporates quantization effects during training to optimize model performance at lower precision <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe choice of numeric representation significantly impacts quantization outcomes. For 4-bit quantization of large language models, specialized number formats like NF4 with double quantization have demonstrated remarkable effectiveness, fully recovering 16-bit LoRA performance in fine-tuning scenarios and outperforming standard FP4 formats by approximately one percentage point on benchmarks like MMLU <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\nInnovative techniques have pushed the boundaries of low-bit quantization efficacy. Adaptive Precision Task Quantization (APTQ) achieves comparable perplexity to higher-precision models even at average bit rates as low as 3.0-3.5, outperforming other post-training quantization approaches at reduced precision <Paper corpusId=\"267897495\" paperTitle=\"(Guan et al., 2024)\" isShortName></Paper>. This demonstrates that advanced algorithms can maintain performance while achieving compression beyond uniform 4-bit quantization.\n\nMixed-precision approaches offer significant advantages over uniform quantization by strategically allocating different bit-widths across a model. Research shows that constraining models to an average of 4 bits while keeping first and last layers at 8 bits improves accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>. Similarly, mixed-precision configurations (such as ANT4-8) that combine 4-bit and 8-bit precision can achieve original model accuracy in CNNs and less than 1% accuracy loss for Vision Transformers and BERT models <Paper corpusId=\"251928917\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n\nThe activation precision plays a crucial role alongside weight quantization. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower precision dramatically increases model accuracy. For 4-bit weights, higher precision activations maintain performance within 1% of full-precision models across tasks ranging from MNIST classification to language modeling <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This finding suggests that asymmetric quantization approaches may offer better performance-compression trade-offs than symmetric schemes.\n\nQuantized distillation has emerged as a particularly effective technique across various bit-widths and architectures. It significantly outperforms other methods for 2-bit and 4-bit quantization, achieving accuracy within 0.2% of the teacher model at 8 bits and showing relatively minor accuracy loss at 4-bit precision <Paper corpusId=\"3323727\" paperTitle=\"(Polino et al., 2018)\" isShortName></Paper>. On language tasks, distillation enables 4-bit quantized models to achieve better perplexity than their teachers and improve BLEU scores compared to models trained with standard loss functions.\n\nFrequency-Aware Quantization (FAQ) provides substantial improvements for low-bit training. FAQ-trained 8-bit networks typically match or exceed full-precision accuracy with just one epoch of fine-tuning, while 4-bit networks require significantly longer fine-tuning (approximately 110 epochs) but ultimately match or exceed full-precision performance <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>. In most cases, well-trained 4-bit models show no accuracy advantage when precision is increased to 8-bit, suggesting that 4-bit can be sufficient with appropriate training techniques <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>.\n\nThe effectiveness of quantization methods varies significantly across model architectures. For example, QUIK quantization excels for LLaMA models but underperforms for Mistral, while AWQ shows greater robustness for Mistral <Paper corpusId=\"276575417\" paperTitle=\"(Kharinaev et al., 2025)\" isShortName></Paper>. This variability highlights the importance of selecting quantization techniques based on specific model architectures rather than applying one-size-fits-all approaches.\n\nPost-training methods like Optimal Channel Scaling (OCS) demonstrate the limitations of simpler quantization approaches. While effective at 8-bit and 6-bit precision, OCS experiences drastic accuracy drops at 4-bit <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. This highlights the need for more sophisticated techniques when targeting very low bit-widths.\n\nFor spiking neural networks, Mixed-precision Integer (MINT) quantization allows effective compression to both 8-bit and 4-bit precision with minimal accuracy impact (less than 1% drop). In some cases, quantized models even outperform full-precision baselines by small margins (e.g., 0.2% on VGG-16 TinyImageNet), suggesting quantization can provide regularization benefits <Paper corpusId=\"265043878\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Krishnamoorthi, 2018)",
                        "snippets": [
                            "Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model."
                        ],
                        "paper": {
                            "corpus_id": 49356451,
                            "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper",
                            "authors": [
                                {
                                    "authorId": "2065915235",
                                    "name": "Raghuraman Krishnamoorthi"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 1021
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0.79931640625
                    },
                    {
                        "id": "(Guan et al., 2024)",
                        "snippets": [
                            "Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B",
                            "The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy."
                        ],
                        "paper": {
                            "corpus_id": 267897495,
                            "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2120170158",
                                    "name": "Ziyi Guan"
                                },
                                {
                                    "authorId": "2301516332",
                                    "name": "Hantao Huang"
                                },
                                {
                                    "authorId": "2286850679",
                                    "name": "Yupeng Su"
                                },
                                {
                                    "authorId": "2351687700",
                                    "name": "Hong Huang"
                                },
                                {
                                    "authorId": "2287187433",
                                    "name": "Ngai Wong"
                                },
                                {
                                    "authorId": "2288350991",
                                    "name": "Hao Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Design Automation Conference",
                            "n_citations": 16
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Garg et al., 2021)",
                        "snippets": [
                            "Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."
                        ],
                        "paper": {
                            "corpus_id": 231918483,
                            "title": "Confounding Tradeoffs for Neural Network Quantization",
                            "authors": [
                                {
                                    "authorId": "9273400",
                                    "name": "Sahaj Garg"
                                },
                                {
                                    "authorId": "153408223",
                                    "name": "Anirudh Jain"
                                },
                                {
                                    "authorId": "134183581",
                                    "name": "Joe Lou"
                                },
                                {
                                    "authorId": "2060968473",
                                    "name": "Mitchell Nahmias"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT."
                        ],
                        "paper": {
                            "corpus_id": 251928917,
                            "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization",
                            "authors": [
                                {
                                    "authorId": "2109865181",
                                    "name": "Cong Guo"
                                },
                                {
                                    "authorId": "145107889",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "1831521",
                                    "name": "Jingwen Leng"
                                },
                                {
                                    "authorId": "2117940902",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "145338263",
                                    "name": "Fan Yang"
                                },
                                {
                                    "authorId": "2117415805",
                                    "name": "Yun-Bo Liu"
                                },
                                {
                                    "authorId": "2151671216",
                                    "name": "Minyi Guo"
                                },
                                {
                                    "authorId": "2124167",
                                    "name": "Yuhao Zhu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Micro",
                            "n_citations": 60
                        },
                        "score": 0.7783203125
                    },
                    {
                        "id": "(Lam et al., 2020)",
                        "snippets": [
                            "For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 211677681,
                            "title": "Quantized Neural Network Inference with Precision Batching",
                            "authors": [
                                {
                                    "authorId": "2347284",
                                    "name": "Maximilian Lam"
                                },
                                {
                                    "authorId": "1515569180",
                                    "name": "Zachary Yedidia"
                                },
                                {
                                    "authorId": "103876904",
                                    "name": "Colby R. Banbury"
                                },
                                {
                                    "authorId": "1805668",
                                    "name": "V. Reddi"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Polino et al., 2018)",
                        "snippets": [
                            "Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss."
                        ],
                        "paper": {
                            "corpus_id": 3323727,
                            "title": "Model compression via distillation and quantization",
                            "authors": [
                                {
                                    "authorId": "36060478",
                                    "name": "A. Polino"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                },
                                {
                                    "authorId": "3311387",
                                    "name": "Dan Alistarh"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 732
                        },
                        "score": 0.880859375
                    },
                    {
                        "id": "(McKinstry et al., 2018)",
                        "snippets": [
                            "FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."
                        ],
                        "paper": {
                            "corpus_id": 52197199,
                            "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference",
                            "authors": [
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2248110488",
                                    "name": "John V. Arthur"
                                },
                                {
                                    "authorId": "3121907",
                                    "name": "Izzet B. Yildiz"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 94
                        },
                        "score": 0.87548828125
                    },
                    {
                        "id": "(Esser et al., 2019)",
                        "snippets": [
                            "In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest."
                        ],
                        "paper": {
                            "corpus_id": 67788003,
                            "title": "Learned Step Size Quantization",
                            "authors": [
                                {
                                    "authorId": "2357931",
                                    "name": "S. K. Esser"
                                },
                                {
                                    "authorId": "46571359",
                                    "name": "J. McKinstry"
                                },
                                {
                                    "authorId": "2064431971",
                                    "name": "Deepika Bablani"
                                },
                                {
                                    "authorId": "2730753",
                                    "name": "R. Appuswamy"
                                },
                                {
                                    "authorId": "1944330",
                                    "name": "D. Modha"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 810
                        },
                        "score": 0.783203125
                    },
                    {
                        "id": "(Kharinaev et al., 2025)",
                        "snippets": [
                            "At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral."
                        ],
                        "paper": {
                            "corpus_id": 276575417,
                            "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2346980509",
                                    "name": "Artyom Kharinaev"
                                },
                                {
                                    "authorId": "2291142916",
                                    "name": "Viktor Moskvoretskii"
                                },
                                {
                                    "authorId": "2183482391",
                                    "name": "Egor Shvetsov"
                                },
                                {
                                    "authorId": "2334636989",
                                    "name": "Kseniia Studenikina"
                                },
                                {
                                    "authorId": "2346982876",
                                    "name": "Bykov Mikhail"
                                },
                                {
                                    "authorId": "2257279157",
                                    "name": "E. Burnaev"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8193359375
                    },
                    {
                        "id": "(Kim et al., 2020)",
                        "snippets": [
                            "On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated",
                            "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."
                        ],
                        "paper": {
                            "corpus_id": 218862856,
                            "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training",
                            "authors": [
                                {
                                    "authorId": "49476045",
                                    "name": "Jangho Kim"
                                },
                                {
                                    "authorId": "1713608836",
                                    "name": "Kiyoon Yoo"
                                },
                                {
                                    "authorId": "3160425",
                                    "name": "Nojun Kwak"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.86865234375
                    },
                    {
                        "id": "(Yin et al., 2023)",
                        "snippets": [
                            "Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization."
                        ],
                        "paper": {
                            "corpus_id": 265043878,
                            "title": "MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks",
                            "authors": [
                                {
                                    "authorId": "1820826857",
                                    "name": "Ruokai Yin"
                                },
                                {
                                    "authorId": "2265595883",
                                    "name": "Yuhang Li"
                                },
                                {
                                    "authorId": "65951652",
                                    "name": "Abhishek Moitra"
                                },
                                {
                                    "authorId": "9352814",
                                    "name": "P. Panda"
                                }
                            ],
                            "year": 2023,
                            "venue": "Asia and South Pacific Design Automation Conference",
                            "n_citations": 13
                        },
                        "score": 0.81005859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Recommendations",
                "tldr": "For most NLP applications, 4-bit quantization represents the optimal trade-off point, offering significant memory savings (70-80%) with minimal performance degradation (1-3%), though layer-specific strategies that maintain higher precision for sensitive components can further improve results. When deploying quantized models, practitioners should consider task complexity, model architecture, and available computational resources, with 8-bit recommended for critical applications requiring maximum accuracy and 4-bit or lower best suited for resource-constrained environments. (7 sources)",
                "text": "\nWhen implementing quantization in practice, empirical research offers several clear recommendations to optimize the trade-off between model compression and performance preservation. The most consistent finding is that 4-bit precision represents a practical sweet spot for most NLP applications, delivering substantial compression (7-8\u00d7) while typically incurring only minor accuracy degradation (1-3%) across diverse model architectures <Paper corpusId=\"231906389\" paperTitle=\"(Ghamari et al., 2021)\" isShortName></Paper>. This 4-bit threshold allows an average memory footprint reduction of approximately 70% compared to original models without significant performance decreases <Paper corpusId=\"276902966\" paperTitle=\"(Giagnorio et al., 2025)\" isShortName></Paper>.\n\nLayer-specific quantization strategies can significantly improve results compared to uniform approaches. For instance, when working with architectures like SqueezeNet, applying 4-bit quantization selectively to specific layers (e.g., 'fire-9-expand3x3') can minimize accuracy drops while still achieving substantial model compression <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>. This targeted approach recognizes that not all network components show equal sensitivity to precision reduction.\n\nFor applications where performance is paramount, 8-bit quantization offers an excellent compromise. With 8-bit precision, models typically show minimal accuracy decreases (around 2.2% in mean score) while reducing model size by approximately 4\u00d7 <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This makes 8-bit particularly suitable for production systems where reliability is critical but efficiency gains are still needed.\n\nWhen extreme compression is required, practitioners should consider the empirically established 3-bit threshold, beyond which performance degradation transitions from linear to exponential collapse. While 4-bit quantization generally maintains >95% of baseline performance, dropping to 2-bit precision typically preserves >90% of baseline performance but risks significant degradation for complex tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This suggests that 2-bit quantization should be used cautiously and primarily for applications where some performance trade-off is acceptable.\n\nFor federated learning scenarios with heterogeneous client devices, mixed-precision approaches offer compelling advantages. Systems with mixed-precision clients can save over 65% of energy consumption compared to homogeneous 32-bit systems while maintaining high accuracy. Interestingly, performance gains for lower-precision clients (4-bit) show diminishing returns beyond 16-bit precision for higher-tier devices, suggesting that 16-bit represents a practical upper limit for mixed-precision systems <Paper corpusId=\"270257725\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nWhen quantizing language models for code-specific tasks or specialized domains, using a domain-specific calibration dataset becomes increasingly important as quantization becomes more aggressive. Such calibration data helps limit performance degradation at extreme compression levels (3 and 2 bits) <Paper corpusId=\"276902966\" paperTitle=\"(Giagnorio et al., 2025)\" isShortName></Paper>.\n\nFor data valuation and influence estimation applications, practitioners can effectively use quantized representations to drastically reduce storage requirements. Even at 1-bit precision, quantized approaches remain only a few percentage points behind 16-bit performance while reducing storage needs by up to 16\u00d7 (from 16.54 GB to just 1.03 GB) <Paper corpusId=\"276106922\" paperTitle=\"(Ananta et al., 2025)\" isShortName></Paper>. This indicates that even extremely low-bit representations can retain sufficient information for certain meta-learning applications.\n\nWhen choosing between compression techniques, quantization consistently outperforms pruning in terms of robustness. While 4-bit quantization typically results in less than 5% accuracy loss, unstructured pruning with matched sparsity can cause 15-20% performance degradation <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This clear advantage makes quantization the preferred choice for most model compression scenarios.",
                "citations": [
                    {
                        "id": "(Ghamari et al., 2021)",
                        "snippets": [
                            "We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models",
                            "The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy."
                        ],
                        "paper": {
                            "corpus_id": 231906389,
                            "title": "Quantization-Guided Training for Compact TinyML Models",
                            "authors": [
                                {
                                    "authorId": "103148446",
                                    "name": "Sedigh Ghamari"
                                },
                                {
                                    "authorId": "40408041",
                                    "name": "Koray Ozcan"
                                },
                                {
                                    "authorId": "40844014",
                                    "name": "Thu Dinh"
                                },
                                {
                                    "authorId": "38363587",
                                    "name": "A. Melnikov"
                                },
                                {
                                    "authorId": "2062260709",
                                    "name": "Juan Carvajal"
                                },
                                {
                                    "authorId": "39497207",
                                    "name": "Jan Ernst"
                                },
                                {
                                    "authorId": "1691331",
                                    "name": "S. Chai"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 17
                        },
                        "score": 0.7744140625
                    },
                    {
                        "id": "(Giagnorio et al., 2025)",
                        "snippets": [
                            "The empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance."
                        ],
                        "paper": {
                            "corpus_id": 276902966,
                            "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
                            "authors": [
                                {
                                    "authorId": "2343501642",
                                    "name": "Alessandro Giagnorio"
                                },
                                {
                                    "authorId": "2079107343",
                                    "name": "A. Mastropaolo"
                                },
                                {
                                    "authorId": "2041943516",
                                    "name": "Saima Afrin"
                                },
                                {
                                    "authorId": "1719962",
                                    "name": "M. D. Penta"
                                },
                                {
                                    "authorId": "1801330",
                                    "name": "G. Bavota"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8017578125
                    },
                    {
                        "id": "(Yee et al., 2020)",
                        "snippets": [
                            "For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."
                        ],
                        "paper": {
                            "corpus_id": 225583435,
                            "title": "Face Recognition and Machine Learning at the Edge",
                            "authors": [
                                {
                                    "authorId": "2004954858",
                                    "name": "Joanne Ling Sin Yee"
                                },
                                {
                                    "authorId": "2412102",
                                    "name": "U. U. Sheikh"
                                },
                                {
                                    "authorId": "1951977",
                                    "name": "M. Mokji"
                                },
                                {
                                    "authorId": "2111988471",
                                    "name": "S. Rahman"
                                }
                            ],
                            "year": 2020,
                            "venue": "IOP Conference Series: Materials Science and Engineering",
                            "n_citations": 2
                        },
                        "score": 0.791015625
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."
                        ],
                        "paper": {
                            "corpus_id": 244531228,
                            "title": "LiteEdge: Lightweight Semantic Edge Detection Network",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "118657651",
                                    "name": "Hasan Al-Banna Mohamed"
                                },
                                {
                                    "authorId": "15678675",
                                    "name": "Zuowen Wang"
                                },
                                {
                                    "authorId": "4765464",
                                    "name": "Bodo Rueckauer"
                                },
                                {
                                    "authorId": "1704961",
                                    "name": "Shih-Chii Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
                            "n_citations": 3
                        },
                        "score": 0.77001953125
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."
                        ],
                        "paper": {
                            "corpus_id": 277452419,
                            "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2352948034",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2274202084",
                                    "name": "Zuchao Li"
                                },
                                {
                                    "authorId": "2269488794",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2343636012",
                                    "name": "Gui-Song Xia"
                                },
                                {
                                    "authorId": "2306994733",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2268745050",
                                    "name": "Liangpei Zhang"
                                },
                                {
                                    "authorId": "2275194788",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Yuan et al., 2024)",
                        "snippets": [
                            "Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients",
                            "As shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy."
                        ],
                        "paper": {
                            "corpus_id": 270257725,
                            "title": "Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation",
                            "authors": [
                                {
                                    "authorId": "2305216766",
                                    "name": "Jinsheng Yuan"
                                },
                                {
                                    "authorId": "98553648",
                                    "name": "Zhuangkun Wei"
                                },
                                {
                                    "authorId": "48544782",
                                    "name": "Weisi Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Wireless Communications and Networking Conference",
                            "n_citations": 1
                        },
                        "score": 0.89794921875
                    },
                    {
                        "id": "(Ananta et al., 2025)",
                        "snippets": [
                            "Performance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation."
                        ],
                        "paper": {
                            "corpus_id": 276106922,
                            "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2343738049",
                                    "name": "Moses Ananta"
                                },
                                {
                                    "authorId": "2191731497",
                                    "name": "Farid Adilazuarda"
                                },
                                {
                                    "authorId": "2306265527",
                                    "name": "Z. M. K. Zuhri"
                                },
                                {
                                    "authorId": "2257345523",
                                    "name": "Ayu Purwarianti"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.7412109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.266772
    }
}