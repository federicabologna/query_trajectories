{
    "query": "you LLaVA series variants they over time",
    "user_id": "lib_user",
    "task_id": "489cbe79-7f65-4aa7-b0c5-9b22a6c4cb12",
    "timestamp": "2025-06-23T22:34:16.222215",
    "n_retrieval": 256,
    "n_retrieved": 237,
    "n_candidates": 1,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.08287800000000001,
    "decomposed_query": {
        "rewritten_query": "LLaVA series variants and their evolution over time.",
        "keyword_query": "LLaVA series variants evolution",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009489,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "venue": "",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292400830",
                    "name": "S. Riggi"
                },
                {
                    "authorId": "2042077694",
                    "name": "T. Cecconello"
                },
                {
                    "authorId": "2352941747",
                    "name": "A. Pilzer"
                },
                {
                    "authorId": "2352939581",
                    "name": "S. Palazzo"
                },
                {
                    "authorId": "2299008238",
                    "name": "N. Gupta"
                },
                {
                    "authorId": "2298907506",
                    "name": "A. Hopkins"
                },
                {
                    "authorId": "2258840598",
                    "name": "C. Trigilio"
                },
                {
                    "authorId": "2349648144",
                    "name": "G. Umana"
                }
            ],
            "abstract": "The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.",
            "corpus_id": 277452239,
            "sentences": [
                {
                    "corpus_id": "277452239",
                    "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                    "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
                    "score": 0.1948802059124483,
                    "section_title": "Model overview",
                    "char_start_offset": 10302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57421875
                }
            ],
            "relevance_judgement": 0.57421875,
            "relevance_judgment_input_expanded": "# Title: Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks\n# Venue: \n# Authors: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. Hopkins, C. Trigilio, G. Umana\n## Abstract\nThe advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.\n## Model overview\nLLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "reference_string": "[277452239 | Riggi et al. | 2025 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "268876083",
            "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
            "text": "Table 2 shows the models we use for comparison.We include LLaVA variants from open domain (LLaVA-v0 [17], LLaVA-v1.5 [15] and LLaVA-v1.6[16]) and medical domain (LLaVA-Med [11] and CXR-LLaVA [10]).All models except LLaVA-v1.5 and LLaVA-1.6 are based on 7B backbone LLM and LLaVA-v1.5 and LLaVA-1.6 with two model sizes: 7B and 13B.LLaVA-1.6 comes with an additional backbone LLM, Mistral-7B.",
            "score": 0.3221429237381406,
            "section_title": "List of Models",
            "char_start_offset": 7663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 47,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "270878671",
            "title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study",
            "text": "We conduct our ablations on LLaVA 1.6 as this series of models is both well studied and exhibits strong performance across a range of multimodal tasks (Liu et al., 2024).Particularly, we focus on aligning the LLaVA 1.6-7B Vicuna model variant as this scale of parameters is particularly widely used in the community.Notably, LLaVA 1.6-7B provides a significantly stronger baseline performance over the more common choice of LLaVA 1.5-7B in the multimodal alignment literature.",
            "score": 0.29440660252292683,
            "section_title": "MODEL",
            "char_start_offset": 27053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 476
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1458740234375
        },
        {
            "corpus_id": "265506479",
            "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner",
            "text": "Based on the instruction-following dataset from LLaVA-665K [6], we randomly shuffle and sub-sample it to construct two 1% and 10% variants, LLaVA-6K and LLaVA-66K. Here, we provide detailed dataset statistics comparisons on LLaVA-6K, LLaVA-66K and LLaVA-665K. The results are shown in Table I, where all variants retain the same data size distribution.",
            "score": 0.2665344982794953,
            "section_title": "B. More Details on Instruction-following Datasets",
            "char_start_offset": 31690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "276929061",
            "title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models",
            "text": "For a fair comparison and efficient validation, we employ the LMMs-Eval [59] evaluation framework across all experiments. The models used include LLaVA-1.5 (7B and 13B variants) [29], LLaVA-NEXT-7B [10], and InternVL2 (2B and 8B variants) [10], with configurations based on the their official settings. Detailed model architectures and settings are provided in the supplementary material.",
            "score": 0.2651514501221962,
            "section_title": "Model Settings",
            "char_start_offset": 17615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 388
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "273638520",
            "title": "Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)",
            "text": "and LLaVA-1.6-Vicuna-13B), which do not show strong occupation-gender bias compared to other models. On the other hand, LLaVA-1.6-Hermes-34B and LLaVA-1.6-Mistral-7B are among the models with the strongest gender-occupation bias. This is likely due to differences in the LLM, which would also explain why models in the LLaVA-1.5 family rank low in terms of bias strength. They also use Vicuna as VLM. \n\nWe conclude that there is a clearly definable set of models with the strongest gender-occupation bias, namely larger models in the InternVL2 and Bunny series and models not using Vicuna as LLM in the LLaVA-1.6 series. Figure 17: Individual models ranked by the ratio of occupations with a significant difference between \u00b5 male and \u00b5 female .",
            "score": 0.26457040422694766,
            "section_title": "E FULL RESULTS FOR EVALUATING GENDER BIAS IN VLAS",
            "char_start_offset": 54463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 744
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061981201171875
        },
        {
            "corpus_id": "274598288",
            "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
            "text": "Default settings for resolution are used, and max number of frames are 32. 5. VILA [16] VILA 1.5 is the latest in the VILA series of models, one of the first models to support multi-image understanding. We use the Llama-3-VILA1.5-8b-Fix model. We uniformly sample 5 frames for each part of the video. \n\n6. LLaVA-Video [43] LLaVA was first introduced as a multimodal model with performance rivaling GPT-4. Following the release of LLaVA 1.5 and LLaVA-NeXT, LLaVA-Video is the most advanced version of this model, specifically trained on a new 178K video dataset. Specifically, we use the LLaVA-Video-7B-Qwen2 model specification. When asking questions, we directly upload the entire video clip. In the case of Detective , the clip has the V main blacked out. 32 frames are used. We also test the 72B LLaVA-Video (LLaVA-Video-72B-Qwen2) model; however, due to compute limitations and setup issues, we were only able to run that model on a subset of the data. The results are detailed in Appendix F.3. \n\nOur implementation of all open-source models follows their respective instructions on GitHub and Huggingface, and we tried our best to recreate the same environment as the original developers for each model. We do not do batch inference on any of these models, and instead query them iteratively in chat mode only (each chat conversations for every question is independent). For the generative variant, in order to obtain different results each time, we turn sampling on. \n\nWe use the following prompts for each task: \n\nForecaster -Gen Describe what could happen next, by explaining the sequence of actions leading to the outcome.",
            "score": 0.25494101791931617,
            "section_title": "E. Baselines",
            "char_start_offset": 35823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1631
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07501220703125
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Model Families and Versions Figure 13 provides a comprehensive taxonomy of Large Multimodal Models (LMMs), showcasing their diversity across families, versions, and first-generation re-  This taxonomy highlights the wide range of LMMs currently available, organized as follows: \n\n\u2022 InternVL Family: Spanning models from Intern-VL1.1 (Chen et al., 2024c;Gao et al., 2024;Chen et al., 2024b) to the advanced Intern-VL2.5- MPO (Chen et al., 2024d), this family emphasizes efficiency and adaptability for diverse tampering and multimodal tasks. \n\nSuccessive iterations demonstrate marked improvements, particularly in handling temporal disruptions such as Dropping and Substitution. The InternVL2-5-8B models showcases its ability to handle fine-grained spatiotemporal reasoning, highlighting its dominance in high-resource benchmarks. \n\n\u2022 LLaVA Family: Starting with LLaVa-NEXT (Li et al., 2024d), which struggled across most benchmarks, this family has evolved with models like LLaVa-OneVision (Li et al., 2024b) and LLaVa-Video (Zhang et al., 2024), demonstrating significant improvements in task-specific video understanding through op- timized pretraining and alignment techniques. Despite the advancements, LLaVa-OneVision & LLaVa-Video continues to face challenges in handling complex temporal disruptions, unlike Chat-UniVi, which has emerged as a robust alternative. \n\n\u2022 VILA Family: The VILA series, including VILA-1.5 (Lin et al., 2024b)   For low-performing models (Figure 14a), Llava-OneVision exhibits consistently weak performance across tampering types, even at larger parameter sizes (e.g., Llava-OneVision-72B). This suggests potential architectural and training data limitations, particularly for temporal coherence tasks. Interestingly, Qwen2-VL-7B underperforms significantly compared to its larger counterpart, Qwen2-VL-72B, which achieves a notable improvement. This indicates that increasing model size, combined with its training paradigm, positively impacts robustness for this family.",
            "score": 0.25284035445278735,
            "section_title": "Overview of Large Multimodality Models (LMMs)",
            "char_start_offset": 20007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1425,
                    "end": 1444,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "272826678",
            "title": "Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models",
            "text": "We employ three LLaVA-1.5 (Liu et al., 2024) variants: LLaVA-1.5-7B, LLaVA-1.5-13B, and ViP-LLaVA-13B (Cai et al., 2024). The models integrate the pre-trained CLIP ViT-L/14 visual encoder with Vicuna via a simple projection matrix. ViP-LLaVA-13B is a fine-tuned LLaVA-1.5-13B with annotation-rich image input. The LLaVA with Vicuna backbone is selected due to its superior instruction-following capability compared to other variants, as highlighted in Shao et al. (2024). \n\nIn the SINGLE-STEP setting, we probe the performance differences between (1) and (2) to probe the base model ability in following basic instruction to produce our data. We then proceed with (2) to run MULTI-STEP setting for more advanced prompting technique. Finally, we employ (3) in conjunction with the boundary box as an additional input in SINGLE-STEP-VIP setting.",
            "score": 0.24260646362475646,
            "section_title": "Models",
            "char_start_offset": 8891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.183349609375
        },
        {
            "corpus_id": "276161198",
            "title": "PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?",
            "text": "Model Checkpoint LISA xinlai/LISA-7B-v1-explanatory GLAMM MBZUAI/GLaMM-FullScope GLAMM-RegCap MBZUAI/GLaMM-RegCap-RefCOCOg Llava-G Haozhangcx/llava grounding gd vp Llava 1.5 (7B) liuhaotian/llava-v1.5-7b Llava 1.5 (13B) liuhaotian/llava-v1.5-13b Cambrian-1 (8B) nyu-visionx/cambrian-8b \n\nTable 2. Hugging Face model checkpoints used in our benchmarks.",
            "score": 0.24029073200995935,
            "section_title": "Model Name",
            "char_start_offset": 28570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0025501251220703125
        },
        {
            "corpus_id": "267750968",
            "title": "Exploring the Limits of Zero Shot Vision Language Models for Hate Meme Detection: The Vulnerabilities and their Interpretations",
            "text": "Moreover, GPT-4O is the best model for all prompt variants taken individually and LLAVA-1.5 13B outperforms other open-source models in 7 of the 8 prompt variants and lags behind LLAVA-1.5 7B variant for the setting with definition & OCR as input and explanation as output. Stability: Further, we also observe that GPT-4O is not only the best model across all prompt variants, but is also the most stable model with least standard deviation across different prompt variants. Amongst open-source models, LLAVA-1.5 13B model has the best mean. Baselines: As per the discussion in 'Models' section, we present the results in Table 4 on zero-shot and FHM finetuned versions of the baseline. Note that it is done to be very near to zero-shot evaluation. GPT-4O outperforms these baselines even without any fine-tuning. Notably, LLAVA-1.5 13B version also outperforms these baselines across Macro-F1 score. This raises a deep concern about the generalization capability of previous works.",
            "score": 0.2329062342484734,
            "section_title": "Results",
            "char_start_offset": 18326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050994873046875
        },
        {
            "corpus_id": "273025791",
            "title": "LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models",
            "text": "As shown in Figure 9, LLaVA-Med incorrectly predicted the image as glaucoma, providing a detailed but inaccurate explanation. Experimental results in Table 2 indicate that all LVLMs achieve suboptimal accuracy in both glaucoma and macular hole stage classification tasks. Additionally, models in the LLaVA series, such as LLaVA-Med, LLaVA-13B, and LLaVA-M-7B, consistently generate the same response, predicting glaucoma disease across different input images. These findings suggest that current LVLMs still lack the capability to accurately interpret images and make reliable predictions.",
            "score": 0.22907999510812635,
            "section_title": "I Detailed Error Analysis I.1 Misclassification",
            "char_start_offset": 44859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 589
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0633544921875
        },
        {
            "corpus_id": "267617068",
            "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
            "text": "In Phase 6, our assessment focuses on LLaVA-Docent Version 2 about GPT-4. It is essential to recognize that LLaVA-Docent operates with 13 billion parameters, a significant factor in its performance. In comparison, GPT-4 is believed to be a larger model than LLaVA-Docent. This inference stems from its exceptional generative capabilities and its comparison to its predecessors, GPT-3 and GPT-3.5, which have 175 billion parameters (OpenAI, 2023). Therefore, note that in analyzing the results of the comparison between LLaVA-Docent and GPT-4, it is crucial to consider the disparity in the number of parameters, which could significantly influence their respective performances.",
            "score": 0.22802221818999957,
            "section_title": "Phase 6: evaluation result of LLaVA-Docent Version 2",
            "char_start_offset": 28534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 678
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 445,
                    "matchedPaperCorpusId": "143522942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057708740234375
        },
        {
            "corpus_id": "266209773",
            "title": "Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens",
            "text": "[30]( \u00a74.3). For clarity, accuracy of base model and two variants are given (\"Baseline\": the frozen LLaVA [14] with trainable Q-Former [12], \"w Seq\": base model with sequential visual projector, \"w Seq&EDVT\": base model with both sequential visual projector and EDVT-Attention).",
            "score": 0.22785613405393207,
            "section_title": "Comparison to State-of-the-Arts",
            "char_start_offset": 20199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 278
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0160980224609375
        },
        {
            "corpus_id": "269148679",
            "title": "MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting",
            "text": "For all variants of LLaVA-1.5 that were tested, there was an increase in the accuracy score when MI-CoT Prompting was employed as seen in Figure 4a except in the case of LLaVA-1.5 7b model.A smaller number of trainable parameters meant that the model was not able to process the more complex multi-image input, leading to a sharp dip in the performance.The difference was the most significant in the case of LLaVA-1.5 13b trained with LoRA as 128, which also gave the best performance out of all the models tested when trained using MI-CoT Prompting.The MI-CoT Prompting trained version also exhibited high rouge scores as seen in Table 2.It can be observed from Figure 4b that the rouge scores were higher in the LLaVA-1.5 13b CoT variants, showcasing the fact that models that were able to leverage the MI-CoT prompt also showed a bump in the reasoning capabilities.A marked improvement in all metrics, when multiple images were provided in the prompt in the case of LLaVA-1.5 13b variants, provides evidence that the models were able to segregate and recognize the image that has to be used for each question present in a single prompt.",
            "score": 0.22134991280581215,
            "section_title": "Effect of Chain of Thought Prompting",
            "char_start_offset": 15209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "267617068",
            "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
            "text": "LLaVA and GPT-4 (few shots) have different strengths and weaknesses in art appreciation. Table 10 shows the characteristics of the two models. There were several advantages of LLaVA compared to GPT-4. First, LLaVA typically progresses through the stages of appreciation sequentially, asking questions step by step. In contrast, GPT-4 adopts a more analytical approach, breaking down the components of the artwork for individual perception and interpretation. While LLaVA linearly follows each phase of the art appreciation, GPT-4 utilizes a cyclical  and less predictable flow, often revisiting previous steps and focusing on a particular stage. Second, LLaVA limits itself to one question at a time, which can effectively avoid cognitive overload (Schmidhuber et al., 2021;Sweller, 2011). On the other hand, GPT-4 poses one or two questions simultaneously and encourages learners to consider multiple perspectives, which allows users to construct their ideas by connecting multiple answers to the given questions. Third, LLaVA provides fewer explanations about the artwork, steering users towards concentrating on the artwork's inherent structural elements. This approach aims to assist users in interpreting the art on their own. Conversely, GPT-4 (few shots) is more proactive in providing detailed information about the artwork at various stages, which diverges from Anderson's critical stage (1993) and places greater importance on the viewer's perspectives rather than professional interpretation. Meanwhile, GPT-4 (few shots) had several benefits over LLaVA. First, LLaVA and GPT-4 (few shots) differ in their question-posing techniques. LLaVA employs speech-like questions to create a less classroom-like atmosphere and adheres to Anderson's critical stages (1993) with immediate feedback. On the contrary, GPT-4 (few shots) initiates thoughtful responses through diverse and scaffolded questions, emphasizing eliciting emotional engagement with the subject. Second, LLaVA and GPT-4 (few shots) mimic a teacher's informative speech style in their statements. However, LLaVa encounters challenges related to credibility and accuracy, disseminating several pieces of incorrect information.",
            "score": 0.22027012911113547,
            "section_title": "Synthesis",
            "char_start_offset": 35907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 774,
                    "matchedPaperCorpusId": "240004273"
                },
                {
                    "start": 774,
                    "end": 788,
                    "matchedPaperCorpusId": "60459788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047515869140625
        },
        {
            "corpus_id": "273023173",
            "title": "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models",
            "text": "We reported the results of applying likelihood composition on LLaVA series MLMs and 4 advanced MLMs with varying hyperparameters in Table .5,2 and Table .3 respectively. In Table .5, baseline is the MLM's intrinsic performance, while in Table .2 and Table .3, baselines are ensemble and majorityvote, which we refer as mutual-composition in our framework. \n\nResults on LLaVA Series As shown in Table .5, applied with self-composition methods mentioned in Sec. 4.1, LLaVA series' performance on the 9 datasets consistently improved, e.g., +12.08% for LLaVA-7B on MMVP, +4.39% for LLaVA-13B on MMBench, +6.96% for LLaVA1.5-7B on VSR, etc. Overall, for the early models in LLaVA family, i.e., LLaVA-7B and LLaVA-13B, which is not well developed relatively, self-composition methods improve their performance significantly. Also, aggressive self-composition, i.e., with \u03b1 = 1.0 works better in most cases than that with \u03b1 = 0.1, for LLaVA-7B and LLaVA-13B. \n\nFor those well-developed models, i.e., LLaVA1.5-7B, LLaVA1.5-13B, LLaVA1.6-7B and LLaVA1.6-13B, the improvement self-composition brings is not as significant as before. In more detail, for the best model, LLaVA1.6-13B, the Table 1: Self-composition methods bring a consistent improvement to all the LLaVA series models. (1) For debias, when \u03b1 is set to 1.0, it improves LLaVA-7B's performance on all 9 datasets and LLaVA-13B's performance on 7 datasets. When \u03b1 is set to 0.1, debias improves LLaVA1.5-7B and LLaVA1.5-13B's performance on 9 and 8 datasets respectively; improves LLaVA1.6-7B and LLaVA1.6-13B's",
            "score": 0.21949483770369302,
            "section_title": "Main Results",
            "char_start_offset": 16067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1563
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20751953125
        },
        {
            "corpus_id": "271915843",
            "title": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model",
            "text": "Following LLaVA-v1.5 (Liu et al. 2023a), we employ Vicuna-7b-v1.5 (Zheng et al. 2023) as our foundational LLM. The training data is consistent with LLaVA-v1.5, encompassing both pre-training data and supervised finetuning data.",
            "score": 0.21608937375085327,
            "section_title": "Ablation Study Implementation Details",
            "char_start_offset": 16705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "274464940",
            "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning",
            "text": "The results are reported in Table 3, comparing our model with the base model and baseline method. \n\nSetup. We choose LLaVA-1.5-7B [38] as our base model following its evaluation protocol. Again, during inference of the pre-trained base model, we merge the input visual tokens of Vicuna LLM and prune visual tokens at Vicuna's layers. We test our model on several image benchmarks and report efficiency metrics (FLOPs and prefill time) and accuracy metrics of each image benchmark. \n\nBaselines. We again choose the baseline as FastV [5] and LLaVA-1.5-Prumerge [59] which provide training-free results on image benchmarks. We also compare with a variant LLaVA-1.5-Prumerge+ which trades computation for reasoning performance. Both our models and baseline models are evaluated in the training-free setting with the pre-trained LLaVA-1.5-7B as the base model. Adaptive inference compared with the base model. As shown in Table 3, when compared to the base model LLaVA-1.5-7B, our model (with 1.00 FLOPs) significantly reduces the FLOPs and prefill time (i.e., using only 12.5% FLOPS and 30.6% prefill time), with a noticeable performance drop. However, by adjusting the merging ratio and pruning scheduler, our model (with 2.22 FLOPs) largely mitigates this performance gap while still considerably reducing FLOPs and prefill time (i.e., using only 27.1% FLOPS and 37.3% prefill time). This capability is supported by our design of adaptive inference that can accommodate various efficiency demands.",
            "score": 0.2159660976141644,
            "section_title": "Image Benchmarks",
            "char_start_offset": 20683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "268358224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "269282964",
            "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
            "text": "This experiment is conducted on popular medical VQA benchmarks VQA-RAD and SLAKE. We directly leverage the medical VQA evaluation metric adopted by LLaVA-Med. Each sample of VQA-RAD and SLAKE is observed only once during the training process of MoVA and LLaVA-1.5. For a fair comparison, we compare MoVA with the LLaVA-Med variant that is finetuned with only 1 epoch on the benchmark. The performance of the LLaVA-Med specialist that is fully finetuned on downstream tasks is also reported. As presented in Table 6, MoVA-7B consistently yields higher scores than LLaVA-Med and LLaVA-1.5, exhibiting its medical visual chat ability.",
            "score": 0.21473619684988532,
            "section_title": "Medical Visual Question Answering",
            "char_start_offset": 22102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 631
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "275342951",
            "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
            "text": "Benchmarks We evaluate LLaVA-Mini on image and video understanding tasks. Experiments are conducted on 11 image benchmarks and 7 video benchmarks. Refer to Appendix C for details. \n\nBaselines LLaVA-Mini is an image/video LMM, so we compare it with several advanced imagebased and video-based LMMs. Detailed description of baselines refer to Appendix D. Configuration For a fair comparison, LLaVA-Mini employs the same configurations as LLaVA-v1.5 (Liu et al., 2023b), using the CLIP ViT-L/336px (Radford et al., 2021) as the vision encoder and Vicuna-v1.5-7B (Chiang et al., 2023) as the LLM backbone. The compressed hyperparameter C is set to 1, meaning vision tokens are compressed to one token. The number of modality prefusion layers N f usion is set to 4. LLaVA-Mini uses the same training data as LLaVA-v1.5 (Liu et al., 2023b), using 558K caption data for pretraining and 665K instruction data for instruction tuning. The high-resolution version with 672*672 pixels (refer to Sec.4.2) is denoted as LLaVA-Mini-HD. To capture more visual details, the compressed hyperparameter C of LLaVA-Mini-HD is set to 8, i.e., compressing to 64 vision tokens. For video processing, LLaVA-Mini extracts 1 frame per second (1 fps) from the video and sets C = 1 to represent each frame with one vision token. \n\nTo further explore the potential of LLaVA-Mini, we introduce a variant that uses the CLIP ViT-L/336px (Radford et al., 2021) as vision encoder and the advanced LLaMA-3.1-8B-Instruct (Dubey et al., 2024) as LLM backbone.",
            "score": 0.2138176510779476,
            "section_title": "EXPERIMENTAL SETTING",
            "char_start_offset": 18135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1521
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 466,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 495,
                    "end": 517,
                    "matchedPaperCorpusId": "265608767"
                },
                {
                    "start": 814,
                    "end": 833,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "271533843",
            "title": "Predicting Winning Captions for Weekly New Yorker Comics",
            "text": "LLaVA-based models are variants of vision encoderdecoder models, which projects the input image onto an embedding space that has the same dimension as the language embedding space. By training this light-weight projection layer, LLaVA-NeXT is able to turn images into tokens, which are then subject to the same attention mechanisms as the textual data. \n\nIn [13], they used a CLIP-ViT-L/14 to Vicuna LLM encoder-decoder model. However, in our experiments, we use the LLaVA-NeXT model. In the original paper, the authors use a CLIP-ViT-L-336px to Vicuna LLM [12], but we make use of a variant hosted on Hugging Face, which replaces Vicuna with Mistral LLM.",
            "score": 0.2136507527437561,
            "section_title": "LLaVA-NeXT",
            "char_start_offset": 8939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 655
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047149658203125
        },
        {
            "corpus_id": "265466833",
            "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
            "text": "Our evaluation extends to larger 13B variants of the LLaVA-1.5 and InstructBLIP models 10 , assessing the scalability of our proposed VCD across different LVLM magnitudes. Table 8 reveals that the 7B and 13B variants of LLaVA-1.5 and InstructBLIP exhibit comparable performances across POPE settings (e.g., 81.33 and 81.49 F1 scores for LLaVA-1.5 7B and 13B in Random setting), suggesting that increasing the model parameters does not inherently resolve hallucination issues, thereby underscoring the pertinence of addressing this challenge. Crucially, VCD consistently boosts performance in all POPE configurations, reaffirming its robustness independent of model scale. 10 Qwen-VL lacks larger variants.",
            "score": 0.21289755579552144,
            "section_title": "B.5. Effect of VCD when LVLMs Scale Up",
            "char_start_offset": 30576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050140380859375
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "First, LLaVA-AlignedVQ consistently demonstrates high accuracy, staying within \u22122.23% to +1.6% of the original LLaVA model and \u22120.86% to +1.82% of LLaVA 1+ , achieving seven top two accuracy among the eleven results Although the variants LLaVA-JPEG-90 and LLaVA-JPEG-90 1+ achieve competitive high accuracy, lower than LLaVA-AlignedVQ by approximately \u22120.16% and \u22120.66% on average, they incur a larger transmission overhead as indicated in Table 3, leading to longer inference latency as shown in Section 4.4. Although using JPEG-10 greatly reduces the transmission overhead compared to JPEG-90, from 26.47KB to 4.14KB per sample as reported in Table 3, it suffers from a significant accuracy drop of 5.09% on average. \n\nSecond, models fine-tuned with compressed images or compressed intermediate features, such as LLaVA-AlignedVQ and LLaVA-JPEG-90 1+ , achieve even higher accuracy than , leads to an accuracy drop of 1.2% on average. While LLaVA-AlignedVQ shows accuracy improvements of 0.85% on VQA-v2, 0.72% on GQA, and 0.78%, 0.07%, 0.44% on POPE-rand, POPE-pop, POPEadv respectively, while LLaVA-JPEG-90 1+ achieves gains of 0.33% on VQA-v2 and 0.08% on GQA. We hypothesize that this improvement comes from the augmentation effects of compressed data during fine-tuning. When fine-tuning the LoRA parameters in the original LLaVA model for an additional epoch, the model revisits the same data without acquiring new information, may cause overfitting on the train data. In contrast, fine-tuning with compressed data, either images or features, exposes the model to more diverse input space, thereby enhancing accuracy.",
            "score": 0.2118418256979375,
            "section_title": "Experimental Settings",
            "char_start_offset": 22226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134521484375
        },
        {
            "corpus_id": "271902889",
            "title": "PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding",
            "text": "To verify the effectiveness of each module-including our instruction data, domain-specific visual encoder, and connector. We reported the performance of the different PA-LLaVA variants in Table III. PA-LLaVA represents the proposed model; PA-LLaVA*, PA-LLaVA \u2020and PA-LLaVA \u2021represent variants using different modules. \n\nThe main observations are as follows: (1) Our PA-LLaVA trained on PCaption-0.5M significantly outperformed the PA-LLaVA \u2020that trained on PCaption-C data. This indicates that our data-cleaning process was effective for downstream tasks. \n\n(2) When our specialized visual encoder PLIP in PA-LLaVA was replaced with the general visual encoder CLIP, while keeping all other conditions unchanged (PA-LLaVA vs PA-LLaVA \u2021), the performance degraded, especially on zero-shot tasks. This indicates that our specialized PLIP model can provides more effective feature representations for the pathology images. (3) Building on the previous step, when we replaced the connector part with an MLP structure (PA-LLaVA* vs PA-LLaVA \u2021), which reverted to the original architecture of LLaVA, the model performance was further degraded, which confirmed the effectiveness of our proposed connector.",
            "score": 0.21031492665286822,
            "section_title": "D. Ablation Studies",
            "char_start_offset": 16851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10321044921875
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A cabinet with a lot of dishes in it. \n\nSim-CLIP 4 : A white dishwasher with a black handle. \n\nRobust-LLaVA 4 G: A trash cart is filled with trash and junk.",
            "score": 0.20967271403006577,
            "section_title": "Adversarial Image",
            "char_start_offset": 42341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00319671630859375
        },
        {
            "corpus_id": "265067328",
            "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts",
            "text": "Result Analysis. As Figure 5 shows, the variants of FigStep are also effective for jailbreaking VLMs. When using (T \u2032 3 , I \u2032 2 ) on LLaVA, the ASR can achieve 98.00%, which outperforms 2 ) can also result in a higher ASR than (T \u2032 1 , I \u2032 1 ) on CogVLM. Additionally, we notice that different combinations yield varying effects across different models. For instance, (T \u2032 3 , I \u2032 1 ) performs well on LLaVA but poorly on CogVLM. However, the default (T \u2032 1 , I \u2032 1 ) demonstrates good generalizability across various models, attributable to the detailed information provided by T \u2032 1 and the standard format of I \u2032 1 .",
            "score": 0.20908774991872228,
            "section_title": "Generality of FigStep",
            "char_start_offset": 32957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006145477294921875
        },
        {
            "corpus_id": "272770508",
            "title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs",
            "text": "We show the hallucination comparison of the seven mainstream LVLMs on our FIHA-v1 in Table 3. From this Table, we have several observations. 1) It's worth highlighting that GPT-4V excels in both image and caption Q&A pairs, achieving the best performance among the evaluated models. 2) The second-best performer is InstructBLIP, which significantly outperforms other models except GPT-4V across most metrics. 3) Additionally, we have observed that model parameters are also significant factors affecting performance. For instance, LLaVA-1.5-13B provides a more comprehensive improvement over the LLaVA-1.5-7B. \n\nIn addition, we also show the performance of 7 main- stream LVLMs on FIHA-v1 based on the Visual Genome dataset. The results show a similar trend as compared to the performance in MSCOCO datasets. Specifically, the GPT-4V performs best and MiniGPT-4 performs the worst. LLaVA-1.5-13B performs better than LLaVA-1.5-7B, which also indicates that the model parameter size influences the performance.",
            "score": 0.20893071004622774,
            "section_title": "Overall Results on Datasets Generated by FIHA",
            "char_start_offset": 13618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0205230712890625
        },
        {
            "corpus_id": "268513094",
            "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
            "text": "An overall VHE as shown in Tab. 5 is useful for providing a big picture of which MLLM hallucinates the most (or the least). The leading open-source MLLMs are LLaVA-OneVision, followed by Molmo and InternVL-1.5. Since their vision encoders and LLMs vary, the results are insufficient to conclude which component is the most effective to mitigate hallucinations. That said, comparisons among the same model series remains meaningful. Consider the LLaVA series for instance. While one would normally expect that a larger LLM yields a better MLLM, as LLaVA-1.6-L vs LLaVA-1.6, the difference between LLaVA-1.5-L and LLaVA-1.5 is marginal (0.270 vs 0.265). In order to analyze and consequently understand such an counterintuitive result, PhD enables a zoom-in analysis in mode-oriented (Sec. 4.3) and task-oriented (Sec. 4.4) styles, unavailable in the previous benchmarks. One more advantage of PhD compared to its predecessors lies in its discrimination ability. The relatively small performance gap between GPT-4o and the top open-source models as measured by POPE and AMBER might lead to an overly optimistic interpretation that the open-source alternatives are catching up with the proprietary model. In fact, a substantial gap remains, as revealed by PhD. Fig. 3 further illustrate the qualitative results, where GPT-4o exhibits fewer hallucinations in its response. This is achieved either with stronger visual encoders, as the cases of LLaVA-OneVision, InternVL-1.5, and Cambrian-1 using SoViT-400m/14, InternViT-6B or hybrid vision structure, or supporting higher image resolutions, see Molmo and MiniGPT4-v2 that accept multiscale or larger input.",
            "score": 0.20846512648171836,
            "section_title": "Using PhD for Overall VHE",
            "char_start_offset": 22800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033843994140625
        },
        {
            "corpus_id": "273638520",
            "title": "Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)",
            "text": "We evaluate 22 open-source VLAs of varying sizes ranging from 1B parameters to 34B parameters. Our methodology is applicable to all VLAs, however it requires access to output probabilities which are generally not available for API-models such as GPT-4V (Achiam et al., 2023) or Gemini (Gemini Team et al., 2023). The VLAs evaluated in this study are taken from different series, namely LLaVA-1.5 (Liu et al., 2024a), LLaVA-1.6 (Liu et al., 2024b), MobileVLM-V2 (Chu et al., 2024), Bunny (He et al., 2024), Phi-3.5-Vision-Instruct (Abdin et al., 2024), Qwen-VL-Chat (Bai et al., 2023), and InternVL2 (Chen et al., 2024a;b). Here, LLaVA-1.5 also includes BakLLaVA (SkunkworksAI, 2023) and LLaVA-RLHF (Sun et al., 2024). The full model list and details are in Appendix C.",
            "score": 0.20656260968331347,
            "section_title": "EXPERIMENTAL EVALUATION",
            "char_start_offset": 15775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 768
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 698,
                    "end": 716,
                    "matchedPaperCorpusId": "262824780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044097900390625
        },
        {
            "corpus_id": "276903707",
            "title": "Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
            "text": "Table 4. LoFi results on LLaVA-NEXT [21]. We evaluate LoFi using \u03b2 = 4 as a representative configuration. Compared to its application on LLaVA-1.5, LoFi applied to LLaVA-NEXT achieves significantly greater computational cost reductions while maintaining high accuracy and consistency. \n\nacross GQA, VQAv2, and TextVQA at comparable or lower computational costs (TFLOPs). For instance, on GQA, dynamic pruning attains 87.3% consistency and 60.8% accuracy, compared to fixed ratios that reach at most 85.9% consistency and 60.1% accuracy. Similar trends are observed in VQAv2 and TextVQA. These results confirm that dynamically adjusting the pruning ratio effectively preserves both accuracy and consistency while maintaining computational efficiency.",
            "score": 0.20523968676524876,
            "section_title": "GQA VQAv2 TextVQA",
            "char_start_offset": 25807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 749
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "274306239",
            "title": "Cross-modal Information Flow in Multimodal Large Language Models",
            "text": "We investigate the current state-of-the-art and open-source multimodal large language models from the LLaVA series: LLaVA-1.5-7b, LLaVA-1.5-13b [27], \n\nLLaVA-v1.6-Vicuna-7b [28] and Llama3-LLaVA-NEXT-8b [2], which achieve state-of-the-art performance across a diverse range of 11 tasks including GQA. These models are trained on similar publicly available data but with different architectures and model sizes, which allows us to explore cross-modal interaction and processing over different architectures and minimize interference of unknown factors from training data. All these models have the same image encoder (CLIP-ViT-L-336px [35]) but with different LLM: Vicuna-v1.5-7b [47] with 32 layers (transformer blocks) in LLaVA-1.5-7b and LLaVA-v1.6-Vicuna-7b, Vicuna-v1.5-13b [47] with 40 layers in LLaVA-1.5-13b and Llama3-8b [14] with 32 layers in Llama3-LLaVA-NEXT-8b, where Vicuna-v1.5 is the standard and dense transformer architecture [41] and Llama3 adopts grouped query attention [4]. \n\nIn terms of image processing, LLaVA-1.5-7b and LLaVA-1.5-13b directly feed the original fixed-length image patch features from the image encoder into the LLM as input tokens. In contrast, LLaVA-v1.6-Vicuna-7b and Llama3-LLaVA-NEXT-8b employ a dynamic high-resolution technique, which dynamically adjusts image resolution, resulting in variable-length image patch features with higher resolution. Due to space limitations, we will primarily present the results for the model LLaVA-1.5-13b in the subsequent sections of this paper, while similar findings for other models are presented in Appendix E.",
            "score": 0.20487583663204825,
            "section_title": "Models",
            "char_start_offset": 16408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1595
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276611328125
        },
        {
            "corpus_id": "265499116",
            "title": "Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
            "text": "with GPT-4V and LLaVA-1.5 \n\nIn this section, we give a few samples in HALLUSIONBENCH and share our observations. Each figure is self-contained for readability, where we highlight the control pairs, the responses of GPT-4V and LLaVA-1.5, the failures of those models, and the corresponding part of the answers.",
            "score": 0.20304817140688994,
            "section_title": "A. More Case Analysis on HALLUSIONBENCH",
            "char_start_offset": 25615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 28,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004383087158203125
        },
        {
            "corpus_id": "266933417",
            "title": "Hallucination Benchmark in Medical Visual Question Answering",
            "text": "The model's performance is measured by the classification accuracy of the prediction's token.If the model provides a token other than the given options, the prediction is regarded as wrong and irrelevant (i.e.# irr in Table 1).If the model provides a token in the given options but a wrong answer, then the prediction is regarded as wrong only.We conduct an ablation study for various prompt styles, aiming to rigorously assess the models' performance (Table 2).The ablation study with the largest open source model that we use, LLaVA-v1.5-13Bmodel, confirms the effect of different prompting and shows that L + D0 prompt is the best strategy for hallucination evaluation, which we use for further evaluation (Table 7).\n\nThe evaluation of hallucination for various models shows that the best LLaVA variant model is LLaVA-v1.5-13Bmodel (Table 1).GPT-4-turbo-vision model outperforms LLaVA-v1.5-13Bmodel on average, but LLaVA-v1.5-13Bmodel performs better in FAKE and SWAP scenarios.Also, regarding the number of irrelevant answers, LLaVA-v1.5-13Bperforms better than other models including GPT-4-turbo-vision.This is also confirmed by qualitative analysis of samples of response (Table 6).",
            "score": 0.20298261323886518,
            "section_title": "EXPERIMENT AND RESULTS",
            "char_start_offset": 2978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 719
                },
                {
                    "start": 721,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039794921875
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "Benchmarks. We evaluate LLaVA-AlignedVQ on eight diverse Vision Question Answering (VQA) benchmarks: VQA-v2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for open-ended questions, VizWiz (Gurari et al., 2018) for questions from visually impaired users, TextVQA (Singh et al., 2019) for text-rich queries, POPE (Li et al., 2023b) for model's degree of hallucination on three sampled subsets of COCO (Lin et al., 2014), random, common, andadversarial, MMBench (Liu et al., 2025) for all-round shuffling on multiple choice answers, LLaVA-Wild (Liu et al., 2023) and MM-Vet (Yu et al., 2023) for visual conversations on a diverse range of tasks. POPE reports F1 Score, LLaVA-Wild and MM-Vet utilize the answer score evaluated by GPT-4o (Achiam et al., 2023), and all the other datasets use accuracy. LLaVA-AlignedVQ uses the same codebook to compress intermediate feature for all these benchmarks. \n\nAlternatives for Comparison. We compare the accuracy and latency of LLaVA-AlignedVQ against the following alternative approaches: \n\n\u2022 LLaVA-Ori: Transmits raw images directly to the cloud for processing, with all computations handled on the cloud side. We make two main observations. First, LLaVA-AlignedVQ consistently demonstrates high accuracy, staying within \u22122.23% to +1.6% of the original LLaVA model and \u22120.86% to +1.82% of LLaVA 1+ , achieving seven top two accuracy among the eleven results Although the variants LLaVA-JPEG-90 and LLaVA-JPEG-90 1+ achieve competitive high accuracy, lower than LLaVA-AlignedVQ by approximately \u22120.16% and \u22120.66% on average, they incur a larger transmission overhead as indicated in Table 3, leading to longer inference latency as shown in Section 4.4.",
            "score": 0.2010291106587475,
            "section_title": "Experimental Settings",
            "char_start_offset": 21038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 128,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 137,
                    "end": 161,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 269,
                    "end": 289,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04559326171875
        },
        {
            "corpus_id": "261883964",
            "title": "Excalibur: A new ensemble method based on an optimal combination of aggregation tests for rare-variant association testing for sequencing data",
            "text": "Black: evolution of power given the evolution of cohort size, based on empirical power ID n\u02da6, n\u02da7, n\u02da8 and n\u02da9 (Tables 1 and S5). Red: evolution of power given the evolution of proportion of protective variants, based on empirical power ID n\u02da11, n1 2 and n\u02da13 (Tables 1 and S5). Blue: evolution of power given the evolution of proportion of causal variants, based on empirical power ID n\u02da2, n\u02da3, n\u02da4 and n\u02da5 (Tables 1 and S5). Magenta: evolution of power given the inclusion of only rare variants versus rare and common variants, based on empiric al power ID n\u02da18 and n\u02da10 (Tables 1 and S5). Turquoise: evolution of power given the evolution of causal MAF cutoff, based on empirical power ID n\u02da1 and n\u02da11 causal variants, based on empirical power ID n\u02da2, n\u02da3, n\u02da4 and n\u02da5 (Tables 1 and S5). Kind of variant: evolution of power given the inclusion of only rare variants versus rare and common variants, based on empirical power ID n\u02da18 and n\u02da10 (Tables 1 and S5) and based on type I error ID n\u02da4 and n\u02da5 (Tables 1 and S3). causal MAF cutoff: evolution of power given the evolution of causal MAF cutoff, based on empirical power ID n\u02da1 and n\u02da11 (Tables 1 and S5). Region size: evolution of power given the evolution of region size, based on empirical power ID n\u02da17 and n\u02da16 (Tables 1 and S5) and based on type I error ID n\u02da8, n\u02da3 and n\u02da9 (Tables 1  and S3). For example, Excalibur_baseline (see column Test) has an increase of power of 0.056 when increasing proportion of cases / controls (see column Prop case). This comes from the total_evol column (see S10 Table column total_evol for 1. Proportion case/control for Excali-bur_baseline at alpha 0.05). (XLSX) S13 Table. Computational time. Computational time of our 4 ensemble methods and 59 state-of-the-art tests, based on 18,000 empirical power simulations. The average, minimum and maximum computational time are given in seconds. Best: minimal time needed to perform 20,000 genetic regions based on the minimum computational time, given in hours.",
            "score": 0.20062510354876348,
            "section_title": "Discussion",
            "char_start_offset": 40272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015430450439453125
        },
        {
            "corpus_id": "273638057",
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "text": "Pre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
            "score": 0.20059573278853018,
            "section_title": "Model Fine-tuning",
            "char_start_offset": 11891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "268157336"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1263427734375
        },
        {
            "corpus_id": "276938169",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
            "text": "In this section, we compare our model against state-of-theart video MLLMs on seven different video benchmarks, summarizing the results in Table 3 Table 3. We compare BIMBA with state-of-the-art video MLLMs across seven diverse video question-answering benchmarks. BIMBA-LLaVA achieves the highest performance on all datasets when using the Qwen2-7B LLM backbone (third section). Since different MLLMs leverage varying LLM backbones and training data, we also conduct a fair comparison by evaluating our model against four baselines trained using the same 370K instruction-tuning dataset and using Vicuna-7B and LLaMA3.2-8B LLM decoders (second section). In this evaluation setting, we observe that our model consistently outperforms all baselines across all benchmarks. \n\nrely on varying backbones and training datasets, to ensure a fair comparison, we first fine-tune four competitive models, as well as the two default variants of our BIMBA model on our 370K instruction-tuning dataset. These include PLLaVA [75], LLaVA-NeXT (Video) [44], LLaMA-3.2 [52] (Video) and PLLaVA (LLaMA-3.2), a variant of PLLaVA using the stronger LLaMA-3.2 [52] image MLLM. The results in the second section of the table show that our model achieves better results than all these baselines. In particular, we note that when using the same Vicuna-7B [11] model, BIMBA-LLaVA outperforms both PLLaVA and LLaVA-NeXT (Video) on all benchmarks, achieving an average accuracy of 55.23% compared to 49.42% for LLaVA-NeXT and 50.40% for PLLaVA. Likewise, our BIMBA-LLaMA variant surpasses LLaMA-3.2 (Video) by 3.09% on average and PLLaVA (LLaMA-3.2) by 4.21%.",
            "score": 0.19829426625323637,
            "section_title": "Comparison with the State of the Art",
            "char_start_offset": 24350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1630
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02691650390625
        },
        {
            "corpus_id": "274965690",
            "title": "PolySmart @ TRECVid 2024 Video Captioning (VTT)",
            "text": "When video data is directly input to the model, we observe notable performance differences across the three model variants. Table 1 shows that the Vanilla LLaVA-NeXT-Video (LV-V) generally lags in metrics such as BLEU (BL), METEOR (ME), and CIDEr (CI) compared to the fine-tuned and vanilla LLaVA (LV-FT and LV). This suggests that the video variant may not fully leverage sequential data without additional training. Notably, LV-V achieves a higher SPICE (SP) score, indicating a better understanding of semantic relationships in certain scenarios. However, its performance inconsistency implies limited generalization when processing raw video as input.",
            "score": 0.19759471610845003,
            "section_title": "Video as Input",
            "char_start_offset": 4606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 655
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0792236328125
        },
        {
            "corpus_id": "277272125",
            "title": "On the Perception Bottleneck of VLMs for Chart Understanding",
            "text": "Building on the observed advantages of LLaVAs based on enhanced CLIPs, we aim to further verify whether these enhanced CLIPs can raise the performance ceiling of their corresponding LLaVAs. If we further scale up the taskspecific training data -it is possible that the original CLIP already encodes all the required information and the LLaVA training can extract that knowledge through more instruction tuning data. To this end, we conduct larger-scale chart understanding tuning in the third stage of LLaVA training to mitigate the extraction bottleneck. In this specific ablation experiment, we experiment with individual datasets to avoid potential confounders such as the transfer effect between different datasets. Specifically, we choose DVQA and PlotQA as they offer large training sets suitable to this scaling study. \n\nSpecifically, we conduct training for both CLIP and LLaVA using the DVQA and PlotQA datasets separately, leading to the two specialized models: LLaVA-PlotQA and LLaVA-DVQA. For CLIP training, we utilize a total of 2 million samples from DVQA and 3 million samples from PlotQA. We still incorporate both standard training data and hard negative variants, following the hard negative generation strategy and hyperparameter configuration detailed in Section 3.4. For LLaVA training, we adhere to the three-stage training process using the LLaVA-v1.5-13B model, as outlined in Section 4.1. In the third and final chart-specific tuning stage, we train LLaVA models using 800K samples from each dataset separately, allowing us to systematically investigate the performance ceiling under this setting.",
            "score": 0.19693601092549573,
            "section_title": "Scaling Chart Understanding Tuning",
            "char_start_offset": 16536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1622
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.106689453125
        },
        {
            "corpus_id": "278368242",
            "title": "Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding",
            "text": "We consider LLaVA-v1.5-7B (Liu et al., 2024a) and LLaVA-Med-v1.5-Mistral-7B (Li et al., 2023b) as the multi-modal LLMs for quantifying their uncertainty on VQA and Slake, respectively. Large Language and Vision Assistant (LLaVA) is a vision and language model that connects a vision encoder (CLIP (Radford et al., 2021)) with a language model (LLaMA 2 (Touvron et al., 2023)) to handle image-text queries. LLaVA-Med-v1.5-Mistral-7B is the fine-tuned variant of LLaVA on the medical domain with Mistral-7B (Jiang et al., 2023) as the language model. The fine-tuning is done by curriculum learning on biomedical image-caption pairs from the PubMed Central (Zhang et al., 2023a) dataset.",
            "score": 0.19629817778067288,
            "section_title": "Multi-Modal LLMs",
            "char_start_offset": 18460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 684
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 45,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1602783203125
        },
        {
            "corpus_id": "273403785",
            "title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias",
            "text": "We evaluate the performance and biases of various VLMs across different sizes and architectures, including both open and closed-source models. For the open-source models, we assess the LLaVa v1.6 series and the PaliGemma-2 series, specifically: llava-v1. 6-34b-hf, llava-v1.6-vicuna-7b, llava-v1.6-mistral-7b-hf Liu et al. (2023), google/paligemma2-10b-pt-224, and google/paligemma2-10b-pt-448 Steiner et al. (2024). Additionally, we evaluate two CLIP models, CLIP L-224 and CLIP L-336, with the latter serving as the Vision Transformer (ViT) component for all the LLaVa models mentioned. For the closed-source models, we include Google's May 2024 release: gemini-1.5-flash-001 Radford et al. (2021); Goldin et al. (2024). For reproducibility, we set the temperature to 0 in all cases. More details about hardware and API versioning can be found in Appendix A.3",
            "score": 0.19556272466486574,
            "section_title": "Models",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 861
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 699,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021087646484375
        },
        {
            "corpus_id": "272647612",
            "title": "MemeIntent: Benchmarking Intent Description Generation for Memes",
            "text": "However, three over four metrics 12 gave a statistically higher score for Llava in experiments where zero-shot and NoBK input were used. Besides, SelfCheckGPT-NLI gave statistically higher scores for Llava when where AutoBK was used. Therefore, none of these models can entirely outperform the other across settings. \n\n12 except BLEU which did not show statistical significance Learning setup (RQ2). In general, when Llama was used, few-shot was better than zero-shot. Conversely, the opposite happened when Llava was used. The superior performance of few-shot learning in Llama is aligned with the intuition that having demonstrations is useful. Meanwhile, Llava's inferior few-shot performance has been discussed by its authors that Llava was not explicitly trained to take multiple images as input 13 . \n\nThe general trend above does have a few exceptions: SelfCheckGPT gave statistically higher scores for few-shot learning in Llava AutoBK, and zero-shot learning in Llava HumanBK.",
            "score": 0.19519968769707413,
            "section_title": "Automatic Evaluation",
            "char_start_offset": 23510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 985
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01322174072265625
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio. LLaVA-NeXT-Video [52] fine-tunes LLaVA-NeXT on video data, with a variant that applies DPO [34] for improved performance. LITA [11] employs a slow-fast design [9,41] to capture spatial and temporal information more effectively. \n\nTraining-free video LLMs extend image LLMs for video understanding without requiring additional fine-tuning on video data. As a pioneering approach, IG-VLM [13] constructs a grid-view image from video frames, which is then fed directly into a frozen image LLM with a specially designed prompt. While promising, the image grid approach has limitations, such as reduced resolution and the limited number of frames it can include, which we further discuss in the next section. FreeVA [40] explores various temporal aggregation methods, but similarly uses a limited number of frames. The current state-of-the-art SF-LLaVA [45] adopts the slow-fast design, which is proven to be effective in action recognition [9,41], and in LITA, as mentioned earlier. SF-LLaVA designs a slow pathway compressing fewer frames, and a fast pathway heavily compressing more frames. While both SF-LLaVA and our method use a twostream design, our Thumbnail-and-Sampling strategy leads to significantly better performance on various video understanding benchmarks, while maintaining a better token efficiency. We extend the discussion in Sec. 4.",
            "score": 0.1951707073121611,
            "section_title": "Related Works",
            "char_start_offset": 4774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "54463801"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "54463801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "score": 0.1948802059124483,
            "section_title": "Model overview",
            "char_start_offset": 10302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "270562057",
            "title": "AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
            "text": "We benchmark different models with F1 scores and inference time per sample on the POPE dataset with LLaVA-1.5 [45] in Table A9. Experimental results show that our model performs the best with slight inference overhead. We also test AGLA-small, an AGLA variant using a much smaller matching model with 120M parameters, which is more efficient but achieves a competitive F1 score as well.",
            "score": 0.19394530807687277,
            "section_title": "A13. Efficiency",
            "char_start_offset": 29495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0127716064453125
        },
        {
            "corpus_id": "269756882",
            "title": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks",
            "text": "In the same model series, larger versions, evidenced by increased parameters such as in LLaMA-7B vs. LLaMA-13B, LLaVA-v0-7B vs. LLaVA-v0-13B, and LaVIN-7B vs. LaVIN-13B, generally outperform their smaller counterparts across most datasets. It suggests that models with more parameters and extensive training are likely to achieve superior performance. ( 5) Latest models: The updated model iterations, such as the LLaMA, Qwen-VL, mPLUG-OWL, and LLaVA series, demonstrate significant improvements over their predecessors, highlighting how advancements in novel architectures and extended training contribute substantially to enhanced model efficacy. Additionally, the latest models, such as Qwen2.5-VL and MiciCPM-V2.6, showcase exceptional performance. Remarkably, newer models with fewer parameters (e.g., Qwen2.5-VL-7B-Instruct) can match the performance of earlier models with significantly more parameters (e.g., Qwen2-VL-72B-Instruct), underscoring the higher information density of the latest models. ( 6) \n\nVarious instructions: On one hand, performance peaks vary within the same model when applied to different datasets, contingent on the instructions used. On the other hand, across various models, the optimal performance on a specific dataset often requires model-specific instructions. Despite its importance, the area of selecting the most suitable instructions for diverse datasets and models is under-researched and represents a promising avenue for future exploration. (7) More challenge tasks: Closed-source models, such as Gemini-V, significantly outperform opensource models on more challenging datasets, including MOSI-7 and MOSEI-7, which involve multiple classification categories, MNRE with a long-tail effect, and complex reasoning datasets like PuzzleVQA and MMMU. These results highlight considerable room for improvement in opensource models when addressing such tasks. Closed-source models demonstrate superior performance on sophisticated tasks like PuzzleVQA and multimodal relation extraction for several technical reasons.",
            "score": 0.19351630696715943,
            "section_title": "Models",
            "char_start_offset": 46311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2055
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07171630859375
        },
        {
            "corpus_id": "274423466",
            "title": "I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt Generation for Text-Guided Multi-Mask Inpainting",
            "text": "Data Annotation Quality We computed the CLIPSim score between our art dataset images and the grounded descriptions generated by Kosmos-2 to assess the quality Figure 4. Effects of LLaVA sampling temperature on the quality and diversity of prompt generation. For CLIPSim, we report the similarity value scaled by 2.5 to map it into the [0, 1] interval, allowing for easier comparison, as done in [12]. of object identification. We also calculated the similarity between bounding box crops and object-level descriptions generated by LLaVA. The average similarity scores are 0.32 and 0.28, respectively, indicating strong global and local alignment of the generated annotations. This is supported by empirical testing and human inspection while creating datasets like LAION-5B [32], which used a threshold of 0.28 to include image-text pairs. Prompt Generation Quality and Plausibility Table 1 compares our fine-tuning approach for LLaVA, which includes mask coloring and prompt-answer formatting, against the base LLaVA model's visual instructionfollowing abilities. The base model, limited to visual instruction-following, struggles with multi-mask prediction, often defaulting to general image descriptions. Even in a simplified single-mask scenario (using the Brush2Prompt paper's prompt), it inaccurately describes hidden objects, focusing on visible entities, leading to lower scores than our fine-tuned variants. \n\nWe fine-tuned two models: LLaVA-1Mask, trained on single-mask examples, and LLaVA-MultiMask, trained in a multi-mask setting. LLaVA-1Mask performs well in single- mask predictions, but LLaVA-MultiMask surprisingly outperforms it, even in single-mask scenarios (1Pred), suggesting that multi-mask training enhances prompt generation capabilities. Additionally, we evaluated LLaVA-MultiMask in a full multi-mask test, focusing on a single selected mask (LLaVA-MultiMask-LastPred). Despite having access to 18% fewer unmasked regions on average, performance degradation was minimal, underscoring the robustness of the multi-mask model.",
            "score": 0.191148165233391,
            "section_title": "Results and Discussion",
            "char_start_offset": 21438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "233296711"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034698486328125
        },
        {
            "corpus_id": "274141390",
            "title": "Generative Timelines for Instructed Visual Assembly",
            "text": "Semantic Cues Ins. Rem. Repl. Swap Ins. Rem. Repl. Swap Avg. Zero-shot MiniGPT-4 [56] 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Llava-1.5 [8] 12.5 7.5 2.5 0.0 0.0 0.0 0.9 0.0 2.9 GPT-4o 60.0 96. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Llava-1.5 [8] 8.8 7.5 2.5 0.0 0.0 0.0 1. instance at a time. Instructed visual assembly, in contrast, requires managing a large collection of at least 20 images per instruction, making these models unsuitable for the task. However, they were the closest available open-source models to serve as our baselines. \n\nIn the case of MiniGPT-4, when prompted with several images (our collection) and an instructed assembly task, it tends to produce generic captions for some images in the collection. We will showcase examples of these failure cases later in the manuscript. One notable advantage of MiniGPT-4 is the use of Q-former, which reduces the number of tokens needed to represent a single image. This allowed us to utilize this model without major modifications since our entire task fit within the context limit of MiniGPT-4. Conversely, for Llava-1.5 [8], a single image is represented by 576 tokens; with a collection of 20 images, the input sequence increases to approximately 11,520 tokens, far exceeding the original design limitations of LLMs [35]. We attempted to use vanilla Llava-1.5 but encountered significant context limitations that prevented the model from producing any meaningful output. We then modified Llava to reduce the token count per image by trying methods such as averaging all tokens, max pooling, or using the [CLS] token. We determined that using the [CLS] token was most effective for our task. Despite these efforts, Llava-1.5 was still unable to perform assembly tasks adequately.",
            "score": 0.19091169503250027,
            "section_title": "Assembly Accuracy (%) Positional Cues",
            "char_start_offset": 35145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "274437635",
            "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
            "text": "Visualization of LVIS-VQA. Fig. 6 shows the dynamic token reduction process of Dynamic-LLaVA on LVIS-VQA (single-round), while Fig. 7 illustrates the same process in LVIS-VQA (multi-round). In Fig. 6, a user asks a question to Dynamic-LLaVA, and the associated input image is shown, with regions marked in gray indicating the dynamically reduced token patches determined by Dynamic-LLaVA. Drawing from the identified mask and the provided input information, the model produces a",
            "score": 0.19023747501060154,
            "section_title": "A.6 VISUALIZATION",
            "char_start_offset": 40983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 478
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279541015625
        },
        {
            "corpus_id": "229340359",
            "title": "Sub-Linear Memory: How to Make Performers SLiM",
            "text": "The discussed variant (5-9) corresponds to GPT-2. We consider this configuration for simplicity and use it in experiments. However, as we further show, our findings can be easily extended to other modifications.",
            "score": 0.19023747501060154,
            "section_title": "Transformer and Performer Architectures",
            "char_start_offset": 9249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0006694793701171875
        },
        {
            "corpus_id": "273654435",
            "title": "Improving Multimodal Large Language Models Using Continual Learning",
            "text": "We study 9 choices for LLaVA's LLM of varying scales and instruct-tuning: 6 Pythia models (160M -2.8B) [17], Phi2 (3B) [18], and 2 LLaMA 2 (7B) models. Refer to Appendix A.3 for details on the base LLMs.",
            "score": 0.19023747501060154,
            "section_title": "LLM:",
            "char_start_offset": 2762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 203
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0526123046875
        },
        {
            "corpus_id": "268513172",
            "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
            "text": "We evaluate SQ-LLaVA on ten benchmarks, covering a range of academic Visual Question Answering (VQA) tasks and recent instruction tuning tasks designed for large vision language models.The academic VQA includes VQA-v2 [10] and VizWiz [11].GQA [13] is a fine-grained real-world visual reasoning and questionanswering benchmark.ScienceQA [25] is a benchmark with rich subjects (natural science, language science, and social science).TextVQA [39] requires the model to recognize the texts in the image.LLaVA (in the wild) and MM-Vet [53] use GPT4 to assess the capability of the models for testing.With manually designed questions, MM-Bench and MMBench-CN [23] evaluate the model's vision-related reasoning and perception for English and Chinese, respectively.POPE [20] is a benchmark for evaluating object hallucination [34].\n\nIn Table 1, we quantitatively compare between SQ-LLaVA and existing models.SQ-LLaVA-7B and SQ-LLaVA-13B trained on two instruction datasets [5,21] outperform previous methods in six out of ten visual instruction tuning tasks.To be specific, SQ-LLaVA-7B achieves 17.2% improvement over LLaVA-v1.5-7BTable 1: Comparison with state-of-the-art methods on ten benchmarks.After training on the same instruction data, SQ-LLaVA traind on [21] and SQ-LLaVA * trained on [5] surpass their baseline model LLaVA-v1.5 and ShareGPT4V on 9 out of 10 and 6 out of 10 benchmarks in the 7B scale, and 8 out of 10 and 6 out of 10 in the 13B scale.The best results are bold and the second-best results are underlined.on the LLaVA (in the wild) benchmark, indicating the superior capabilities of our model in tasks such as detailed description and complex reasoning.Also, SQ-LLavA-7B improves over previous methods on ScienceQA, indicating that our model excels in understanding and reasoning over scientific content and can effectively handle multi-modal information.",
            "score": 0.19021952133242953,
            "section_title": "Zero-shot Multilingual Capability",
            "char_start_offset": 23087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 823
                },
                {
                    "start": 825,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "260611572"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "52176506"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "265308687"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1255,
                    "end": 1259,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "265308687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.078369140625
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "It is a core aspiration in AI to build general-purpose assistants with Large Multimodal Models (LMM) [67]. LLaVA-OneVision is an open model, continuing to advance the line of research in building large vision-and-language assistant (LLaVA) [83] that can follow diverse instructions to complete a variety of computer vision tasks in the wild. As a cost-efficient recipe, it is typically developed by connecting vision encoders with large language models (LLM) using a simple connection module. \n\nThe first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 [81] significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time. \n\nLLaVA-NeXT provides an extendable and scalable prototype, which facilitates several parallel explorations, reported in the LLaVA-NeXT blog series [82,169,65,64,68]: https://llava-vl.github.io/blog/ \u2022 The Video blog [169] shows that the image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer, due to the design of AnyRes to digest any vision signals as a sequence of images. \u2022 The Stronger blog [65] demonstrates the LLM model scaling succuss of this cost-efficient strategy. By simply scaling up the LLM, it achieves performance comparable to GPT-4V on selected benchmarks. \n\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success.",
            "score": 0.1886942306780868,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2398681640625
        },
        {
            "corpus_id": "250644543",
            "title": "Integrating Fr\u00e9chet distance and AI reveals the evolutionary trajectory and origin of SARS\u2010CoV\u20102",
            "text": "Members with an MAE below the threshold were considered corrected final members for variant 0, whereas those with an MAE above the threshold were filtered out as outliers (see Figure 4C), resulting in final variant 0. The outliers were reintroduced into the sample pool for subsequent UMAP cycles.\n\nThis process was iteratively applied to correct the members of pre-variants 1 to 4, maintaining consistency with the procedure outlined above (see Figure 4B-4C).These processes finally identified 5 variants from 0 to 4.\n\nTo visualize the evolutionary state of these five variants, we plotted a heatmap of these five variants with all 84 feature Frs (Figure 4D).This heatmap showed that variant 0 was close to the reference sequence with few feature alternations (Figure 4D), but variant 2-4 had undergone a series of mutations.Variant 4 had already mutated most of its 84 features (Figure 4D) in 8 week data.This rapid evolution of variants indicated that SARS-CoV-2 rapidly mutated once it adapted to human immunity and that our system was sensitive enough to discriminate variants.This variant diversity emerging within eight weeks also suggests that SARS-CoV-2 might remain in the human community for a long time before being reported.\n\nAfter the first five variants were identified, the weekly window was moved to identify the next group of variants.Following the same algorithm and practice, 34 variants were identified (Figure 4E).These 34 variants and their compositions were used for the analysis of variant evolution.",
            "score": 0.188313807562176,
            "section_title": "Quantitative variant identification",
            "char_start_offset": 22948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 299,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 518
                },
                {
                    "start": 520,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1237
                },
                {
                    "start": 1239,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1525
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0007734298706054688
        },
        {
            "corpus_id": "266361969",
            "title": "VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering",
            "text": "Training Data. To train the VQA model, we construct a training set by using the top-C (e.g. C = 5 ) retrieved images {I 1 , ..., I c , ..., I C } for a CIR method (e.g., SPRC), and generated QA pairs x QA . We further introduce an indicator variant y c , where y c = 1 if I c is the target image, else y c = \u22121. Then, the training set for training the VQA method can be represented as {(I c , y c , x QA )} Finetune LLaVA. As shown in Fig. 3 (b), using LLaVA as an example, we froze the backbone model while leveraging the LoRA trainable [28]. LLaVA takes a candidate image I c and a question Q k as the input, and outputs the prediction of the answer A \u2032 k . For finetuning LLaVA, we also predict the probability \n\nTo finetune LLaMA, the training loss is defined by considering all the QA pairs. When I c is the target image (i.e., y c = 1), we require LLaVA to correctly answer all the questions, i.e., p k \u2243 1. In contrast, when I c is not the target image (i.e., y c = \u22121), LLaVA is expected to incorrectly answer at least one question. Thus, the training loss can be written as \n\nIn this manner, we can obtain a VQA model that can also be used to distinguish target images from failure-retrieved images, thereby benefiting the CIR performance.",
            "score": 0.1882840431940035,
            "section_title": "VQA for Boosting CIR",
            "char_start_offset": 13130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03753662109375
        },
        {
            "corpus_id": "268247968",
            "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
            "text": "We selected LLaVA-v1.5 (Liu et al., 2023a) as a representative VLM due to its superior capabilities and its open-sourced training code. LLaVA-v1.5-7B and LLaVA-v1.5-13B are both evaluated.",
            "score": 0.18815710551034956,
            "section_title": "Target Models",
            "char_start_offset": 9835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037811279296875
        },
        {
            "corpus_id": "267782659",
            "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models",
            "text": "Comparison among TinyLLaVA Variants. Here, we thoroughly compare various variants of TinyLLaVA models (See more details in Table A1 of Appendix). For reference, we name the variants under three design axes: training recipe, vision encoder, and language model, and format their names as TinyLLaVA-{recipe name}-{vision encoder}-{language model}. For instance, TinyLLaVA-base-C-TL is interpreted as trained under the base recipe, with CLIP and TinyLlama as backbones. We find that smaller TinyLLaVA variants can achieve results comparable to their larger counterparts when using appropriate combinations of data and training recipes, as shown in Figure 9. \n\nComparison with other LMMs. Finally, we compare our TinyLLaVA variants to the state-of-the-art LMMs as shown in Table 3. Our TinyLLaVA-share-Sig-Phi with 3.1B parameters achieves comprehensive superiority over LLaVA-1.5 [37] with 7B parameters. It is worth noting that TinyLLaVA-share-Sig-Phi achieves comparable results to MoE-LLaVA [35] on VQAv2 [17] with fewer parameters and outperforms it in terms of POPE accuracy [55]. These findings highlight the promising potential of thorough explorations into the design space of LMMs. \n\nSummary. In this part, we observe that smaller model vari-  ants can achieve results comparable to their larger counterparts when using appropriate combinations of data and training recipes. Meanwhile, our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL.",
            "score": 0.18703682781130299,
            "section_title": "Overall Comparison among Different LMMs.",
            "char_start_offset": 19192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1516
                }
            ],
            "ref_mentions": [
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "258740697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "269157480",
            "title": "Self-Supervised Visual Preference Alignment",
            "text": "and the improved model (SeVa-7B), respectively. As shown in Fig. 6, Our SeVa demonstrates superiority over LLaVA in various aspects: stronger OCR ability, where SeVa could recognize the exact number of the gas price; less hallucinations can be observed in the second picture, where SeVa accurately comprehends the process of 'chocolate cake recipe' and produces the correct answers. It is also surprising that SeVa could potentially recovers more world knowledge after DPO training, as it gives detailed and accurately response to the introduction of a movie (the third picture). Finally, we found that through our DPO training, the models provide more detailed and helpful answers, as suggested by the last picture of a meal ordering. Following Vicuna [6], we also conduct a competition game between SeVa and LLaVA-1.5 under 7B and 13B settings on LLaVA W and MMVet, respectively. We introduce GPT-4 as judge to evaluate the score of each model's responses (similar to the evaluation process in these two",
            "score": 0.1868093331154565,
            "section_title": "Question vg:",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034942626953125
        },
        {
            "corpus_id": "273025917",
            "title": "Toward a Holistic Evaluation of Robustness in CLIP Models",
            "text": "Modern large multimodal models (LLMs), such as LLaVA [18], typically use a frozen pre-trained visual encoder from CLIP as their visual backbone, with instruction fine-tuning applied to the linear projector and the language model components. This raises an important question: how does the interaction between a shared visual encoder and distinct language models affect the classification performance of LLaVA compared to CLIP-like models? \n\nDriven by this, we compare the classification accuracy of CLIP and LLaVA to investigate how the interaction between the shared visual encoder and their distinct language models influences overall performance. In this section, \"LLaVA\" and \"CLIP\" refer to their training paradigms rather than specific model implementations. We also include SigLIP [46] as another representative of CLIP-like models. \n\nOur evaluation is conducted on three splits of the ImageNet-D dataset [19]: Background, Texture, and Material. This dataset, generated by a text-to-image diffusion model, poses significant classification challenges. We adopt a VQAstyle approach for LLaVA's classification, providing it with Observations: We report the results on ImageNet-D in Table III and summarize the observations as follows. First, extending the findings of [19], which uses CLIP (ViT/14) solely as a category selection model, we observe that the interactions between the language and vision encoders in selection networks can vary significantly. When the category list is easy for CLIP, LLaVA models with the same visual encoder do not improve classification. However, when CLIP struggles with the category list, LLaVA with the same visual encoder offers classification benefits. For example, in row 1, when the most confused categories of ResNet-50 are easy for CLIP, LLaVA models with the same visual encoder show no improvement. Similarly, in row 2, when SigLIP-SO-14 performs well at classification, LLaVA models exhibit a performance drop. However, in row 2, when the category list is challenging for CLIP (SigLIP-SO-14), LLaVA provides over a 20% improvement across three splits.",
            "score": 0.18635093353247117,
            "section_title": "IX. VISUAL AND LANGUAGE ENCODER INTERACTION: A CLASSIFICATION PERSPECTIVE",
            "char_start_offset": 43551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "268724151"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "268724151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037811279296875
        },
        {
            "corpus_id": "267751035",
            "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
            "text": "\u2022 LLaVA-1.5 (Liu et al., 2023a) is an endto-end LMM extended from Vicuna (Chiang et al., 2023), augmented with vision encoder.",
            "score": 0.18609192118893153,
            "section_title": "F Large Multimodal Models",
            "char_start_offset": 30477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015838623046875
        },
        {
            "corpus_id": "270924146",
            "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models",
            "text": "Model group 1  models are: LLama3.1-ins-8b, LLama3-ins-8b, LLama2-7b-chat (Touvron et al., 2023), Chatglm3-6b, Chatglm2-32k-7b(Du et al., 2021), Vicuna-v1.5-7b, Vicuna-v1.5-16k-7b(Chiang et al., 2023) and Qwen2-7b-ins (Bai et al., 2023). More details about these models can be found in the Appendix B.",
            "score": 0.18609192118893153,
            "section_title": "Graph semantic reasoning",
            "char_start_offset": 16122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 301
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 95,
                    "matchedPaperCorpusId": "258740923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004451751708984375
        },
        {
            "corpus_id": "278714870",
            "title": "Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models",
            "text": "To demonstrate the generalizability of the proposed CICD as a broadly applicable plug-and-play module, we evaluate its performance across six different LVLMs spanning three model families: Instruct-BLIP (Dai et al., 2023), LLaVA-1.5 (Liu et al., 2024c), LLaVA-Next (Liu et al., 2024d), Qwen-VL-Chat (Bai et al., 2023b), Qwen2-VL-Instruct (Wang et al., 2024b), and Qwen2.5-VL-Instruct (Bai et al., 2025). InstructBLIP and LLaVA-1.5 are both built upon Vicuna-7B (Chiang et al., 2023) as their language backbone, while LLaVA-Next is based on LLaMA3. The Qwen-VL series, including Qwen2-VL-Instruct and Qwen2.5-VL-Instruct, is developed using the QwenLM framework. As for vision encoders, InstructBLIP, LLaVA-1.5, LLaVA-Next, and Qwen-VL-Chat utilize the pretrained CLIP model (Radford et al., 2021), whereas Qwen2-VL-Instruct and Qwen2.5-VL-Instruct adopt a fine-tuned ViT (Dosovitskiy et al., 2021). All models are implemented at the 8B parameter scale.",
            "score": 0.18561794991785363,
            "section_title": "B.2 Evaluated LVLMs",
            "char_start_offset": 30851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 952
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 774,
                    "end": 796,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 871,
                    "end": 897,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0810546875
        },
        {
            "corpus_id": "270440975",
            "title": "Yo'LLaVA: Your Personalized Language and Vision Assistant",
            "text": "Training.Unless stated otherwise, we use 5 images and k = 16 tokens to learn the subject.Each conversation is single-turn (one question and one answer).We use AdamW [38] with a 0.001 learning rate and LLaVA-1.5-13B[10] as the base model.Training images include \u223c200 negative images per subject (\u223c100 hard negatives from retrieval and \u223c100 easy negatives randomly sampled).We train <sks> is a male Asian model with white hair.\n\n<sks> is wearing a black jacket with a skeleton design on the front.\n\n<sks> is a fashionable individual with short, styled, platinum blonde hair, often seen in modern, stylish outfits.Table 4: Example generated descriptions for a subject, which can be used in place of the image as references (In this case, a person).each subject for up to 15 epochs, saving the best checkpoint based on recognition accuracy on the train set.All experiments are conducted on a single A6000 GPU.\n\nDataset.We collect a new dataset of 40 subjects: Person (10), Pets (5), Landmarks (5), Objects (15), and Fiction Characters (5).The dataset is divided into train and test splits.The number of images per subject varies from 10-20 images.Please refer to Appendix C for more details about our dataset.\n\nBaselines.We choose Vanilla LLaVA [2] as our main baseline.We consider two main variants of LLaVA: Naive LLaVA, which is simply LLaVA itself without any personalized information; and LLaVA + Personalized Description, where LLaVA is assisted with personalized descriptions about the subject.We employ two methods to acquire personalized descriptions: (1) Human-written: We manually write a description for each subject (see Table 4, \"Human\"), mimicking a real scenario where a user describes a personalized subject to LMMs.(2) Automated description: We first prompt LLaVA to generate captions for all training images of this subject.",
            "score": 0.1855584317026384,
            "section_title": "Experimental Setup",
            "char_start_offset": 20095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 9,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 425
                },
                {
                    "start": 427,
                    "end": 495
                },
                {
                    "start": 497,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1205
                },
                {
                    "start": 1207,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "276580647",
            "title": "Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models",
            "text": "Vision-Language Models take in images and text from human, then generate text as output. There are multiple ways to create a VLM, LLaVA [28] is one of them. Their technique is as follows: visual inputs,   are encoded using a vision encoder trained using contrastive learning, for example CLIP [36], to   . A multi-layer perceptron,  , is then used to project   into embeddings   , that are of the same dimension as the language model's token embeddings. These embeddings are then inserted into the input embedding sequence   . The result of this process is used as the input to a language model, such as LLaMA [49]. Instruction-tuned vision-language models are used in multiple tasks for their ability to understand and use images to generate a response following a human instruction. This paper experiments with two instruction-tuned VLMs: LLaVA [28] and Qwen2-VL [51]. \n\nThe LLaVA series of models are one of the pioneers in the VLM scene. We fine-tuned the LLaVA-NeXT-Mistral-7B variant as one of our baselines. It uses CLIP ViT-L/14 [36] as its vision encoder, and Mistral-7B-Instruct-v0.2 [22] as the language model. Qwen2-VL [51] is one of the best small (below 10 billion parameters) VLMs at the time of writing. We also fine-tuned it as a stronger baseline. It uses a 675M ViT with Multimodal Rotary Position Embedding (M-RoPE) as the vision encoder, and the Qwen2-7B-Instruct [54] as the language model.",
            "score": 0.18551041771951812,
            "section_title": "Vision-Language Models",
            "char_start_offset": 6682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1412
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0618896484375
        },
        {
            "corpus_id": "273026182",
            "title": "Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities",
            "text": "a. Semantic Entropy for VQ Answering (lower is better) b. Semantic Entropy for VQ Reasoning (lower is better)   For PaliGemma, adding image and context information significantly reduces uncertainty in VQ answering, making the model much more confident in its predictions. It seems that providing additional context, regardless of its content, leads the model to be more self-assured. This intriguing pattern shows large overconfidence in PaliGemma, which does not always have to be beneficial as it can, e.g., lead to silent failures, where the model is extremely confident in its wrong predictions (Bender et al., 2021;Jaeger et al., 2023). \n\nFor all LLaVA models, we observe overall an inverse relationship between answer uncertainty and reasoning uncertainty, with LLaVA 1.5 exhibiting the highest uncertainty in VQ answering but the lowest in VQ reasoning. \n\nWhen the image is added, LLaVA-Vicuna and LLaVA-NeXT show reduced uncertainty in VQ answering but increased uncertainty in VQ reasoning, as the models, in the question-only configuration, only acknowledge the absence of the image and therefore reason with high confidence. Complementary context slightly decreases model uncertainty, indicating a marginal increase in confidence for both VQ answering and reasoning. This effect is minor though, as shown in Figure 3 b. where all LLaVA models exhibit nearly identical semantic entropies for I+Q and Q+I+C+, as well as for Q+I A and Q+I A +C+. Contradictory contextual information, on the other hand, significantly increases uncertainty in the model answers. Its effect on reasoning is also particularly pronounced in LLaVA 1.5 but remains relatively minor for LLaVA-Vicuna and LLaVA-NeXT. Thus, the LLaVA models appear to be slightly influenced by reinforcing information sources but are more easily unsettled by contradictory ones. \n\nModel Failure Detection through Semantic Entropy Interpretating VLMs' behavior through the lense of model uncertainty is crucial for identifying and understanding failures, including hallucinations and silent failures. Specifically, for PaliGemma, silent failures cannot be dismissed due to the model's extreme overconfidence, despite its prediction accuracy being comparable to that of LLaVA models.",
            "score": 0.1853261034629924,
            "section_title": "MODEL UNCERTAINTY",
            "char_start_offset": 22581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 620,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0318603515625
        },
        {
            "corpus_id": "276938169",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
            "text": "\u2022 BIMBA-LLaVA. We implement this variant by applying BIMBA on the image-pretrained MLLM LLaVA-NeXT [44], which uses CLIP [59] vision encoder and Vicuna-7B [11] LLM. \u2022 BIMBA-LLaMA. We implement this variant by applying BIMBA on the image-pretrained MLLM LLaMA-3.2 [52], which uses Meta-CLIP [74] vision encoder and LLaMA-3.2-LLM-8B LLM.",
            "score": 0.1849846477560656,
            "section_title": "Our Model Variants",
            "char_start_offset": 17189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 335
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0169830322265625
        },
        {
            "corpus_id": "275336657",
            "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment",
            "text": "This section provides additional details and analyses on the evolution of concepts due to fine-tuning and their recovery using shift vectors. App. A. discusses the correlation between the concepts recovery and the consistency of their shifts. Finally, App. A.6 covers experiments conducted with the LLaVA-1.5 [30].",
            "score": 0.1848785976207565,
            "section_title": "A. Fine-tuning and evolution of concept representations",
            "char_start_offset": 26562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 314
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "274982255",
            "title": "Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning",
            "text": "In the classification task, all models performed well, with LLaVA-7B and LLaVA-13B achieving 97.72% and 99.47% accuracy, respectively. The Claude-3 series and Gemini-1.5-Pro also demonstrated high accuracy, showcasing their strong symbol classification capabilities. This further indicates that the models' direction reasoning is not hindered by their ability to recognize the images.",
            "score": 0.18476670353457772,
            "section_title": "A. Object Classification",
            "char_start_offset": 9597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0224151611328125
        },
        {
            "corpus_id": "278033639",
            "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona",
            "text": "We trained the Pictor model using both the LLaVA 1.5B and 7B models. For the 1.5B model, we employed LoRA with a rank of 16, an alpha value of 64, a batch size of 16, and a learning rate of 2e-5 over 5 epochs. In the case of the 7B model, LoRA was configured with a rank of 16, an alpha value of 32, a batch size of 16, and a learning rate of 5e-5, also for 3 epochs. Both models employed the Adam optimizer (Kingma and Ba, 2014) with no weight decay and utilized a cosine learning rate schedule with a 3% warmup ratio. All training for the Pictor model was conducted on an NVIDIA A100 GPU. \n\nFor the DST and policy models (T5-small and base variants), we used a batch size of 16, learning rate of 1e-3, and trained them for 10 epochs using the AdamW (Loshchilov and Hutter, 2017) optimizer with no weight decay. These models were trained on an NVIDIA A6000 GPU.",
            "score": 0.1846693395814469,
            "section_title": "D Baseline Training Details",
            "char_start_offset": 33885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 862
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021240234375
        },
        {
            "corpus_id": "269149486",
            "title": "AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception",
            "text": "In this section, we compare the performance of our proposed AesExpert models (three versions) with 16 state-of-the-art MLLMs, including the popular GPT-4V [61] and Gemini Pro Vision [11], as well as 14 state-of-the-art variants with open sources, i.e.LLaVA (LLaMA-2-Chat-7B) [39], LLaVA-1.5-7B(Vicuna-v1.5-7B)[38], LLaVA-1.5-13B(Vicuna-v1.5-13B)[38], ShareGPT4V (Vicuna-v1.5-7B)[5], Q-Instruct (LLaVA-v1.5-7B)[53], mPLUG-Owl2 (LLaMA-2-7B) [64], InstructBLIP (Vicuna-7B) [7], IDEFICS-Instruct (LLaMA-7B) [23], Otter (MPT-7B) [33], Qwen-VL (QWen-7B) [3], GLM (ChatGLM-6B) [9], MiniGPT-v2 (LLaMA-2-Chat-7B) [4], TinyGPT-V (Phi-2) [66] and MiniGPT-4 (Vicuna-7B) [70].Detailed information about these models can be found in [67].The experimental results are listed in Table 3.\n\nFrom Table 3, we can find that the three versions of AesExpert achieve the top three results.Among them, AesExpert based on LLaVA-1.5-13Bachieves the best performance, which is significantly better than the most advanced GPT-4V.For the existing open-sourced models, ShareGPT4V [5] performs best, but lags behind our AesExpert (L-13B) by more than 10% on overall score.",
            "score": 0.1841775274862673,
            "section_title": "Performance Comparison",
            "char_start_offset": 22234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 724
                },
                {
                    "start": 724,
                    "end": 771
                },
                {
                    "start": 773,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.024993896484375
        },
        {
            "corpus_id": "14828196",
            "title": "From Primordial Soup to Platform-Based Competition: Exploring the Emergence of Products, Systems, and Platforms",
            "text": "What we call a variant is analogous to an allele in genetics. While the NK modeling literature typically assumes the existence of only two alleles per \"gene\" or design decision (noting that this is without loss of generality since more complex genes or decisions can be broken down and encoded as binary sequences), we explicitly allow multiple variants for a single type of component. Moreover, in some of our experiments we allow components to differ in the number of variants that are available. These variants arrive stochastically over time, enlarging the product design space in the same way that innovation enlarges the design space of products in the real world. As in the real world, variants are not necessarily functionally equivalent; some may offer higher performance or better compatibility with other components than others. However, we do not consider gradual evolution within a family of variants. The Intel 80386 interacted with other system components in much the same way as its predecessor the 80286, whereas in our model the characteristics of later arrivals are uncorrelated with their predecessors.",
            "score": 0.18409416019529046,
            "section_title": "Literature review: components, products, systems, and platforms",
            "char_start_offset": 10807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0026416778564453125
        },
        {
            "corpus_id": "276421413",
            "title": "MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation",
            "text": "LLaVA-V1.5-7B, LLaVA-V1.5-13B, MiniCPM-8B, LLaVA-Next-0.5B, LLaVA-Next-7B, Qwen2VL-2B, Qwen2VL-7B, LLaVA-OneVision-0.5B, LLaVA-OneVision-7B, VILA1.5-3B, VILA1.5-8B, mplug-Ow3-1B, mplug-Ow3-2B, mplug-Ow3-7B use 16-bit floating-point precision, while Qwen2VL-72B and LLaVA-OneVision-72B use 4-bit quantization. Their output length is limited to a maximum of 256 tokens. The models utilize the default values for Temperature, Top-k Sampling, and Top-p Sampling as specified in their Hugging Face inference code.",
            "score": 0.1840417563898511,
            "section_title": "G Experimental Settings",
            "char_start_offset": 30570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02606201171875
        },
        {
            "corpus_id": "270440492",
            "title": "VLind-Bench: Measuring Language Priors in Large Vision-Language Models",
            "text": "Open-source LVLMs LLaVA-NEXT 72B (Qwen 1.5 72B Chat) [10] 94.4 95.7 76.1 58.6 75.5 46.7 LLaVA-NEXT 34B (Nous Hermes 2 Yi 34B) [10] 80.5 85.8 61.7 61.1 67.2 44.5 LLaVA-1.5 13B (Vicuna v1.5 13B) [13] 59.9 92.1 40.9 42.0 31.5 20.9 LLaVA-1.5 7B (Vicuna v1.5 7B) [13] 0.0 0.0 --0.0 0.0 + RLAIF-V [29] 17.9 8.3 48.1 25.0 54.3 35.7 InstructBLIP 13B [5] 66.6 79.5 54.2 57.8 46.7 28.0 InstructBLIP 7B [5] 58.6 73.5 28.2 14.6 27.2 21.0 OmniLMM 12B (Zephyr 7B \u03b2) [28] 88.1 97.7 78.6 81.4 79.5 66.4 MiniCPM-V-2 2.8B [28] 76.2 98.3 56.5 68.1 49.0 34.1 \n\nBackbone LLMs Qwen 1.5 72B Chat [2] 75.8 -69.9 -74.2 -Nous Hermes 2 Yi 34B [16] 83.1 -75.3 -77.8 -Vicuna v1.5 13B [22] 57.9 -80.0 -69.2 -Vicuna v1.5 7B [22] 0.0 ---0.0 -Zephyr 7B \u03b2 [23] 62.3 -45.7 -40.7 -",
            "score": 0.1840417563898511,
            "section_title": "Statistics",
            "char_start_offset": 17446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 745
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0119171142578125
        },
        {
            "corpus_id": "268819996",
            "title": "Uncovering Bias in Large Vision-Language Models with Counterfactuals",
            "text": "Pretend you are the person shown in the image.Please describe yourself using as much detail as possible, including your thoughts and emotions in this moment.Keywords I am studying how different groups are perceived by Americans.I'm not interested in your personal beliefs, but in how you think they are viewed by others.What are the 5 keywords that describe the characteristics of people like the person in this image?\n\nTable 3.Text prompts used in this study.BakLLaVA exhibits the greatest Identity Attack and Flirtation scores.BakLLaVA generally has the highest Identity Attack scores across models, although Instruct-BLIP often matches or approaches similar levels.Values of this score are also significantly higher for the keywords prompt than other evaluated prompts.For Flirtation, Bak-LLaVA consistently exhibits the highest values across models when presented with the pretend prompt.\n\nModel size has little effect on toxicity.We evaluate one model (LLaVA) at multiple model sizes (7b and 13b).The distribution of Toxicity, Insult, Identity Attack, and Flirtation scores for these two models are generally very similar across prompts.In the physical-gender and physicalrace evaluation settings, LLaVA-13b produces lower toxicity and insult scores for the keywords prompt; however, there are negligible differences in these scores between LLaVA-7b and LLaVA-13b for the race-gender evaluation setting.\n\nBase LLM has a relatively small impact on observed toxicity.Two of our evaluated LVLMs (LLaVA-7b and InstructBLIP) were derived from the same base LLM (Vicuna-7b).Despite this similarity, these two models often diverge significantly in our toxicity evaluations, with InstructBLIP exhibiting much higher scores than LLaVA-7b (e.g., over 3x higher median Toxicity for physical-race attributes and the keywords prompt).This suggests that other differences in model architecture and/or training have a larger influence on their production of toxic content than the choice of base LLM.",
            "score": 0.18233034339943438,
            "section_title": "Emotions",
            "char_start_offset": 17393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 46,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 418
                },
                {
                    "start": 420,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 772
                },
                {
                    "start": 772,
                    "end": 892
                },
                {
                    "start": 894,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1408
                },
                {
                    "start": 1410,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0290374755859375
        },
        {
            "corpus_id": "265609270",
            "title": "HallE-Control: Controlling Object Hallucination in Large Multimodal Models",
            "text": "The LLaVA we used in all experiments is pretrained on on LCS-558k which is subset of LAION (Schuhmann et al., 2021), CC (Sharma et al., 2018) and SBU (Ordonez et al., 2011) data, and finetuned on Instruct-158K instruction data. We use Vicuna version 1.3 as initialized language decoder and CLIP-Large as vision encoder. The 158K finetuning data consists of detailed caption, complex reasoning, and conversation. \n\nWe employed the RAM detector, specifically the RAM-14M variant, which uses a Swin-Large backbone. In the data generation stage, we focused on the 'detailed caption 23K' file from the LLaVA Instruction set comprising 158K entries. This file was generated using a specific prompt provided by LLaVA repository1 . Our preprocessing involved adding brackets '[]' around objects filtered by the RAM detector to uniquely identify them in the captions. \n\nHallucination control only involves only finetuning a linear layer added before lm head layer. We freeze all other layers and only finetune control layer with 3 epochs. The dataset is generated data with associated \u03b5 value. We have 10K contextual only detailed caption data and 23K parametric joint detailed caption data. \n\nA.8 More Experiments on LLaVA 1.5 \n\nThe experiments on the main text are based on LLaVA 1.3, we provide experiments on LLaVA 1.5 in",
            "score": 0.1822905748842368,
            "section_title": "A.7 Experiment Settings",
            "char_start_offset": 30051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1316
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "14579301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019989013671875
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "A straightforward approach of using VQ for feature compression is to compress the intermediate features between adjacent transformer blocks. However, this naive approach results in a severe drop in accuracy. In Figure 2c, we present the accuracy results on the VQA-v2 dataset (Goyal et al., 2017) using LLaVA with Vanilla VQ applied after different transformer blocks. Compared to the accuracy 79.13% of the original LLaVA model (i.e, the pink dashed line), applying VQ results in an accuracy drop of 10.51% to 37.94% across different blocks (i.e., the blue line). What's worse, the earlier the partitioning point in the model, the greater the accuracy drop. \n\nVQ variants with less quantization errors cannot mitigate the accuracy gaps. We also explored other VQ variants, such as Residual VQ and Grouped VQ introduced in Section 2.1, to mitigate this accuracy loss as they reduce quantization errors. As shown in Figures 2a and 2b, we experimented with different numbers of codebooks for Residual VQ and various number of groups for Grouped VQ. While these variants improved accuracy from the range of 41.19% \u2212 68.62% to 72.65% \u2212 73.75% with more codebooks, and to 71.15% \u2212 73.34% with more groups, a significant gap of 5.38% \u2212 7.98% still remains between the VQ-applied models and the original model. Moreover, these VQ variants come at the cost of increased compressed feature size, thereby more transmission overhead. Specifically, with n codebooks and g group, the compressed feature size is enlarged by n \u00d7 g. \n\nWe further experimented with a combination of Residual VQ and Grouped VQ, shown as the green line in Figure 2c. When using 3 codebooks and 8 groups, accuracy was consistently improved across blocks, resulting in an accuracy drop of 2.57% \u2212 5.68% compared to the original model. LLaVA-Ori: the baseline, i.e., the original LLaVA accuracy. VQ np gq@blockX: the accuracy when using VQ with p codebook and q group after the Xth transformer block.",
            "score": 0.18222276708786708,
            "section_title": "BACKGROUND AND MOTIVATIONS",
            "char_start_offset": 9749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 296,
                    "matchedPaperCorpusId": "8081284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "239999732",
            "title": "Data-driven analysis of amino acid change dynamics timely reveals SARS-CoV-2 variant emergence",
            "text": "that will eventually characterize a variant typically show an exponential-like temporal growth 29 ; one meaningful example is shown in Supplementary Fig. S1. \n\nWe harnessed the peculiarity of these temporal trends to scout emerging variants. To that end, we applied standard time-series clustering techniques to group together changes showing similar prevalence behavior over a one-month-long period. We regarded as warnings of possible variant emergence those clusters characterized by (1) a positive trend in the prevalence time-series of the constituting amino acid changes, and (2) being sufficiently different from clusters that caused previous warnings (to avoid extracting twice a set describing the same candidate variant). To assess whether these clusters successfully match known variants (named 'hit'), we tested their similarity against the characterizing changes of the most widespread SARS-CoV-2 lineages, as defined by Pangolin, collected in the 'lineage dictionary' (see Supplementary Tables S1 and S2). The dictionary contains all Pangolin lineages that, at the calculation date, have appeared in at least 5000 sequences in the full dataset, with the exception of a few variants of interest highlighted by the news, for which we allowed a lower threshold of 1000 sequences. We also used community analysis to assess whether and how the emergence of new variants can lead to a reorganization in the dynamics of amino acid changes, as observed through their co-occurrence within clusters. \n\nOur approach must not be considered an alternative to phylogenetic analysis, which takes into account the entire evolutionary history of the viral genome. Indeed, the phylogenetic approach works by accumulating individual sequences along the tree of the species, which is built incrementally. When a branch is particularly rich of sequences, it becomes a candidate variant to be investigated (see Supplementary Fig. S2). Our approach, instead, is purely big data-driven; single sequences are not considered (and are not even known). We study aggregate counts of sequences and perform a posteriori analysis (as hinted in Supplementary Fig. S3). Other methods have previously complemented phylogenesis, namely by describing typical SARS-CoV-2 mutational profiles across different countries and regions 30,31 , proposing statistical indicators for location-based mutation evolution 32 , and observing changes that become recurrently prevalent in different locations, thus suggesting selective advantages 2 .",
            "score": 0.18221044760039842,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2510
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 97,
                    "matchedPaperCorpusId": "235393839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001495361328125
        },
        {
            "corpus_id": "270923718",
            "title": "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts",
            "text": "We evaluate: Moondream2-1.6b (Moondream, 2024) based on Phi-1.5 (Li et al., 2023c), LLaVA-1.5 (Liu et al., 2023a) with Vicuna-7b (Peng et al., 2023) as the LLM backbone, LLaVA-1.6 (LLaVA-Next) (Liu et al., 2024), an improvement over LLaVA-1.5 with higher image resolution and better visual reasoning, uses Mistral-7b (Jiang et al., 2023), PaliGemma-3b (Google, 2024), based on open components from the SigLip (Zhai et al., 2023) image encoder and the Gemma (Team et al., 2024) language model, Mantis-bakllava-7b (Jiang et al., 2024b) fine-tuned from BakLLaVA (SkunkworksAI, 2023) and derived from LLaVA but using Mistral-7b (Jiang et al., 2023), Mantis-Idefics2-8b (Jiang et al., 2024b), the current stateof-the-art Mantis variant, based on Idefics2-8b (Lauren\u00e7on et al., 2024).",
            "score": 0.18209778217503145,
            "section_title": "A Evaluated Models",
            "char_start_offset": 25793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 778
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 428,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "271902811",
            "title": "\u201cImage, Tell me your story!\u201d Predicting the original meta-context of visual misinformation",
            "text": "Figure 20 reports the results of the qualitative analysis of multimodal zero-shot Llava for a random sample of 100 test images. Llava makes less correct predictions than GPT4, 13.6% against 20.4%, but more partially correct predictions, 18.6% against 11.6%. There are two major differences with GPT4. Firstly, there is no content filter in place. Secondly, Llava does not often abstain from answering. Hence, more errors are due to wrong answers based on text and image content than with GPT4.",
            "score": 0.18200644785743067,
            "section_title": "L Qualitative analysis of multimodal zero-shot Llava",
            "char_start_offset": 34904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 493
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007755279541015625
        },
        {
            "corpus_id": "277468152",
            "title": "CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation",
            "text": "Table 9 lists all the models evaluated with their corresponding generation configs. (Bai et al., 2025) 20480 20480 0.0 QVQ-72B-Preview (Team, 2024) 100000 100000 0.0 llava-onevision-72b-ov-chat (Li et al., 2024a) 20480 20480 0.0 gemma-3-27b-it (Team et al., 2025) 8192 8192 0.0 Aria (Li et al., 2024b) 20480 20480 0.0 MiniCPM-V-2 6 (Yao et al., 2024) 20480 20480 0.0 Qwen2.5-VL-3B-Instruct (Bai et al., 2025) 20480 20480 0.0 Llama-3.1-405B-Instruct (Dubey et al., 2024) 100000 100000 0.0 DeepSeek-R1 (Guo et al., 2025) 100000 100000 0.6 DeepSeek-V3 (Liu et al., 2024) 100000 100000 0.0 R1-Distill-Llama-70B (Guo et al., 2025) 100000 100000 0.6 Llama-3.3-70B-Instruct (Dubey et al., 2024) 20480 20480 0.0 QwQ-32B (Team, 2025)",
            "score": 0.18200644785743067,
            "section_title": "B.3 Generation Configuration",
            "char_start_offset": 25325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0005908012390136719
        },
        {
            "corpus_id": "269148679",
            "title": "MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting",
            "text": "We conducted a variety of experiments with both text and multimodal LLMs to gauge the difference in performance that comes about due to the change in the modality.LLaMA2-7b and Mistral-7b are the current state-of-the-art open-source LLMs for textual input.These models were tested with text-only inputs.We use these LLMs to highlight the difference in the level of performance between fine-tuned models versus using them straight out of the box, aka through zero-shot prompting.For the ablation study, we also experimented with GPT-4, which is the current state-of-the-art model for multimodal question-answering.\n\nLLaVA and LLaVA-1.5 being multimodal were provided with the figures along with the textual input.All the models were trained on A100 GPU and were fine-tuned for 5 epochs with a batch size of 8. Weighted Adam optimizer was utilised and the learning rate was set to 2e-4.\n\nWe also experimented with different LoRA values in the case of the LLaVA-1.5 model.LoRA or Low-Rank Adaptation [30], is a method to represent the weight changes during the training process in lower-ranked matrices.This is especially useful while fine-tuning general-purpose LLMs, as it speeds up the training process.A lower LoRA rank means fewer parameters are learned during the adaptation process, however, it results in a faster training process as well.We tested the 7b (7 billion) and 13b (13 billion) variants of LLaVA which correspond to the number of learning parameters.The different LLaVA configurations also formed the basis of our comparison of the performance of (MI-CoT) Prompting.For fine-tuning, open-source base model checkpoints from huggingface were utilized.",
            "score": 0.18133609914392948,
            "section_title": "Models",
            "char_start_offset": 11268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 884
                },
                {
                    "start": 886,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1665
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041473388671875
        },
        {
            "corpus_id": "271404168",
            "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
            "text": "ChatGPT, Copilot, and Gemini are developed by proprietary companies, whereas LLaVA-NeXT is an open-source multimodal LLM. Notably, at the time of our experiments, LLaVA-NeXT has achieved state-of-the-art results across various multimodal LLM benchmarks among open-source LLMs. Due to hardware constraints, our experiments were conducted with the 7B parameter variant of LLaVA-NeXT, despite the more advanced state-of-the-art 34B parameter model. Nevertheless, including LLaVA-NeXt in our evaluation covers an important landscape of LLM research. The LLaVA-NeXt model was run on a server with an Nvidia Titan RTX 24GB graphical processing unit. On the other hand, the proprietary models were evaluated via their Application Programming Interfaces (APIs). We have made the program code of the experiments publicly available on the OSF 1 . \n\nDataset: Our study compiled an evaluation dataset from previous research on misleading charts circulated on the internet [24]. The dataset was collected through search engines and social media. The chart images are annotated with the issues identified from the original web page or social media post. A total of 74 unique chart issues were identified in the study, providing a diverse sample of misleading charts that the general public may encounter. \n\nIn the initial phase of our exploratory study, we focused on the five most frequently identified issues: (1) Truncated Axis, (2) 3D Chart, (3) Missing Title, (4) Dual Axis, and (5) Misrepresentation. We subsequently expanded to include the ten most common issues in Experiment Two by including (6) Missing Axis Title, (7) Missing Legend, (8) Inconsistent Tick Intervals, (9) Not Data, and (10) Selective Data. In the third experiment, the study was further broadened to cover up to 21 issues due to a tie for the 20th spot, with both the 20th and 21st issues appearing an equal number of 25 times in the dataset. These additional issues included (11) 21) Inconsistent Binning Size.",
            "score": 0.18133233808881186,
            "section_title": "METHODOLOGY",
            "char_start_offset": 18407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "248266819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045013427734375
        },
        {
            "corpus_id": "270559956",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "text": "To test the impact of reducing the number of tokens in LLaVA-NeXT models, we constraine the grid size, resulting in a maximum token count of 1728 (either 336x672 or 672x336 pixels). As shown in Table 6, reducing the vision token count leads to significant performance drops across all evaluated tasks. For example, the performance of the 13B model on DocVQA decreases from 69.8 to 53.9 when the token count is restricted, with similar trends observed in other variants. This analysis highlights the trade-off between token count and model performance: while reducing tokens can enhance computational efficiency, it may lead to a compromise in accuracy. \n\nIn contrast, ELVA models demonstrate strong performance along with improved efficiency in both speed and memory usage, underlining their robustness in handling high-resolution text-centric tasks efficiently. The ELVA models effectively balance performance and efficiency, outperforming the LLaVA-NeXT variants with reduced token counts. times that of LLaVA-NeXT, we find this a fair tradeoff for the gains in efficiency and performance. Our approach remains competitive and more resourceefficient compared to several contemporary models. For detailed analysis, please refer to Appendix A. \n\nRegarding memory usage, practical deployment often uses quantization (Dettmers et al., 2023), significantly reducing memory costs. For instance, the LLaVA-NeXT-13B model originally requires two V100 GPUs but can run on a single V100 with quantization, albeit with more latency and some performance loss. Despite these drawbacks, quantization shows promise and will likely improve. Our ELVA models, built for efficiency, complement these advancements, promising even greater value when combined with quantization techniques.",
            "score": 0.1812472676762138,
            "section_title": "Ablations with LLaVA-NeXT Variants",
            "char_start_offset": 22947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1769
                }
            ],
            "ref_mentions": [
                {
                    "start": 1315,
                    "end": 1338,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044769287109375
        },
        {
            "corpus_id": "278129311",
            "title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization",
            "text": "To gain a deeper understanding of the evaluator model, we conduct a series of variant experiments. The key findings, as presented in Tab. 5, are summarized as follows: \n\n\u2022 We compare UV-CoT with its self-evaluated variant, where the evaluator model is the same as the target model (initialed with LLaVA-1.5-7B). Although the selfevaluated version exhibits a performance decrease of 3.2% compared to the original UV-CoT, it still outperforms LLaVA-1.5-7B (+4.8% on average) across all evaluated datasets and achieves performance comparable to the larger OmniLMM-12B model (-0.2% on average). This demonstrates that UV-CoT maintains robust performance even under self-evaluation, highlighting its efficiency without requiring larger model scales. \u2022 For OmniLMM-12B and LLaVA-1.5-7B, we incorporate a CoT process using bounding boxes generated by UV-CoT. The CoT-enhanced versions significantly outperform their original counterparts, achieving average performance gains of 4.7% for LLaVA-1.5-7B and 7.4% for OmniLMM-12B across datasets. Remarkably, these models were not fine-tuned for the CoT process, indicating that the bounding box information alone substantially",
            "score": 0.18077971336756438,
            "section_title": "Insight of Evaluator Model",
            "char_start_offset": 20655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1165
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0521240234375
        },
        {
            "corpus_id": "273901322",
            "title": "SCITUNE: Aligning Large Language Models with Human-Curated Scientific Multimodal Instructions",
            "text": "We use the LLaVA codebase (Liu et al., 2023) for multimodal adapter training with SciTune instructions. We train the model for 1 epoch with 128 batch size with a 0.002 learning rate and 2048 context length. LLaMA-SciTune 7B and 13B model variants took 6.5 and 11.2 hours to train, respectively with 8 x A100 GPUs. We use Pytorch Fully Sharded Data Parallel (FSDP) to recursively wrap the language models decoder layers in the taskspecific instruction finetuning stage. We finetuned the models for 12 epochs with the ScienceQA training example to make a fair comparison with LLaVA.",
            "score": 0.18061574652368273,
            "section_title": "B Training Details",
            "char_start_offset": 25544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04681396484375
        },
        {
            "corpus_id": "276482570",
            "title": "Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing",
            "text": "The Llamba models were distilled using MOHAWK [Bick et al., 2024] from Meta's Llama-3.x family [Touvron et al., 2023]. Specifically, Llamba-3.1-1B was distilled from Llama-3.2-1B-Instruct, Llamba-3B from Llama-3.2-3B-Instruct, and Llamba-8B from Llama-3.1-8B-Instruct.",
            "score": 0.1799858346934754,
            "section_title": "Distillation",
            "char_start_offset": 9645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004199981689453125
        },
        {
            "corpus_id": "274437586",
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "text": "LLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
            "score": 0.1799858346934754,
            "section_title": "Model architectures",
            "char_start_offset": 36898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398681640625
        },
        {
            "corpus_id": "269983640",
            "title": "SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge",
            "text": "Training. We implement two versions of the hierarchical filtering model, one using LLaVA-1.5vicuna-7b [37] and the other using Qwen-VL-Chat [27]. In each version, we use same hyperparameters to fine-tune two same LVLMs with LoRA [24] as the website filter and the content filter, respectively. Whether fine-tuning LLaVA-1.5-vicuna-7b or Qwen-VL-Chat, the entire training process is facilitated on two Nvidia A100 GPUs, using a batch size of 128 over 3 epochs. \n\nBaselines. We incorporate 15 representative LVLMs into the proposed framework including Gemini 1.5 Pro [20], GPT-4V [19], GPT-4o, InternVL-1.5 [43], LLaVA-1.6 [23], LLaVA-1.5 [37] XCom-poser2 [26], Monkey [25], CogVLM [28], MiniCPM-V2 [33], mPLUG-Owl2 [29], Qwen-VL [27], MMAlaya [32], Xtuner [30] and VisualGLM [31]. We implement Gemini 1.5 Pro, GPT-4V and GPT-4o via their official webs and APIs. We implement other LVLMs based on VLMEvalKit [34]. \n\nEvaluation. We evaluate LVLMs on four datasets including GQA [44], InfoSeek [45], A-OKVQA [46] and the proposed UDK-VQA. The reason behind selecting GQA, InfoSeek, and A-OKVQA is to evaluate the generalization capability of our framework across datasets that do not necessitate up-to-date knowledge. In addition to evaluating LVLMs via VLMEvalKit, we design additional matching patterns for each LVLM with respect to its answer format. For example, we additionally use the pattern \"The answer is XXX.\" for XComposer2 as it often answers in this format.",
            "score": 0.17996746526692659,
            "section_title": "Settings",
            "char_start_offset": 17992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 975,
                    "end": 979,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "257102433"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "249375629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0147857666015625
        },
        {
            "corpus_id": "276938169",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
            "text": "Likewise, our BIMBA-LLaMA variant surpasses LLaMA-3.2 (Video) by 3.09% on average and PLLaVA (LLaMA-3.2) by 4.21%. These results highlight the effectiveness of our selective-scan-based token compression mechanism, which can be seamlessly integrated into existing MLLMs to enhance performance across a wide range of video understanding tasks. \n\nFurthermore, since recent MLLMs [34,47,61,62,66,83] have been trained with significantly larger instructiontuning datasets and stronger LLM backbones, we also trained a variant of BIMBA-LLaVA using approximately 1.6 million video instruction samples and the Qwen2-7B LLM backbone. The results in the third section of Table 3 show that this BIMBA-LLaVA variant achieves state-of-the-art performance on multiple challenging video question-answering benchmarks, including NExT-QA [73], EgoSchema [51], VNBench [85], LongVideoBench [71], and Video-MME [15]. On the EgoSchema benchmark, our model surpasses the previous best method, LongVU [61], by 3.54%, demonstrating its superior ability to comprehend egocentric videos and handle questions that require long context understanding. Similarly, on VNBench, which focuses on needle-in-the-haystack questions, our approach outperforms LLaVA-Video [83] by 7.11%, highlighting its strong capability to extract key information from very long videos. Additionally, BIMBA-LLaVA (Qwen2-7B) substantially outperforms models based on proprietary LMMs. Specifically, it surpasses VideoTree [69] (GPT-4) by 7.7% on NExT-QA and 10.04% on EgoSchema. Finally, on benchmarks requiring long video comprehension, such as LongVideoBench [71], Video-MME [15], and MLVU [86] our model sets a new state-of-the-art, further demonstrating its effectiveness in processing and understanding hour-long videos.",
            "score": 0.17995589597179037,
            "section_title": "Comparison with the State of the Art",
            "char_start_offset": 25866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "234763093"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "261031047"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "271329356"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "271329356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03759765625
        },
        {
            "corpus_id": "62841719",
            "title": "Network connectivity dynamics affect the evolution of culturally transmitted variants",
            "text": "Additionally, it gives us more information about how relevant agents' choices were in the firsts rounds. Therefore, in some Network connectivity dynamics affect the evolution of culturally transmitted variants 11 analyses we will also present time to convergence (TC) or the number of rounds it takes for the population to reach full convergence (defined as H = 0 bit) for the first time. \n\nWe calculated the adaptiveness (A) of the cultural system at each round (t) as the frequency of high-quality variants (quality is measured by s, see above) at that round, \n\n(Equation 3) \n\nwhere n(t) is the number of high quality variants at round t and N(t) is the total set of variants produced in round t. \n\nA number of researchers have used the notion of relative fitness and change in genotype frequencies in the Wright-Fisher model (Fisher, 1930;Wright, 1931) as an equivalent to Bayesian models, establishing a connection between biological and cultural evolution (Reali, & Griffiths, 2010). Other researchers on cultural evolution have developed models that link demography and cultural adaptiveness, using a variety of mathematical approaches (Mesoudi, 2011, Shennan, 2001). These models describe how a trait changes in frequency over time. For example, the average change in the frequency of cultural variants \u2206\u0305 is given by the Price equation as described in Henrich (2004): 4) \n\nwhere the first term of Equation ( 4) represents change due to selection (selective transmission) and the second term represents noisy interference. In order to make these equations helpful for studying cultural evolution, researchers usually make a series of tractability assumptions depending on the specific conditions of their study, e.g. all agents copy the most skilled agent in Henrich (2004). Following Vaesen (2012) Equation ( 4) can be reduced to: \n\nwhere  \u210e is the frequency of a variant in the subsequent generation or ( + 1), and \u0305 corresponds to the z value of the earlier generation or (). \n\nTwo considerations apply to our study when we calculate average change in high quality variants. First, we use relative fitness equations to account for the adaptiveness of high quality variants, that is to say, we only Network connectivity dynamics affect the evolution of culturally transmitted variants 12 consider the frequency of cultural variants having s = 1.",
            "score": 0.17994730033008458,
            "section_title": "Quantifying convergence and adaptiveness",
            "char_start_offset": 19631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1378
                },
                {
                    "start": 1381,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 855,
                    "matchedPaperCorpusId": "4797951"
                },
                {
                    "start": 961,
                    "end": 987,
                    "matchedPaperCorpusId": "8289211"
                },
                {
                    "start": 1142,
                    "end": 1156,
                    "matchedPaperCorpusId": "12736303"
                },
                {
                    "start": 1156,
                    "end": 1172,
                    "matchedPaperCorpusId": "12999588"
                },
                {
                    "start": 1360,
                    "end": 1374,
                    "matchedPaperCorpusId": "15549778"
                },
                {
                    "start": 1766,
                    "end": 1780,
                    "matchedPaperCorpusId": "15549778"
                },
                {
                    "start": 1792,
                    "end": 1805,
                    "matchedPaperCorpusId": "19892232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016679763793945312
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "We evaluate LLaVA-AlignedVQ using an NVIDIA Jetson AGX Xavier as the edge device and a workstation equipped with an A100 GPU as the cloud server. To simulate various network conditions, we measure the inference latency of LLaVA-AlignedVQ's visual encoder under different transmission bandwidths, ranging from 0.25Mbps to 4Mbps. We compare the inference latency of our edge-cloud collaborative solution, LLaVA-AlignedVQ, with two cloud-only approaches, LLaVA-JPEG-90 and LLaVA-JPEG-10, which transmit JPEG-compressed images. \n\nFigure 6 reports the inference latency, including the edge computation time, the transmission time, and the cloud execution time. For LLaVA-JPEG variants, the edge computation time is ignored as JPEG compression usually takes less than 1ms with proper optimization. Since the large language model is executed on the cloud in all setups, its execution time is excluded from this comparison. Overall, LLaVA-AlignedVQ achieves an inference speedup of 2-15\u00d7 compared to the cloud-only solution using JPEG90compressed images. Against JPEG10-compressed images, our solution still provides a 1.19-2.52\u00d7 inference speedups at poor bandwidth conditions (0.25-1 Mbps). Although JPEG-10 compression is competitive in latency, it compromises accuracy as highlighted in Section 4.2. (Zeghidour et al., 2021) and Grouped VQ (Yang et al., 2023) on the task performance of LLaVA-AlignedVQ. Table 4 shows that increasing the number of codebooks and groups yields no or minimal accuracy improvement but results in increased transmission overhead, which scales proportionally with the number of codebooks and groups. Therefore, adding more codebooks and groups is unnecessary in LLaVA-AlignedVQ.",
            "score": 0.17989461365852405,
            "section_title": "Performance on Inference Latency",
            "char_start_offset": 24813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1702
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0190887451171875
        },
        {
            "corpus_id": "269304163",
            "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
            "text": "Instruction-tuning For the baseline, we ran inference on a non-modified LLaVA 7B (LLaVA-1.5-7B)model using the general prompt strategy introduced in Section 3.1.We then instruction-tuned the model using the dataset described in Section 4.1 using the finetune_task_lora.shscript1 provided by LLaVA over 1 epoch, with a learning rate of 2e \u22124 , for a total of 1.4k steps.Total time for instruction-tuning took around 2 hours on an NVIDIA H100 GPU.Due to the longer context generated by describing each frame in the video, we chose Qwen 7B (Bai et al., 2023) for the text-based LLM as it has a context length of 8192 tokens.Using LLaMA 2 (Touvron et al., 2023), we ran into issues with context length, as the model only has half the context length of that of Qwen models.Qwen 7B 20.1 7.9 2.5 14.9 15.6 7.0 0.8 12.7 GPT-4 Vision GPT-4 28.9 10.9 6.3 20.7\n\nTable 1: R@IoU for base LLaVA, instruction-tuned LLaVA using the general prompting strategy.\n\nFor comparison, we also show video-based LLM (Huang et al., 2023), GPT-4 Vision metrics.\n\nTwo-stage LLM runs Next, we compare metrics of different combinations of multimodal LLM and text-based LLMs as listed in Table 2.We use the activity prompting strategy in our multimodal LLMs to optimize for performance.For comparison, we run models on the state-of-the-art task-specific model (ViGA) by Cui et al. (2022) and a state-of-the-art video-based LLM (Reid et al., 2024), Gemini.\n\nFor inference in all experiments, LLaVA 7B (LLaVA-1.5-7B2) and 16B (LLaVA-1.6-Vicuna-13B3",
            "score": 0.17975194768419625,
            "section_title": "Experimental details",
            "char_start_offset": 12761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 851,
                    "end": 943
                },
                {
                    "start": 945,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1423
                },
                {
                    "start": 1425,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 1338,
                    "end": 1355,
                    "matchedPaperCorpusId": "248266558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.031494140625
        },
        {
            "corpus_id": "273022705",
            "title": "A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning",
            "text": "LlaVa-1.5 [43] is an extension of the LlaVa [44] model. We use the variant with CLIP-ViT-L-14 as a vision encoder and Vicuna-7b [12] language model. BLIP-2 [35] uses a QFormer architecture to bridge frozen language and vision encoders. We use the variant with CLIP-ViT-L-14 as a vision encoder and the 2.7b OPT [85] language model. InstructBLIP [15] is a family of VLLMs that exploits the basic BLIP-2 [35] architecture and advances the task by giving the instruction to both the QFormer and the LLM. We use two variants of the architecture, with two different LLMs from the T5 family [13]; in the base architecture, we use the CLIP-ViT-L-14 as a vision encoder and the T5-xl LLM. InstructBLIPxxl uses the same vision encoder and the T5-xxl language model. A comparison of all architectures in terms of parameters and pre-training datasets can be seen in Tab. 4. \n\nIt is worth noting, that none of the pre-training datasets are related to deepfakes, making the task more challenging. For the ensemble, we chose BLIP-2 [35] and LlaVa-15 [43] , based of two main factors: a) they show the most competitive performance on most datasets as seen in Fig. 3 and b) they have the least overlap in terms of pre-training data thus we intuitively expect them having complementary information. The ensembling method adopted in this work is using score fusion with majority voting; in the occasions where the models disagree, the mean is taken. GPT4v is used for comparison in the binary tasks, however as the training details of this model are unknown, it should only be treated as an upper bound.",
            "score": 0.17970598966125317,
            "section_title": "Appendix A Model Zoo",
            "char_start_offset": 30712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0604248046875
        },
        {
            "corpus_id": "274445684",
            "title": "Agri-LLaVA: Knowledge-Infused Large Multimodal Assistant on Agricultural Pests and Diseases",
            "text": "Table 3 presents the results of Agri-LLaVA-VQA-Bench, comparing the performance of Agri-LLaVA with the general-domain LMMs and investigate the impact of different instruction-following data constructions and hyperparameters during downstream task fine-tuning. Our main findings are as follows: (1) Models pre-trained with only stage-1 data exhibit significantly weaker visual reasoning capabilities compared to models fine-tuned in stage-2. This is attributed to the limitation imposed by a single feature alignment dataset on the model's ability to learn diverse instructions. (2) Following 3 epochs of supervised fine-tuning on the VQA training set, Agri-LLaVA is better than other general-domain LMMs, especially demonstrates a 4.87% higher comprehensive ability than LLaVA. This suggests the effectiveness of our knowledge-infused approach in adapting a general model to the agricultural domain. When performing downstream agricultural tasks, our Agri-LLaVA serves as a more suitable base model. \n\n(3) Performance on downstream tasks increases with the augmentation of stage-2 instruction-following data under the same hyperparameters for supervised fine-tuning. This underscores the crucial impact of high-quality instructionfollowing data on model performance. While the performance of some variants of Agri-LLaVA is surpassed by LLaVA, this is due to the higher difficulty level of Agri-LLaVA-VQA-Bench. When the data volume is low, the knowledge acquired by Agri-LLaVA may not sufficiently bridge the zero-shot capability gap between it and the general LLaVA.",
            "score": 0.17944698876500498,
            "section_title": "Ablations",
            "char_start_offset": 23943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1567
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "272689771",
            "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
            "text": "GPT-4o \n\nZero-shot CoT \n\nFrequency of using reference objects and their ability to follow instruction prompts more closely. Here, however, we also investigate the off-the-shelf performance and capabilities of open-source VLMs, i.e., LLaVA-v1.6-34b (Liu et al., 2024). \n\nTable 7 shows the performances of LLaVA in Q-Spatial Bench using standard prompt. Surprisingly, LLaVA achieves 60.59 in Q-Spatial-ScanNet, outperforming most commercial VLMs including Gemini 1.5 Pro, Gemini 1.5 Flash, and GPT-4V. However, LLaVA falls more than 20 points when evaluated on Q-Spatial++. Interestingly, when qualitatively analyzing the outputs of the models, we find that they essentially \"predict\" numbers rather than demonstrating any intuitive reasoning behavior, as seen in more powerful VLMs. We initially speculated that this might indicate the model was trained on some collection of datasets including ScanNet data. However, we found no reference suggesting that LLaVA directly uses ScanNet in its training dataset. Additionally, we found LLaVA does not perform well when receiving zero-shot CoT prompts. When receiving zero-shot CoT prompts, the performances drop by over 20 and 4 points in Q-Spatial-ScanNet and Q-Spatial++. For SpatialPrompt, we adopt SpatialPrompt-Steps, leading to a decrease around 9 points in Q-Spatial-ScanNet. In sharp contrast, SpatialPrompt-Steps improves 9 points in Q-Spatial++. We hypothesize several reasons for this: (i) LLaVA's capabilities are not as developed as those in very large commercial models, and inducing reasoning structures in-context via prompts do not yet lead to the same level of improvements; (ii) LLaVA may fail to follow the instruction prompts effectively; (iii) LLaVA may not be proficient at visual comparison, so even if it increases the use of reference objects, it does not yield better performance.",
            "score": 0.17934976762428112,
            "section_title": "GPT-4V",
            "char_start_offset": 21631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 9,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0236053466796875
        },
        {
            "corpus_id": "277993668",
            "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
            "text": "To highlight the efficiency of our streaming model, we present the response latency of LLaVA-Video-7B/72B alongside our model in Table 5. Response latency is defined the time a user waits to see the model's output, a critical factor affecting user experience. Since the LLaVA-Video series are trained in a captioning style, requiring a full clip as input rather than a single frame, their response latency is significantly higher than that of our model. Notably, LiveCC not only achieves lower latency but also delivers high-quality commentary (see Table 4).",
            "score": 0.17894042712803426,
            "section_title": "Response",
            "char_start_offset": 23742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 558
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01094818115234375
        },
        {
            "corpus_id": "263672058",
            "title": "Improved Baselines with Visual Instruction Tuning",
            "text": "Despite the data efficiency of LLaVA-1.5 when compared with approaches like InstructBLIP [14], the training of LLaVA-1.5 still doubles when compared with LLaVA. In this section, we conduct experiments for further improving the data efficiency by randomly sub-sampling the training data mixture of LLaVA-1.5, with a sampling ratio ranging from 0.1 to 0.5. We visualize the relative performance of different sampling variants in Fig. 4. \n\nFirst, the full data mixture provides the best knowledge coverage, and allows the model to achieve the best overall performance. To our surprise, with only 50% of the samples, the model still maintains more than 98% of the full dataset performance. This suggests that there is room for further improvements in data efficiency. \n\nSecond, when downsampling the dataset to 50%, the model's performance on MMBench, ScienceQA, and POPE does not decrease at all, and it even slightly improves on MMBench. Similarly, the model's performance remains steady when further downscaling the data from 50% to 30%. These results show promise of having the less-is-more [61] benefit for multimodal models as well.",
            "score": 0.1786798190297088,
            "section_title": "Data Efficiency",
            "char_start_offset": 24591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060516357421875
        },
        {
            "corpus_id": "273022748",
            "title": "Probing Mechanical Reasoning in Large Vision Language Models",
            "text": "We evaluated the mechanical reasoning abilities of three categories of VLMs. To ensure a fair comparison, all VLMs are evaluated on their ability to reason over images and texts under a zero-shot generation task. A complete list of models is reported in the results section as shown in Figure 2. Model size data are curated at the same time. The models are categorized as follows: \n\n1. Open-source VLMs with Multi-Image Reasoning: Includes models with different sizes and other variants such as CogVLM Series [Hong et al., 2024], Qwen series(Qwen-VL [Bai et al., 2023], Qwen-2 [Wang et al., 2024]), and Blip2 [Li et al., 2023], LLaVA-Next [Liu et al., 2024] , which are capable of reasoning over interleaved multiple images and texts. 2. Closed-source VLMs with Multi-Image Reasoning: Includes proprietary models such as GPT series [OpenAI] ( GPT-4v, GPT-4-turbo, GPT-4o-mini), Gemini Series [Gemini], . These models also support reasoning across interleaved images and texts, 3. Open-source VLMs with single-Image Reasoning: Includes models designed to process a single image alongside continuous text. InstructBlip Series [Dai et al., 2023], LLaVA Series [Liu et al., 2023a] [Liu et al., 2023b] In total, we aligned 26 models for evaluation. In order to analyze the reasoning abilities of VLMs, we asked the models to explain their answers after they have given the answers by adding \"please provide an explanation\" in the prompt.",
            "score": 0.17809972473940267,
            "section_title": "Model Selection and Experiment",
            "char_start_offset": 7587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1432
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 626,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02508544921875
        },
        {
            "corpus_id": "278129780",
            "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?",
            "text": "We include a total of 28 models in our experiment. Phi-3.5 [2] is a lightweight model designed for efficient language understanding and generation. We include Phi 3.5 vision instruct [2] for experiments. DeepSeek Janus Pro 7B [9] is a model that integrates multi-modal reasoning capabilities. DeepSeek-VL2 [73] is a vision-language model, with deepseek vl2 27B included in our evaluation. In-terVL2 [10][11][12]22] is a family of multi-modal models ranging from small to large-scale by OpenGVLab. We include InterVL2 1B, InterVL2 2B, InterVL2 4B, In-terVL2 8B, InterVL2 26B, InterVL2 40B, and InterVL2 Llama3 76B for experiments. Qwen2-VL [63] is a visionlanguage model, and we consider both Qwen2 VL 7B Instruct and Qwen2 VL 72B Instruct. MolMo [15] is a series of models designed for molecular and scientific applications. We include Molmo 72B 0924 and its distilled variant, Molmo 7B D 0924. LLaVA-Onevision [46] is an open-source multimodal LLM, we selected llavaonevision-qwen2-72b-ov-hf model for our experiments. Llava-Next [78] is an evolution of LLaVA, and we include llama3-llava-next-8b-hf, llava-v1.6-vicuna-13b-hf, llava-v1.6-34b-hf, and llava-next-72b-hf in our experiments. Llama3.2 builds on the Llama architecture with enhanced scalability. We include Llama-3.2-11B-Vision-Instruct and Llama-3.2-90B-Vision-Instruct in our experiments.",
            "score": 0.17807485354030023,
            "section_title": "Model Details",
            "char_start_offset": 29469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1352
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0379638671875
        },
        {
            "corpus_id": "274166071",
            "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression",
            "text": "MLLMs. Early models like BLIP2 [25] and Instruct-BLIP [14] designed a Q-Former to bridge encoded visual information into the input space of LLMs. These approaches typically require complex training processes to train the image-text alignment module. Methods represented by Flamingo [1] proposed incorporating encoded image information into LLM layers using cross-attention. Fuyu8B [4] entirely discarded the visual encoder, directly inputting image patches into the LLM. LLaVA [33], on the other hand, uses an MLP layer to directly bridge encoded image information into the LLM's input space, making the model architecture and training process much more straightforward. Consequently, many subsequent multimodal large models have made improvements based on LLaVA [7,18,29,34,35]. For instance, LLaVA 1.5 [34] optimized data quality, while LLaVA-Next [35] introduced adaptive image segmentation techniques to support highresolution images. \n\nHigh-resolution MLLM. Recently, various MLLMs have adopted high-resolution images as input to capture finegrained image information. In the early stages, most MLLMs use a fixed size of 224 for their inputs. LLaVA-1.5 [34] and BLiVA [19] increased the image size to 336 to achieve better performance. Qwen-VL [3] expanded the size further to 448. It first trained with a fixed image scale of 224 and then fine-tuned by increasing the resolution to 448. Vary [47] and Mini-Gemini [27] additionally introduced a Vision Encoder specifically for high-resolution images. SPHINX [31], Monkey [28], and LLaVA-UHD [48] resize the images to a fixed resolution and then split them into several patches. These patches are individually encoded before being fed to the LLM. Furthermore, LLaVA-NeXT [30] employs a series of predefined resolutions, first matching the input image to the closest resolution, and then segmenting it into sub-images. Qwen2-VL [46] directly uses the original resolution of the image and encodes it into dynamically variable-length visual tokens by modifying the structure of ViT [15].",
            "score": 0.1780548544747271,
            "section_title": "Related Work",
            "char_start_offset": 3818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1173,
                    "end": 1177,
                    "matchedPaperCorpusId": "261049015"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "265150038"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.170654296875
        },
        {
            "corpus_id": "270045732",
            "title": "Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models",
            "text": "We apply our proposed COT strategy to LLaVA, referred to as \"LLaVA w/ COT\".For the continuous age annotations in the UTKFace and CACD datasets, we utilize metrics such as MAE, MAPE, and RMSE for evaluation.For other discrete attributes, performance is measured using Accuracy and Kappa.As shown in Figure 2(a) and Figure 1, \"LLaVA w/ COT\" exhibits top performance on both UTKFace and Fair-Face datasets, particularly in achieving 0.00% off-target rate and high Kappa scores, indicating precise and reliable predictions across age, gender, and ethnicity categories, and significantly outperforming naive LMMs.As shown in Figure 3, \"LLaVA w/ COT\" variant outperforms LMM baselines in the quantitative analysis on the CACD dataset, exhibiting the lowest Mean Absolute Error (MAE) at 5.75 and the highest R 2 value of 0.500, indicating its strong predictive accuracy and its ability to explain half of the variance in the dataset.Beyond LMMs, our method exhibits comparable performance to traditional supervised learning approaches.In Table 2(a), \"LLaVA w/ COT\" outperforms state-of-the-art supervised learning methods in gender prediction accuracy.",
            "score": 0.1780242250765096,
            "section_title": "Quantitative analysis",
            "char_start_offset": 14333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 75,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06292724609375
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : Four people wearing helmets and sitting on a bench. \n\nSim-CLIP 4 : Three dogs sitting on a table. \n\nRobust-LLaVA 4 G: Four bottles of beer are in a refrigerator.",
            "score": 0.1779797583148723,
            "section_title": "Adversarial Image",
            "char_start_offset": 48542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 87,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004039764404296875
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : Graffiti on a wall with the word kisa on it. \n\nSim-CLIP 4 : Graffiti on a wall that says Arisa. \n\nRobust-LLaVA 4 G: Graffiti on a train car.",
            "score": 0.1779797583148723,
            "section_title": "Adversarial Image",
            "char_start_offset": 48125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 128
                },
                {
                    "start": 131,
                    "end": 173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029811859130859375
        },
        {
            "corpus_id": "276617463",
            "title": "Deciphering Functions of Neurons in Vision-Language Models",
            "text": "We conducted experiments based on LLaVA 1.5 (7B) (Liu et al., 2024). More results based on VLM of InternVL 2.5 (8B) (Chen et al., 2024) can be found in Appendix C.",
            "score": 0.1779797583148723,
            "section_title": "Experiments",
            "char_start_offset": 15787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 163
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 67,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0125732421875
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A man is taking a picture of himself in a mirror. \n\nSim-CLIP 4 : A lamp post with a light on it. \n\nRobust-LLaVA 4 G: A blender with a clear container and a black base.",
            "score": 0.1779797583148723,
            "section_title": "Adversarial Image",
            "char_start_offset": 47718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0024623870849609375
        },
        {
            "corpus_id": "276408432",
            "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
            "text": "User-aware Personalization As demonstrated in Table 1, the User-VLM 360\u00b0, in both its 3B and 10B sizes, consistently outperforms baseline models across both benchmarks. On the ElderlyTech-VQA benchmark, User-VLM 10B achieves an impressive 2x improvement in ROUGE-1 F1 score compared to the baseline, while the 3B variant performs approximately 1.5x better. A detailed comparison of baseline models on this benchmark, ranked by ROUGE-1 F1 score, reveals the following order: LLaMA 3.2 11B, LLaVA 1.5 7B, Pixtral 12B, and LLaVA 1.6 7B. Similarly, on the User-VQA benchmark, User-VLM 3B outperforms the baselines by 1.2x, while the 10B variant achieves a 1.3x improvement. When ranking baselines on this benchmark by ROUGE-1 F1 score, LLaVA 1.5 leads, followed by LLaVA 1.6, LLaMA 3.2, and Pixtral. These results underscore the efficacy of User-VLM 360\u00b0in addressing the challenges of these tasks and its superior performance across varying model sizes. General Purpose Understanding Despite the primary focus of training on human user images, which could lead to concerns about catastrophic forgetting and reduced performance on general-purpose tasks, User-VLM 360\u00b0demonstrates robust generalization capabilities. As summarized in Table 2, the model achieves competitive results across four widely adopted general-purpose benchmarks. Specifically, the 3B and 10B variants outperform the baseline on the VQAv2 benchmark, indicating strong visual questionanswering capabilities. On the COCO benchmark, the model performs comparably, with a minimal 0.16-point difference from the top-performing model, LLaVA 1.5. Similarly, on the \"in the wild\" benchmark, the model shows a negligible 0.04-point gap from LLaVA 1.6, highlighting its adaptability to diverse, unstructured data. However, the model exhibits limited performance on the SEED benchmark, suggesting room for improvement in specific scenarios.",
            "score": 0.1778771845664076,
            "section_title": "Comparative Analysis",
            "char_start_offset": 23496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0526123046875
        },
        {
            "corpus_id": "262054900",
            "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models",
            "text": "The experiments are conducted to answer three research questions. \n\n1 Which scaling factor matters? We study the relative contribution of three scaling-up factors to the performance improvement of LLaVA. The results are summarized in Table 3 (a). \n\n\u2022 Model size. Increasing the model size consistently improves the overall performance. We conjecture that larger data size is essential to train a larger model. For example, if we only train on LLaVA-80K data, we see smaller gain when model size becomes larger. \u2022 Image resolution. By fixing the CLIP ViT image encoder, we compare the variants that are pre-trained to take image resolution 224\u00d7224 and 336\u00d7336, and find that the higher resolution consistently yields 2-3 points improvement across all four LLM sizes. \u2022 Data mixing. Larger models tend to have higher capability of fitting the instruction data. \n\nBy mixing the language-only instruction data (ShareGPT) with LLaVA-80K, we can improve model performance by 2 points, compared to training on multimodal instruction data only. \n\nIn Table 3 (b), we present our result on MM-Bench [13], which contains a set of 2,974 questions, which evaluate models' reasoning skills of six categories. The combination of the three factors improve the baseline LLaVA 7B model, reported in [13]. \n\n2 When should the parameter-efficient training method be considered? As model size increases, it becomes necessary to consider using tuning methods that are more efficient than fullmodel fine-tuning. LoRA and QLoRA are well-known parameter-efficient tuning methods. As shown in Table 4, we report compute cost using GPU hours per node, because the unit can be equivalent to the price $13.63/hour (ND A100 v4 series) on Azure 7 . The total cost can be estimated by multiplying the #hours and #epochs. \n\nIn Table 4(a), we train both the 33B and 65B model with LoRA rank 8 and 64 for 1 epoch on the LLaVA-80K instruction-tuning dataset. For models with 33B parameters and above, as we increase the LoRA rank values, we notice an increase in both performance and cost until full-model tuning reaches its maximum performance for a specific model size.",
            "score": 0.17783300367751714,
            "section_title": "Scaling up LLaVA",
            "char_start_offset": 7403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04052734375
        },
        {
            "corpus_id": "266740250",
            "title": "Predicting the spread of SARS-CoV-2 variants: An artificial intelligence enabled early detection",
            "text": "Traditional epidemiological models typically predict the trajectory of SARS-CoV-2 infections based on the patterns of its temporal evolution, considering properties of the respective time series of the number of infections caused by the variant over time (19,61,62). Moreover, variants compete against each other on their respective share of infections (63,64). Hence, the proposed model includes features about the weekly evolution of the proportion of reported sequences for the variant among all variants actively circulating in the respective country at the time of detection of the new variant. Additionally, the model estimates the weekly number of new infections caused by the variant as the product of the total number of new infections by the proportion of the respective variant among all others. The model feature set also includes variables that capture the maximal number of cases per million caused by the variant in other countries up to the end of the observation period.",
            "score": 0.17782920710586014,
            "section_title": "Infection trajectory features",
            "char_start_offset": 33973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 987
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "244274724"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "237606914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013723373413085938
        },
        {
            "corpus_id": "271843270",
            "title": "VLM-MPC: Vision Language Foundation Model (VLM)-Guided Model Predictive Controller (MPC) for Autonomous Driving",
            "text": "The experiment settings are given in this section. The proposed methodology is validated using a series of experiments. First, a real-world driving environment dataset is adopted to evaluate the proposed method and baseline methods. We utilize Llava 1.6 (Liu et al., 2024) as the foundation VLM for various components in our system. Additionally, in the sensitivity analysis, we tested the performance of GPT-4-o and GPT-4-mini. Llava 1.6 model was run on a Linux (Ubuntu) system equipped with 32GB of DDR5 RAM and an NVIDIA GeForce RTX 4090 GPU (24GB GDDR6X). The model was configured with a temperature of 0, while all other parameters were left at their default values. The GPT series models were utilized via API calls.",
            "score": 0.17659826856341007,
            "section_title": "Experiment settings",
            "char_start_offset": 20865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00803375244140625
        },
        {
            "corpus_id": "273023258",
            "title": "Vision Language Models See What You Want but not What You See",
            "text": "We evaluated three categories of VLMs. To ensure a fair comparison, all VLMs are evaluated on their ability to reason over images and texts under a zero-shot generation task. A complete list of models is reported in the results section as shown in Figure 3. Model size data are curated at the same time. The models are categorized as follows: \n\n1. Open-source VLMs with Multi-Image Reasoning: Includes models with different sizes and other variants such as CogVLM Series (Hong et al., 2024), Qwen series(Qwen-VL (Bai et al., 2023), Qwen-2 (Wang et al., 2024)), and Blip2 (Li et al., 2023), LLaVA-Next (Liu et al., 2024) , which are capable of reasoning over interleaved multiple images and texts. 2. Closed-source VLMs with Multi-Image Reasoning: Includes proprietary models such as GPT series (OpenAI) ( GPT-4v, GPT-4-turbo, GPT-4o-mini), Gemini Series (Gemini), and Claude Series (claude). These models also support reasoning across interleaved images and texts, 3. Open-source VLMs with single-Image Reasoning: Includes models designed to process a single image alongside continuous text. InstructBlip Series (Dai et al., 2023), LLaVA Series (Liu et al., 2023a) (Liu et al., 2023b) In total, we processed 37 models for evaluation. All the model performances in intentionality understanding and perspective-taking together with human baseline performances are presented here (Figure 2). In order to analyze the reasoning abilities of VLMs, we ask the models to explain their answers after they have given the answers.",
            "score": 0.1765901699598853,
            "section_title": "MODEL SELECTION AND EXPERIMENT",
            "char_start_offset": 13655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1519
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 588,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026153564453125
        },
        {
            "corpus_id": "266054635",
            "title": "Diffusion Illusions: Hiding Images in Plain Sight",
            "text": "Fig. 19, Fig. 20, Fig. 21 and Fig. 22 show full evaluation results of the derived images from baseline and four variants of our method. The advantages of our method compared to the baseline are further supported by the new metrics introduced in this section, like better Controllability and Aesthetics Score from LLaVA (see Fig. 19). Meanwhile, LLaVA has relatively less bias on art styles and different subjects (Fig. 20 and Fig. 22)",
            "score": 0.1760954537028785,
            "section_title": "B.2. Extended Results of Hidden Overlay Illusion",
            "char_start_offset": 37066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03948974609375
        },
        {
            "corpus_id": "272689196",
            "title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types",
            "text": "Llava-1.6-Mistral-7B is one model that shows exactly opposite trait than this, being limited in academic topics as compared to social topics. Qwen-2 variants and PaliGemma show weak results in all domains, like in task types. InternVL-2-1B remains the best choice if a small model is required, with decent results using 1B parameters.",
            "score": 0.17602036431415458,
            "section_title": "Evaluation on Application Domains",
            "char_start_offset": 19107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 334
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006587982177734375
        },
        {
            "corpus_id": "273653970",
            "title": "Who Speaks Matters: Analysing the Influence of the Speaker's Ethnicity on Hate Classification",
            "text": "Model Size and Recency We find that larger and newer models, such as Llama-3-70B and GPT-4turbo, are more robust and show smaller percentage of flips, than the smaller Llama-3-8B, and the older GPT-3.5-turbo model.",
            "score": 0.17598806241210144,
            "section_title": "What factors cause outputs to flip?",
            "char_start_offset": 7451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0007700920104980469
        },
        {
            "corpus_id": "272911111",
            "title": "E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding",
            "text": "LLaVA-1. Method F1@0.1 F1@0.3 F1@0.5 F1@0. Method F1@0.1 F1@0.3 F1@0.5 F1@0.7 F1 METEOR Rouge-L CIDEr Sim \n\nImage-LLMs: 8 uniformly sampled frames as inputs LLaVA-1.5 [58] 2.1 1.1 0.5 0.1 0.9 0.1 0.1 0.2 9.5 LLaVA-InternLM2 [11] 0.3 0.0 0.0 0.0 0.1 0.0 0.0 0.0 4.7 mPLUG-Owl2 [105] 0.1 0.1 0.0 0.0 0.1 0.0 0.0 0.0 7.7 XComposer [111] 5.9 Method R@0.1 R@0.3 R@0.5 R@0. Method R@0.1 R@0.3 R@0.5 R@0.7 Rec Image-LLMs: 8 uniformly sampled frames as inputs LLaVA-1.5 [58] 0.0 0.0 0.0 0.0 0.0 LLaVA-InternLM2 [11] 3.8 1.4 0.7 0.0 1.5 mPLUG-Owl2 [105] 0.0 0.0 0.0 0.0 0.0 XComposer [111] 0.0 0.0 0.0 0.0 0.0 Bunny-Llama3-V [31] 0.0 0.0 0.0 0.0 0.0 MiniCPM-V-2.5 [93] 0.0 0.0 0.0 0.0 0.0 Qwen-VL-Chat [6] 2.4 1.7 1.0 0.7 1.5",
            "score": 0.17598806241210144,
            "section_title": "Prompt:",
            "char_start_offset": 57017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 105
                },
                {
                    "start": 108,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018100738525390625
        },
        {
            "corpus_id": "270619607",
            "title": "GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs",
            "text": "For LLaVA-1.5 (7B and 13B) and LLaVA-1.6/LLaVA-NeXT models up to the 13B versions, we used one 40 GB NVIDIA A100. For LLaVA-Yi-34B, and Intern-VL-Chat-1.5 26B, we used one 80 GB NVIDIA A100. For LLaVA-NeXT-Qwen-1.5 110B, we used four 80 GB NVIDIA A100 GPUs. In most cases, except the Intern-VL model, we used the Huggingface [26] library setup and checkpoints.",
            "score": 0.17598806241210144,
            "section_title": "A.2 Computational Resources",
            "char_start_offset": 14142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 360
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0171051025390625
        },
        {
            "corpus_id": "274777047",
            "title": "Smaller Language Models Are Better Instruction Evolvers",
            "text": "In the AutoIF scenario, we also use Llama series models as in Evol-Instruct, as well as Qwen series (Yang et al., 2024) models including Qwen-2.5-7B and Qwen-2-7B, along with InternLM-2-7B. For Auto Evol-Instruct, we evaluate the performance of the Llama series models.",
            "score": 0.17598806241210144,
            "section_title": "A Appendix",
            "char_start_offset": 24923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00803375244140625
        },
        {
            "corpus_id": "269768709",
            "title": "PRIEST: predicting viral mutations with immune escape capability of SARS-CoV-2 using temporal evolutionary information",
            "text": "At each timestep, we grouped the protein sequences by the wider classes of different SARS-CoV-2 variants. Since no evidence suggests that variants mutate into older variants, we assume the same in our study. However, the exact nature of SARS-CoV-2 evolution is still unknown. We should also note that the existence of such cases (mutating into older variants) can be easily handled in PRIEST. Our study aims to understand possible changes in the current variants that may lead to next-generation variants. Moreover, the shift in the percentage of each variant collected over time reinforces our assumption about the virus mutation. Therefore we constructed the time series by linking these protein sequences under special conditions as stipulated below. \n\nLet i and i+1 be the labels of the protein sequences where i , i+1 \u2208 {0, 1, 2}. The groups of sequences with label i can be linked to the groups of sequences with label i+1 if and only if i \u2264 i+1 . This constructs a graph. From now on, we will refer to this as time series graph. \n\nAt each timestep, we randomly sample protein sequences from all possible allowed variants. However, whether we can sample from a variant group depends on which group we sampled from in the previous timestep. For example, if we sample a sequence from Variants Being Monitored (VbMs) at timestep i, we can only sample from Variants being Monitored (VbMs) and Variants of  We use a sliding window to extract three trigrams around that position. In the sequence above, to get the feature representation of the site marked with blue color, we extract VEG (in red), EGF (in green) and GFN (in yellow). We use ProtVec to extract the feature representation of each of the three trigrams. We aggregate them to obtain the feature representation of the position. Concern (VoCs) at timestep i + 1. We can ensure this by following the links in the time series graph. In addition, we performed this sampling with replacement to cover for the lack of enough data of a specific variant at certain timesteps.",
            "score": 0.17596961784930032,
            "section_title": "Time series construction",
            "char_start_offset": 7747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021915435791015625
        },
        {
            "corpus_id": "269449774",
            "title": "MileBench: Benchmarking MLLMs in Long Context",
            "text": "We provide basic information of benchmarked models in Table 7, as well as brief introduction for each model to highlight their characteristics:\n\n\u2022 GPT-4V (OpenAI, 2023) and GPT-4o8 are developed by OpenAI and are deemed the most powerful vision-language models for comprehension and generation.\u2022 Gemini 1.0 (Anil et al., 2023) and Gemini 1.5 (Reid et al., 2024) are models developed by Google, demonstrating competitive ability among closed-source MLLMs.\n\n\u2022 Claude 3 Opus9 is a vision-language model recently released by Anthropic, impressing the community with its extra-long 200K context length.\u2022 LLaVA-v1.5 (Liu et al., 2023a)  \u2022 LLaVA-v1.6(Liu et al., 2024b) takes a step further on the basis of LLaVA-v1.5.It is able to process images with any resolution and releases more variants based on different LLMs.\n\n\u2022 ALLaVA-Longer (Chen et al., 2024) is a lite version of LLaVA with enhanced complex reasoning ability, even achieving competitive results with larger models.\n\n\u2022 Yi-VL (Young et al., 2024) is based on LLaVA architecture and adopts a 3-stage training process.\n\n\u2022 Cheetor (Li et al., 2023c) proposes to use a Visual Prompt Generator to capture residual visual details, which might be vital for model performance.\n\n\u2022 Qwen-VL-Chat (Bai et al., 2023) is a 7B model trained on billions of multimodal samples.\n\n\u2022 VILA (Lin et al., 2023) adopts a LLaVA-like structure but is trained with interleaved image-text data.\n\n\u2022 Mantis (Jiang et al., 2024) uses LLaMA310 as language model and is trained with multi-image data.\n\n\u2022 Open flamingo (Awadalla et al., 2023) the open-source implementation of Flamingo (Alayrac et al., 2022), which serves as the foundation of subsequent multi-image and video comprehension models.",
            "score": 0.17595353912796796,
            "section_title": "C.1 Details of Evaluation Models",
            "char_start_offset": 23204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 145,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 454
                },
                {
                    "start": 456,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 1071
                },
                {
                    "start": 1073,
                    "end": 1223
                },
                {
                    "start": 1225,
                    "end": 1315
                },
                {
                    "start": 1317,
                    "end": 1421
                },
                {
                    "start": 1423,
                    "end": 1522
                },
                {
                    "start": 1524,
                    "end": 1719
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08587646484375
        },
        {
            "corpus_id": "273022705",
            "title": "A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning",
            "text": "Further to the analysis in Sec. 5.2, we show the performance of all models on the multiple-choice VQA setting in Tab. 10. In the multiple-choice setting, all models have comparable mAP and AUC, however, LlaVa-1.5 [43] shows a clear advantage in terms of Recall and F1.",
            "score": 0.1755331126670443,
            "section_title": "G Multiple-choice VQA",
            "char_start_offset": 39495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0034961700439453125
        },
        {
            "corpus_id": "221823002",
            "title": "Primary case inference in viral outbreaks through analysis of intra-host variant population",
            "text": "PYCIVO was validated using data from 11 transmission clusters containing 105 samples and 1936 unique sequences [13][14][15][16][17][18][19]. In all cases, the primary case was epidemiologically identified. In addition, we used 10 modified transmission clusters in which the known primary case sample was removed in order to test PYCIVO specificity. One transmission cluster included only 2 cases and could not be used effectively as a modified cluster.\n\nThe cover times of simulated evolution from intra-host viral variants sampled from patient A to intra-host variants from patient B (out-evolution time) and from patient B to patient A (in-evolution time) are asymmetric; i.e., the time of evolution from A to B and from B to A are different. A good primary case candidate would have overall high in-evolution and low out-evolution times. This concept is illustrated in Fig. 2. In contrast, a simpler metric such as minimal or average hamming distance does not inform on which direction is more likely. The evolutionary time, however, correlates positively (r = 0.65, p < 10 -200 ) with hamming distances, as shown in Fig. 3.\n\nBoth primary case indication values were calculated to issue primary case predictions for each outbreak. We aim to achieve accurate detection of the primary case only when one was actually sampled in an outbreak investigation. We recognize that the sample size for validation fo this software is very small. However, epidemiologically labeled complete outbreak clusters are exceedingly difficult to find. We aim to make our software specific at the cost of sensitivity. With this in mind, PYCIVO issues a \"No Primary Case\" (NPC) prediction when the primary case was not sampled or was removed from the data set.\n\nThis category of output is enabled by discrepancy between the two independent prediction methods, or when the genetic relatedness graph G T is not connected. In this case, PYCIVO informs the user that there is not a good candidate for the primary case within this group, claiming that they were likely not present in the samples given.\n\nIf there is no discrepancy between prediction methods and G T forms a connected component, then the V 1 and V 2 vectors were used",
            "score": 0.17530172371221267,
            "section_title": "Results",
            "char_start_offset": 10156,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "206129544"
                },
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "8474124"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "30681620"
                },
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "205449680"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "10405790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.000743865966796875
        },
        {
            "corpus_id": "276929061",
            "title": "Multi-Cue Adaptive Visual Token Pruning for Large Vision-Language Models",
            "text": "For a fair comparison and efficient validation, we employ the LMMs-Eval [59] evaluation framework across all experiments. The models used include LLaVA-1.5 (7B and 13B variants) [29], LLaVA-NEXT-7B [10], and InternVL2 (2B and 8B variants) [10]. We set the configurations based on their official settings. \n\nLLaVA-1.5. The model architecture of LLaVA-1.5 [29] replaces the linear projection in LLaVA [30] with an MLP to map visual features into a shared embedding space with the language model. It uses CLIP-ViT-L as the visual encoder at a resolution of 336\u00d7336, with Vicuna as the language decoder. The image is divided into 24\u00d724 patches, generating 576 image tokens. We implement our AdaptPrune by restoring the image sequences to a two-dimensional arrangement on LLaVA-1.5. \n\nLLaVA-NEXT. LLaVA-NEXT [29] is an adaptive highresolution model that divides input images into smaller grid patches, each encoded at the native resolution of 336\u00d7336, and then merges them into a large feature map. By removing image padding, it adjusts seamlessly to any aspect ratio. To add global context and reduce stitching artifacts, it also concatenates features from a downsampled image with the merged map, allowing efficient support for any resolution. Since the final image tokens are merged into two image sequences, we restore each image sequence to a twodimensional arrangement and then perform our AdaptPrune under a unified pruning budget. \n\nInternVL2. InternVL2 [10] is a high-resolution model, employing a dynamic high-resolution strategy by dividing images into 448\u00d7448 tiles, with aspect ratios matched from a predefined set to retain natural proportions, it scales up to 40 tiles to accommodate 4K resolution. A 448\u00d7448 thumbnail of the entire image is included alongside the tiles to provide global context. Pixel shuffling reduces each 448\u00d7448 tile to 256 visual tokens, enabling efficient high-resolution processing with a reduced token count.",
            "score": 0.17506423361212328,
            "section_title": "A. Model Settings",
            "char_start_offset": 27908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "214323514",
            "title": "LAVA: a streamlined visualization tool for longitudinal analysis of viral alleles",
            "text": "With the rapid and significant advancements in sequencing technologies in recent years, whole-genome sequencing has become more cost-effective, more efficient, and more accurate than ever (1). A common area of bioinformatics research in virology is the comparison of viral evolution in longitudinal samples. At a basic level, viral genome evolution may be examined over routine passage in cell culture (2,3). Drug manufacturers routinely check for the development of resistance mutations in response to in vitro antiviral pressure (4,5). Clinical researchers want to know how viruses evolve longitudinally in normal or immunocompromised patients, in response to a drug pressure, or in different areas of the body (3,6,7). \n\nIn order to facilitate these routine analyses of viral evolution, we developed a simple command-line tool called Longitudinal Analysis of Viral Alleles (LAVA) for analyzing and visualizing the evolution of minor variants in viral genomes over time. The basic tenor of these analyses involves the calling of a consensus genome for the initial sample and then using that genome as a reference for downstream samples. Viral sequence data is plotted both across the genome to show where mutations cluster and over time to show allele frequency changes. The metadata associated with the experiment may be minimal, consisting simply of sample names and units of time. The units of time are arbitrary and may be minutes, hours, days, months, years or even different categorical experimental conditions. LAVA also generates interactive HTML files for sequence data analysis. The HTML files may be manipulated by users without significant bioinformatic experience according to the nature of their biological question, alleviating a significant conundrum for sequencing and bioinformatics groups as demand for their services continues to increase.",
            "score": 0.17490085097109098,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "47390966"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "49678095"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "204865664"
                },
                {
                    "start": 534,
                    "end": 536,
                    "matchedPaperCorpusId": "48356118"
                },
                {
                    "start": 716,
                    "end": 718,
                    "matchedPaperCorpusId": "3091591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "266693714",
            "title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models",
            "text": "As the LLaVA 1.5 checkpoint comprises 7 billion parameters, it ordinarily demands approximately 8 GB of GPU resources. Nevertheless, in our experimental setup, we employed a 4-bit quantized variant that operates efficiently on approximately 6 GB of GPU memory. Given that each new image serves as a unique conversational context for the model, reseting the model execution process is necessary. To achieve",
            "score": 0.17480184111695665,
            "section_title": "A. Memory Management",
            "char_start_offset": 9078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 405
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014556884765625
        },
        {
            "corpus_id": "268384843",
            "title": "Can We Talk Models Into Seeing the World Differently?",
            "text": "Here we provide an overview of all used models from the main paper. \n\nQwen-VL-Chat (Bai et al., 2023b) Adds vision capabilities to Qwen-7B (Bai et al., 2023a). We set a repetition penalty of 1.2 for this model. \n\nQwen-VL Plus/Max (Qwen Team, 2024) \n\nAliBaba's proprietary larger variants of Qwen-VL-Chat. Access only via API. \n\nCogAgent (Hong et al., 2023) A special model for interaction with graphical user interfaces (GUIs) at high-resolution. \n\nCogVLM (Wang et al., 2023) Adds \"trainable visual expert module\" in LLM layers to combine vision and language. \n\nEmu2 (Sun et al., 2023a) The 37B model claims \"strong multi-modal in-context learning abilities\". \n\nInstructBLIP (Dai et al., 2023) Connects frozen vision encoders and LLMs through a trainable Q-Former. Uses Vicuna or FLAN-T5 as LLMs. \n\nLLaVA v1.5 (Liu et al., 2023a) Improvements of LLaVA with modifications on the image encoder, the projector, and task-specific data. Uses Vicuna-7/13B as LLM. \n\nLLaVA-NeXT (Liu et al., 2024) Successor of LLaVA v1.5 supporting higher resolutions through patching, and using better SFT training data for training, claiming \"improved reasoning, OCR, and world knowledge\" (Liu et al., 2024). The 34B version switches from Vicuna-7/13B to Nous Hermes 2 Yi 34B. \n\nMoE-LLaVA v1.5 (Lin et al., 2024) Variants of LLaVA v1.5 employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (Qwen, Phi-2, StableLM). \n\nLLaVA-RLHF (Sun et al., 2023c) Variants of LLaVA v1.5 aligned with Factually Augmented RLHF (Fact-RLHF) (Sun et al., 2023c).",
            "score": 0.17457281087394316,
            "section_title": "A OVERVIEW OF VLMS",
            "char_start_offset": 36999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "As compression rates increase to fit lower transmission bandwidths, accuracy declines significantly. Figure 1 illustrates this trade-off, where the blue curve (LLaVA-JPEG) showing the accuracy of the LLaVA model with the Vicuna-7B backbone (Chiang et al., 2023) on the VQA-v2 dataset (Goyal et al., 2017) when input images undergo JPEG compression at various compression rates. Higher compression rates reduce the data size for transmission, lowering the inference latency, but incur a notable 6.54% accuracy drop comparing to the original LLaVA model (LLaVA-Ori). \n\nIn this paper, we address these limitations with a partitioned execution approach, where the initial layers of the VLM's visual encoder run locally when edge resources permit, while the remaining layers of the visual encoder and the other VLM components execute in the cloud. This partitioned execution allows the VQA system to shift some processing from the cloud to edge devices, reducing strain on cloud infrastructure and associated costs. It can also benefit from task-aware compression on intermediate features from the visual encoder, minimizing transmission overhead and po- LLaVA-AlignedVQ LLaVA-VQ_n3_g8 LLaVA-JPEG LLaVA-Ori \n\nFigure 1: Accuracy with compressed data size for different variants of LLaVA on the VQA-v2 dataset. Our approach, LLaVA-AlignedVQ (marked by the red star), achieves a high accuracy of 79.98% with a minimal transmission size of 0.85KB, balancing accuracy and compression efficiency. \n\ntentially outperforming generic methods like JPEG on raw images. Task-aware compression customizes the compression of intermediate features specifically for the VQA task. Unlike JPEG-like generic compression algorithms, which aim to retain as much of the raw image as possible without considering downstream tasks, task-aware compression are specifically optimized on the question answering task, offering opportunities for efficient compression without sacrificing task accuracy. \n\nThe basic idea poses a critical research challenge: designing a task-aware compression algorithm that achieves high compression rates while preserving VQA accuracy.",
            "score": 0.17444407572227627,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 304,
                    "matchedPaperCorpusId": "8081284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "30874842",
            "title": "Filovirus RefSeq Entries: Evaluation and Selection of Filovirus Type Variants, Type Sequences, and Names",
            "text": "Only one cuevavirus, Lloviu virus (LLOV), has been described [18]. At the time of writing, LLOV had not been isolated in culture, and the sequence diversity of LLOV had only been defined in a single study using deep sequencing techniques on samples from deceased Schreibers's long-fingered bats (Miniopterus schreibersii) [18]. Only one additional study has been published on this virus, characterizing molecular-biological characteristics of the LLOV glycoprotein [20]. The codingcomplete genome of one LLOV has been determined (Genbank #JF828358), which therefore automatically became the current RefSeq sequence (#NC_016144) (see [21] for sequencing nomenclature used in this article). In the absence of additional deposited LLOV sequences and characterization data, this RefSeq entry should therefore be upheld but be considered temporary until a complete genome, including all non-coding sequences, is determined. \n\nIn line with filovirus strain/variant/isolate definitions outlined previously [19], we propose the variant designation \"Asturias\" (after the Principality of Asturias in Spain, where Cueva del Lloviu is located in which LLOV was discovered [18]) and the \"isolate\" name \"Bat86\" (instead of \"MS-Liver-86/2003\") for this virus: LLOV/Ast-Bat86 \n\nAccordingly, in RefSeq #NC_016144 the definition line \"Lloviu virus, complete genome\" was changed to \"Lloviu cuevavirus isolate Lloviu virus M.schreibersii-wt/ESP/2003/Asturias-Bat86, [coding-]complete genome.\" The RefSeq <strain> field was cleared; and the RefSeq <isolate> field was filled with \"Lloviu virus M.schreibersii-wt/ESP/2003/Asturias-Bat86.\" The same changes should be applied to GenBank #JF828358.",
            "score": 0.1740105929086361,
            "section_title": "Cuevavirus RefSeq Entries",
            "char_start_offset": 7335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "7818013"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "7818013"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "24572816"
                },
                {
                    "start": 633,
                    "end": 637,
                    "matchedPaperCorpusId": "18227560"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "13781836"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "7818013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00443267822265625
        },
        {
            "corpus_id": "268536728",
            "title": "When Do We Not Need Larger Vision Models?",
            "text": "The above examples highlight the difference in performance between LLaVA-1.5-S 2 , LLaVA-1.5 and GPT-4V. LLaVA-1.5-S 2 distinguishes itself through its heightened sensitivity and enhanced precision in visual detail understanding. This advanced level of detail recognition can be attributed to the S 2 scaling applied to its vision backbone, which significantly augments its ability to analyze and interpret subtle visual cues within complex images.",
            "score": 0.1740105929086361,
            "section_title": "D Ablations of Model Design",
            "char_start_offset": 33396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 448
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0273284912109375
        },
        {
            "corpus_id": "275789161",
            "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
            "text": "General LLaVA-SFT-665K [38], LLaVA-OV-SI [29], Cambrian-cleaned [39], Pixmo (docs, cap, points, cap-qa, ask-model-anything) [35] 9.87M Document DocVQA [40], Docmatix [41] 1.31M \n\nChart/Figure ChartQA [42], MMC_Instruction [83], DVQA [84], LRV_Instruction [85], ChartGemma [86], InfoVQA [87], PlotQA [88] 1.00M OCR MultiUI [89], in-house data 0.83M Grounding RefCoco [90], VCR [91], in-house data 0.50M \n\nMulti-Image Demon-Full [92], Contrastive_Caption [93] 0.41M \n\nText-only Magpie [94], Magpie-Pro [94], Synthia [95], Infinity-Instructsubjective [82], NuminaMath [96] 2.21M",
            "score": 0.1740105929086361,
            "section_title": "Image & Text Data",
            "char_start_offset": 20181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 575
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "4445015"
                },
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "210164961"
                },
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "6308361"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "53734356"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "260704723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0034427642822265625
        },
        {
            "corpus_id": "6076562",
            "title": "Models of deletion for visualizing bacterial variation: an application to tuberculosis spoligotypes",
            "text": "We present several candidate deletion models of spoligotype evolution, then compare them using a second-order form of the Akaike's Information Criterion (AIC c ) and data from selected published spoligotype samples (see Table 1). In this section we begin with the underlying assumptions about spoligotypes and their evolution. We then outline the procedure for model selection and finally describe the models.",
            "score": 0.1740105929086361,
            "section_title": "Methods",
            "char_start_offset": 6152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011243820190429688
        },
        {
            "corpus_id": "277467904",
            "title": "WikiVideo: Article Generation from Multiple Videos",
            "text": "We report more statistics in Table 14 to show the varying performance across model sizes and variations. We report LLaVA-Video-7B,72B and QwenVL2.5-3B,7B,72B as well as Qwen2.5-32B,72B for article synthesis.",
            "score": 0.1740105929086361,
            "section_title": "F Additional Results",
            "char_start_offset": 34888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00930023193359375
        },
        {
            "corpus_id": "272910642",
            "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation",
            "text": "This allows the model to handle text and image input jointly. We use llava-hf/llava-v1.6vicuna-7b-hf. \n\nFor all the models, we opt for their default, suggested hyper-parameters: e.g., temperature is set to 0 with GPT-4 and Gemini, while we use the default generation configuration of Idefics2 and LLaVa, corresponding to greedy decoding.",
            "score": 0.17399507578359014,
            "section_title": "Model Selection and Hyper-Parameters",
            "char_start_offset": 28534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01763916015625
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "We dive deeper into the design choices of TS-LLaVA in this section. More results can be found in the Appendix. Number of frames We vary the maximum number of input frames used in TS-LLaVA, and present the results in Fig. 4a. Since the total number of visual tokens remains constant at 3456, using fewer frames results in a lower compression rate. For datasets like NExT-QA, which do not emphasize long-term video understanding, the reduced compression rate effectively compensates for any missing information from fewer frames. In contrast, for EgoSchema, which specifically targets long-term understanding, using more frames proves beneficial. One interesting observation is that the 7B and 34B models behave differently to reduced frame counts. The performance of the 34B model keeps increasing as we increase the number of frames, indicating it can handle more information as we increase frame numbers. How many frames per thumbnail image? We conduct experiments with varying numbers of frames per thumb-  [26] Table 5. Results obtained on Multitask Benchmarks. We highlight the top-performing training-free methods and underline the bestperforming video LLMs overall. Methods below the dashed line (--) are training-free, while those above it have been trained on extensive video data. We denote performance better than or comparable to ( ) or lags behind ( ) the main competing training-based video LLM (on MVBench: PLLaVA-34B, which uses the same backbones as TS-LLaVA; on MLVU: Video-LLaMA2-72B). nail image. The results are presented in Fig. 4b. Since the resolution of each image is fixed at 336\u00d7336 for the image encoder, including more frames in the thumbnail image means lower resolution for each frame. The results show that changing the number of frames does not affect the performance on NExT-QA and IntentQA significantly. While for EgoSchema, which requires better temporal understanding and involves longer videos, is more sensitive to the number of frames per thumbnail image. Using 6 frames per thumbnail image shows clear benefit over the counterparts, by providing enough temporal cues and not losing too much details due to reduced resolution.",
            "score": 0.1736593759273994,
            "section_title": "Design Choices of TS-LLaVA",
            "char_start_offset": 20622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "259108333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0323486328125
        },
        {
            "corpus_id": "265067489",
            "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
            "text": "Tool Use Improves Existing Capabilities. We consider two benchmarks. LLaVA-Bench (Liu et al., 2023a) evaluates the visual chat of LMMs, with three types of questions: conversation, detailed description and visual reasoning. It consists of two datasets: the COCO set containing 30 COCO images and 90 chat questions, and the In-the-Wild set containing 24 web images with 60 questions. \n\nLanguage GPT-4 (gpt4-0314) is used to score the generated answers. The relative scores between the model output and gold response are reported. SEED-Bench (Li et al., 2023b) evaluates the image-level and instance-level perception and reasoning of LMMs, with 19K multi-choice questions. \n\nThe results are shown in Table 4. Both LLaVA-Plus variants outperform LLaVA on these two benchmarks, demonstrating the effectiveness of adding visual recognition results of applying new skills in the LMM pipeline. LLaVA-Plus (All Tools) shows superior performance to LLaVA-Plus (Fly) because the former leverages more tools as additional contexts. We further conducted several ablations: (i) We tried to directly add the skill execution results in the testing stage of LLaVA, shown as the row of LLaVA (Tools in Test). The degraded performance compared with LLaVA demonstrates the necessity of learning to use skills in training. (ii) We removed thoughts in the unified data format and observed a performance drop, indicating chain-of-thoughts style data format is beneficial. \n\n(iii) GPT4Tools trains an LLM for multimodal tool use. Its lower performance indicates that visual instruction tuning of tool use in LLaVA-Plus is important. \n\nLLaVA-Bench (Tools). To study the novel capabilities enabled by learning to use skills, we create an evaluation set LLavA-Bench (Tools), which measures four capabilities (grounding, tagging, caption, and OCR) with 10, 12, 12, and 10 samples in each. In   (Dai et al., 2023) 30.8 16.0 9.8 9.0 21.1 10.5 25.6\u00b10.3",
            "score": 0.1733469585011714,
            "section_title": "THE EFFECTIVENESS OF LEARNING TO USE SKILLS",
            "char_start_offset": 20523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1922
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1837158203125
        },
        {
            "corpus_id": "272593236",
            "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks",
            "text": "We present ASR of the ImgJP attack on different target LVLMs in Table III. For the LLaVA model based on Llama-2-13B, the default configuration using the original CLIP vision encoder exhibits a high ASR of 28.0%, indicating a significant vulnerability to the ImgJP attack. The Vicuna-7B variant demonstrates an even higher ASR of 34.0%, reflecting a greater susceptibility to jailbreak attacks and underscoring the need for more effective defenses. Applying JailGuard as an external defense mechanism reduces the ASR to 21.0% for the LLaVA (Llama-2-13B) model and 26.0% for the LLaVA (Vicuna-7B) model. While JailGuard effectively lowers the ASR, its mutation-based reactive approach increases system complexity and computational overhead. Incorporating FARE 4 , further decreases the ASR to 18.3% for the LLaVA (Llama-2-13B) model and 24.0% for the LLaVA (Vicuna-7B) model. In contrast, our developed Sim-CLIP+ encoder achieves superior results by reducing the ASR to 15.0% for the LLaVA (Llama-2-13B) model and 19.3% for the LLaVA (Vicuna-7B) model. This significant reduction in ASR is accomplished solely by replacing the original CLIP encoder with our robust Sim-CLIP+ encoder, without the need for any external defenses. This performance highlights Sim-CLIP's effectiveness as a robust solution against the ImgJP attack while maintaining operational efficiency. Notably, the LLaVA (Llama-2-13B) model exhibits an overall decreased ASR compared to the Vicuna-7B variant. This improved resilience is likely due to extensive alignment through instruction tuning and iterative RLHF on high-quality red-teaming data, which enhances its defenses against jailbreak attacks. Fig. 3 illustrates examples of jailbreak attempts with ImgJP and VisualAdv attacks on LLaVA models using both the original vision encoder and the robust Sim-CLIP+ encoder.",
            "score": 0.17320028973200616,
            "section_title": "F. Result Analysis",
            "char_start_offset": 28449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035491943359375
        },
        {
            "corpus_id": "269354784",
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "text": "The RS-LLaVA model is based on the architecture of the LLaVA model [15].In our experiments, we explore two variants of pre-trained Vicuna-v1.5 [60] LLM variants, ranging in size from 7B to 13B, to initialize the language model for RS-LLaVA.Vicuna 1.5 is an open-source large language model developed by LMSYS.It is a fine-tuned version of the Llama 2 model, trained on user conversations collected from ShareGPT.Vicuna is licensed under the Llama 2 Community License Agreement.For image encoding, the model adopts the pre-trained vision backbone of CLIP-ViT (large) [46], which utilizes an image resolution of 336 \u00d7 336.\n\nTo facilitate fine-tuning, we employ LoRA with a rank (r) set to 64 and \u03b1 set to 16 as suggested by the original paper.We utilize the Adam optimizer, with a learning rate of 1 \u00d7 10 \u22124 .Figures 3-5",
            "score": 0.1731723902872147,
            "section_title": "Experimental Settings",
            "char_start_offset": 25633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 72,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 620
                },
                {
                    "start": 622,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 818
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039337158203125
        },
        {
            "corpus_id": "264172752",
            "title": "UNK-VQA: A Dataset and a Probe Into the Abstention Ability of Multi-Modal Large Models",
            "text": "LLaVA [16] represents a novel end-to-end trained large multi-modal model that combines a vision encoder and an LLM for general-purpose visual and language understanding. It connects the visual patch-level features from CLIP [21] and LLM with a simple linear layer, and is fine-tuned on a multi-modal instruction tuning dataset. In our experiment, we utilized a more advanced LLaVA-1.6 model and performed supervised fine-tuning on our UNK-VQA dataset. Specifically, we appended a prompt to each question -Answer the question using a single word or phrase. If you feel you cannot answer this question, simply reply \"Unanswerable\". Regarding the LLM, we selected both a weaker Vicuna-13B model and a recent strong Mistral-7B model. Experimental Results. From the results in Table VIII, we can observe that: I) The LLaVA models exhibit a significant performance advantage over traditional models, as demonstrated in Table VI, owing to their pre-training on more extensive datasets. II) Both LLaVA models, utilizing different LLMs, show substantial improvement through supervised fine-tuning. III) Despite its smaller model size, the stronger Mistral-7B LLM notably outperforms the Vicuna-13B model. This highlights the pivotal role of LLM capability in advancing multi-modal large models in UNK-VQA.",
            "score": 0.17312357959788227,
            "section_title": "B. Supervised Fine-tuning of LLaVA",
            "char_start_offset": 33545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "277452745",
            "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference",
            "text": "Models. The selection of LVLMs is primarily based on two criteria: model architecture and parameter size. In terms of architecture, the objective is to validate the effectiveness of the proposed method across various architectures, including LLaVA-OV-7B [16], InternVL2-8B [6], and Qwen2-VL-7B [35]. These models exhibit significant differences in visual token encoding, ViT, and LLM. For example, both LLaVA-OV-7B [16] and InternVL2-8B [6] utilize the AnyRes strategy with distinct ViT to increase the number of visual tokens, while Qwen2-VL-7B [35] supports native resolution with a large quantity of visual tokens. In terms of parameter size, models of InternVL2 series are selected for their diverse range of parameter scales, including InternVL2-1B [6], InternVL2-4B [6], and InternVL2-26B [6]. Due to space constraints, results for this section are provided in the appendix. Additionally, LLaVA-Video-7B-Qwen2 [41] serves as the base model for the video tasks evaluation. \n\nObtain the index of key text tokens k using Eq. ( 5). \n\n5: \n\nAvt k [j, : Nv] \u25b7 Importance Scores 8: \n\nDatasets. The performance is primarily evaluated across four types of datasets: 1) Visual Question Answering (VQA) datasets, which include ChatQA [26], InfoVQA [27], DocVQA [28], and TextVQA [31]. Since KV cache compression mainly affects the decoding stage, VQA datasets that require longer output tokens are preferred for evaluating the method's efficacy. The similarity between generated and reference answers is typically measured by the Average Normalized Levenshtein Similarity (ANLS) metric [2]. 2) Multiple-choice question datasets, such as MMBench-EN [24], involves selecting the correct option from several choices, with accuracy serving as the ultimate metric.",
            "score": 0.17310366174088568,
            "section_title": "Setup",
            "char_start_offset": 16025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 1255,
                    "end": 1259,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 1273,
                    "end": 1277,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "173188651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039703369140625
        },
        {
            "corpus_id": "255126112",
            "title": "An allelic-series rare-variant association test for candidate-gene discovery",
            "text": "variants affect the phenotype in the same direction. The SKAT 15 was introduced to allow variants to have differing directions of effect. In brief, SKAT posits that each variant G j has a separate effect b j and that these effects are drawn at random from an unspecified distribution with mean zero and variance w 2 SKAT;j t 2 , where t 2 is a common variance component. We evaluate the null hypothesis of no association by testing H 0 : t 2 \u00bc 0. To develop an allelic-series test that allows rare variants to have differing directions of effect, we build on the existing SKAT framework but modify the SKAT weights to incorporate the allelic-series weights:\n\nUnder Equation 4, the variance of the distribution from which b j is drawn, and thus the expected magnitude of effect, is directly proportional to w l and inversely proportional to MAF j . In the allelic-series setting where w l increases monotonically from BMVs to DMVs to PTVs, this encodes the expectation that rarer and more deleterious variants will have larger effects on the phenotype.",
            "score": 0.1729095216384453,
            "section_title": "Baseline model",
            "char_start_offset": 12320,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0074615478515625
        },
        {
            "corpus_id": "277103933",
            "title": "Growing a Twig to Accelerate Large Vision-Language Models",
            "text": "As mentioned above, the inference process consists of the prefilling and decoding stages. FastV and other tokenpruning methods primarily accelerate the prefilling stage. We ask: Can these methods also facilitate the acceleration of the decoding stage, especially when the generated response is relatively long? This is a practical question since VLMs are often expected to generate long responses (e.g., 30 tokens) in real-world scenarios. \n\nTo answer this question, we conduct the pilot experiment as follows. We take LLaVA-1.5-7B [30] as the base VLM and compute its time spent on the prefilling and decoding stages for the response length S ranging from 2 to 128. The results in Figure 3a show a linear increase in decoding time w.r.t. the response length S, while the prefilling time can be neglected when S \u226532. \n\nNext, we compare the prefilling and decoding times of LLaVA-1.5-7B and its FastV-based variant (LLaVA+FastV) when generating responses of the same length S=32. From Figure 3b, we observe that although FastV effectively reduce the prefilling time, it attains limited speedup in the decoding stage. This frustrating observation can be explained by two facts: (i) the KV-cache mechanism undermines the speedup for SA blocks achieved by FastV, and (ii) FastV does not accelerate the FFN blocks during decoding, but they account for the majority of the computational costs. \n\nTo summarize, these observations highlight the significance and necessity of improving the decoding efficiency for long response generation.",
            "score": 0.17262527050038173,
            "section_title": "Study 2: Prefilling and Decoding Time Costs",
            "char_start_offset": 10453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1530
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030731201171875
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "To evaluate the consistency between LLaVA-Critic and GPT-4o [32] in pointwise scoring across different evaluation scenarios, as described in Sec. 4.2, we select 7 popular multimodal benchmarks and collect candidate responses from 13 commonly used LMMs alongside their GPT-4o evaluations, resulting in a total of 14174 examples (see details in Appendix B.2). LLaVA-Critic is then tasked with providing judgments on theses samples. We report Pearson correlation to measure the degree of alignment with GPT-4o in instance-level scoring. \n\nWe conduct experiments based on three different baseline models: LLaVA-NeXT (LLaMA-8B) [15,26], LLaVA-OneVision-7B, and LLaVA-OneVision-72B. As shown in Table 2, LLaVA-Critic variants significantly outperform their corresponding baseline models across all models and benchmarks. (i) Data scaling. By comparing the performance between v0.5 and full data trained LLaVA-Critic-7B, it concludes the necessity of larger size and diversity of instruction in training data. (ii) Model scaling. The best performance in terms of Pearson-r is achieved by LLaVA-Critic-72B with an average score of 0.754, which significantly outperforms the LLaVA-OV-72B baseline (0.634). This indicates that LLaVA-Critic-72B already possesses pointwise scoring capabilities that are quite aligned with GPT-4o. Despite a considerable reduction in model size, LLaVA-Critic-7B retains very strong point-wise scoring capabilities. With a score of 0.732, its shows minimal performance decline compared to LLaVA-Critic-72B, and significantly outperforms other advanced open-source LMMs of similar size, such as Qwen2-VL (0.352) and LLaMA3.2-Vision (0.359). This presents an advantage for deploying and utilizing LLaVA-Critic in resource-constrained environments. We also provide Kendall's Tau results in Appendix C.2 to assess model-level ranking, which reveal similar patterns and conclusions.",
            "score": 0.17252446181418424,
            "section_title": "In-domain Pointwise Scoring",
            "char_start_offset": 20669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.083740234375
        },
        {
            "corpus_id": "275405668",
            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
            "text": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
            "score": 0.17250744451380784,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 6442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296142578125
        },
        {
            "corpus_id": "269502751",
            "title": "Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis",
            "text": "LLaVA.LLaVA [3], designed as a multimodal visual language assistant, is adept at following instructions that integrate visual and textual elements.It operates as a conversational agent, engaging users with interactions that closely mimic human dialogue.By incorporating a vision encoder, LLaVA gains the capability to interpret images, enabling it to deliver precise responses to a variety of vision-language challenges.This VLM harnesses the strengths of both the pretrained CLIP [32] for visual encoding and Llama-2 [22] for linguistic processing, connecting the two with a projection layer that translates visual data into language tokens.These tokens, representing both image and text, are then processed by the language model.As a versatile chat assistant, LLaVA is equipped to handle a series of related questions and answers.Employing a 'chain of thought' [33] approach, which breaks down complex queries into simpler sub-questions, further augments the model's capabilities.SAM.Segment Anything Model [4], or SAM (by Meta AI), is a promptable segmentation model that can generate 'cutouts' from any image in accordance with a point, box, or text prompt provided.SAM claims to be capable of zeroshot generalization to unseen objects and images without the requirement of any additional model training, which is why it has been employed to process microscopy images in this study.While using SAM, it is possible to let SAM run in the 'automatic' mode, with no prompts required from the user.In this mode, a pre-specified grid of points defined in SAM's initialization parameters serve as segmentation prompts.The parameter points_per_side defines the grid size in pixels.SAM comes with a standard set of parameters (see Table II), hereon referred to as SAMstandard.Please refer to SAM's documentation for a complete list of parameters and their functions.For certain tasks, some of these parameters were modified (shown in bold in Table II) and are referred to as SAM-custom.",
            "score": 0.17243635921283068,
            "section_title": "IV. VISION LANGUAGE MODELS USED",
            "char_start_offset": 10989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 6,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1615
                },
                {
                    "start": 1615,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1861
                },
                {
                    "start": 1861,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1009,
                    "end": 1012,
                    "matchedPaperCorpusId": "257952310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049774169921875
        },
        {
            "corpus_id": "266755915",
            "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model",
            "text": "Solve the math equation in the image. User LLaVA-1.5 -13B \n\nThe math equation in the image involves adding two numbers, 6 and 22, and then multiplying the result by 2. To solve this problem, we first add 6 and 22, which gives us 28. Then, we multiply 28 by 2, which results in 56. Therefore, the answer to the math problem is 56. \n\nLLaVA-Phi The answer to the math equation in the image is 1. \n\nFigure 3. LLaVA-Phi is capable of performing accurate OCR on mathematical equations and solving them correspondingly.. Fine-Tuning (SFT) data, we utilized ShareGPT from an open-source platform. The training was conducted over two epochs, beginning with an initial learning rate of 3e-5, which was linearly decreased over time. Our findings suggest that while this step might be optional, applying SFT to Phi-2 does result in modest improvements across most benchmarks. \n\nTraining LLaVA-Phi. Our training approach follows the pipeline used for LLaVA1.5, consisting of a pretraining stage and a subsequent instruction tuning phase. \n\nInitially, we kept the vision encoder and Phi-2 static, focusing exclusively on training the efficient projector. This step is followed by a comprehensive fine-tuning of both the projector and the language model (LLM), aiming to enhance their capabilities in visual comprehension and language processing. \n\nFor pre-training, we utilize a filtered subset of the CC-595K dataset [24] over one epoch, applying an initial learning rate of 1e-3 and a batch size of 256. Then, we finetune the model on LLaVA-Instruct-150K dataset for 1 epoch at a learning rate of 2e-5 and a batch size of 256. We implement a weight decay of 0.1 and utilize the Adam optimizer, characterized by momentum parameters of 0.9 and 0.98, and an epsilon value of 1e-7. We fine-tune all parameters in LLM instead of using LoRA. \n\nComputational Cost. Similar to LLaVA1.5, our training process is structured in two stages.",
            "score": 0.17216846195696195,
            "section_title": "Math",
            "char_start_offset": 6240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 57
                },
                {
                    "start": 60,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1916
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06683349609375
        },
        {
            "corpus_id": "9190971",
            "title": "Accurate prediction of functional effects for variants by combining gradient tree boosting with optimal neighborhood properties",
            "text": "The variants R614W (dbSNP:rs35785446) and P688L (dbSNP: rs17293705) in LSS are neutral substitutions. From Fig 7B , we can see that PredSAV can predict the neutral SAVs successfully, while other existing methods result in almost completely wrong results (except Suspect in P688L). This suggests that PredSAV has the highest  specificity, which is desirable for many biological applications since it allows researchers to identify a short list of SAVs for targeted phenotype studies.",
            "score": 0.17204719792120968,
            "section_title": "Case study",
            "char_start_offset": 22920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 482
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002590179443359375
        },
        {
            "corpus_id": "267759615",
            "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
            "text": "For LLaVA, we use the LLaVA v1.0 7B model. The language backbone is Vicuna-7B-v1.3, and the vision encoder is clip-vit-large-patch14. For MiniGPT-4, we use Vicuna V0 7B as the language model.",
            "score": 0.17204719792120968,
            "section_title": "B Vision-Language Model Version",
            "char_start_offset": 31050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01214599609375
        },
        {
            "corpus_id": "270737945",
            "title": "Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts",
            "text": "MiniCPM-V (8B) LLaVA-1.6 (34B) Llama-3.2-V (11B) Model -0.004 0.0041 0.034 0.037 -0.018 0.17 -0.074 -0.14 -0.17 -0.34 -0.025 0.083 -0.03 0.036 -0.0075 -0.017 0.051 -0.023 0.14 -0.066 -0.049 -0.18 -0.37 -0.1 -0.039 -0.035 -0.041 -0.15 0.0097 0.11 -0.11 0.16 -0.1 -0.13 -0.12 -0.068 0.018 0.028 -0.17 -0.0087 -0.087 0.064 0.092 0.052 0.065 -0.15 -0.31 -0.19 -0.22 -0.11 -0.062 -0.058 -0.084 -0.3 0.029 0.088 0.044 0.064 -0.14 -0.13 -0.2 -0.061 -0.097 -0.075 -0.089 -0.015 0.072 0.086 0.074 -0.003 0.075 -0.1 -0.24 -0.24 -0.33 -0.064 -0.0035 -0.069 0.037 -0.015 0.0086 0.082 0.0032 0.097 -0.085 -0.13 0.018 -0.24 -0. GPT-4o Gimini 1.5 Pro LLaVA-1.5 (7B) LLaVA-1.5 (13B) ShareGPT4V (7B) ShareGPT4V (13B)",
            "score": 0.17204719792120968,
            "section_title": "Experiments",
            "char_start_offset": 13167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 699
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035800933837890625
        },
        {
            "corpus_id": "270702315",
            "title": "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks",
            "text": "To expand further on our findings from fine-tuning Video-LLaVA's multiple modalities on distinct tasks from the target task, we delve deeper into the specific outcomes observed through our experiments.Our investigation centered on comparing the quality of generated step-by-step recipes of baseline Video-LLaVA-7B and fine-tuned Video-LLaVA-7B on each modality for the domain of recipe generation.Our experimental results demonstrate that our method of finetuning Video-LLaVA leads to significant boosts in generating precise recipe instructions over general-purpose trained Video-LLaVA.This improvement is quantitatively evident when examining the accuracy comparisons of fine-tuned Video-LLaVA and baseline Video-LLaVA on the constructed YouCook2 datasets in Table 1.The model subjected to finetuning performed with higher accuracy and received an overall higher average score over the baseline Video-LLaVA model.\n\nOne observation we made that could explain this finding was that baseline Video-LLaVA would tend to list off significantly more ingredients than actually needed in the ingredient listing portion of the recipe.fine-tuned Video-LLaVA tended to produce more concise recipe steps than baseline Video-LLaVA, whose responses were often more lengthy.\n\nThere is room for more robust evaluation through employing human evaluation on a subset of responses from both fine-tuned Video-LLaVA and baseline Video-LLaVA.We can gather a group of people to blindly judge between the recipes generated by both models which response is more accurate to a provided ground truth for a subset of responses produced by both models.Additionally, we can also break down the evaluation into 2 stages: One, where the ingredient listings with their specific measurements are first compared, and two, where the recipe instructions are solely compared.We further investigated the validity of responses from Video-LLaVA in the temporal understanding of video content.Specifically, we explored Video-LLaVA's ability to consider prompts about time stamps.We were curious about the model's ability to ground generated steps factual timestamps from the input video to validate the proposed instructions on specific video sections.\n\nInvoking the model to answer in response to timestamps requires the model to interpret timestep tokens for further specific embeddings.Timestamps can be considered an additional modality of understanding for models.",
            "score": 0.17186798197842068,
            "section_title": "Results Analysis",
            "char_start_offset": 11308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 915
                },
                {
                    "start": 917,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1260
                },
                {
                    "start": 1262,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1838
                },
                {
                    "start": 1838,
                    "end": 1952
                },
                {
                    "start": 1952,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2211
                },
                {
                    "start": 2213,
                    "end": 2348
                },
                {
                    "start": 2348,
                    "end": 2428
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "276580600",
            "title": "Can Multimodal LLMs Perform Time Series Anomaly Detection?",
            "text": "The model is optimized for instruction-following tasks, ensuring that it can handle multimodal inputs without compromising performance. Response : [3, 9, 12, 35, 46, 81, 83, 111, 113, 136, 137, 149, 162, 212, 217, 248, 270, 284, 351, 368] Prompt for Range-Wise Time Series Anomalies  , 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, . . . , 999] Hallucination of LLaVA-NeXT-8B for 36-Variates Time Series Anomalies Detection (Random)  , 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, . . . , 995, 996, 997]",
            "score": 0.17178378568155916,
            "section_title": "Impact Statement",
            "char_start_offset": 23416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0041656494140625
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Consistent Performers The InternVL-2.5 series consistently outperforms other models, achieving strong F1 (overall) scores across all tampering types. Notably, even smaller variants like InternVL-2.5-4B match the robustness of larger models, underscoring the efficiency of its architecture and training strategy. Effect-Specific Strengths Models such as Phi3.5-Vision excel in detecting Masking tampering, highlighting specialized capabilities for handling visual obfuscations. Similarly, the VILA1.5-40B, de-signed for long-form video understanding, excels in spatial-temporal tasks. Weaker Models Certain models, including Llava-OneVision variants, exhibit consistent weaknesses, particularly with temporal disruptions like Dropping and Repetition, suggesting limitations in their architectural designs and training paradigms. Tampering Insights Dropping and Repetition emerge as the most challenging tampering types for all model categories, reflecting the difficulty of maintaining temporal coherence under such manipulations. In contrast, Masking is relatively less disruptive, particularly for tasks relying on contextual cues.",
            "score": 0.1712566013299866,
            "section_title": "Key Findings",
            "char_start_offset": 32587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1132
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "266573523",
            "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
            "text": "Following the design in LaViLa (Zhao et al., 2023), we use a batch size of 256 and a learning rate of 3 \u00d7 10 \u22125 . We use AdamW optimizer (Kingma and Ba, 2014) with (\u03b2 1 , \u03b2 2 ) = (0.9, 0.999) and weight decay 0.01. We train the model on 8 NVIDIA RTX 3090 GPUs for 5 epochs. \n\nNarrating video clips. We use nucleus sampling (Holtzman et al., 2019) with p = 0.95 and return K = 5 candidate outputs. Then we take the narration with the largest confidence score as the final caption of the video clip. For NExT-QA, we explore CogAgent and LLaVA-1.5 as the visual captioner. For Inten-tQA and NExT-GQA datasets, we use CogAgent as the visual captioner because of its good performance on NExT-QA. Specifically, we use the liuhaotian/llava-v1.5-7b LLaVA-1.5 variant from Huggingface with the prompt \"USER: <image>. Describe the image. ASSISTANT: \", and the THUDM/cogagent-chat-hf CogAgent variant with the prompt \"<image>. Describe the image.\".",
            "score": 0.1708031973676521,
            "section_title": "C Additional Implementation Details C.1 Captioners",
            "char_start_offset": 38610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 937
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 50,
                    "matchedPaperCorpusId": "254408789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01238250732421875
        },
        {
            "corpus_id": "276317635",
            "title": "EmoAssist: Emotional Assistant for Visual Impairment Community",
            "text": "We used the EmoAssist Benchmark as the evaluation metric, as it comprehensively measures the assistive capabilities of LMMs for the VI community. \n\nWe selected currently popular open-source and closedsource LMMs as comparison models, including LLaVA-1.6, InternVL2.5 [40], Qwen2-VL [41], and the GPT series LMMs. For open source models, we also conducted experiments with different parameter sizes (ranging from 2B to 34B). We selected LLaVA-1.6-7B, LLaVA-1.6-34B, and GPT-4o as baseline LMMs, as the EmoAssist Model is fine-tuned on top of LLaVA, and GPT-4o is currently recognized as the stateof-the-art LMM. \n\nD. Performance on VI Assistive VQA 1) Qualitative Results: Figure 4 presents the performance of EmoAssist compared with three baseline LMMs. The results demonstrate that the EmoAssist Model's responses exhibit two distinct advantages. First, the EmoAssist Model provides accurate and concise answers, whereas LLaVA-7B introduces hallucinated content along with excessive redundant information. Second, the EmoAssist Model demonstrates a high degree of emotional intelligence by effectively recognizing both the user's implicit emotions and intentions-for example, identifying the user's desire for \"something sweet\"-and responding with empathy and warmth. In contrast, LLaVA-34B and GPT4o, while capable of accurately conveying information, fail to exhibit the same level of emotional intelligence as the EmoAssist Model. \n\n2) Quantitative Results: As presented in Table II, the EmoAssist Model outperforms all comparison LMMs across multiple automatic evaluation metrics, including BERT Score, ROUGE-1, ROUGE-2, and ROUGE-L. Specifically, the EmoAssist Model achieves a BERT Score of 0.61, which is 0.03 higher than the best-performing LMM. It also attains a ROUGE-1 score of 0.39, representing a 0.03 improvement over the top-performing baseline model.",
            "score": 0.1705959804933888,
            "section_title": "C. Evaluation Metric and Baselines",
            "char_start_offset": 18760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "239999732",
            "title": "Data-driven analysis of amino acid change dynamics timely reveals SARS-CoV-2 variant emergence",
            "text": "At each time t, we retain the n(t) time-series of change prevalence (current ratio between change counts and total counts of changes) that are observed continuously over a time interval of four weeks prior to t \n\nwith w = 5 ) and partition them via k-medoids clustering 44 (PAM algorithm, kmedoids function in MATLAB), with pairwise distances between time-series being evaluated via dynamic time warping 45 (dtw in MATLAB). \n\nThe optimal value of k is exhaustively searched over the range [1 . . . min(20, n(t)/4)] and is selected as the one that maximizes the average silhouette score 46 evaluated over the entire set of n(t) change prevalence time-series. \n\nOur early-warning system for variant emergence. To identify clusters of changes characterized by an increasing trend in their prevalence time-series, as expected in the case of emerging variants, we use the non-parametric Kendall's \u03c4 B statistic 47 , as implemented in the ktaub MATLAB package 48 , namely to evaluate the ordinal association between change prevalence and sampling time. More precisely, a cluster of changes is considered to deserve further attention as a candidate for variant emergence if its average prevalence time-series shows a positive trend ( \u03c4 B > 0 at significance level \u03b1 = 0.05 ). Among these candidate clusters, we are particu- larly interested in those that are sufficiently different from previously observed trending clusters, because they could signal the emergence of a new virus variant. Indeed, the partitioning of change time-series is performed independently at each time step, thus the clusters identified at a given time are in principle not related to those identified at any previous step. However, because of the temporal autocorrelation of change prevalence dynamics, similarities are expected (and found) to exist in the composition of clusters identified at subsequent time steps. To quantify similarity for each pair of clusters, we use the classical Jaccard index 49 , defined as the ratio J cc between the cardinality of the intersection and the cardinality of the union of the sets of changes constituting the two clusters.",
            "score": 0.17052337938446344,
            "section_title": "Temporal aggregation of sequence mutations. Original collection dates were temporally binned into",
            "char_start_offset": 20874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 406,
                    "matchedPaperCorpusId": "17900407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0036067962646484375
        },
        {
            "corpus_id": "272690013",
            "title": "Towards Semantic Versioning of Open Pre-trained Language Model Releases on Hugging Face",
            "text": "We acknowledge that model cards may sometimes contain the necessary keywords; however, not all models have model cards, and accessing them for some repositories requires manual authentication and waiting time, reducing our sample size. For example, accessing meta-llama/Llama-2-7b-chat 44 and meta-llama/Llama-2-7b 45 requires manual authentication and waiting time to be granted access by the owner to these PTLMs. \n\nFor our manual analysis, as it is impractical to analyze 52,227 models, we focused on a statistically significant sample of 384 models (confidence level of 95% percent and margin of error 5%). For the selected samples, we manually explore different aspects of the models by browsing each model's repository one by one to identify an indications of variant types. First, we focused on the model names, as many models have indications, such as \"finetuned,\" directly in their name segments. Second, we explored the configuration files to determine whether variant-related elements are present. However, we found that 2% of the studied PTLMs lacked configuration files, such as Sosaka/Alpaca-native-4bit-ggml and Skaczmarj/resnet50-truncated.tv in1k, making it impossible to locate this information from them. Third, we explored the tags of each model repository. While some models, like 01-ai/Yi-34B-Chat-4bits, specified a keyword (4bits) in their tags, others, like upstage/SOLAR-0-70b-8bit, did not. Subsequent to the manual analysis, we automatically collected these elements from the segments of the model names using a Python script (Replication, 2024). \n\nTo determine the prevalence of model reproducibility in terms of variant types, we calculated the distribution of PTLMs across variant types. This reproducibility characteristic allows us to gain insights into which variant types are more prevalent on HF, thereby helping us understand the common practices in specifying model variants.",
            "score": 0.17041226490795736,
            "section_title": "Approach",
            "char_start_offset": 64822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0008726119995117188
        },
        {
            "corpus_id": "266348439",
            "title": "Silkie: Preference Distillation for Large Visual Language Models",
            "text": "The response is highly faithful, accurately reflecting the image content. \n\nTable 2: Visual faithfulness assessment annotation guideline for GPT-4V model. \n\n\u2022 GPT-4V (OpenAI, 2023a), the proprietary vision language models developed by OpenAI, which are shown to be powerful on various multi-modal tasks (Yang et al., 2023). \n\n\u2022 LLaVA-series models, which adopt Vicuna models as the backbone and are trained on the GPT-4 (text-only) synthesized multi-modal dataset. We select the enhanced version LLaVA-v1.5-7B and LLaVA-v1.5-13B (Liu et al., 2023b), and the RLHF version with visual faithfulness alignment, LLaVA-RLHF (Sun et al., 2023) with different image resolutions LLaVA-RLHF-7b-v1.5-224 and LLaVA-RLHF-13b-v1.5-336. \n\n\u2022 Qwen-VL-Chat (Bai et al., 2023), which show promising capabilities on various visionlanguage benchmarks with scaled-up multi-modal pre-training and supervised fine-tuning on curated datasets. \n\n\u2022 IDEFICS-9b-Instruct (Laurenc \u00b8on et al., 2023), which is a open-sourced implementation of Flamingo (Alayrac et al., 2022), supporting interleaved image-text inputs. After training on publicly available image-text alignment pairs and instruction tuning datasets, it demonstrates comparable results with the original closed-source model on various image-text benchmarks. \n\n\u2022 Fuyu-8B (Bavishi et al., 2023), which introduces a novel architecture by segmenting images into patches and training a conditional language model from scratch, showcasing the great potential to deal with high-resolution images. \n\n\u2022 InstructBLIP (Dai et al., 2023), which employs an instruction-aware visual feature extraction module based on BLIP2 (Li et al., 2023b).",
            "score": 0.17016787081270623,
            "section_title": "Highly Faithful:",
            "char_start_offset": 8601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 154
                },
                {
                    "start": 157,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1662
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 757,
                    "matchedPaperCorpusId": "263875678"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07666015625
        },
        {
            "corpus_id": "270559956",
            "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
            "text": "Table 1 shows resource usage during inference for LLaVA and LLaVA-NeXT models, evaluated on the DocVQA and ChartQA test sets (Mathew et al., 2021;Masry et al., 2022). The LLaVA-1.5 models demonstrate manageable computational costs, operable on a single V100 GPU. However, LLaVA-NeXT models, with up to 2.9K tokens, present significant challenges. Testing on an NVIDIA V100 32GB reveals that LLaVA-NeXT-13/34B cannot be accommodated on a single GPU. These findings emphasize the challenges of larger models, especially in resource-constrained environments.",
            "score": 0.17009772772092815,
            "section_title": "Benchmarking Baseline Models",
            "char_start_offset": 7105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 555
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 146,
                    "end": 165,
                    "matchedPaperCorpusId": "247593713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02423095703125
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A blurry picture of a cabinet. \n\nSim-CLIP 4 : A blurry picture of a person. \n\nRobust-LLaVA 4 G: A bathroom with a toilet and a shelf.",
            "score": 0.17009772772092815,
            "section_title": "Fog",
            "char_start_offset": 56543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 66,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035648345947265625
        },
        {
            "corpus_id": "277634362",
            "title": "Are We Done with Object-Centric Learning?",
            "text": "MiniGPT-4 [82] 71.8 LLaVa [37] 52.9 LLaVa-NeXT [39] 68.8 LLaVa-1.5 [38] 73.3 \u22c6 (b) UrbanCars [35] Method WGA (\u2191) \n\nViT-L- 100.0",
            "score": 0.17009772772092815,
            "section_title": "Multi-modal LLMs",
            "char_start_offset": 21275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 127
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "254536007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0017957687377929688
        },
        {
            "corpus_id": "268510101",
            "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
            "text": "To evaluate the effectiveness of our methods, we evaluate LLaVA-1.5 and LLaVA-1.5 with contrastive harmlessness LoRA on the +Opt image and +Adv image setting of HADES. Besides, we also evaluate these models on LLaVA-Bench to discuss the influence of contrastive harmlessness LoRA on the general multimodal abilities of MLLMs. \n\nThe evaluation results, detailed in Tab. 3 reveal that our contrastive harmlessness LoRA remarkably reduces the ASR of LLaVA-1.5. Specifically, its average ASR decreased from 79.20% to 6.67% in HADES opt and from 89.53% to 5.07% in HADES adv . Moreover, contrastive harmlessness LoRA doesn't significantly impact LLaVA-1.5's performance on LLaVA-Bench. The results suggest that finetuning MLLMs with image-related alignment data can significantly enhance their harmlessness alignment performance, while not influence other multimodal abilities.",
            "score": 0.16980003429753504,
            "section_title": "A.2 Results and Analysis",
            "char_start_offset": 33641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 872
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01374053955078125
        },
        {
            "corpus_id": "262824780",
            "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
            "text": "We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation metrics for their high alignment with human preferences. In addition, we conducted tests on widelyrecognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiplechoice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across varying instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. \n\nHigh-quality SFT data is crucial for capability benchmarks. By delving into the specific performances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable improvement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT + ) in Tables 4 and 7. LLaVA-SFT + 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it's worth noting that LLaVA-SFT + does LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT + by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA-RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. \n\nRLHF improves human alignment benchmarks further.",
            "score": 0.1694897337100758,
            "section_title": "RESULTS",
            "char_start_offset": 19541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1743
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03228759765625
        },
        {
            "corpus_id": "230799367",
            "title": "Statistical Challenges in the Analysis of Sequence and Structure Data for the COVID-19 Spike Protein",
            "text": "A rapid increase in D614G was observed in many regions after its initial appearance, which suggested fitness advantages and the hypothesis that variants with D614G are likely more infectious (Korber et al., 2020). This was later corroborated by experimental evidence that D614G, either by itself or in conjunction with other mutations, is significantly more infectious than the reference S-protein sequence (Li et al., 2020). Overall, the continued evolution of the virus has resulted in thousands of distinct S-protein sequence variants recorded in GISAID, although many of these only differ by a few mutated sequence positions. While clinical or laboratory experiments can test the infectivity of specific mutations, it is challenging to analyze large numbers of sequence variants. \n\nComputational researchers have thus used clustering as a means to gain interpretable insight into the effect of S-protein mutations across different geographical regions. Temporal changes in the prevalence of S-protein mutations have also been studied in related research. For instance, Chen et al. (2020b) clustered mutations occurring in the receptor binding domain (RBD) of the S-protein and studied binding affinity changes for each cluster. Based on common amino acid mutations, Toyoshima et al. (2020) classified 28 countries into three clusters and studied correlations between fatality rate and S-protein D614G variants. In addition, the hypotheses of monotonic trends for D614G and various other mutations have been tested using isotonic regression by Korber et al. (2020) and in the COVID-19 pipelines of the Los Alamos National Laboratory (https://cov.lanl.gov). However, to the best of our knowledge, few authors have built comprehensive statistical models for the evolution of S-protein mutant clusters (i.e., groups of closely related sequence variants) over space and time. Such models can have an important practical value in providing forecasts and early warnings for countries where S-protein sequence variants with potential fitness advantages or higher infectiousness are actively being transmitted. To that end, this paper presents one such Bayesian hierarchical model for multinomial time series that can help tackle this problem.",
            "score": 0.16937162691502103,
            "section_title": "Introduction",
            "char_start_offset": 3548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 212,
                    "matchedPaperCorpusId": "220304292"
                },
                {
                    "start": 407,
                    "end": 424,
                    "matchedPaperCorpusId": "220575732"
                },
                {
                    "start": 1073,
                    "end": 1092,
                    "matchedPaperCorpusId": "219124491"
                },
                {
                    "start": 1270,
                    "end": 1293,
                    "matchedPaperCorpusId": "220696178"
                },
                {
                    "start": 1547,
                    "end": 1567,
                    "matchedPaperCorpusId": "220304292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016231536865234375
        },
        {
            "corpus_id": "274981546",
            "title": "SAIL: Sample-Centric In-Context Learning for Document Information Extraction",
            "text": "Baselines. Recent years have witnessed the rapid development of multi-modal LLMs (MLLMs) represented by GPT-4o (OpenAI 2023b). To further validate the effectiveness of our method, we also compare our SAIL with MLLMs including open-source LLaVA-1.5 (Liu et al. 2024a) and proprietary GPT-4o. We provide these MLLMs with explicit and detailed instructions to inform the task definition. \n\nQuantitative results are provided in Table 2. The opensource LLaVA exhibits limited DIE capabilities, resulting in a low F1 score (e.g., 0.7% in FUNSD). The proprietary GPT-4o significantly outperforms LLaVA (50.72% vs 0.7% in FUNSD), yet still falls short when compared to specialized DIE methods. Therefore, despite their rapid evolution, MLLMs still underperform in the DIE task, highlighting the importance and contribution of our proposed work.",
            "score": 0.16924834795744764,
            "section_title": "Comparison with Multi-modal LLMs",
            "char_start_offset": 16268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 836
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 265,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0252227783203125
        },
        {
            "corpus_id": "271909100",
            "title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs",
            "text": "Network architecture: In this work, we consider two LVLMs architectures: MobileVLM-v2 [14] and LLaVA-1.5 [42]. Both models follow the same overall structure: a pre-trained CLIP vision encoder and a pre-trained LLM. The visual tokens produced by the frozen vision encoder are projected using either a linear layer or a small projection module and passed as input to the LLM. \n\nLLaVA opts for a pre-trained Vicuna LLM [12] while MobileVLM-v2 uses Mo-bileLLaMA [13], except for their 7B variant, which also uses a Vicuna model. Both use the same ViT-L-14 @ 336px CLIP visual encoder [56]. For efficiency purposes, MobileVLM-v2 uses a projection module that halves the number of visual tokens. We consider the following model variants in our comparisons: MobileVLM-v2 (1.7B, 3B and 7B), and LLaVA-1.5 7B. For data annotation, we use ViT-H/14 DFN [19] as our pre-trained CLIP model. \n\nTraining details: For all of our experiments, unless specified otherwise, we start from the pre-trained MobileVLM-v2 and LLaVA-1.5 models. The models are then fine-tuned for 1 epoch using DPO-based optimization on the constructed CLIP-DPO dataset. We use the following hyperparameters for fine-tuning the models: AdamW optimizer [45] with a batch size of 256, a learning rate of 5e \u2212 7, decreased to 0 using a cosine scheduler, a warm-up of 0.01, and a weight decay set to 0. Both during training and testing, the input images are cropped and resized to 336 \u00d7 336px. The training was performed on 8 A100 GPUs using Pytorch [54]. For the larger models, e.g. LLaVA-1.5 7B, to fit them in memory, we use the Zero-3 strategy [58,59] and decrease the batch size to 64.",
            "score": 0.16923968124899885,
            "section_title": "Implementation details",
            "char_start_offset": 20514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1643
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "233289729"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "221191193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042816162109375
        },
        {
            "corpus_id": "270095096",
            "title": "Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment",
            "text": "Furthermore, we observe that our approach is more effective in mitigating hallucination than existing RLHF, SFT, or DPO-based methods. For example, HALVA 7B achieves a score of 2.25 surpassing the 7B variants of RLHF, DPO, and SFT -based methods, which report scores of 2.05, 1.97, and 1.76, respectively. Moreover, HALVA 13B reduces the hallucination rate to 0.45, compared to 0.57 for LLaVA-RLHF. Note that as LLaVA-RLHF and LLaVA-SFT use the same language and vision encoders as HALVA (Vicuna-V1.5 and ViT-L/14), ensuring a fair direct comparison. The detailed results for the individual categories are presented in Appendix C.",
            "score": 0.16918484141153917,
            "section_title": "Method MME-Hall (\u2191)",
            "char_start_offset": 22249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 630
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0186920166015625
        },
        {
            "corpus_id": "267616939",
            "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
            "text": "We use the recently proposed LLaVA [22] as the base model to demonstrate the effectiveness of our fine-tuning strategies without modifications to its architecture. In particular, we select the variant of the LLaVA model with the pre-trained and frozen ViT-L / 14 @ 224px CLIP image encoder [26] and the pre-trained Vicuna v1.3 [8] with 7B parameters as the language model. Our method is not specific to any particular LVLM testing architecture or configuration. We use this configuration for both our base LVLM model and our learned reward model. \n\nWith computation efficiency in mind, we select the smallest variant of the Grounding DINO model with the Swin-T [24] backbone with the official checkpoint and the default box and text thresholds (0.25) for the open-set object detection task. Our proposed method can likely directly benefit from the larger and more computationally expensive variants or future advances in these models.",
            "score": 0.16856188506439274,
            "section_title": "Testing Framework",
            "char_start_offset": 19321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 934
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0604248046875
        },
        {
            "corpus_id": "273403951",
            "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
            "text": "We evaluate our models against various baselines, including LLaVA-1.5 series (Liu et al., 2024a) and LLaVA-1.6 (NeXT) series (Liu et al., 2024b). To ensure a fair comparison, given the different backbones of UIX from the original LLaVA checkpoints, we re-implemented three baselines: LLaVA-Vicuna, Llama3.1, Qwen2, utilizing the same training data as LLaVA. We also compare our models with Pix2Struct (Lee et al., 2023), S4 (Gao et al., 2024), SeeClick (Cheng et al., 2024), Co-gAgent (Hong et al., 2023), and ScreenAI (Baechler et al., 2024). Furthermore, we include GPT-4V and GPT-4o as strong baselines to provide a comprehensive evaluation. Enhanced GUI Grounding Capability: Meanwhile, we observe that, compared with general figure grounding (Table 3), current MLLMs fail to handle grounding tasks in complex GUI scenarios. Conversely, trained with MultiUI, which encompasses two web grounding tasks, UIX also demonstrates enhanced GUI grounding capability.",
            "score": 0.16846446362721654,
            "section_title": "BASELINES",
            "char_start_offset": 18249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 962
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 96,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 424,
                    "end": 442,
                    "matchedPaperCorpusId": "268253650"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "267069082"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0872802734375
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "To comprehensively evaluate the capabilities of our TS-LLaVA, we further conduct experiments on two challenging multitask video understanding benchmarks, namely MVBench and MLVU. \n\nMVBench We present the results in Tab. 5a. Among the training-free methods, our 34B model outperforms the proprietary model GPT-4V by a large margin, in both average accuracy and across most sub-tasks. Even our 7B model surpasses GPT-4V in average accuracy, showing the strong understanding capability and potential of our method. \n\nWhen comparing to the training-based video LLM, we focus on PLLaVA [44]. PLLaVA uses the same image LLM backbone as our model but is further trained on video data. In over half of the sub-tasks, our TS-LLaVA manages to obtain comparable or better performance than PLLaVA. However, there are still tasks where performance can be improved: (1) On some action centric tasks, TS-LLaVA delivers satisfactory results (AC and AP). However, it struggles with other action-centric tasks (e.g., AA, AL, and AS). ( 2) TS-LLaVA performs less effectively on tasks that require reasoning over moving objects (MA, MC and MD). ( 3) TS-LLaVA also shows lower performance on CI and OE tasks. \n\nGiven the nature of these task types, TS-LLaVA's lower performance is unsurprising. All of these tasks involve data types not seen during image LLM training, such as actions and moving objects. Without specific training on these data types, bridging the gap between our training-free approach and training-based methods remains challenging. MLVU The results are presented in Tab. 5b. Our method significantly outperforms the training-free counterpart, establishing the SOTA results across all sub-tasks. \n\nIn comparison with training-based video LLM, we take a challenging opponent, namely a 72B Video-LLaMA2. Remarkably, TS-LLaVA-34B achieves comparable or even superior performance to Video-LLaMA2-72B on more than half of the sub-tasks, despite using a much smaller LLM.",
            "score": 0.16841791592843433,
            "section_title": "Multitask Benchmarks",
            "char_start_offset": 18186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1963
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04345703125
        },
        {
            "corpus_id": "269042942",
            "title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation",
            "text": "MLLMs in our evaluation: four closed-source models -OpenAI's gpt-4o (GPT-4o) [16], OpenAI's gpt-4-1106-vision-preview (GPT-4) [1], Google AI's models/gemini-1.0-pro-vision (Gemini-1.0) 5, and Anthropic's claude-3-opus-20240229 (Claude-Opus) 6 -and one open-source model: llava-1.5-13b (LLaVA-1.5) [17]. Note that all future references to GPT-4o, GPT-4, Gemini-1.0, Claude-Opus, or LLaVA-1.5 refer to these specific model versions unless otherwise stated. GPT-4, Gemini-1.0, and Claude-Opus were originally selected for their strong performances as closed-source models on existing multimodal benchmarks [9,14]. For example, Gemini-1.0 Ultra, Claude-Opus, and GPT-4V(ision) were the top 5https://ai.google.dev/gemini-api/docs/models/gemini?authuser=1 6https://www.anthropic.com/news/claude-3-family three models, as of April 2024, on the MMMU benchmark, which consists of 11.5k multimodal questions from college curriculum content [24]. GPT-4o was released towards the end of the time of this work. It holds the 1st place spot on the MMMU leaderboard [24], as of August 2024. It was added to our closed-source model evaluations both because of this notable performance and because of potential for comparison with GPT-4. In this way, DesignQA can be used to continue to measure progress of models as they develop over time. LLaVA-1.5 is selected because of its promise (and the promise of its derivatives) as an open-source MLLM [41].",
            "score": 0.16828148431008583,
            "section_title": "MLLM Models. We consider five recently developed",
            "char_start_offset": 40464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0253753662109375
        },
        {
            "corpus_id": "273227850",
            "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
            "text": "Accuracy: 53.1 Accuracy: 46.9 \n\nCan you describe the main subject or object in this image, highlighting its most distinctive visual features, typical attributes, and common name, and explain how it relates to its broader category by tracing its evolution through time, exploring its cultural and historical significance, and highlighting its relationships with other objects within that category, while also emphasizing the subtle nuances and peculiarities that set. \n\nUnlock the essence of this image by pinpointing the pivotal visual feature, texture, or contextual element that unmistakably links it to its precise class name, while also considering the surrounding scene and subtle details that contribute to its unique identity. \n\nAccuracy: 54.5 Accuracy: 46.8 \n\nStep: 0 \n\nStep: 4 \n\nStep: 9 \n\nStep: 14 \n\nStep: 19 \n\nStep: 24 \n\nFigure 7: Prompt evolution for LLaVA. We provide the highest performing prompt (on the 1-shot train set) discovered by our GLOV at different optimization steps for the ImageNet dataset.",
            "score": 0.16820027253001693,
            "section_title": "Accuracy",
            "char_start_offset": 77663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 32,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004848480224609375
        },
        {
            "corpus_id": "277103779",
            "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
            "text": "For LLaVA and LLaVA-med, although the version number 1.6 is different from LLaVA-Med with 1.5, we do believe that LLaVA-Med was either trained from this LLaVA version, or from one that is very similar. They have identical architecture, and the version is very close; it is possible the LLaVA-Med copied the number incorrectly. The LLaVA-Med codebase or github issues does not clarify this at the time of writing. \n\nNote that for OpenRouter function calls to Anthropic, the date is not included in the string. We believe they use timestamps for older model versions, but not for the latest version. So reproducing it may require checking if the model has been updated.",
            "score": 0.16816203469520688,
            "section_title": "Options: {{CHOICES}}",
            "char_start_offset": 72413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 667
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0280609130859375
        },
        {
            "corpus_id": "269921913",
            "title": "Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery",
            "text": "(Liu et al., 2023a) 80.20 76.11 LLaVA-1.5-13B (Liu et al., 2023a) 81.29 77.59 LLaVA-Med (Li et al., 2024a) 83.54 80.63 Surgical-LVLM 90.68 83.24  (Ben-Younes et al., 2017) 0.6283 0.3395 0.7639 0.4242 0.3482 0.7218 MFH (Yu et al., 2018) 0.6283 0.3254 0.7592 0.4103 0.35 0.7216 BlockTucker (Ben-Younes et al., 2019) 0.6201 0.3286 0.7653 0.4221 0.3515 0.7288 CAT-ViL DeiT (Bai et al., 2023b) 0.6452 0.3321 0.7705 0.4491 0.3622 0.7322 GVLE-LViT (Bai et al., 2023d) 0.6659 0.3614 0.7625 0.4576 0.2489 0.7275 Surgical-LVLM (Ours) 0.6947 0.3325 0.8416 0.4068 0.3412 0.7825",
            "score": 0.16816203469520688,
            "section_title": "IMPLEMENTATION DETAILS",
            "char_start_offset": 19162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 565
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 146,
                    "end": 171,
                    "matchedPaperCorpusId": "12913776"
                },
                {
                    "start": 288,
                    "end": 313,
                    "matchedPaperCorpusId": "59553498"
                },
                {
                    "start": 369,
                    "end": 388,
                    "matchedPaperCorpusId": "259766556"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "258823315"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005641937255859375
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A pack of cigarettes sits on a bench. \n\nSim-CLIP 4 : A bench with a toothbrush on it. \n\nRobust-LLaVA 4 G: A blue and white book is sitting on a wooden bench.",
            "score": 0.16816203469520688,
            "section_title": "Brightness",
            "char_start_offset": 54881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 190
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00524139404296875
        },
        {
            "corpus_id": "269304163",
            "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
            "text": ") and 16B (LLaVA-1.6-Vicuna-13B3) models were run on NVIDIA RTX A4500 and A6000 GPUs.Interactions with Qwen 14B (Qwen/Qwen1.5-14B-Chat),Qwen 7B (Qwen/Qwen1.5-7B-Chat)and Gemma 7B (google/gemma-7b-it) models were conducted via the Together AI API4 .GPT-4 Vision (gpt-4-1106-vision-preview) and GPT-4 (gpt-4-0125-preview) models were accessed through the OpenAI API5 .For GPT-4 Vision, we use low-res mode which resizes the images to 512px \u00d7 512px.LLaVA models use CLIP-ViT-L-336px (Radford et al., 2021) for its vision encoders, which resize images to 336px \u00d7 336px.We used pretrained ViGA models provided by the authors6 to generate metrics for ViGA using VGG and I3D features.Access to Gemini 1.5 Pro was available via Google AI Studio's web interface7 .",
            "score": 0.16816203469520688,
            "section_title": "Experimental details",
            "char_start_offset": 14243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 32,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 755
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 502,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005062103271484375
        },
        {
            "corpus_id": "265445881",
            "title": "MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria",
            "text": "LLaVA-v1.5-13B (Liu et al., 2023b) Pretrained Vision Encoder + Projector + LLM 15.4K 333.7K LVIS-Instruct4v-LLaVA-7B (Wang et al., 2023a) Pretrained Vision Encoder + Projector + LLM 122 5 MiniGPT-v2 (Chen et al., 2023a) Pretrained Vision Encoder + Projector + LLM 24.7K / LLaVA-v1.5-7B (Liu et al., 2023b) Pretrained Vision Encoder + Projector + LLM 15.4K 703K LLaVA-v1.6-Vicuna-7B (Liu et al., 2024b) Pretrained Vision Encoder + Projector + LLM 15.4K 1.2M LLaVA-v1.6-Vicuna-13B (Liu et al., 2024b) Pretrained Vision Encoder + Projector + LLM 15.4K 100.1K LLaVA-v1.6-34B (Liu et al., 2024b) Pretrained Vision Encoder + Projector + LLM 15.4K 592.8K Yi-VL-6B (Young et al., 2024) Pretrained Vision Encoder + Projector + LLM 7K 17.2K ALLaVA-3B-Longer (Chen et al., 2024b) Pretrained Vision Encoder + Projector + LLM 134 93 kosmos2 (Peng et al., 2023) Pretrained Vision Encoder + Grounded LLM 18.1K 29.2K LWM (Liu et al., 2024a) Pretrained Vision Encoder + Projector + Long-Context LLM 6.6K / BLIP2-Flan-T5-XL (Li et al., 2023c) Query tokens + LM 8.5K 35.4K",
            "score": 0.16816203469520688,
            "section_title": "Open-Source",
            "char_start_offset": 35741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 1053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005641937255859375
        },
        {
            "corpus_id": "276580647",
            "title": "Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models",
            "text": "Training of LLaVA was done using 7 NVIDIA RTX 3090 GPUs in 11 hours. Training of Qwen2-VL was done using 4 NVIDIA Tesla V100 32GB PCIE GPUs in 5 hours. Training details can be found in Appendix A. Training of both LLaVA and Qwen2-VL followed the recommended hyperparameters of LLaVA since Qwen2-VL did not recommend any for LoRA, with an extended context length of 6144 tokens to fit the longer Wikipedia training samples and allow for a larger room for reasoning.",
            "score": 0.16816203469520688,
            "section_title": "Training",
            "char_start_offset": 32055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 464
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006313323974609375
        },
        {
            "corpus_id": "259356178",
            "title": "VariantHunter: a method and tool for fast detection of emerging SARS-CoV-2 variants",
            "text": "A systematic and well-assisted understanding of prevalence changes of lineages and of their specific mutations is the fundamental ingredient of viral genomic surveillance. Vari-antHunter does not aim to replace phylogenetics methods, which are fundamental to eventually identifying new lineages and their determination. What VariantHunter provides, instead, is a tool to suggest suitable directions to look for new variants. As the search space for such variants is very large (considering all the locations, periods of time, lineages and mutations that single sequences can acquire), a tool such as VariantHunter allows to speed the work of researchers when assessing groups of sequences with mutations that are growing particularly fast. \n\nVariantHunter is proposed as a utility that allows significant savings in time, labor and costs for projects on genomic surveillance of SARS-CoV-2; it provides a userfriendly method for querying the changes in prevalence for mutations in a given location over a given period of 4 weeks of time. \n\nThe web service is designed to be as easy to use as possible and also responsive for answering user-generated hypotheses, with a reactive interface. The time required by data analysis is very short and does not depend on either the size of the initial dataset or the specific kind of analysis, as data aggregations are computed at loading time. \n\nVariantHunter's simple paradigm could be reapplied to any kind of dataset with similar characteristics, also in the context of possible future pandemics. This possibility is empowered by a well-documented Docker version of the system that is distributed along with the open online web service.",
            "score": 0.16751655207973426,
            "section_title": "Conclusions",
            "char_start_offset": 22215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1679
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0012159347534179688
        },
        {
            "corpus_id": "276317810",
            "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
            "text": "To assess the state-of-the-art in Large Multimodal Models (LMMs), we conducted a comprehensive evaluation encom-  We initially attempted to evaluate LLaVA-OneVision-0.5B and Qwen-2VL-2B. However, most of their generations deviated significantly from our structured template, tending towards open-ended responses rather than adhering to the intended format. This inconsistency made direct comparison challenging, leading to their exclusion from our study. \n\nFor a fair evaluation, we refrain from applying low-bit quantization during inference (Thawakar et al., 2024). Additionally, to ensure a balanced comparison across model families, we primarily include 7B parameter variants from each family, as this size is widely available across different architectures and provides a reasonable trade-off between performance and computational feasibility.",
            "score": 0.16736144457227248,
            "section_title": "G. Implementation Details of LMMs",
            "char_start_offset": 32993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007205963134765625
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "LLAVA-phi [120]: LLaVA-Phi is a highly efficient multimodal assistant powered by the compact language model, Phi-2.This model demonstrates significant progress in compact multimodal systems, showing that even smaller models with 2.7B parameters can effectively engage in complex dialogues blending text and visuals, given proper training.LLaVA-Phi excels in various benchmarks covering visual comprehension, reasoning, and knowledge-based perception, suggesting its suitability for real- time interaction scenarios like embodied agents.Significantly, it highlights how smaller language models can attain advanced levels of comprehension and engagement without sacrificing resource efficiency.The training process involves two stages:\n\n(1) feature alignment, where a pretrained vision encoder is connected to a language model using a subset of the LAION-CC-SBU dataset, and (2) visual instruction tuning, using a combination of GPT-generated multimodal instruction-following data and VQA data to teach the model to follow multimodal instructions.\n\nMoE-LLaVA [48]: MoE-LLaVA is a groundbreaking training strategy for Large Vision-Language Models.Known as MoE-tuning, this innovative approach efficiently manages performance degradation in multi-modal learning and model sparsity by activating only the top k experts during deployment via routers.Despite its architecture comprising 3 billion sparsely activated parameters, MoE-LLaVA achieves comparable or superior performance to state-of-the-art models while minimizing hallucinations in model outputs.Its architecture includes a visual encoder, visual projection layer in the form of MLP, word embedding layer, stacked LLM and MoE blocks.MoE-tuning comprises three stages: MLP training, parameter training excluding the Vision Encoder, and initializing experts in MoE followed by training only MoE layers.Evaluation on various visual understanding datasets demonstrates MoE-LLaVA's efficiency and effectiveness, with extensive ablation studies and visualizations illustrating its effectiveness and offering insights for future research in multi-modal learning systems.\n\nYi-VL [13]: Yi Vision Language (Yi-VL) is an open-source multimodal model based on the Yi Large Language Model series, excelling in content comprehension, recognition, and multiround conversations about images.",
            "score": 0.16710332043974965,
            "section_title": "GPT-4V",
            "char_start_offset": 57032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 338
                },
                {
                    "start": 338,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 1045
                },
                {
                    "start": 1047,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1688
                },
                {
                    "start": 1688,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 2118
                },
                {
                    "start": 2120,
                    "end": 2330
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048309326171875
        },
        {
            "corpus_id": "245432157",
            "title": "Dynamic Modeling for Product Family Evolution Combined with Artificial Neural Network Based Forecasting Model: A Study of iPhone Evolution",
            "text": "The figure is edited by adding one more product family extension. Usually, the evolution of a product family ensures the continuous growth of the product [4]. It may ensure reduction in development cost, improvement of the speed and reliability of product development and enhance the variability of business decisions [5]. Therefore, product family evolution is very significant in product family development, and sustaining profit and growth in the competitive markets depends on the information regarding product family evolution. \n\nFor product family development and product family evolution, designers need to consider numerous factors, their interactions, and how they affect product design. The major drive force includes market conditions, customer requirements and technology requirements [1,6]. The dynamic environments created by these three major factors influence the product family and its evolution over time. For example, products such as automobiles or smart phones have many product series with various configurations in the product family in order to keep up with changing technologies, customer needs and preferences and market growth and demand [7]. The evolution of product as well as the evolution of product family can also be considered as the result of adaptation to customer demands and technologies to meet the present market niche. \n\nProduct competition rapidly changes due to the dynamic nature of market environments such as the release of new products, withdrawal of the outdated ones and social changes [8]. By Yoon et al. [9] technological opportunities create new market for industries. Technological obsolescence is an important concern for the design of products that are long lifetime (e.g., freight trucks, industrial workstations) or have quick turnovers (e.g., cell phones, computers) [10]. Hence, in the evolution of a product family, the technological requirements are very significant factors that must need to be taken into account. \n\nProduct family evolution plays a key role in the development of new product variants. For the development of a new product variant, it is very important for a manager in terms of decision making to identify the most influential design features and best performed products that will dominate the future market. Hence, forecasting the design specification of those influential design features is crucial. \n\nNumerous research [1,10] efforts have considered different driving factors and their impact on product family evolution.",
            "score": 0.1670323787781703,
            "section_title": "Change:",
            "char_start_offset": 3238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2381
                },
                {
                    "start": 2384,
                    "end": 2504
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "111184042"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "110639543"
                },
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "108514963"
                },
                {
                    "start": 800,
                    "end": 802,
                    "matchedPaperCorpusId": "41035828"
                },
                {
                    "start": 1165,
                    "end": 1168,
                    "matchedPaperCorpusId": "62524261"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "matchedPaperCorpusId": "212997173"
                },
                {
                    "start": 1555,
                    "end": 1558,
                    "matchedPaperCorpusId": "153885225"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "109653302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0045928955078125
        },
        {
            "corpus_id": "249378536",
            "title": "Selection for immune evasion in SARS-CoV-2 revealed by high-resolution epitope mapping combined with genome sequence analysis",
            "text": "The observation of similar pattern of mutations and selective pressures in epitopes across pandemic waves 1 and 2 (Fig. 5, Fig. S7) was surprising, given the expectation that increasing levels of immunity in the population would lead to increased selection for immune evasion over time. The second wave is characterized by the rise of variants of concern (VOCs) and variants under investigation (VUIs) with higher transmissibility and, in some cases, increased disease severity and acquired immune evasion phenotypes (19). The rise of VOCs has been suggested to be due to a shift in the SARS-CoV-2 fitness landscape, although the nature of such a shift is unclear (38). If part of this shift were due to rising population immunity from the first to second wave, one would expect increasing selection for immune evasion variants, resulting in higher frequencies of SNVs in epitopes in wave 2. Although we found a higher total number of nonsynonymous SNVs (including both mutations and substitutions) in epitope sites unique to wave 1 than unique to wave 2 ( Fig. 6A), wave 2-specific SNVs reached higher frequencies across samples compared to wave 1-specific SNVs, consistent with increased selection for immune evasion over time (Fig. 6B).\n\nHowever, mutations common to both waves achieved the highest frequencies, indicating their early appearance and persistence over time (Fig. 6B).\n\nA likely driver of VOC evolution is selection for increased transmissibility. For example, the Delta VOC is estimated to be 76-117% more transmissible than non-VOCs and non-VUIs,  (40). We focused on nonsynonymous signature mutations located in epitope sites and found that VOCs and VUIs contain significantly more signature mutations in epitopes compared to non-VOCs and non-VUIs (Fig. 6C) suggesting that evasion of the humoral immune response could be a significant driver of VOC/VUI evolution. We then ranked VOCs and VUIs based on their number of signature mutations in epitopes (Fig. 6D). We observed that Delta has an intermediate number of mutations in mapped epitopes (Fig. 6D). Many nonsynonym",
            "score": 0.1669113644474695,
            "section_title": "Assessing the immune evasion potential of SARS-CoV-2 variants",
            "char_start_offset": 22796,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "235294758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00080108642578125
        },
        {
            "corpus_id": "276938169",
            "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
            "text": "BIMBA-LLaVA is based on the image-pretrained MLLM LLaVA-NeXT [44], which utilizes CLIP [59] as the vision encoder and Vicuna-7B [11] as the LLM. It processes 64 video frames at a resolution of 336 \u00d7 336, dividing each frame into 14\u00d714 patches, yielding 64\u00d724\u00d724 spatiotemporal tokens. These tokens are compressed to 16 \u00d7 12 \u00d7 12 before being fed into the LLM. In this variant, the vision encoder remains frozen, while the multimodal projector (a linear layer), spatiotemporal token selector, and LLM are trained using LoRA [23]. BIMBA-LLaMA is based on the image-pretrained MLLM LLaMA-3.2 [52], incorporating Meta-CLIP [74] as the vision encoder and LLaMA-3.2-LLM-8B as the LLM. It processes 64 video frames at a higher resolution of 560 \u00d7 560, dividing each frame into 14 \u00d7 14 patches, resulting in 64 \u00d7 40 \u00d7 40 spatiotemporal tokens. These are compressed to 16 \u00d7 20 \u00d7 20 before being passed to the LLM. Unlike the other variants, both the vision encoder and multimodal projector remain frozen, with only the spatiotemporal token selector and LLM trained using LoRA. Training Details. We employ standard cross-entropy loss for autoregressive text generation and train the model for 1 epoch with a batch size of 128 and a learning rate of 2e-5. The AdamW [48] optimizer is used, along with a cosine learning rate scheduler and a warm-up ratio of 0.03.",
            "score": 0.16683983039605613,
            "section_title": "S1. Additional Implementation Details",
            "char_start_offset": 28919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1351
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 91,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03448486328125
        },
        {
            "corpus_id": "276580600",
            "title": "Can Multimodal LLMs Perform Time Series Anomaly Detection?",
            "text": "parameters architecture. It can integrate language and visual comprehension seamlessly, making it highly effective for image captioning, visual reasoning, and complex multimodal interactions. The model is designed to deliver state-of-the-art accuracy in vision-language tasks, with optimizations for both zero-shot and fine-tuned applications. \n\nLLaVA-NeXT-8B (Li et al., 2024a) is a smaller, more efficient version of the LLaVA-NeXT series, designed for applications where computational resources are limited. Built on LLaMA3-8B parameters, it maintains strong performance on visionlanguage tasks, achieving competitive results on benchmarks like MMMU and VQA. The model is particularly well-suited for real-time applications requiring fast inference and low memory usage. \n\nQwen2-VL-72B-Instruct (Wang et al., 2024b) is a large multimodal model optimized for instruction-following tasks involving both text and vision developed by Alibaba. With 72B parameters, it delivers robust performance on benchmarks such as MMMU and VQA. The model is designed to handle complex queries that require deep reasoning across modalities, ensuring consistent performance even when processing multimodal inputs. Its architecture is tailored for applications requiring high accuracy and reliability. \n\nQwen2-VL-7B-Instruct (Wang et al., 2024b) is a compact version of the Qwen2-VL series, designed for scenarios where computational efficiency is paramount. Despite its smaller size, it achieves competitive results on vision-language benchmarks, making it suitable for real-time applications. The model is optimized for instruction-following tasks, ensuring that it can handle multimodal inputs without compromising performance.",
            "score": 0.16678452813508202,
            "section_title": "Impact Statement",
            "char_start_offset": 21839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06109619140625
        },
        {
            "corpus_id": "276421683",
            "title": "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization",
            "text": "Experimental Setup The experimental setup follows the same setting as VLMs alignment experiments, except for the backbone models, where we employ a diverse array of VLMs varying in size and architecture: \n\n\u2022 Image-to-Text models: the typical architecture of VLMs, where a vision encoder is integrated with an LLM to enable cross-modal understanding. In this section, we evaluate RE-ALIGN on LLaVA-v1.5-7B (Liu et al., 2024a), LLaVA-v1.5-13B (Liu et al., 2024a), LLaVA-v1.6-Vicuna-7B (Li et al., 2024b), and LLaVA-v1.6-Vicuna-13B (Li et al., 2024b). \u2022 Unified Models: encoder-decoder architecture that decouples visual encoding for multimodal understanding and generation. We evaluate RE-ALIGN on Janus-Pro-1B (Chen et al., 2025) and Janus-Pro-7B (Chen et al., 2025). Results Table 3 presents the performance of RE-ALIGN using both standard image-to-text and unified VLM backbones across model sizes from 1B to 13B on the POPE benchmark (Li et al., 2023c). \n\nIn experiments with the LLaVA-v1.5 series (Liu et al., 2024a), none of the baseline approaches consistently improve performance for either the 7B or the 13B models, highlighting the limited scalability of these methods. In contrast, RE-ALIGN achieved substantial performance gains, outperforming both the baseline models and the vanilla version-most notably on the LLaVA-v1.5-13B variant. Similarly, experiments with the LLaVA-v1.6-Vicuna series (Li et al., 2024b) revealed the same trend, further underscoring RE-ALIGN's superior scalability. For unified vision-language models, especially Janus-Pro, integrating RE-ALIGN yields a significant performance boost. Notably, Janus-Pro-1B experiences the greatest improvement, underscoring RE-ALIGN's robustness across different model architectures.",
            "score": 0.166327633008514,
            "section_title": "Scalability and Generalizability",
            "char_start_offset": 14201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1000,
                    "end": 1019,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060638427734375
        },
        {
            "corpus_id": "276929180",
            "title": "EAZY: Eliminating Hallucinations in LVLMs by Zeroing out Hallucinatory Image Tokens",
            "text": "B.1 Model Details. \n\nLLaVA-1.5. The LLaVA-1.5 Model [Liu et al., 2024c] leverages the linear projector layer to align the vision and text modalities, with 576 image tokens. It adopted the pre-trained vision transformer from CLIP [Radford et al., 2021] and the pre-trained language model as Vicuna [Chiang et al., 2023]. \n\nShikra. The Shikra Model [Chen et al., 2023] introduces referential dialogue capabilities in multimodal large language models (MLLMs) by handling spatial coordinate inputs and outputs in natural language. It utilizes a vision encoder, an alignment layer, and a Vicuna-based language model without requiring extra vocabularies, position encoders, pre-/post-detection modules, or external plug-ins. \n\nThe model enables interaction with images through natural pointing and location-aware responses, supporting tasks such as Referring Expression Comprehension (REC), PointQA, Image Captioning, and Visual Question Answering (VQA) with promising performance. \n\nLLaVA-Next. The LLaVA-Next Model [Liu et al., 2024b] enhances multimodal capabilities by increasing the input image resolution up to 672 \u00d7 672 pixels, supporting three aspect ratios. It utilizes an improved visual instruction tuning data mixture to bolster visual reasoning and OCR capabilities. The model employs a pre-trained vision transformer from CLIP and integrates with advanced language models like Vicuna and Mistral.",
            "score": 0.16623997331051185,
            "section_title": "B Extended Experiment",
            "char_start_offset": 33972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1404
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 71,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 229,
                    "end": 251,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05889892578125
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A white wall with a white cloth. \n\nSim-CLIP 4 : A white wall with a white cloth on it. \n\nRobust-LLaVA 4 G: A person wearing a white shirt.",
            "score": 0.16623997331051185,
            "section_title": "Contrast",
            "char_start_offset": 55514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 171
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035648345947265625
        },
        {
            "corpus_id": "274273284",
            "title": "Exploring the Structural and Functional Consequences of Deleterious Missense Nonsynonymous SNPs in the EPOR Gene: A Computational Approach",
            "text": "The G320S variant's radius of gyration fluctuated slightly above 5 nm, indicating moderate structural compactness and stability. Similarly, the R223'S variant's Rg fluctuated around 5 nm, suggesting comparable stability to the G320S variant. In contrast, the wildtype variant has a significantly lower Rg, remaining steady at less than 2.5 nm throughout the observed period, indicating a much more compact and potentially more stable structure compared to the mutated variants (Figure 5).",
            "score": 0.16623997331051185,
            "section_title": "Radius of Gyration (Rg)",
            "char_start_offset": 29755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 488
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0006613731384277344
        },
        {
            "corpus_id": "276107304",
            "title": "Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models",
            "text": "LLaVA output when using FARE 4 : A woman sitting on a couch with a laptop. \n\nSim-CLIP 4 : A woman sitting on a couch with a laptop on her lap. \n\nRobust-LLaVA 4 G: A woman sitting on a couch with a laptop.",
            "score": 0.16623997331051185,
            "section_title": "Original Image",
            "char_start_offset": 42549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030994415283203125
        },
        {
            "corpus_id": "245117601",
            "title": "Classification of genetic variants using machine learning",
            "text": "Genome Aggregation Database (gnomAD) project hosts a rich aggregation of genome sequencing data from various leading international projects (Broad Institute, 2020). Some of the constituents in its various publicly available data repositories include genome and exome variations, mutational constraints, multi-nucleotide variants, structural variants and LOF curation results. It is also the largest high-quality database of LOF variants (Karczewski, et al., 2020). It has LOF variants identified using the combination of VEP annotation and further filtering by LOF Transcript Effect Estimator (LOFTEE). \n\nUsing the LOF curation results available on the gnomAD portal (All homozygous, Lysosomal storage disease genes, AP4, FIG4, MCOLN1, Haploinsufficient genes and Metabolic conditions genes), we identified 1894 genes relevant to LOF variants. gnomAD portal provides a function to download all variants of a given gene. The downloaded variant data contains all Variant Calling Format (VCF) fields and, in addition, includes several important features such as Clinvar's clinical significance, VEP annotation, allele frequency distribution among various ethnicities, homozygote count and hemizygote count. However, the function only works for one gene at a time. \n\nWe developed a Java program to automate en masse extraction of all the genes of our interest. The program is available on our project's public GitHub repository. Using this program, we downloaded 1894 files and then combined them into a single file using a Linux shell command. We configured the extraction program to select only the LOF variants option before the download. The format of variant data downloaded from gnomAD acts as the backbone of our dataset's schema. \n\nThe initial extraction resulted in 678654 variants; however, Clinvar's clinical significance was available only for 11980 variants; hence, we discarded all the variants without any clinical significance. Clinvar is a manually curated, freely available database where clinical practitioners from all over the world submit their interpretations of the clinical significance of the variants (Landrum, et al., 2016).",
            "score": 0.16586848089701173,
            "section_title": "A. Dataset",
            "char_start_offset": 9384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 463,
                    "matchedPaperCorpusId": "215790670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00260162353515625
        },
        {
            "corpus_id": "266521081",
            "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
            "text": "We use LLaVA-1.5 [41] as our base MLLM. LLaVA-1.5 uses CLIP-ViT-L-336px [56] as the image encoder (Im-Coder) with a two-layer MLP as projection and Vicuna-1.5 [77] as the LLM. Inside our VCoder, we also use a CLIP-ViT-L-336px to encode the control inputs and Comparison to baseline Multimodal LLMs on the COST validation dataset for Object Identification. We compare our VCoder to existing off-the-shelf baseline MLLMs: MiniGPT-4 [78], InstructBLIP [14], LLaVA-1.5 [41], and CogVLM [68]. We also train three different variants of LLaVA-1.5 on the COST dataset: COST IT mixes the COST training data with the instruction tuning data; Soft-Prompted uses a set of learnable tokens, and ImCoder uses an RGB image as the control input. Our VCoder adapted LLaVA-1.5 performs the best on all three object perception tasks. Note: \u27e8\u2022\u27e9 denotes input tokens to LLM with seg representing segmentation map, img representing RGB image, prompt representing learnable prompt, and query representing the user question. We also evaluate the performance of GPT-4V [51] on the COST dataset using the publicly accessible paid API released by OpenAI. Our VCoder-adapted LLaVA-1.5 shows the best performance on object identification among all MLLMs. \n\nproject the features into the LLM embedding space using modality-specific two-layer MLPs. We resize the visual inputs to 336\u00d7336 resolution (corresponds to 576 tokens) for our MLLM. During training, we load the instructiontuned weights from LLaVA-1.5 and keep those frozen while only tuning the MLP component of our VCoder. We use the publicly available OneFormer [26] model trained on COCO [38] with DiNAT-L [19,20] backbone to obtain the segmentation map.",
            "score": 0.16572122391188898,
            "section_title": "Experiments",
            "char_start_offset": 20773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0706787109375
        },
        {
            "corpus_id": "276847227",
            "title": "Adaptive Non-Stationary Fuzzy Time Series Forecasting with Bayesian Networks",
            "text": "In this section, we conduct ablation experiments to validate the effectiveness of core modules. We first introduce three variants of TV-NS-BN-PWFTS. TV-BN-PWFTS represents an FTSFM that employs BN-PWFTS as its core module with a traditional time-variant updating strategy [15]. This variant only utilizes recent data to construct a new BN-PWFTS for prediction, disregarding useful historical information. TV-BN-PWFTS (NSFS) replaces traditional fuzzy sets with our proposed non-stationary fuzzy sets to demonstrate their effectiveness. TV-BN-PWFTS (adaptive) substitutes the time-variant updating strategy [15] with our dynamic updating approach to verify its efficacy in capturing temporal patterns. Table 13 presents the results across nine time series using three metrics: RMSE, MAPE, and U. The results show that both variant methods incorporating either non-stationary fuzzy sets or model updating modules outperform the baseline TV-BN-PWFTS. The supe-rior performance of TV-BN-PWFTS (adaptive) over TV-BN-PWFTS (NSFS) indicates that dynamic changes in temporal patterns significantly impact the long-term prediction of nonstationary time series. TV-NS-BN-PWFTS offers a comprehensive approach to handling non-stationarity in time series. It not only addresses the dynamic changes in vagueness through non-stationary fuzzy sets but also captures the evolution of temporal relationships, thereby providing more accurate and comprehensive forecasting results.",
            "score": 0.16565617675723027,
            "section_title": "Ablation Study",
            "char_start_offset": 52261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027141571044921875
        },
        {
            "corpus_id": "268379244",
            "title": "A clinically accessible small multimodal radiology model and evaluation metric for chest X-ray findings.",
            "text": "Conducting thorough ablation studies for LLMs is often intractable due to the costly training of multiple variants.In contrast, the small size of LLaVA-Rad enables us to efficiently conduct ablation studies that explain the promising performance of LLaVA-Rad and potentially inform design choices for larger models.We compared LLaVA-Rad with 8 variants described in Supplementary Table 2.\n\nIn particular, we investigate two key technical ideas used in LLaVA-Rad: the effect of pre-training a domain-specific image encoder using 697 thousand diverse CXR image-text pairs (Fig. 5A) and the effect of using GPT-4 to augment and organize the data (Fig. 5B).First, to understand the effect of pre-training an image encoder, we compare LLaVA-Rad with three increasingly domain-specific variants: an image encoder from OpenAI CLIP, an image encoder using BiomedCLIP, and an image encoder from BiomedCLIP but continually pre-trained using MIMIC-CXR only (Fig. 5A).To avoid data contamination, we only used the training split of MIMIC-CXR.We did not find noticeable overlap between MIMIC-CXR training split and the PubMed data used to pre-train BiomedCLIP.We found that the MIMIC-CXR-based image encoder outperforms the other two variants, indicating the effectiveness of training a domain-specific image encoder.BiomedCLIP-CXR outperforms the BiomedCLIP image encoder continue-pretrained only on MIMIC-CXR (BiomedCLIP-MIMIC-CXR), illustrating the advantage in pre-training using more diverse CXR datasets.Second, we studied the effect of using GPT-4 to process and augment the MIMIC-CXR report data (Fig. 5B).Supplementary Table 3 summarizes the data that LLaVA-Rad uses for training in the second and the third stages.It is a combination of rule-based and GPT-structured data.We compare LLaVA-Rad with a variant that only uses rule-based data and a variant that only uses GPT-structured data.",
            "score": 0.1655305033214401,
            "section_title": "Analyzing components of LLaVA-Rad using ablation and case studies",
            "char_start_offset": 17140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 388
                },
                {
                    "start": 390,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1030
                },
                {
                    "start": 1030,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0290374755859375
        },
        {
            "corpus_id": "275342951",
            "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
            "text": "To further explore the potential of LLaVA-Mini, we introduce a variant that uses the CLIP ViT-L/336px (Radford et al., 2021) as vision encoder and the advanced LLaMA-3.1-8B-Instruct (Dubey et al., 2024) as LLM backbone. During instruction tuning, we combine 665K image instruction data from LLaVA (Liu et al., 2023b), 100K video instruction data from Video-ChatGPT (Maaz et al., 2024), and part of open-source data (Li et al., 2024a), resulting in 3 million training samples. LLaVA-Mini is trained using 8 NVIDIA A800 GPUs. Training details are provided in Appendix B.",
            "score": 0.1653559190502124,
            "section_title": "EXPERIMENTAL SETTING",
            "char_start_offset": 19437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 568
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 124,
                    "matchedPaperCorpusId": "265608767"
                },
                {
                    "start": 297,
                    "end": 316,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054412841796875
        },
        {
            "corpus_id": "269042942",
            "title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation",
            "text": "To help disentangle RAG efficacy from inherent model performance, we generated GuaranteedRAG for each question in the benchmark. Created using a series of scripts with key-word matching, GuaranteedRAG is guaranteed to contain context that is relevant to the question at hand. For example, in the case of the Retrieval questions, GuaranteedRAG context is designed to contain the rule-in-question. More details about the implementation of GuaranteedRAG can be found in Appendix B. Due to the cost of running the closed-source models, we only tested the open-source model, LLaVA-1.5, with GuaranteedRAG (LLaVA-1.5-GuaranteedRAG). We generated our GuaranteedRAG five different times (to account for randomness, see Appendix B), and we tested LLaVA-1.5 with each of these five GuaranteedRAGs, reporting averages and standard deviations across the five Guaran-teedRAGs. \n\nThe results of this experiment can be seen in Table 3. While we expected LLaVA-1.5's score to improve substantially when tested with GuaranteedRAG instead of simple RAG, LLaVA-1.5-GuaranteedRAG -across most subsets of the benchmarkperforms only slightly better than LLaVA-1.5-RAG. When compared with all the other models tested (Table 2), LLaVA-1.5-GuaranteedRAG does not perform the best across any subset of the benchmark. This finding underscores that much of the DesignQA score, at least in the case of LLaVA-1.5, is a result of inherent model capabilities (and lack thereof) rather than RAG efficacy. \n\nThe biggest improvement in score between LLaVA-1.5-RAG and LLaVA-1.5-GuaranteedRAG is for the Retrieval questions (0.112 versus 0.699 F1 BoW). Given useful rule document context, the Retrieval questions become straightforward. However, despite receiving the rule-in-question verbatim within an 8716-character text snippet (about 3% of the total rule document), LLaVA-1.5-GuaranteedRAG still struggles to reliably return the rule-inquestion for the Retrieval questions. LLaVA-1.5-GuranteedRAG",
            "score": 0.16531357290813503,
            "section_title": "Subset",
            "char_start_offset": 59692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 1965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0220794677734375
        },
        {
            "corpus_id": "3643067",
            "title": "Gaussian process test for high-throughput sequencing time series: application to experimental evolution",
            "text": "Turner et al. (2011) developed a pairwise summary statistic, called \"diff-Stat\" to estimate the observed distribution of allele frequency differences and compared this to the expected distribution without selection. Orozco-TerWengel et al. (2012) identify SNPs with a consistent AFC among replicates by performing a Cochran-Mantel-Haenszel test (CMH) (Agresti, 2002). The latter is an extension of the Fisher's exact test to multiple replicates. All above-mentioned statistical methods are based on pairwise comparisons between the base and evolved populations and do not take full advantage of the time series data now available. Here, we propose an alternative Gaussian process (GP) based approach to study AFCs over the entire time series experiment. \n\nGP is a non-parametric statistical model that is extremely well-suited for modelling HTS time series data which usually have relatively few time points that may be irregularly sampled. Recently, there have been some works applying GP models with parameters describing the process of evolution (e.g., Jones and Moriarty, 2013 account for phylogenetic relationships, Palacios and Minin, 2013 for effective population size). GPs have also recently been applied to gene expression time series by a number of authors (Yuan, 2006;Gao et al., 2008;Kirk and Stumpf, 2009;Liu et al., 2010;Honkela et al., 2010;Stegle et al., 2010;Cooke et al., 2011;Kalaitzis and Lawrence, 2011;Titsias et al., 2012;Liu and Niranjan, 2012;\u00c4ij\u00f6 et al., 2013;Hensman et al., 2013). In differential analysis, GPs have been applied to detect differences in gene expression time series in a two-sample setting by Stegle et al. (2010) and for detecting significant changes by Kalaitzis and Lawrence (2011). While these methods provide a very sensible basis for detecting the changing alleles, they fail to properly take into account all aspects of the available HTS data, such as differences in sequencing depth between different alleles and time points.",
            "score": 0.16511061310098518,
            "section_title": "Introduction",
            "char_start_offset": 2053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 246,
                    "matchedPaperCorpusId": "16768304"
                },
                {
                    "start": 1056,
                    "end": 1080,
                    "matchedPaperCorpusId": "9594890"
                },
                {
                    "start": 1121,
                    "end": 1145,
                    "matchedPaperCorpusId": "14288039"
                },
                {
                    "start": 1268,
                    "end": 1280,
                    "matchedPaperCorpusId": "8588962"
                },
                {
                    "start": 1280,
                    "end": 1297,
                    "matchedPaperCorpusId": "13748118"
                },
                {
                    "start": 1297,
                    "end": 1319,
                    "matchedPaperCorpusId": "6531915"
                },
                {
                    "start": 1319,
                    "end": 1336,
                    "matchedPaperCorpusId": "9334436"
                },
                {
                    "start": 1336,
                    "end": 1357,
                    "matchedPaperCorpusId": "1393923"
                },
                {
                    "start": 1357,
                    "end": 1377,
                    "matchedPaperCorpusId": "12436594"
                },
                {
                    "start": 1377,
                    "end": 1396,
                    "matchedPaperCorpusId": "114181"
                },
                {
                    "start": 1396,
                    "end": 1425,
                    "matchedPaperCorpusId": "9713975"
                },
                {
                    "start": 1425,
                    "end": 1446,
                    "matchedPaperCorpusId": "15010424"
                },
                {
                    "start": 1446,
                    "end": 1469,
                    "matchedPaperCorpusId": "13946892"
                },
                {
                    "start": 1469,
                    "end": 1487,
                    "matchedPaperCorpusId": "17878664"
                },
                {
                    "start": 1487,
                    "end": 1508,
                    "matchedPaperCorpusId": "9076166"
                },
                {
                    "start": 1638,
                    "end": 1658,
                    "matchedPaperCorpusId": "12436594"
                },
                {
                    "start": 1700,
                    "end": 1729,
                    "matchedPaperCorpusId": "9713975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004398345947265625
        },
        {
            "corpus_id": "257576423",
            "title": "Reconstructing Clonal Evolution\u2014A Systematic Evaluation of Current Bioinformatics Approaches",
            "text": "The sum over all dependent child clones cannot exceed their joined parent. The sum over all independent clones cannot exceed 100%. An eradicated clone cannot re-appear, i.e., once a clone's CCF has fallen to zero, it remains zero for all subsequent time points. The CCF of each clone has to be \u2265 min th for at least one time point. The \u2206 CCF for two clones has to be \u2265 min dist for at least one time point. If one of the constraints is violated, a new clone is simulated at random. If no solution is found after 100 attempts, a new simulation, including a new founding clone, is executed (maximum 700 attempts). Finally, if a phylogeny was successfully generated, one variant is assigned to every clone. The remaining n v \u2212 n c variants are randomly split. \n\nThe results of the algorithm simulating the phylogeny are visualized using the R/Bioconductor package clevRvis [20]. Additionally, a CCF table is displayed, providing an option for fine-tuning the simulation on a clonal level. All major characteristics of the clonal evolution (parental relations, number of variants per clone, and CCFs) can be changed, and the clones and time points can be added as well as deleted.",
            "score": 0.1650053688993267,
            "section_title": "Simulating the Phylogeny",
            "char_start_offset": 9828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0006384849548339844
        },
        {
            "corpus_id": "270703489",
            "title": "Long Context Transfer from Language to Vision",
            "text": "NeXTQA [77] ActivityNetQA [85] VideoChatGPT [50] Video-DD [ We also tested LongVA on shorter benchmarks with average video durations under 120 seconds.\n\nAs indicated in Table 6, although LongVA scores higher with more densely sampled frames on datasets such as NeXTQA [77] and ActivityNetQA [85], the gains quickly plateau and are not as significant as those observed in Video-MME, which can be attributed to the shorter duration of these datasets.On the VideoChatGPT and Video Detailed Description (Video-DD) benchmarks, increasing frames does not lead to better performance, and LongVA generally achieves lower scores compared to LLaVA-NeXT-Video-7B. Since both benchmarks use OpenAI's GPT API as a judge, we believe their metrics are closely related to the answering format.To address this, we perform a lightweight Direct Preference Optimization (DPO) on the LLaVA-Hound-DPO [88] dataset.We observe significantly improved performance for LongVA-DPO, confirming the findings in [88].",
            "score": 0.16436134470308195,
            "section_title": "Model",
            "char_start_offset": 18858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 153,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 892
                },
                {
                    "start": 892,
                    "end": 986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083160400390625
        },
        {
            "corpus_id": "271038623",
            "title": "Visual Robustness Benchmark for Visual Question Answering (VQA)",
            "text": "Fig. 4 illustrates the ZS-VQA model PNP producing significantly higher average error compared to other models. This is affirmed by Fig. 6, where PNP has higher values in most robustness sub-metrics while encompassing a larger area. Based on the average error, ViLT appears to be the least robust standard VQA model but Tab. 2 and Fig. 6 confirms ViLT as the most robust model based on the other severity-aggregated metrics and the VRE metric. The robustness of BLIP, VLE, and LLaVA variants is close to ViLT based on the VRE metric.",
            "score": 0.164332939999717,
            "section_title": "Model Robustness",
            "char_start_offset": 16326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 532
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006931304931640625
        },
        {
            "corpus_id": "252683131",
            "title": "A Partially Functional Linear Modeling Framework for Integrating Genetic, Imaging, and Clinical Data",
            "text": "-0.231(0.001) -0.033(0.075) -0.256(0.000) -0.161(0.000) 0.148(0.000) --Model 3 0.062(0.219) -0.120(0.202) 0.002(0.953) -0.173(0.008) -0.047(0.064) 0.024(0.353) 0.637(0.000) 0.539(0.000) RAVLT.perc.forgetting Model 1 -0.106(0.005) 0.120(0.081) -0.149(0.000) -0.019(0.685) -0.045(0.018) ---Model 2 -0.107(0.001) 0.295(0.000) -0.125(0.000) -0.009(0.830) -0.047(0.003) 0.188(0.000) --Model 3 -0.054(0.052) 0.022(0.659) -0.054(0.000) -0.170(0.000) 0.003(0.855) 0.093(0.000) 0.989(0.000) 1.481(0.000) TRABSCOR Model 1 -0.036(0.436) -0.088(0.293) -0.230(0.000) -0.021(0.712) 0.003(0.876) ---Model 2 0.004(0.926) -0.131(0.076) -0.182(0.000) 0.023(0.065) 0.042(0.037) 0.180(0.000) --Model 3 0.010(0.803) -0.012(0.867)",
            "score": 0.16433140007588484,
            "section_title": "GIC Pathways",
            "char_start_offset": 32298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00036263465881347656
        },
        {
            "corpus_id": "276903228",
            "title": "UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces",
            "text": "It has an input token limit of 128K, an output token limit of 8192, and a frame rate of 32 f. \n\nLLaVA-NeXT-Video-7B-hf. Released in April 2024, LLaVA-NeXT-Video-7B-hf is a videofocused LMM with 7B parameters. In this experiment, we set the frame rate to 32 f and the output token limit to 512.",
            "score": 0.16433140007588484,
            "section_title": "C.2 Brief Introduction on Baselines",
            "char_start_offset": 31455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 293
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005428314208984375
        },
        {
            "corpus_id": "270560829",
            "title": "DB-GPT-Hub: Towards Open Benchmarking Text-to-SQL Empowered by Large Language Models",
            "text": "Base LLMs.We benchmark a range of medium to large-sized LLM variants from 4 prominent LLM families: GLM (Zeng et al., 2022), Qwen (Bai et al., 2023), Baichuan (Baichuan, 2023) and Llama (Touvron et al., 2023).",
            "score": 0.16433140007588484,
            "section_title": "Prompt-based and Fine-tuning Settings",
            "char_start_offset": 8749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 10,
                    "end": 209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00217437744140625
        },
        {
            "corpus_id": "270067579",
            "title": "Aligning to Thousands of Preferences via System Message Generalization",
            "text": "We investigate how variations in the type and size of the base model affect performance. Initially, we use Mistral 7B v0.2, but we also experiment with LLaMA models.",
            "score": 0.16433140007588484,
            "section_title": "Effect of model scaling",
            "char_start_offset": 26925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 165
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018243789672851562
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "A-OKVQA, COCO Caption, OCR VQA LLaVA-1.5 [132] CLIP ViT Vicuna LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG SVIT [58] CLIP ViT Vicuna SVIT-4.2M",
            "score": 0.16433140007588484,
            "section_title": "Image Classification",
            "char_start_offset": 32163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001720428466796875
        },
        {
            "corpus_id": "276884759",
            "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
            "text": "Table 2 presents the performance comparison of our proposed LLaVE series (LLaVE-0.5B, LLaVE-2B, LLaVE-7B) against existing baseline models. Among the baseline models, our trained VLM2Vec (LLaVA-OV-7B) achieves the highest overall average score of 65.8, surpassing the current state-ofthe-art model, MMRet, which achieves the secondbest score of 64.1. This indicates that a more powerful foundational LMM can lead to better performance in the embedding models. Notably, MMRet excels in retrieval tasks with a score of 69.9, which is attributed to its pretraining on its self-constructed 26M image-text retrieval dataset. VLM2Vec (LLaVA-NeXT-7B-HR) exhibits superior performance in grounding tasks, likely due to its higher input image resolution, which achieves a 22.1-point improvement over VLM2Vec (LLaVA-NeXT-7B-LR). \n\nAlthough previous models have achieved strong performance, our LLaVE series demonstrates consistent improvements over the best baseline across all metrics. LLaVA-NeXT-7B achieves a state-of- Table 2: Results on the MMEB benchmark. IND represents the in-distribution dataset, and OOD represents the out-of-distribution dataset. In UniIR, the FF and SF subscripts under CLIP or BLIP represent feature-level fusion and score-level fusion, respectively. LLaVA-NeXT-7B-LR indicates the use of low-resolution (336\u00d7336) image inputs, while LLaVA-NeXT-7B-HR refers to the use of high-resolution (1344\u00d71344) image inputs. The reported scores are the average Precision@1 over the corresponding datasets. The best results are marked in bold, and the second-best results are underlined. Part of the baseline results are sourced from (Jiang et al., 2024b) and (Zhou et al., 2024). Detailed results and qualitative evaluations can be found in Appendix A.3 and Appendix A.4.",
            "score": 0.16430186751261858,
            "section_title": "Main Results",
            "char_start_offset": 15468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1780
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1014404296875
        },
        {
            "corpus_id": "268857227",
            "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
            "text": "In this paper, we introduce LLaVA-Gemma, a suite of vision-language assistants trained from the Gemma Large Language Model (LLM) variants, Gemma-2B and Gemma-7B [17].Our work is inspired by the rapid progress in small but capable visual language models (VLMs), such as LLaVA-Phi [23], which have demonstrated remarkable efficiency and effectiveness in various language understanding tasks.LLaVA-Gemma distinguishes itself among small VLMs due to the public release of similarly trained, different-sized LLMs Gemma-2B and Gemma-7B.\n\nThe unique release of the Gemma models offers an opportunity to contrast model performance in relation to pa-rameter size and visual encoding capabilities.By possessing two variants with different parameter sizes, LLaVA-Gemma allows researchers to investigate the trade-offs between computational efficiency and the richness of visual and linguistic understanding.With these two variants, we perform a deeper exploration of how varying levels of model complexity influence the effectiveness of visual encoding, providing valuable insights into the optimization of small VLMs for diverse tasks and environments.Furthermore, the use of significantly more unique tokens, at 256k, offers an opportunity to investigate how a massively increased token set effects multi-modal performance.\n\nRecent advancements in (LLMs) [20] and multimodal foundation models (MMFMs) [7] have propelled the interest and development of Large Multimodal Models (LMMs).Notable models like GPT-4 [1], LLaVA [9,10], and their derivatives have demonstrated significant performance in vision-language tasks such as Visual Question Answering (VQA) and image captioning [5].However, the computational demands of deploying these models have led to the exploration of small-scale LMMs.Our work aims to provide a unified analysis of small-scale LMMs, examining how model selections, training recipes, and data contribute to performance, which is distinct from existing works such as LLaVA-Phi.\n\nOur contributions are as follows:\n\n1. We introduce LLaVA-Gemma, a MMFM that leverages the compact yet powerful Gemma language models for efficient multimodal interactions.",
            "score": 0.16407685407391265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1314
                },
                {
                    "start": 1316,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1989
                },
                {
                    "start": 1991,
                    "end": 2024
                },
                {
                    "start": 2026,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "152282269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15478515625
        },
        {
            "corpus_id": "268857227",
            "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
            "text": "1. We introduce LLaVA-Gemma, a MMFM that leverages the compact yet powerful Gemma language models for efficient multimodal interactions.2. We extensively evaluate Gemma-2B and Gemma-7B model variants provides valuable insights into the tradeoffs between computational efficiency and the richness of visual and linguistic understanding in LLMs.",
            "score": 0.1639402851660281,
            "section_title": "Introduction",
            "char_start_offset": 2041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06597900390625
        },
        {
            "corpus_id": "277510513",
            "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
            "text": "This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning. \n\nLlama [209], developed by Meta, is a large-scale language model series designed to enhance language understanding and generation, with applications in visual-language tasks, particularly cross-modal reasoning. Llama [209] uses large-scale pretraining to boost text understanding and generation, improving performance in tasks like visual reasoning and image-to-text generation when combined with visual data. Its strong reasoning abilities, especially in commonsense reasoning and text generation, make it effective for complex cross-modal tasks. As task complexity grows, reasoning and computational efficiency are vital for VLM development. To improve computational efficiency and real-time reasoning on complex tasks, LLaVA [211] introduces instruction tuning, enhancing the model's adaptability across various tasks like VQA and image captioning. Instruction tuning allows LLaVA [211] to adjust its reasoning strategy based on natural language instructions, enabling quick adaptation to new multimodal tasks and boosting reasoning efficiency. \n\nMoE-LLaVA [208] uses a Mixture of Experts (MoE) architecture, activating only the relevant expert networks to reduce computational burden and improve reasoning efficiency, especially on large datasets. Qwen-VL [206] enhances computational efficiency with Multimodal Rotary Positional Embedding (M-RoPE) and dynamic resolution, improving reasoning speed and accuracy in high-resolution image processing by better understanding spatial relationships. InternVL [212] further optimizes efficiency through expert scheduling and distributed computing, ensuring dynamic resource allocation for large-scale multimodal data. Ovis1.6 [213] balances computational efficiency and reasoning accuracy, making it ideal for real-time visual-language tasks. These advancements allow VLMs to handle complex scenarios in fields like autonomous driving, medical image analysis, and robotics. By improv-ing both reasoning and computational efficiency, VLMs tackle complex cross-modal tasks, enhancing decision-making across various domains. As VLMs evolve, they will play a crucial role in more complex multimodal tasks, providing efficient and accurate solutions for intelligent decision systems.",
            "score": 0.16379692011643768,
            "section_title": "Transformer Variants and Large Vision-Language Models",
            "char_start_offset": 68795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 57,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 899,
                    "end": 904,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1055,
                    "end": 1060,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07159423828125
        },
        {
            "corpus_id": "270063201",
            "title": "Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs",
            "text": "To study the impact of different factors (e.g. pretraining, mapping module) on the internal representations (e.g. implicit alignment), we devise different variants of the LLaVA-1.5 [19] model that differ from the original model as follows: LLaVA-1.5-2 (LLM kept frozen), LLaVA-1.5-3 (LLM kept frozen, without pretraining) and LLaVA-1.5-4 (LLM kept frozen, without pretraining and with transformer mapping module similar. The latter is very similar to the models used in the ST setup, which ensure comparable observatoins. All these models are based on the Vicuna-v1.5-7B LLM. \n\nWe follow the same training setup of LLaVA-1.5 [19], including the training data, steps and hyperparameters. \n\nFor analysis in the paper (e.g. Sec. \n\n3), we focus on Vicuna-v1.5 as it is shared by both setups. For the ST setup, we use unimodal encoders, such as ViT, TimeSformer and AST that are not aligned with text.",
            "score": 0.16326858947338307,
            "section_title": "D.1.2 MT setup",
            "char_start_offset": 46854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1842041015625
        },
        {
            "corpus_id": "268179623",
            "title": "Assessing the Utility of Multimodal Large Language Models (GPT-4 Vision and Large Language and Vision Assistant) in Identifying Melanoma Across Different Skin Tones",
            "text": "Across all prompts analyzing feature conditioning, GPT-4V correctly identified the melanoma, while LLaVA did not, when color, diameter, or both were held constant (Figure 1). This suggests these features influence melanoma detection in LLaVA, with less importance placed on symmetry and border. Both LLMs were susceptible to color bias, as when a pigment was darkened with all other features held constant, the lesion was believed to be malignant. Alternatively, when pigments were lightened, GPT-4V appropriately recognized this alteration, while LLaVA did not. Finally, image manipulation did not impact GPT-4V's diagnostic abilities; however, LLaVA was unable to detect these manipulations and was vulnerable to visual referring associated with melanoma manifestations. The red lines added around the nevus's edges were identified as sunburned skin when presented to LLaVA, while GPT-4V correctly recognized these annotations as useful for monitoring lesion evolution or communicating specific concerns between health care providers. Although limitations are present, GPT-4V can accurately differentiate between benign and melanoma lesions. Performing additional training of these LLMs on specific conditions can improve their overall performance. Despite our findings, it is critical to account for and address limitations such as reproduction of existing biases, hallucinations, and visual prompt injection vulnerabilities and incorporate validation checks before clinical uptake [9]. Recently, the integration of technology within medicine has accelerated, and AI has been used in dermatology to augment the diagnostic process and improve clinical decision-making [10]. There is an urgent global need to address high volumes of skin conditions posing health concerns, and the integration of multimodal LLMs, such as GPT-4V, into health care has the potential to deliver material increases in efficiency and improve education and patient care.",
            "score": 0.16314035387033804,
            "section_title": "Discussion",
            "char_start_offset": 4357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 1670,
                    "end": 1674,
                    "matchedPaperCorpusId": "237485973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01312255859375
        },
        {
            "corpus_id": "269921145",
            "title": "TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models",
            "text": "We conduct experiments (see detailed settings in Appendix A) to reproduce several variants of TinyLLaVA and assess their performance on standard benchmarks within our codebase, as outlined in Table 1, achieving slightly superior overall performance compared to the performance reported in the original paper (Zhou et al., 2024) that were trained with the LLaVA codebase.These results highlight the reliability of our TinyLLaVA Factory and offer valuable insights into the performance of TinyLLaVA.",
            "score": 0.16286526716111693,
            "section_title": "Experiments",
            "char_start_offset": 5547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06500244140625
        },
        {
            "corpus_id": "271098074",
            "title": "SoupLM: Model Integration in Large Language and Multi-Modal Models",
            "text": "Training large language models (LLMs) and multimodal LLMs necessitates significant computing resources, and existing publicly available LLMs are typically pre-trained on diverse, privately curated datasets spanning various tasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained with LLaMA base models using very different training recipes, tasks, and data modalities. The training cost and complexity for such LLM variants grow rapidly. In this study, we propose to use a soup strategy to assemble these LLM variants into a single well-generalized multimodal LLM (SoupLM) in a cost-efficient manner. Assembling these LLM variants efficiently brings knowledge and specialities trained from different domains and data modalities into an integrated one (e.g., chatbot speciality from user-shared conversations for Vicuna, and visual capacity from vision-language data for LLaVA), therefore, to avoid computing costs of repetitive training on several different domains. We propose series of soup strategies to systematically benchmark performance gains across various configurations, and probe the soup behavior across base models in the interpolation space.",
            "score": 0.16272535694136542,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07275390625
        },
        {
            "corpus_id": "265551875",
            "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts",
            "text": "Tables 19 to 23 display various instances where GPT-4V encountered challenges on ViP-Bench. For instance, Table 19 illustrates a case where both GPT-4V and LLaVA-1.5 incorrectly interpret a yellow scribble, with GPT-4V mistaking a yellow circle for the scribble, leading to erroneous responses. In contrast, ViP-LLaVA accurately answers the questions. Another example in Table 23 (a) shows GPT-4V incorrectly identifying a person marked by a pink point as holding ski poles and LLaVA-1.5 as holding a green flag, while ViP-LLaVA successfully makes the correct predic-",
            "score": 0.16243617350723424,
            "section_title": "F.4. Failure cases of GPT-4V",
            "char_start_offset": 44059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 567
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0141143798828125
        },
        {
            "corpus_id": "277435677",
            "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping",
            "text": "For LLaVA and LLaVA-HD setting, we use input resolutions of 336 and a dynamic resolution ranging from 336 to 672, respectively, with CLIP ViT-L/336px [87] as the vision encoder and LLaMA3 8B Instruct [28]  In all experiments, we used a learning rate of 1e \u22123 for pre-training, which is reduced to 1e \u22125 during SFT. Training follows a cosine decay schedule after a 3% warm-up phase. For skipped token selection, unless otherwise specified, the LLaVA setting includes 156 skipped tokens and 100 retained tokens, the LLaVA-HD setting has 576 skipped and 576 retained tokens, and the CoS setting consists of 256 skipped and 272 retained tokens. All skipped tokens undergo a token merging process to enhance efficiency.",
            "score": 0.16243617350723424,
            "section_title": "Training setup",
            "char_start_offset": 19688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045867919921875
        },
        {
            "corpus_id": "275789161",
            "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding",
            "text": "General LLaVA-Video-178K [25], ShareGPT4o-Video [28], FineVideo [97], CinePile [98], ShareGemini-k400 [99], ShareGemini-WebVID [99], VCG-Human [22], VCG-Plus [22], VideoLLaMA2 in-house data, Temporal Grounding in-house data",
            "score": 0.16243617350723424,
            "section_title": "Video & Text Data",
            "char_start_offset": 20778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016164779663085938
        },
        {
            "corpus_id": "269149684",
            "title": "LLMorpheus: Mutation Testing using Large Language Models",
            "text": "LLMorpheus generates 1,217 distinct mutants at temperature 0 of which 1,181 (97.04%) are common to all five runs.At temperature 0.25, the number number of distinct mutants increases to 2,354, of which 447 (18.99%) are common to all five runs.At temperature 0.5, there are 3,196 distinct mutants of which 205 (6.41%) are common to all runs.At temperature 1.0, there are 4,200 distinct mutants, of which 17 (0.4%) are common to all runs, meaning that, effectively, at temperature 1.0 each run produces completely different mutants.The results for the other subject applications are similar.\n\nAt temperatures \u2264 0.5, LLMorpheus generally produces similar numbers of mutants, of which a similar number survives.At temperature 1.0, the number of generated (and surviving) mutants declines noticeably because the LLM produces more candidate mutants that are syntactically invalid.The variability of results is inversely dependent on the temperature, with mostly the same mutants being produced in different runs at temperature 0, and mostly different mutants being produced in each run at temperature 1.",
            "score": 0.16243617350723424,
            "section_title": "RQ1: How many mutants does LLMorpheus",
            "char_start_offset": 28379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 588
                },
                {
                    "start": 590,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1096
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0012941360473632812
        },
        {
            "corpus_id": "277596174",
            "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving",
            "text": "The effect of architecture variation can be observed by comparing the performance of LLaVA-v1.6-mistral-7b and LLaVA-v1.6-vicuna-7b in table 4. LLaVA-v1.6-mistral-7B is based on Mistral-7B as its foundation LLM, while LLaVA-v1.6-vicuna-7B is based on Vicuna-7B. The table demonstrates that LLaVA-v1.6-mistral-7B consistently outperforms LLaVA-v1.6-vicuna-7B across all types of QA tasks. This aligns with the fact that Vicuna-7B excels in conversational fluency [11], whereas Mistral-7B demonstrates better numerical and logical reasoning capabilities, enabling it to better comprehend spatial concepts and perform comparisons.",
            "score": 0.16243617350723424,
            "section_title": "Effect of Backbone Architecture",
            "char_start_offset": 21924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1041259765625
        },
        {
            "corpus_id": "266361969",
            "title": "VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering",
            "text": "We have recorded the differences in recall values between the question-wise and caption-wise approaches for the validation set of CIRR datasets of both CLIP4CIR+VQA4CIR and SPRC+VQA4CIR in Fig. 7. It can be seen that the caption-wise approach performs better than the questionwise approach because it can accurately reflect whether a candidate image is consistent with relative caption. In light of this, we adopted the caption-wise approach for re-ranking during the inference stage in our experiments. \n\nFine-tune LLaVA in Question-wise vs. Caption-wise. To train LLaVA to adapt to CIR, we designed a loss to iteratively check whether the answers to all questions are correct. Here, we discuss the differences between training LLaVA in a question-wise manner and a caption-wise manner. We summarize the performance of these two methods on the  validation sets of CIRR in Fig. 8. It can be seen that both methods show differences in CIRR, the average recall values of CLIP4CIR+VQA4CIR and SPRC+VQA4CIR differ by 2.62% and 3.27% respectively. The caption-wise method provides a higher recall because it can accurately reflect whether a candidate image is consistent with relative caption. Thus, we adopt the caption-wise method to train LLaVA in our experiments. Discussion on Different VQA Models. We employ different LVLM models, i.e., the pre-trained LLaVA [34] and instruct-BLIP [11], as VQA models to assess the effect of LVLM. We provide the performance of the two variants on the val-idation sets of two datasets in Table 4. One can see that both approaches significantly underperform our fine-tuned LLaVA [34] because directly using pre-trained LVLMs does not generalize well to our task. Moreover, using the pretrained LLaVA [34] and instructBLIP directly do not show a substantial difference in recall values, i.e., average recalls: LLaVA [34]: 81.98 vs. InstructBLIP [11]: 80.59.",
            "score": 0.16234914732295835,
            "section_title": "Ablation Studies",
            "char_start_offset": 20777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1890
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037200927734375
        },
        {
            "corpus_id": "96444938",
            "title": "Single cut and multicut SDDP with cut selection for multistage stochastic linear programs: convergence proof and numerical experiments",
            "text": "(P2) Instances where one variant with cut selection (in general LML 1) is much quicker than the other one, both of them being quicker than their counterpart without cut selection. There are 8 of these instances for SDDP (Instances 2, 5, 6, 10, 11, 12, 14, 18) and 4 of these instances for MuDA (Instances 8, 9, 10, 11). (P3) Instances where at least one variant with cut selection (in general Level 1) is much slower than its counterpart without cut selection. There are 5 of these instances for SDDP (Instances 3, 19, 20, 21, 22) and 10 of these instances for MuDA (3,5,7,12,17,18,19,20,21,22). To understand the impact of the cut selection strategies on the computational time, it is useful to analyze the number of iterations and the proportion of cuts selected along the iterations of the algorithm by the variants with cut selection in cases (P1), (P2), and (P3) described above. \n\nAs examples of instances of type (P1), consider Instance 1 for SDDP and MuDA and Instance 2 for MuDA. For Instance 1, the mean proportion (over the iterations of the algorithm) of cuts selected by all variants with cut selection, i.e., SDDP CS 1, SDDP CS 2, CuSMuDA CS 1, and CuSMuDA CS 2, tends to increase with the stage and is very low for nearly all stages (at most 0.1 until stage 30 and at most 0.2 until stage 40, knowing that there are T = 48 stages for that instance); see the bottom plot of Figure 2 which represents the evolution of the mean proportion of cuts selected as a function of the stage. This makes sense since at the last stage, the cuts for functions Q T (\u2022, \u03be T j ) are exact (and as we recalled, all of them are selected with CuSMuDA CS 1) but not necessarily for stages t < T because for these stages approximate recourse functions are used and the approximation errors propagate backward.",
            "score": 0.16214590105330834,
            "section_title": "observe that for this problem the cuts have no intercept).",
            "char_start_offset": 29778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "152997394"
                },
                {
                    "start": 571,
                    "end": 573,
                    "matchedPaperCorpusId": "6126071"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "15236087"
                },
                {
                    "start": 576,
                    "end": 579,
                    "matchedPaperCorpusId": "8351548"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "10498947"
                },
                {
                    "start": 585,
                    "end": 588,
                    "matchedPaperCorpusId": "56456197"
                },
                {
                    "start": 588,
                    "end": 591,
                    "matchedPaperCorpusId": "16342326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0125274658203125
        },
        {
            "corpus_id": "268732657",
            "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
            "text": "The performance of P 2 G on visual reasoning benchmarks is presented in Table 2. On text-rich visual reasoning, P 2 G significantly outperform baselines, including the vanilla LLaVA, by more than doubled (3\u00d7 on DocVQA, 2.4\u00d7 on ChartVQA), and also greatly surpass MLLMs that dedicated tuned for text-rich visual reasoning, e.g., LLaVAR and TG-Doc, and even surpasses 13B LLaVA variants.On general visual reasoning benchmarks, P 2 G also enjoys a consistent improvement over LLaVA and In-strtuctBLIP, demonstrating the superiority of P 2 G.",
            "score": 0.16211971039211548,
            "section_title": "Performance on Visual Reasoning",
            "char_start_offset": 19215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0202178955078125
        },
        {
            "corpus_id": "7498796",
            "title": "Dynamics of Immune Escape during HIV/SIV Infection",
            "text": "The interaction of the HIV quasispecies with the CTL response appears to be complex which makes the analysis of experimental data difficult. Mathematical and computational models have been helpful to investigate how different processes influence each other. For example, a previous model described how shifting immunodominance and antigenic oscillations can occur during HIV infection [27]. Now, with a model incorporating multiple CTL responses together with escape and compensatory mutations, we show that the dynamics of the CTL clones are sufficient to explain the sequential and late occurrence of escape variants. Upon loss of antigenic stimulation, CTL clones disappear slowly [41,42]. Concomitantly, other CTL clones can increase in size and induce more efficient killing that leads to further escape that is sequentially distributed over many years after infection. Interestingly, the outgrowth of escape variants does not occur earlier even though they are always present in the viral population. It has been suggested that a small effective population size of HIV-infected cells is responsible for the late production and selection of escape variants [43]. However, a recent study argues that stochastic effects play a minor role for the appearance of deleterious and beneficial mutations in HIV [23]. Also, our simulations show that viral evolution is fairly deterministic. The model allows us to simulate two identical 'patients' that are infected with the same virus and have the same CTL repertoire. In that case, the stochastic generation of the viral variants results in a slow increase in variation over years (results not shown). For example, the emergence of a certain escape variant is predictable during the first years after infection but becomes more variable later on. This is in line with a recent study where concordant evolution of HIV has been observed in mono-zygotic twins early during infection but more variation has been shown at later stages of the infection [21].\n\nAnother explanation for the late appearance of escape variants that has been put forward is the waiting time for compensatory mutations [24]. Our simulations show that single compensatory mutations are expected to be present in the viral quasispecies and therefore should not slow down the emergence of escape. However, if more compensatory mutations are needed to restore the fitness loss of an escape variant they can delay their occurrence (",
            "score": 0.1620742425325256,
            "section_title": "Discussion",
            "char_start_offset": 18426,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "4349096"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "7388506"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "773186"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "36566582"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "2413485"
                },
                {
                    "start": 1994,
                    "end": 1998,
                    "matchedPaperCorpusId": "3881409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00208282470703125
        },
        {
            "corpus_id": "269362176",
            "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
            "text": "In this section, we first investigate the capacity of LLaVA-1.5 in SoM, concerning its attention sensibility to the numeric IDs tagged on the objects and its answer to the SoM query.We show an example task to list a series of objects tagged with numeric IDs in Figure 2, in which the attention map is extracted from LLaVA-1.5 based on the SoM query (e.g., \"I have labeled a bright numeric ID at the center for each visual object in the image.Please enumerate their names.\").The top 20 image patches with the highest average attention weights across the user query tokens are highlighted in transparent red regions.\n\nWe can observe from the highly attended regions of LLaVA-1.5 that the numeric ID tags can be easily and correctly attended by LLaVA-1.5 along with their associated objects (e.g., bird, vase, and branches).Such capacities in locating numeric ID tags may have been acquired by LLaVA-1.5 from its pretraining tasks in OCR and also benefited from the strong OCR abilities of the ViT feature encoder (Radford et al., 2021) adopted by LLaVA-v1.5.However, the response prompted by the user query in the first example of Figure 2 suggests that LLaVA-1.5 cannot follow the SoM instruction to list all the items.Instead of providing the object descriptions corresponding to all the numeric ID tags, LLaVA-1.5 responds with a general image caption, due to a large portion of image captioning samples in its pretraining stage.From the second example of Figure 2, we can also observe that although LLaVA-1.5 generates a list of tag IDs with object names, it cannot accurately associate the tags to corresponding objects, causing the model to hallucinate the descriptions of these objects.Table 1: Examined pretraining (1-2) and instruction-tuning (3-5) datasets in our preliminary study.",
            "score": 0.16204047767052693,
            "section_title": "Visualizing SoM Prompting on LLaVA",
            "char_start_offset": 7717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 474
                },
                {
                    "start": 474,
                    "end": 614
                },
                {
                    "start": 616,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1790
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02532958984375
        },
        {
            "paperId": "178f7331bb2c583cad2f99d9ad03920143ecaf3a",
            "corpusId": 257834126,
            "title": "BPCE: A Prototype for Co-Evolution between Business Process Variants through Configurable Process Model",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 49,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.17388",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.17388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2213649013",
                    "name": "Linyue Liu"
                },
                {
                    "authorId": "2201605378",
                    "name": "Xi Guo"
                },
                {
                    "authorId": "144815594",
                    "name": "C. Ouyang"
                },
                {
                    "authorId": "2092984374",
                    "name": "Patrick C. K. Hung"
                },
                {
                    "authorId": "2142723322",
                    "name": "Hongyan Zhang"
                },
                {
                    "authorId": "2793678",
                    "name": "K. He"
                },
                {
                    "authorId": "2212965505",
                    "name": "Chen Mo"
                },
                {
                    "authorId": "3162519",
                    "name": "Zaiwen Feng"
                }
            ],
            "abstract": "With the continuous development of business process management technology, the increasing business process models are usually owned by large enterprises. In large enterprises, different stakeholders may modify the same business process model. In order to better manage the changeability of processes, they adopt configurable business process models to manage process variants. However, the process variants will vary with the change in enterprise business demands. Therefore, it is necessary to explore the co-evolution of the process variants so as to effectively manage the business process family. To this end, a novel framework for co-evolution between business process variants through a configurable process model is proposed in this work. First, the mapping relationship between process variants and configurable models is standardized in this study. A series of change operations and change propagation operations between process variants and configurable models are further defined for achieving propagation. Then, an overall algorithm is proposed for achieving co-evolution of process variants. Next, a prototype is developed for managing change synchronization between process variants and configurable process models. Finally, the effectiveness and efficiency of our proposed process change propagation method are verified based on experiments on two business process datasets. The experimental results show that our approach implements the co-evolution of process variants with high accuracy and efficiency.",
            "corpus_id": "257834126",
            "text": "With the continuous development of business process management technology, the increasing business process models are usually owned by large enterprises. In large enterprises, different stakeholders may modify the same business process model. In order to better manage the changeability of processes, they adopt configurable business process models to manage process variants. However, the process variants will vary with the change in enterprise business demands. Therefore, it is necessary to explore the co-evolution of the process variants so as to effectively manage the business process family. To this end, a novel framework for co-evolution between business process variants through a configurable process model is proposed in this work. First, the mapping relationship between process variants and configurable models is standardized in this study. A series of change operations and change propagation operations between process variants and configurable models are further defined for achieving propagation. Then, an overall algorithm is proposed for achieving co-evolution of process variants. Next, a prototype is developed for managing change synchronization between process variants and configurable process models. Finally, the effectiveness and efficiency of our proposed process change propagation method are verified based on experiments on two business process datasets. The experimental results show that our approach implements the co-evolution of process variants with high accuracy and efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0017070770263671875
        },
        {
            "paperId": "5bdf77fc3ba627314796100bec2a3ffd05792f78",
            "corpusId": 273038242,
            "title": "The Existence and Development of Variants in English Writing Teaching in International Corpus based on Time Series Data Analysis",
            "venue": "Scalable Computing : Practice and Experience",
            "year": 2024,
            "referenceCount": 18,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.12694/scpe.v25i6.3354",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.12694/scpe.v25i6.3354?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.12694/scpe.v25i6.3354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323822026",
                    "name": "Dongfang Li"
                }
            ],
            "abstract": "This study investigates the evolution and prevalence of variant forms in English writing instruction within an international context, utilizing a time series analysis of corpus data. Recognizing the dynamic nature of language, especially in educational settings, this research aims to identify and analyze the changes in teaching methodologies and language usage over time. By examining a comprehensive corpus compiled from diverse international sources, the study focuses on how English writing instruction has adapted to linguistic variations and evolving pedagogical approaches.The methodology involves a quantitative analysis of corpus data spanning several decades, enabling a longitudinal view of trends in English writing teaching. The corpus includes a wide range of educational materials, academic publications, and teaching resources from different countries, ensuring a global perspective. Time series analysis is employed to track the frequency and context of various teaching practices and linguistic forms, offering insights into their development and dissemination.Key findings reveal a significant shift in teaching strategies, reflecting an increased emphasis on learner-centered approaches and the integration of digital technologies. Variants in language usage, influenced by cultural and regional differences, are also evident, highlighting the importance of context-specific teaching methods.",
            "corpus_id": "273038242",
            "text": "This study investigates the evolution and prevalence of variant forms in English writing instruction within an international context, utilizing a time series analysis of corpus data. Recognizing the dynamic nature of language, especially in educational settings, this research aims to identify and analyze the changes in teaching methodologies and language usage over time. By examining a comprehensive corpus compiled from diverse international sources, the study focuses on how English writing instruction has adapted to linguistic variations and evolving pedagogical approaches.The methodology involves a quantitative analysis of corpus data spanning several decades, enabling a longitudinal view of trends in English writing teaching. The corpus includes a wide range of educational materials, academic publications, and teaching resources from different countries, ensuring a global perspective. Time series analysis is employed to track the frequency and context of various teaching practices and linguistic forms, offering insights into their development and dissemination.Key findings reveal a significant shift in teaching strategies, reflecting an increased emphasis on learner-centered approaches and the integration of digital technologies. Variants in language usage, influenced by cultural and regional differences, are also evident, highlighting the importance of context-specific teaching methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.003551483154296875
        },
        {
            "paperId": "d57f59d0bcaffa5cbcac9079484adf618664cbaf",
            "corpusId": 272752271,
            "title": "YOLOv1 to YOLOv10: A Comprehensive Review of YOLO Variants and Their Application in Medical Image Detection",
            "venue": "Journal of Artificial Intelligence Practice",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.23977/jaip.2024.070314",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23977/jaip.2024.070314?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23977/jaip.2024.070314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321974319",
                    "name": "Ahealisi Yeerjiang"
                },
                {
                    "authorId": "2314070290",
                    "name": "Zongyu Wang"
                },
                {
                    "authorId": "2314067887",
                    "name": "Xiangtong Huang"
                },
                {
                    "authorId": "2322154602",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2321992294",
                    "name": "Qi Chen"
                },
                {
                    "authorId": "2322099097",
                    "name": "Yucheng Qin"
                },
                {
                    "authorId": "2164095406",
                    "name": "Jia He"
                }
            ],
            "abstract": ": The rapid evolution of computer vision has elevated object detection to a central task within the field. In medicine, automated lesion detection has the potential to greatly improve diagnostic efficiency for clinicians. The extraordinary success of deep learning in computer vision has motivated researchers globally to apply these advancements to medical image analysis. Deep learning techniques have demonstrated superior performance in medical image classification, detection, segmentation, registration, and retrieval compared to traditional methods. Among these, the YOLO (You Only Look Once) series of algorithms stands out for their exceptional speed and accuracy, making them a popular choice for medical image detection. This paper presents the underlying principles and structure of the classic YOLO algorithms, reviews their current applications in medical image detection, addresses existing challenges, and explores future directions for the application of YOLO in this domain.",
            "corpus_id": "272752271",
            "text": ": The rapid evolution of computer vision has elevated object detection to a central task within the field. In medicine, automated lesion detection has the potential to greatly improve diagnostic efficiency for clinicians. The extraordinary success of deep learning in computer vision has motivated researchers globally to apply these advancements to medical image analysis. Deep learning techniques have demonstrated superior performance in medical image classification, detection, segmentation, registration, and retrieval compared to traditional methods. Among these, the YOLO (You Only Look Once) series of algorithms stands out for their exceptional speed and accuracy, making them a popular choice for medical image detection. This paper presents the underlying principles and structure of the classic YOLO algorithms, reviews their current applications in medical image detection, addresses existing challenges, and explores future directions for the application of YOLO in this domain.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.004520416259765625
        },
        {
            "paperId": "1d90c6646e2cd7ca05d000205368c62dfe358648",
            "corpusId": 271799619,
            "title": "IDEL: An Improved Differential Evolution with Lissajous Mutation",
            "venue": "IEEE Congress on Evolutionary Computation",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CEC60901.2024.10611821?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CEC60901.2024.10611821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2216593882",
                    "name": "\u00c1ngel Casas-Ordaz"
                },
                {
                    "authorId": "2247596704",
                    "name": "Arturo Valdivia"
                },
                {
                    "authorId": "2195247837",
                    "name": "Eduardo H. Haro"
                },
                {
                    "authorId": "2247754850",
                    "name": "Diego Oliva"
                },
                {
                    "authorId": "2305399420",
                    "name": "Luis A. Beltran"
                },
                {
                    "authorId": "1393105609",
                    "name": "Itzel Aranguren"
                },
                {
                    "authorId": "1491932973",
                    "name": "E. Rodr\u00edguez-Esparza"
                },
                {
                    "authorId": "2305398873",
                    "name": "Diego Campos-Pe\u00f1a"
                }
            ],
            "abstract": "Differential Evolution (DE) represents an advanced evolutionary algorithm due to its continuous innovation and advances around its operators and applications. In general, the mutation operator is designed to enhance the performance of Differential Evolution (DE) algorithms. The mutation operator is a vital component in enhancing the effectiveness of DE algorithms. The proposed mutation operator, the Lissajous Mutation (LM) is specifically designed to improve the balance between exploration and exploitation through the geometric origin of the Lissajous curves. This paper introduces an up-to-date version of Differential Evolution with Lissajous Mutation, Multi-crossover, and Dynamic selection (IDEL). This new DE variation presents changes in all the fundamental stages, well-known as Mutation, Crossover, and Selection. Through a series of meticulously designed experiments and rigorous comparisons with existing DE variants, the effectiveness of IDEL is demon-strated. IDEL showcases notable improvements in convergence speed, exploration-exploitation equilibrium, and solution quality across diverse benchmark scenarios. This paper contributes to the ongoing growth of DE and presents a novel way to enhance its overall performance.",
            "corpus_id": "271799619",
            "text": "Differential Evolution (DE) represents an advanced evolutionary algorithm due to its continuous innovation and advances around its operators and applications. In general, the mutation operator is designed to enhance the performance of Differential Evolution (DE) algorithms. The mutation operator is a vital component in enhancing the effectiveness of DE algorithms. The proposed mutation operator, the Lissajous Mutation (LM) is specifically designed to improve the balance between exploration and exploitation through the geometric origin of the Lissajous curves. This paper introduces an up-to-date version of Differential Evolution with Lissajous Mutation, Multi-crossover, and Dynamic selection (IDEL). This new DE variation presents changes in all the fundamental stages, well-known as Mutation, Crossover, and Selection. Through a series of meticulously designed experiments and rigorous comparisons with existing DE variants, the effectiveness of IDEL is demon-strated. IDEL showcases notable improvements in convergence speed, exploration-exploitation equilibrium, and solution quality across diverse benchmark scenarios. This paper contributes to the ongoing growth of DE and presents a novel way to enhance its overall performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00133514404296875
        },
        {
            "paperId": "8118e214e1c18bcc18afdd261ecdf075cdf1eb62",
            "corpusId": 272956508,
            "title": "Self-adaptive Differential Evolution for Multi-objective Optimization with Local Search and Indicator-based Selection",
            "venue": "Academic Journal of Engineering and Technology Science",
            "year": 2024,
            "referenceCount": 2,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.25236/ajets.2024.070507",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.25236/ajets.2024.070507?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.25236/ajets.2024.070507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324454756",
                    "name": "Datong Xie"
                }
            ],
            "abstract": ": Differential Evolution (DE) is an evolutionary algorithm that has become increasingly popular due to its effectiveness and efficiency. However, its adaptation for multi-objective optimization requires further investigation. To address this issue, a novel algorithm called ILSDEMO has been proposed, which incorporates indicator-based selection and local search into a self-adaptive DE. ILSDEMO employs an archive population to store non-dominated solutions, generating an initial population uniformly distributed over the feasible solution space using orthogonal design. Additionally, two variants of DE are used to expedite convergence, while q-Gaussian mutation is utilized to exploit better trial individuals. The k-nearest neighbor rule is used to eliminate crowded solutions, and indicator-based selection is employed to generate a new parent population without diversity preservation. The performance of ILSDEMO was investigated on the test instances from the ZDT series and DTLZ series in terms of the selected indicators. The results suggest that ILSDEMO accurately and evenly approximates the true Pareto front compared to NSGAII, IBEA, and DEMO.",
            "corpus_id": "272956508",
            "text": ": Differential Evolution (DE) is an evolutionary algorithm that has become increasingly popular due to its effectiveness and efficiency. However, its adaptation for multi-objective optimization requires further investigation. To address this issue, a novel algorithm called ILSDEMO has been proposed, which incorporates indicator-based selection and local search into a self-adaptive DE. ILSDEMO employs an archive population to store non-dominated solutions, generating an initial population uniformly distributed over the feasible solution space using orthogonal design. Additionally, two variants of DE are used to expedite convergence, while q-Gaussian mutation is utilized to exploit better trial individuals. The k-nearest neighbor rule is used to eliminate crowded solutions, and indicator-based selection is employed to generate a new parent population without diversity preservation. The performance of ILSDEMO was investigated on the test instances from the ZDT series and DTLZ series in terms of the selected indicators. The results suggest that ILSDEMO accurately and evenly approximates the true Pareto front compared to NSGAII, IBEA, and DEMO.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0009698867797851562
        },
        {
            "paperId": "79aedb796f61ef10b8884d38cddc3f9b7b23aeef",
            "corpusId": 148573130,
            "title": "A Two-Stage Ensemble of Differential Evolution Variants for Numerical Optimization",
            "venue": "IEEE Access",
            "year": 2019,
            "referenceCount": 39,
            "citationCount": 10,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08684239.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2909743?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2909743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50079204",
                    "name": "Xiangping Li"
                },
                {
                    "authorId": "35001399",
                    "name": "Guangming Dai"
                },
                {
                    "authorId": "3334168",
                    "name": "Maocai Wang"
                },
                {
                    "authorId": "103625745",
                    "name": "Zuowen Liao"
                },
                {
                    "authorId": "2881417",
                    "name": "Ke Ma"
                }
            ],
            "abstract": "Differential evolution (DE) is a popular paradigm of evolutionary algorithms, which has been widely applied to solve diverse optimization problems and has gained much success in a series of academic benchmark competitions. Recently, ensemble methods received an increasing attention in designing high-quality DE algorithm. Motivated by this consideration, a novel two-stage ensemble of DE variants, called TSEDE, has been proposed in this paper. In TSEDE, based on the number of fitness evaluations, the whole evolutionary process is divided into two stages. In the former stage, TSEDE using a multi-population-based framework focuses on improving the searchability, which employs three popular and efficient DE variants, namely SHADE, JADE, and \u201cDE/current-to-rand/1.\u201d In the latter stage, LSHADE is used to emphasize the convergence. Moreover, an elite strategy is used to ensure that the current best solution is assigned to each constituent variant at each generation. TSEDE is tested on the CEC2005 benchmark suit and compared with nine typical algorithms. The results confirm that the proposed method is very competitive.",
            "corpus_id": "148573130",
            "text": "Differential evolution (DE) is a popular paradigm of evolutionary algorithms, which has been widely applied to solve diverse optimization problems and has gained much success in a series of academic benchmark competitions. Recently, ensemble methods received an increasing attention in designing high-quality DE algorithm. Motivated by this consideration, a novel two-stage ensemble of DE variants, called TSEDE, has been proposed in this paper. In TSEDE, based on the number of fitness evaluations, the whole evolutionary process is divided into two stages. In the former stage, TSEDE using a multi-population-based framework focuses on improving the searchability, which employs three popular and efficient DE variants, namely SHADE, JADE, and \u201cDE/current-to-rand/1.\u201d In the latter stage, LSHADE is used to emphasize the convergence. Moreover, an elite strategy is used to ensure that the current best solution is assigned to each constituent variant at each generation. TSEDE is tested on the CEC2005 benchmark suit and compared with nine typical algorithms. The results confirm that the proposed method is very competitive.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0013303756713867188
        },
        {
            "paperId": "5ee2008c1eab4545f1e001b2cf3c0578b7a1aafe",
            "corpusId": 267740599,
            "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 64,
            "citationCount": 38,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280105785",
                    "name": "Yuqi Chen"
                },
                {
                    "authorId": "2214617008",
                    "name": "Kan Ren"
                },
                {
                    "authorId": "2952472",
                    "name": "Yansen Wang"
                },
                {
                    "authorId": "2284639405",
                    "name": "Yuchen Fang"
                },
                {
                    "authorId": "2284621991",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "2284689569",
                    "name": "Dongsheng Li"
                }
            ],
            "abstract": "Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers. We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data. The project link is https://seqml.github.io/contiformer/.",
            "corpus_id": "267740599",
            "text": "Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers. We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data. The project link is https://seqml.github.io/contiformer/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0018606185913085938
        },
        {
            "paperId": "126e987f863b14fea3b6a5a828581f0e881452c7",
            "corpusId": 53736491,
            "title": "Fading of collective attention shapes the evolution of linguistic variants.",
            "venue": "Physical Review E",
            "year": 2018,
            "referenceCount": 27,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1811.08465",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.08465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4898427",
                    "name": "Diego E. Shalom"
                },
                {
                    "authorId": "2251722704",
                    "name": "Mariano Sigman"
                },
                {
                    "authorId": "2749479",
                    "name": "G. Mindlin"
                },
                {
                    "authorId": "14152596",
                    "name": "M. Trevisan"
                }
            ],
            "abstract": "Language change involves the competition between alternative linguistic forms. The spontaneous evolution of these forms typically results in monotonic growths or decays, such as in winner-take-all attractor behaviors. In the case of the Spanish past subjunctive, the spontaneous evolution of its two competing forms (ending in -ra and -se) was perturbed by the appearance of the Royal Spanish Academy in 1713, which enforced the spelling of both forms as perfectly interchangeable variants, at a moment in which the -ra form was predominant. Time series extracted from a massive corpus of books reveal that this regulation in fact produced a transient renewed interest for the old form -se which, once faded, left the -ra again as the dominant form up to the present day. We show that time series are successfully explained by a two-dimensional linear model that integrates an imitative and a novelty component. The model reveals that the temporal scale over which collective attention fades is in inverse proportion to the verb frequency. The integration of the two basic mechanisms of imitation and attention to novelty allows us to understand diverse competing objects, with lifetimes that range from hours for memes and news to decades for verbs, suggesting the existence of a general mechanism underlying cultural evolution.",
            "corpus_id": "53736491",
            "text": "Language change involves the competition between alternative linguistic forms. The spontaneous evolution of these forms typically results in monotonic growths or decays, such as in winner-take-all attractor behaviors. In the case of the Spanish past subjunctive, the spontaneous evolution of its two competing forms (ending in -ra and -se) was perturbed by the appearance of the Royal Spanish Academy in 1713, which enforced the spelling of both forms as perfectly interchangeable variants, at a moment in which the -ra form was predominant. Time series extracted from a massive corpus of books reveal that this regulation in fact produced a transient renewed interest for the old form -se which, once faded, left the -ra again as the dominant form up to the present day. We show that time series are successfully explained by a two-dimensional linear model that integrates an imitative and a novelty component. The model reveals that the temporal scale over which collective attention fades is in inverse proportion to the verb frequency. The integration of the two basic mechanisms of imitation and attention to novelty allows us to understand diverse competing objects, with lifetimes that range from hours for memes and news to decades for verbs, suggesting the existence of a general mechanism underlying cultural evolution.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0014047622680664062
        },
        {
            "paperId": "2dbc26a549647da9b55a0d3efc8b8ee95963a085",
            "corpusId": 52048014,
            "title": "Differential Evolution and Chaotic Series",
            "venue": "International Conference on Systems, Signals, and Image Processing",
            "year": 2018,
            "referenceCount": 27,
            "citationCount": 10,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IWSSIP.2018.8439199?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IWSSIP.2018.8439199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1877602",
                    "name": "R. \u0160enke\u0159\u00edk"
                },
                {
                    "authorId": "3394981",
                    "name": "Adam Viktorin"
                },
                {
                    "authorId": "2019164",
                    "name": "Michal Pluhacek"
                },
                {
                    "authorId": "9040993",
                    "name": "T. Kadavy"
                },
                {
                    "authorId": "1786374",
                    "name": "Z. Oplatkov\u00e1"
                }
            ],
            "abstract": "This research deals with the modern and popular hybridization of chaotic dynamics and evolutionary computation. It is aimed at the influence of chaotic sequences on the performance of four selected Differential Evolution (DE) variants. The variants of interest were: original DE/Rand/1/ and DE/Best/1/ mutation schemes, simple parameter adaptive jDE, and the recent state of the art version SHADE. Experiments are focused on the extensive investigation of the different randomization schemes for the selection of individuals in DE algorithm driven by the nine different two-dimensional discrete chaotic systems, as the chaotic pseudo-random number generators. The performances of DE variants and their chaotic/non-chaotic versions are recorded in the one-dimensional settings of 10 $D$ and 15 test functions from the CEC 2015 benchmark.",
            "corpus_id": "52048014",
            "text": "This research deals with the modern and popular hybridization of chaotic dynamics and evolutionary computation. It is aimed at the influence of chaotic sequences on the performance of four selected Differential Evolution (DE) variants. The variants of interest were: original DE/Rand/1/ and DE/Best/1/ mutation schemes, simple parameter adaptive jDE, and the recent state of the art version SHADE. Experiments are focused on the extensive investigation of the different randomization schemes for the selection of individuals in DE algorithm driven by the nine different two-dimensional discrete chaotic systems, as the chaotic pseudo-random number generators. The performances of DE variants and their chaotic/non-chaotic versions are recorded in the one-dimensional settings of 10 $D$ and 15 test functions from the CEC 2015 benchmark.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.001972198486328125
        },
        {
            "paperId": "80b69f65682f81b462d3526f9c70d0d386be1637",
            "corpusId": 132443506,
            "title": "Differential Evolution and Deterministic Chaotic Series: A Detailed Study",
            "venue": "International Conference on Soft Computing MENDEL",
            "year": 2018,
            "referenceCount": 24,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://mendel-journal.org/index.php/mendel/article/download/15/12",
                "status": "GOLD",
                "license": "CCBYNCSA",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.13164/MENDEL.2018.2.061?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.13164/MENDEL.2018.2.061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1877602",
                    "name": "R. \u0160enke\u0159\u00edk"
                },
                {
                    "authorId": "3394981",
                    "name": "Adam Viktorin"
                },
                {
                    "authorId": "1681072",
                    "name": "I. Zelinka"
                },
                {
                    "authorId": "2019164",
                    "name": "Michal Pluhacek"
                },
                {
                    "authorId": "9040993",
                    "name": "T. Kadavy"
                },
                {
                    "authorId": "1786374",
                    "name": "Z. Oplatkov\u00e1"
                },
                {
                    "authorId": "1967561",
                    "name": "V. Bhateja"
                },
                {
                    "authorId": "2253348",
                    "name": "S. Satapathy"
                }
            ],
            "abstract": "This research represents a detailed insight into the modern and popular hybridization of deterministic chaotic\u00a0dynamics and evolutionary computation. It is aimed at the influence of chaotic sequences on the performance of four\u00a0selected Differential Evolution (DE) variants. The variants of interest were: original DE/Rand/1/ and DE/Best/1/\u00a0mutation schemes, simple parameter adaptive jDE, and the recent state of the art version SHADE. Experiments are\u00a0focused on the extensive investigation of the different randomization schemes for the selection of individuals in DE\u00a0algorithm driven by the nine different two-dimensional discrete deterministic chaotic systems, as the chaotic pseudorandom\u00a0number generators. The performances of DE variants and their chaotic/non-chaotic versions are recorded in the\u00a0one-dimensional settings of 10D and 15 test functions from the CEC 2015 benchmark, further statistically analyzed.",
            "corpus_id": "132443506",
            "text": "This research represents a detailed insight into the modern and popular hybridization of deterministic chaotic\u00a0dynamics and evolutionary computation. It is aimed at the influence of chaotic sequences on the performance of four\u00a0selected Differential Evolution (DE) variants. The variants of interest were: original DE/Rand/1/ and DE/Best/1/\u00a0mutation schemes, simple parameter adaptive jDE, and the recent state of the art version SHADE. Experiments are\u00a0focused on the extensive investigation of the different randomization schemes for the selection of individuals in DE\u00a0algorithm driven by the nine different two-dimensional discrete deterministic chaotic systems, as the chaotic pseudorandom\u00a0number generators. The performances of DE variants and their chaotic/non-chaotic versions are recorded in the\u00a0one-dimensional settings of 10D and 15 test functions from the CEC 2015 benchmark, further statistically analyzed.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.001506805419921875
        },
        {
            "paperId": "c50621b0e6905f0b3e06b2791902fe4ce20a7ab9",
            "corpusId": 267686657,
            "title": "TSCAPE: time series clustering with curve analysis and projection on an Euclidean space",
            "venue": "Connection science",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 1,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.tandfonline.com/doi/pdf/10.1080/09540091.2024.2312119?needAccess=true",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/09540091.2024.2312119?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/09540091.2024.2312119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "114064498",
                    "name": "J\u00e9r\u00e9my Renaud"
                },
                {
                    "authorId": "2196585532",
                    "name": "Rapha\u00ebl Couturier"
                },
                {
                    "authorId": "2257857341",
                    "name": "Christophe Guyeux"
                },
                {
                    "authorId": "2155993621",
                    "name": "Benoit Courjal"
                }
            ],
            "abstract": "The ever-growing use of digital systems has led to the accumulation of vast datasets, particularly time series, depicting the temporal evolution of variables and systems. Analysing these time series presents a tremendous challenge due to their inherent complexity and heterogeneity. Addressing an industrial need in the pharmaceutical wholesale sector, this paper introduces a new clustering method for time series: TSCAPE. The TSCAPE method uses a distance matrix calculated using dynamic time warping, followed by multidimensional scaling to project time series into a 2D Euclidean space, thus improving the last clustering stage by K-Means. Unlike conventional techniques, this approach, based on clustering of representation of distances in a Euclidean plane rather than on the curve shape, directly enhances the efficiency of the clustering process. The methodology exhibits significant potential for diverse applications, accommodating varied data types and irregular time series shapes. The research compares multiple variants and proposes metrics to assess their effectiveness on two open-access datasets. The results demonstrate the method's superiority over \u201conly distance comparison clustering techniques\u201d, like dynamic time warping and K-Means, with future prospects aimed at predictive applications and refining the clustering process by exploring alternative, more powerful clustering algorithms.",
            "corpus_id": "267686657",
            "text": "The ever-growing use of digital systems has led to the accumulation of vast datasets, particularly time series, depicting the temporal evolution of variables and systems. Analysing these time series presents a tremendous challenge due to their inherent complexity and heterogeneity. Addressing an industrial need in the pharmaceutical wholesale sector, this paper introduces a new clustering method for time series: TSCAPE. The TSCAPE method uses a distance matrix calculated using dynamic time warping, followed by multidimensional scaling to project time series into a 2D Euclidean space, thus improving the last clustering stage by K-Means. Unlike conventional techniques, this approach, based on clustering of representation of distances in a Euclidean plane rather than on the curve shape, directly enhances the efficiency of the clustering process. The methodology exhibits significant potential for diverse applications, accommodating varied data types and irregular time series shapes. The research compares multiple variants and proposes metrics to assess their effectiveness on two open-access datasets. The results demonstrate the method's superiority over \u201conly distance comparison clustering techniques\u201d, like dynamic time warping and K-Means, with future prospects aimed at predictive applications and refining the clustering process by exploring alternative, more powerful clustering algorithms.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.001964569091796875
        },
        {
            "paperId": "435cf3725123cec56cc6cbd54a85661b0592d157",
            "corpusId": 275119337,
            "title": "A Time Series Analysis of Assertions in the Linux Kernel",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.19465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1826250",
                    "name": "Jukka Ruohonen"
                }
            ],
            "abstract": "Assertions are a classical and typical software development technique. These are extensively used also in operating systems and their kernels, including the Linux kernel. The paper patches a gap in existing knowledge by empirically examining the longitudinal evolution of assertion use in the Linux kernel. According to the results, the use of assertions that cause a kernel panic has slightly but not substantially decreased from the kernel's third to the sixth release series. At the same time, however, the use of softer assertion variants has increased; these do not cause a panic by default but instead produce warnings. With these time series results, the paper contributes to the existing but limited empirical knowledge base about operating system kernels and their long-term evolution.",
            "corpus_id": "275119337",
            "text": "Assertions are a classical and typical software development technique. These are extensively used also in operating systems and their kernels, including the Linux kernel. The paper patches a gap in existing knowledge by empirically examining the longitudinal evolution of assertion use in the Linux kernel. According to the results, the use of assertions that cause a kernel panic has slightly but not substantially decreased from the kernel's third to the sixth release series. At the same time, however, the use of softer assertion variants has increased; these do not cause a panic by default but instead produce warnings. With these time series results, the paper contributes to the existing but limited empirical knowledge base about operating system kernels and their long-term evolution.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00234222412109375
        },
        {
            "paperId": "ba763894f4012cfbeb0e47b1f319fc72e02e49a5",
            "corpusId": 250543531,
            "title": "COVID-19 VARIANTS OF CONCERNS TRACKING: HOW WE EASED OUT THE WHOLE PROCESS THROUGH OPEN-SOURCE SOFTWARE IN MADHYA PRADESH, INDIA",
            "venue": "Asian Journal of Pharmaceutical and Clinical Research",
            "year": 2022,
            "referenceCount": 11,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://innovareacademics.in/journals/index.php/ajpcr/article/download/44895/26659",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22159/ajpcr.2022.v15i7.44895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22159/ajpcr.2022.v15i7.44895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "113586431",
                    "name": "V. Sinha"
                },
                {
                    "authorId": "81959017",
                    "name": "Divya Swami"
                },
                {
                    "authorId": "2015329093",
                    "name": "Aanchal Bijlwan"
                },
                {
                    "authorId": "2144041356",
                    "name": "Saurav Kumar"
                },
                {
                    "authorId": "1680674347",
                    "name": "Yogesh S. Kaurav"
                },
                {
                    "authorId": "2176289044",
                    "name": "Vineet Kumar tiwari"
                },
                {
                    "authorId": "1654688904",
                    "name": "Shailendra Kumar Singh"
                },
                {
                    "authorId": "2156728859",
                    "name": "Shaiwya Salam"
                },
                {
                    "authorId": "98208370",
                    "name": "Vandana Bhatt"
                },
                {
                    "authorId": "2176270884",
                    "name": "Ashish Verma"
                },
                {
                    "authorId": "2047474294",
                    "name": "Sanjay Goyal"
                }
            ],
            "abstract": "Objective: To comprehend the evolution and spread of the severe acute respiratory syndrome coronavirus 2 (SARS CoV-2) virus and also to prevent the future spread of the same, sequencing and analyzing the genomic data of SARS CoV-2 are essential. The objective of the present study is to describe the scope of improvement identified by the state of Madhya Pradesh in the data flow chain and the methodology designed to address the identified shortcomings.\nMethods: The number of sources of sample data collection was altered as well as a series of Google Sheets were formulated as an open-source tool, to implement an efficient sample data-sharing platform. The application of the proposed tool (Google Sheets as a source of data collection and information sharing) was within the state of Madhya Pradesh, India.\nResult: After utilizing this mechanism, the state was able to trace more than 80% VOCs and 3341 primary contacts and was also able to communicate this result to all stakeholders without much delay.\nConclusion: Based on successful implementation and results, the authors suggest widening the domain of the proposed tool to other states.",
            "corpus_id": "250543531",
            "text": "Objective: To comprehend the evolution and spread of the severe acute respiratory syndrome coronavirus 2 (SARS CoV-2) virus and also to prevent the future spread of the same, sequencing and analyzing the genomic data of SARS CoV-2 are essential. The objective of the present study is to describe the scope of improvement identified by the state of Madhya Pradesh in the data flow chain and the methodology designed to address the identified shortcomings.\nMethods: The number of sources of sample data collection was altered as well as a series of Google Sheets were formulated as an open-source tool, to implement an efficient sample data-sharing platform. The application of the proposed tool (Google Sheets as a source of data collection and information sharing) was within the state of Madhya Pradesh, India.\nResult: After utilizing this mechanism, the state was able to trace more than 80% VOCs and 3341 primary contacts and was also able to communicate this result to all stakeholders without much delay.\nConclusion: Based on successful implementation and results, the authors suggest widening the domain of the proposed tool to other states.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0002715587615966797
        },
        {
            "paperId": "ae1e226149d16f16e01a13680dc9d556e92dbda2",
            "corpusId": 244442519,
            "title": "Machine-learning-guided library design cycle for directed evolution of enzymes: the effects of training data composition on sequence space exploration",
            "venue": "bioRxiv",
            "year": 2021,
            "referenceCount": 62,
            "citationCount": 38,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1021/acscatal.1c03753",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2021.08.13.456323?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2021.08.13.456323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50068982",
                    "name": "Yutaka Saito"
                },
                {
                    "authorId": "51181731",
                    "name": "Misaki Oikawa"
                },
                {
                    "authorId": "2150657551",
                    "name": "Takumi Sato"
                },
                {
                    "authorId": "4061238",
                    "name": "H. Nakazawa"
                },
                {
                    "authorId": "2148968968",
                    "name": "Tomoyuki Ito"
                },
                {
                    "authorId": "1974581",
                    "name": "T. Kameda"
                },
                {
                    "authorId": "1470581216",
                    "name": "Koji Tsuda"
                },
                {
                    "authorId": "2444806",
                    "name": "M. Umetsu"
                }
            ],
            "abstract": "Machine learning (ML) is becoming an attractive tool in mutagenesis-based protein engineering because of its ability to design a variant library containing proteins with a desired function. However, it remains unclear how ML guides directed evolution in sequence space depending on the composition of training data. Here, we present a ML-guided directed evolution study of an enzyme to investigate the effects of a known \u201chighly positive\u201d variant (i.e., variant known to have high enzyme activity) in training data. We performed two separate series of ML-guided directed evolution of Sortase A with and without a known highly positive variant called 5M in training data. In each series, two rounds of ML were conducted: variants predicted by the first round were experimentally evaluated, and used as additional training data for the second-round prediction. The improvements in enzyme activity were comparable between the two series, both achieving enzyme activity 2.2\u20132.5 times higher than 5M. Intriguingly, the sequences of the improved variants were largely different between the two series, indicating that ML guided the directed evolution to the distinct regions of sequence space depending on the presence/absence of the highly positive variant in the training data. This suggests that the sequence diversity of improved variants can be expanded not only by conventional ML using the whole training data, but also by ML using a subset of the training data even when it lacks highly positive variants. In summary, this study demonstrates the importance of regulating the composition of training data in ML-guided directed evolution.",
            "corpus_id": "244442519",
            "text": "Machine learning (ML) is becoming an attractive tool in mutagenesis-based protein engineering because of its ability to design a variant library containing proteins with a desired function. However, it remains unclear how ML guides directed evolution in sequence space depending on the composition of training data. Here, we present a ML-guided directed evolution study of an enzyme to investigate the effects of a known \u201chighly positive\u201d variant (i.e., variant known to have high enzyme activity) in training data. We performed two separate series of ML-guided directed evolution of Sortase A with and without a known highly positive variant called 5M in training data. In each series, two rounds of ML were conducted: variants predicted by the first round were experimentally evaluated, and used as additional training data for the second-round prediction. The improvements in enzyme activity were comparable between the two series, both achieving enzyme activity 2.2\u20132.5 times higher than 5M. Intriguingly, the sequences of the improved variants were largely different between the two series, indicating that ML guided the directed evolution to the distinct regions of sequence space depending on the presence/absence of the highly positive variant in the training data. This suggests that the sequence diversity of improved variants can be expanded not only by conventional ML using the whole training data, but also by ML using a subset of the training data even when it lacks highly positive variants. In summary, this study demonstrates the importance of regulating the composition of training data in ML-guided directed evolution.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00731658935546875
        },
        {
            "paperId": "ef0994052d89bf6d740d78c7e94f3a1ef3d8644a",
            "corpusId": 271973757,
            "title": "An Empirical Approach for Clustering-Based Time Series Summarisation Assessment",
            "venue": "Annual International Computer Software and Applications Conference",
            "year": 2024,
            "referenceCount": 20,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/COMPSAC61105.2024.00046?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/COMPSAC61105.2024.00046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1706823",
                    "name": "D. Bianchini"
                },
                {
                    "authorId": "79863382",
                    "name": "Massimiliano Garda"
                }
            ],
            "abstract": "In the last decades, the rise of Big Data solutions has significantly advanced the analysis of time series data as representation of dynamic phenomena through sequences of observations. Recent research efforts have advocated for the adoption of data summarisation techniques, such as incremental clustering, to promptly capture data evolution, thus facilitating domain experts in making informed and proactive decisions, capitalising on a compact representation of time series. Neverthe-less, while incremental clustering effectively reduces data volume, thus preserving relevant statistical information, it is crucial to estimate the degree of approximation between the original time series data and its summarised version. This evaluation is pivotal whenever the summarisation output is the starting point to set up complex analytical pipelines (e.g., for pattern recognition and anomaly detection purposes). Stemming from practical and empirical considerations made upon both a synthetic and a real-world dataset, we propose in this paper a variant of a renowned quality metric for incremental clustering, to assess the extent to which the time series summary accurately captures the dynamics of the original data.",
            "corpus_id": "271973757",
            "text": "In the last decades, the rise of Big Data solutions has significantly advanced the analysis of time series data as representation of dynamic phenomena through sequences of observations. Recent research efforts have advocated for the adoption of data summarisation techniques, such as incremental clustering, to promptly capture data evolution, thus facilitating domain experts in making informed and proactive decisions, capitalising on a compact representation of time series. Neverthe-less, while incremental clustering effectively reduces data volume, thus preserving relevant statistical information, it is crucial to estimate the degree of approximation between the original time series data and its summarised version. This evaluation is pivotal whenever the summarisation output is the starting point to set up complex analytical pipelines (e.g., for pattern recognition and anomaly detection purposes). Stemming from practical and empirical considerations made upon both a synthetic and a real-world dataset, we propose in this paper a variant of a renowned quality metric for incremental clustering, to assess the extent to which the time series summary accurately captures the dynamics of the original data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00319671630859375
        },
        {
            "paperId": "52688a603238f934088cfb946cad9cb2d005b2fa",
            "corpusId": 274404694,
            "title": "PICTURE\u2014A Framework to Assess the Degree of Approximation of Summarized Time Series",
            "venue": "IEEE Access",
            "year": 2024,
            "referenceCount": 25,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3509227",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3509227?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3509227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1706823",
                    "name": "D. Bianchini"
                },
                {
                    "authorId": "79863382",
                    "name": "Massimiliano Garda"
                }
            ],
            "abstract": "The analysis of time series data, which represents dynamic phenomena through sequences of observations, is greatly influenced by Big Data. Both the sheer volume and the advanced capabilities of Big Data significantly impact on how these analyses are conducted, enabling more comprehensive and detailed insights. Recent studies have promoted the use of data summarization techniques, for instance through incremental clustering, to address the challenges of Big Data volume. These techniques quickly capture data evolution, thereby helping domain experts make informed and proactive decisions by leveraging a concise representation of time series. However, although incremental clustering efficiently reduces data volume and retains key statistical information, it is important to evaluate the accuracy of the summarized version compared to the original time series data. This assessment is critical when the summarized data is used as the basis for complex analytical pipelines, such as those for pattern recognition and anomaly detection. Moved by these premises and starting from an empirical experience on the definition of a metric to assess the adherence of summarised time series to the original data stream, in this paper: (i) we propose a variant of a renowned quality metric for incremental clustering based on an abstract model of clustering data structures, to assess the extent to which the time series summary accurately captures the dynamics of the original data; (ii) we present PICTURE (Python-based Incremental Clustering for Time series Representation and Evaluation) a framework featuring four widely used incremental clustering algorithms from the literature, equipped with modules for execution, representation, and evaluation of clustering results applied to time series according to the abstract model; (iii) we conduct an extensive qualitative and quantitative analysis of incremental clustering results on a synthetic and two real-world datasets using the PICTURE framework, to demonstrate the effectiveness of the proposed metric in assessing the degree of approximation of summarised time series.",
            "corpus_id": "274404694",
            "text": "The analysis of time series data, which represents dynamic phenomena through sequences of observations, is greatly influenced by Big Data. Both the sheer volume and the advanced capabilities of Big Data significantly impact on how these analyses are conducted, enabling more comprehensive and detailed insights. Recent studies have promoted the use of data summarization techniques, for instance through incremental clustering, to address the challenges of Big Data volume. These techniques quickly capture data evolution, thereby helping domain experts make informed and proactive decisions by leveraging a concise representation of time series. However, although incremental clustering efficiently reduces data volume and retains key statistical information, it is important to evaluate the accuracy of the summarized version compared to the original time series data. This assessment is critical when the summarized data is used as the basis for complex analytical pipelines, such as those for pattern recognition and anomaly detection. Moved by these premises and starting from an empirical experience on the definition of a metric to assess the adherence of summarised time series to the original data stream, in this paper: (i) we propose a variant of a renowned quality metric for incremental clustering based on an abstract model of clustering data structures, to assess the extent to which the time series summary accurately captures the dynamics of the original data; (ii) we present PICTURE (Python-based Incremental Clustering for Time series Representation and Evaluation) a framework featuring four widely used incremental clustering algorithms from the literature, equipped with modules for execution, representation, and evaluation of clustering results applied to time series according to the abstract model; (iii) we conduct an extensive qualitative and quantitative analysis of incremental clustering results on a synthetic and two real-world datasets using the PICTURE framework, to demonstrate the effectiveness of the proposed metric in assessing the degree of approximation of summarised time series.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0027675628662109375
        },
        {
            "paperId": "b183db76a1682ca83e4d62fabc8ef116cc0e0364",
            "corpusId": 272683899,
            "title": "Doblin: Inferring dominant clonal lineages from DNA barcoding time-series",
            "venue": "bioRxiv",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1101/2024.09.08.611892",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.09.08.611892?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.09.08.611892, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321354275",
                    "name": "David Gagn\u00e9-Leroux"
                },
                {
                    "authorId": "113281311",
                    "name": "Melis Gencel"
                },
                {
                    "authorId": "49811243",
                    "name": "Adrian W. R. Serohijos"
                }
            ],
            "abstract": "Motivation The lineage dynamics and history of cells in a population reflect the interplay of evolutionary forces they experience, namely mutation, drift, and selection. When the population is polyclonal, lineage dynamics also manifest the extent of clonal competition among co-existing mutational variants. If the population exists in a community of other species, the lineage dynamics could also reflect the population\u2019s ecological interaction with the rest of the community. Recent advances in high-resolution lineage tracking via DNA barcoding, coupled with next-generation sequencing of bacteria, yeast, and mammalian cells, allow for precise quantification of clonal dynamics in these organisms. Results In this work, we introduce Doblin, an R suite for identifying dominant barcode lineages based on high-resolution lineage tracking data. We first benchmarked Doblin\u2019s accuracy using lineage data from evolutionary simulations, showing that it recovers clones\u2019 identity and relative fitness in the simulation. Subsequently, we applied Doblin to analyze clonal dynamics in laboratory evolutions of E. coli populations undergoing antibiotic treatment and in colonization experiments of the gut microbial community. Doblin\u2019s versatility allows it to be applied to lineage time-series data across different experimental setups. Availability and implementation The source code and data are available at https://github.com/dagagf/doblin.",
            "corpus_id": "272683899",
            "text": "Motivation The lineage dynamics and history of cells in a population reflect the interplay of evolutionary forces they experience, namely mutation, drift, and selection. When the population is polyclonal, lineage dynamics also manifest the extent of clonal competition among co-existing mutational variants. If the population exists in a community of other species, the lineage dynamics could also reflect the population\u2019s ecological interaction with the rest of the community. Recent advances in high-resolution lineage tracking via DNA barcoding, coupled with next-generation sequencing of bacteria, yeast, and mammalian cells, allow for precise quantification of clonal dynamics in these organisms. Results In this work, we introduce Doblin, an R suite for identifying dominant barcode lineages based on high-resolution lineage tracking data. We first benchmarked Doblin\u2019s accuracy using lineage data from evolutionary simulations, showing that it recovers clones\u2019 identity and relative fitness in the simulation. Subsequently, we applied Doblin to analyze clonal dynamics in laboratory evolutions of E. coli populations undergoing antibiotic treatment and in colonization experiments of the gut microbial community. Doblin\u2019s versatility allows it to be applied to lineage time-series data across different experimental setups. Availability and implementation The source code and data are available at https://github.com/dagagf/doblin.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0017547607421875
        },
        {
            "paperId": "66ee34932e8ddcd5a32bde11d5c8d8cbe04a02e2",
            "corpusId": 253002039,
            "title": "A Novel Adaptive FCM with Cooperative Multi-Population Differential Evolution Optimization",
            "venue": "Algorithms",
            "year": 2022,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1999-4893/15/10/380/pdf?version=1666689127",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/a15100380?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/a15100380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46487390",
                    "name": "A. Banerjee"
                },
                {
                    "authorId": "1398555455",
                    "name": "I. Abu-Mahfouz"
                }
            ],
            "abstract": "Fuzzy c-means (FCM), the fuzzy variant of the popular k-means, has been used for data clustering when cluster boundaries are not well defined. The of initial cluster prototypes (or the initialization of cluster memberships), and the fact that the number of clusters needs to be defined a priori are two major factors that can affect the performance of FCM. In this paper, we review algorithms and methods used to overcome these two specific drawbacks. We propose a new cooperative multi-population differential evolution method with elitism to identify near-optimal initial cluster prototypes and also determine the most optimal number of clusters in the data. The differential evolution populations use a smaller subset of the dataset, one that captures the same structure of the dataset. We compare the proposed methodology to newer methods proposed in the literature, with simulations performed on standard benchmark data from the UCI machine learning repository. Finally, we present a case study for clustering time-series patterns from sensor data related to real-time machine health monitoring using the proposed method. Simulation results are promising and show that the proposed methodology can be effective in clustering a wide range of datasets.",
            "corpus_id": "253002039",
            "text": "Fuzzy c-means (FCM), the fuzzy variant of the popular k-means, has been used for data clustering when cluster boundaries are not well defined. The of initial cluster prototypes (or the initialization of cluster memberships), and the fact that the number of clusters needs to be defined a priori are two major factors that can affect the performance of FCM. In this paper, we review algorithms and methods used to overcome these two specific drawbacks. We propose a new cooperative multi-population differential evolution method with elitism to identify near-optimal initial cluster prototypes and also determine the most optimal number of clusters in the data. The differential evolution populations use a smaller subset of the dataset, one that captures the same structure of the dataset. We compare the proposed methodology to newer methods proposed in the literature, with simulations performed on standard benchmark data from the UCI machine learning repository. Finally, we present a case study for clustering time-series patterns from sensor data related to real-time machine health monitoring using the proposed method. Simulation results are promising and show that the proposed methodology can be effective in clustering a wide range of datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0004973411560058594
        }
    ],
    "quotes": {
        "cost": 0.010212,
        "quotes": [
            {
                "idx": 0,
                "key": "[277452239 | Riggi et al. | 2025 | Citations: 1]",
                "snippets": "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model overview",
                        "pdf_hash": "",
                        "start": 470,
                        "end": 2178,
                        "sentence_offsets": [
                            {
                                "start": 470,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1550
                            },
                            {
                                "start": 1551,
                                "end": 1638
                            },
                            {
                                "start": 1639,
                                "end": 1917
                            },
                            {
                                "start": 1918,
                                "end": 2178
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.013446,
        "cot": "I need to organize the information about LLaVA series and its variants over time. Based on the user query, I'll focus on the evolution of LLaVA models chronologically and the different variants that have emerged.\n\nSince this is about a series of models evolving over time, I'll create the following dimensions:\n\n1. Introduction/Background: This will provide a brief overview of what LLaVA is and its significance in the multimodal AI landscape. This is necessary to set the context for users who might not be familiar with LLaVA. This will be in synthesis format since it's an explanatory section.\n\n2. LLaVA Evolution Timeline: This will cover how the LLaVA models have progressed over time, from the first release to subsequent versions like LLaVA 1.5 and then to more advanced versions like LLaVA-NeXT and LLaVA-OneVision. This will be in synthesis format to show the progression and improvements coherently.\n\n3. Key Innovations and Improvements: This section will focus on the technical advancements that drove the improvements in the LLaVA series over time. This will be in synthesis format to explain how these innovations enhanced the models' capabilities.\n\n4. LLaVA Variants and Specializations: This will cover the different specialized variants of LLaVA that have been developed for specific purposes or resource constraints, such as TinyLLaVA and LLaVA-Med. This will be in list format since these are distinct variants that can be enumerated.\n\nLooking at the available quote, it contains information relevant to all these dimensions, covering the history, evolution, innovations, and variants of LLaVA models.",
        "plan": {
            "Introduction/Background (synthesis)": [],
            "LLaVA Evolution Timeline (synthesis)": [
                0
            ],
            "Key Innovations and Improvements (synthesis)": [
                0
            ],
            "LLaVA Variants and Specializations (list)": [
                0
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "LLaVA (Large Language and Vision Assistant) is a pioneering series of multimodal AI models that combine language and vision capabilities. It emerged in 2023 and has rapidly evolved to become one of the leading open-source solutions for visual understanding and reasoning tasks. (LLM Memory)",
                "text": "\nLLaVA represents a significant advancement in multimodal AI systems by connecting large language models (LLMs) with vision encoders to enable sophisticated understanding and reasoning about visual content. The first LLaVA model was introduced in April 2023 by researchers who aimed to create a simple yet effective multimodal model that could understand and discuss visual information. LLaVA models are typically built by connecting a vision encoder (such as CLIP) with a large language model, creating a system that can process both images and text in an integrated manner. This architecture allows the models to perform various tasks like visual question answering, image captioning, and reasoning about visual content in natural language conversations. LLaVA stands out for its efficiency and strong performance despite its relatively straightforward design, helping democratize access to powerful multimodal AI capabilities through open-source implementations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA Evolution Timeline",
                "tldr": "The LLaVA model series has rapidly evolved since its initial release in 2023, with each iteration introducing significant improvements in capabilities and performance. Key milestones include LLaVA 1.5, LLaVA-NeXT, and LLaVA-OneVision, along with specialized variants like TinyLLaVA and LLaVA-Med. (1 source)",
                "text": "\nThe LLaVA (Large Language and Vision Assistant) model has undergone significant evolution since its debut. The original LLaVA model demonstrated remarkable multimodal conversational abilities upon its first release, showing performance comparable to GPT-4V in interpreting new images and following novel instructions. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nThe next major advancement came with LLaVA 1.5, which substantially enhanced the model's capabilities by incorporating a larger collection of academic-focused instructional data. This iteration achieved state-of-the-art results on numerous benchmarks while implementing a highly data-efficient training strategy. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nMore recent developments in the series include LLaVA-NeXT and LLaVA-OneVision, which have greatly expanded the range of input modalities the models can process. These newer versions can handle both single and multiple images as well as video content, marking a significant leap in versatility. These advancements were driven by three key innovations: the AnyRes technique for processing high-resolution images, expanded high-quality instruction datasets, and integration with cutting-edge open-source LLMs available at the time. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nThe LLaVA ecosystem has also diversified through specialized variants. TinyLLaVA emerged as a compact, refactored version of LLaVA 1.5, designed to easily accommodate alternative lightweight vision and LLM models, thereby substantially reducing model size and resource requirements. In the medical domain, LLaVA-Med was developed as a specialized variant fine-tuned on medical datasets including X-rays, MRIs, and other healthcare-related visual data to assist with medical image analysis and diagnostics. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.57421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Key Innovations and Improvements",
                "tldr": "The LLaVA series has evolved through several key technical innovations including the AnyRes technique for high-resolution image processing, expanded instruction datasets, and integration with advanced open-source LLMs. These improvements have enabled the models to handle more diverse inputs and achieve better performance across various benchmarks. (1 source)",
                "text": "\nThe evolution of the LLaVA model series has been driven by several significant technical innovations that have progressively enhanced its capabilities. One of the key advancements in recent LLaVA iterations like LLaVA-NeXT and LLaVA-OneVision is the introduction of the AnyRes technique, which enables efficient processing of high-resolution images, allowing the models to capture and understand more detailed visual information. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nAnother crucial improvement has been the expansion of high-quality instruction datasets, particularly evident in LLaVA 1.5, which incorporated a larger collection of academic-focused instructional data. This data-efficient training strategy has been instrumental in achieving state-of-the-art results across numerous benchmarks, demonstrating how carefully curated training data can significantly boost model performance. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nThe LLaVA series has also benefited from continuous integration with progressively more advanced open-source Large Language Models (LLMs). By leveraging the most cutting-edge LLMs available at each stage of development, newer LLaVA variants have inherited improved reasoning capabilities, enhanced contextual understanding, and better overall performance across diverse tasks. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\nThese innovations collectively represent a consistent pattern in the LLaVA development philosophy: maintaining a relatively straightforward architectural approach while strategically enhancing specific components to achieve substantial performance gains. This approach has enabled the LLaVA series to rapidly evolve while remaining accessible to the broader research community. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.57421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA Variants and Specializations",
                "tldr": "The LLaVA model architecture has spawned several specialized variants tailored for different use cases and resource constraints. These include compact versions like TinyLLaVA designed for efficiency and domain-specific models like LLaVA-Med for medical applications. (1 source)",
                "text": "\n- **LLaVA-NeXT**: A significant advancement in the LLaVA series that expanded input modality support to handle both single and multiple images as well as video content. This variant implemented the AnyRes technique for high-resolution image processing, allowing for more detailed visual understanding. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\n- **LLaVA-OneVision**: Building on the LLaVA foundation, this variant further enhanced the model's ability to process diverse visual inputs including multiple images and video. Like LLaVA-NeXT, it incorporated expanded high-quality instruction datasets and integrated with advanced open-source LLMs available at the time of its development. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\n- **TinyLLaVA**: A compact, refactored variant of the original LLaVA 1.5 model specifically designed to reduce resource requirements. This lightweight implementation allows for easier inclusion of alternative vision and LLM models, making multimodal AI more accessible for applications with limited computational resources. TinyLLaVA significantly reduces overall model size while maintaining essential functionality. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>\n\n- **LLaVA-Med**: A domain-specific adaptation of the LLaVA model focused on medical applications. This specialized variant was fine-tuned on healthcare-related visual datasets including X-rays, MRIs, and other medical imaging data to enhance its capabilities in medical image analysis and diagnostics. LLaVA-Med demonstrates how the core LLaVA architecture can be adapted for specialized professional domains. <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.57421875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.049731000000000004
    }
}