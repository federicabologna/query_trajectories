{
    "query": "What are the specific inference-time scaling laws identified in recent research, and how do they differ from traditional training scaling laws?",
    "user_id": "lib_user",
    "task_id": "6d52be79-7ac9-4fe9-8030-6b6d242b56f8",
    "timestamp": "2025-06-23T21:25:35.415906",
    "n_retrieval": 256,
    "n_retrieved": 266,
    "n_candidates": 40,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.359451,
    "decomposed_query": {
        "rewritten_query": "Specific inference-time scaling laws and how they differ from traditional training scaling laws.",
        "keyword_query": "inference-time scaling laws traditional training scaling laws",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009696,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Kinetics: Rethinking Test-Time Scaling Laws",
            "venue": "",
            "year": 2025,
            "reference_count": 102,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003376054",
                    "name": "Ranajoy Sadhukhan"
                },
                {
                    "authorId": "2300556424",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "2366124482",
                    "name": "Haizhong Zheng"
                },
                {
                    "authorId": "2364951887",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "2301065578",
                    "name": "Beidi Chen"
                }
            ],
            "abstract": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
            "corpus_id": 279244494,
            "sentences": [],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: Kinetics: Rethinking Test-Time Scaling Laws\n# Venue: \n# Authors: Ranajoy Sadhukhan, Zhuo Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen\n## Abstract\nWe rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.\n",
            "reference_string": "[279244494 | Sadhukhan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 73,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292419803",
                    "name": "Jaihoon Kim"
                },
                {
                    "authorId": "2328014454",
                    "name": "Taehoon Yoon"
                },
                {
                    "authorId": "2322353657",
                    "name": "Jisung Hwang"
                },
                {
                    "authorId": "2322097538",
                    "name": "Minhyuk Sung"
                }
            ],
            "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
            "corpus_id": 277313756,
            "sentences": [
                {
                    "corpus_id": "277313756",
                    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
                    "text": "Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling [49,51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference. \n\nFor diffusion models [46,48], which are widely used for generation tasks, research on inference-time scaling has been growing in the context of reward-based sampling [21,27,45]. Given a reward function that measures alignment with user preferences [23] or output quality [28,44], the goal is to find the sample from the learned data distribution that best aligns with the reward through repeated sampling. Fig. 1 showcases diverse applications of inference-time scaling, enabling the generation of faithful images that accurately align with complex user descriptions involving objects quantities, logical relationships, and conceptual attributes. Notably, na\u00efve generation from text-to-image models [25,43] often fails to fully meet user specifications, highlighting the effectiveness of inference-time scaling. \n\nOur goal in this work is to extend the inference-time scaling capabilities of diffusion models to flow models. Flow models [29] power state-of-the-art image [14,25] and video generation [7,60], achieving high-quality synthesis with few inference steps, enabled by trajectory stratification techniques during training [32]. Beyond just speed, recent pretrained flow models, equipped with enhanced text-image embeddings [41] and advanced architectures [14], significantly outperform previous pretrained diffusion models in both image and video generation quality. \n\nDespite their advantages in generating high-quality results more efficiently than diffusion models, flow models have an inherent limitation in the context of inference-time scaling. Due to their ODE-based deterministic generative process, they cannot directly incorporate particle sampling at intermediate steps, a key mechanism for effective inference-time scaling in diffusion models.",
                    "score": 0.4824396548212185,
                    "section_title": "Introduction",
                    "char_start_offset": 1224,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 283
                        },
                        {
                            "start": 286,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 613
                        },
                        {
                            "start": 616,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1427
                        },
                        {
                            "start": 1430,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1991
                        },
                        {
                            "start": 1994,
                            "end": 2175
                        },
                        {
                            "start": 2176,
                            "end": 2380
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 197,
                            "matchedPaperCorpusId": "221665105"
                        },
                        {
                            "start": 641,
                            "end": 644,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 786,
                            "end": 789,
                            "matchedPaperCorpusId": "271874728"
                        },
                        {
                            "start": 789,
                            "end": 792,
                            "matchedPaperCorpusId": "276094284"
                        },
                        {
                            "start": 864,
                            "end": 868,
                            "matchedPaperCorpusId": "258437096"
                        },
                        {
                            "start": 1319,
                            "end": 1322,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1553,
                            "end": 1557,
                            "matchedPaperCorpusId": "252734897"
                        },
                        {
                            "start": 1747,
                            "end": 1751,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 1848,
                            "end": 1852,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92333984375
                }
            ],
            "relevance_judgement": 0.92333984375,
            "relevance_judgment_input_expanded": "# Title: Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing\n# Venue: arXiv.org\n# Authors: Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung\n## Abstract\nWe propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.\n## Introduction\nOver the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling [49,51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference. \n\nFor diffusion models [46,48], which are widely used for generation tasks, research on inference-time scaling has been growing in the context of reward-based sampling [21,27,45]. Given a reward function that measures alignment with user preferences [23] or output quality [28,44], the goal is to find the sample from the learned data distribution that best aligns with the reward through repeated sampling. Fig. 1 showcases diverse applications of inference-time scaling, enabling the generation of faithful images that accurately align with complex user descriptions involving objects quantities, logical relationships, and conceptual attributes. Notably, na\u00efve generation from text-to-image models [25,43] often fails to fully meet user specifications, highlighting the effectiveness of inference-time scaling. \n\nOur goal in this work is to extend the inference-time scaling capabilities of diffusion models to flow models. Flow models [29] power state-of-the-art image [14,25] and video generation [7,60], achieving high-quality synthesis with few inference steps, enabled by trajectory stratification techniques during training [32]. Beyond just speed, recent pretrained flow models, equipped with enhanced text-image embeddings [41] and advanced architectures [14], significantly outperform previous pretrained diffusion models in both image and video generation quality. \n\nDespite their advantages in generating high-quality results more efficiently than diffusion models, flow models have an inherent limitation in the context of inference-time scaling. Due to their ODE-based deterministic generative process, they cannot directly incorporate particle sampling at intermediate steps, a key mechanism for effective inference-time scaling in diffusion models.",
            "reference_string": "[277313756 | Kim et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 69,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279750963",
                    "name": "Nanye Ma"
                },
                {
                    "authorId": "2058178039",
                    "name": "Shangyuan Tong"
                },
                {
                    "authorId": "2340654494",
                    "name": "Haolin Jia"
                },
                {
                    "authorId": "2307548497",
                    "name": "Hexiang Hu"
                },
                {
                    "authorId": "2269866136",
                    "name": "Yu-Chuan Su"
                },
                {
                    "authorId": "2326256475",
                    "name": "Mingda Zhang"
                },
                {
                    "authorId": "2340726220",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2324838111",
                    "name": "Yandong Li"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "2325917399",
                    "name": "Xuhui Jia"
                },
                {
                    "authorId": "2324769373",
                    "name": "Saining Xie"
                }
            ],
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "corpus_id": 275570556,
            "sentences": [],
            "relevance_judgement": 0.919921875,
            "relevance_judgment_input_expanded": "# Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps\n# Venue: arXiv.org\n# Authors: Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, T. Jaakkola, Xuhui Jia, Saining Xie\n## Abstract\nGenerative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.\n",
            "reference_string": "[275570556 | Ma et al. | 2025 | Citations: 69]"
        },
        {
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 130,
            "influential_citation_count": 10,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314347409",
                    "name": "Yangzhen Wu"
                },
                {
                    "authorId": "48064856",
                    "name": "Zhiqing Sun"
                },
                {
                    "authorId": "2257057803",
                    "name": "Shanda Li"
                },
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "2257099254",
                    "name": "Yiming Yang"
                }
            ],
            "abstract": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
            "corpus_id": 271601023,
            "sentences": [
                {
                    "corpus_id": "271601023",
                    "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
                    "text": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
                    "score": 0.49301297287461976,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving\n# Venue: International Conference on Learning Representations\n# Authors: Yangzhen Wu, Zhiqing Sun, Shanda Li, S. Welleck, Yiming Yang\n## Abstract\nWhile the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.\n",
            "reference_string": "[271601023 | Wu et al. | 2024 | Citations: 130]"
        },
        {
            "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.06802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340178423",
                    "name": "Jun Wan"
                }
            ],
            "abstract": "In 2020, OpenAI proposed the first type of Scaling Laws, describing the relationships between model loss and the scale of parameters, data, and training computation. In 2024, OpenAI proposed the second type of Scaling Laws, describing the relationship between model inference performance and inference computation. In this paper, we analyze LLMs training and inference processes from the perspective of lossless compression using conditional Kolmogorov complexity, and unify these two types of Scaling Laws. We find that both types of Scaling Laws improve approximation of conditional Kolmogorov complexity by increasing execution steps of Turing machine. The first type of Scaling Laws increases execution steps by increasing number of model parameters. The second type of Scaling Laws increases execution steps by increasing the number of intermediate tokens.",
            "corpus_id": 275471424,
            "sentences": [
                {
                    "corpus_id": "275471424",
                    "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity",
                    "text": "The Scaling Laws in LLMs can primarily be categorized into two types: \n\n\u2022 Pre-training Scaling Laws \u2022 Inference Scaling Laws Kaplan et al. (2020) systematically studied the impact of model parameter scale on language model performance and proposed corresponding Scaling Laws. It focuses on resource optimization during the pre-training process, in other words, to improve model performance through increasing key resources such as data volume, model parameter count, and computational power. The proposal of such Scaling Laws also laid an important foundation for training and optimization of LLMs. Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks.",
                    "score": 0.4995514562166905,
                    "section_title": "Two Types of Scaling Laws",
                    "char_start_offset": 13784,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 69
                        },
                        {
                            "start": 72,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1555
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.89013671875,
            "relevance_judgment_input_expanded": "# Title: Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity\n# Venue: arXiv.org\n# Authors: Jun Wan\n## Abstract\nIn 2020, OpenAI proposed the first type of Scaling Laws, describing the relationships between model loss and the scale of parameters, data, and training computation. In 2024, OpenAI proposed the second type of Scaling Laws, describing the relationship between model inference performance and inference computation. In this paper, we analyze LLMs training and inference processes from the perspective of lossless compression using conditional Kolmogorov complexity, and unify these two types of Scaling Laws. We find that both types of Scaling Laws improve approximation of conditional Kolmogorov complexity by increasing execution steps of Turing machine. The first type of Scaling Laws increases execution steps by increasing number of model parameters. The second type of Scaling Laws increases execution steps by increasing the number of intermediate tokens.\n## Two Types of Scaling Laws\nThe Scaling Laws in LLMs can primarily be categorized into two types: \n\n\u2022 Pre-training Scaling Laws \u2022 Inference Scaling Laws Kaplan et al. (2020) systematically studied the impact of model parameter scale on language model performance and proposed corresponding Scaling Laws. It focuses on resource optimization during the pre-training process, in other words, to improve model performance through increasing key resources such as data volume, model parameter count, and computational power. The proposal of such Scaling Laws also laid an important foundation for training and optimization of LLMs. Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks.",
            "reference_string": "[275471424 | Wan | 2025 | Citations: 1]"
        },
        {
            "title": "A Simple Model of Inference Scaling Laws",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261494026",
                    "name": "Noam Levi"
                }
            ],
            "abstract": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
            "corpus_id": 273507997,
            "sentences": [
                {
                    "corpus_id": "273507997",
                    "title": "A Simple Model of Inference Scaling Laws",
                    "text": "Advancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
                    "score": 0.5477506554923403,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 509
                        },
                        {
                            "start": 512,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1101
                        },
                        {
                            "start": 1104,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1664
                        },
                        {
                            "start": 1667,
                            "end": 1883
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 243,
                            "matchedPaperCorpusId": "203592013"
                        },
                        {
                            "start": 484,
                            "end": 508,
                            "matchedPaperCorpusId": "258509679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88818359375
                },
                {
                    "corpus_id": "273507997",
                    "title": "A Simple Model of Inference Scaling Laws",
                    "text": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
                    "score": 0.4396506628741437,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84521484375
                }
            ],
            "relevance_judgement": 0.88818359375,
            "relevance_judgment_input_expanded": "# Title: A Simple Model of Inference Scaling Laws\n# Venue: arXiv.org\n# Authors: Noam Levi\n## Abstract\nNeural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.\n## Introduction\nAdvancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
            "reference_string": "[273507997 | Levi | 2024 | Citations: 13]"
        },
        {
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338865687",
                    "name": "Chien-Ping Lu"
                }
            ],
            "abstract": "As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.",
            "corpus_id": 275336968,
            "sentences": [
                {
                    "corpus_id": "275336968",
                    "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                    "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
                    "score": 0.5303859718760001,
                    "section_title": "Related Work",
                    "char_start_offset": 4305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1221,
                            "end": 1225,
                            "matchedPaperCorpusId": "246473179"
                        },
                        {
                            "start": 1803,
                            "end": 1806,
                            "matchedPaperCorpusId": "6519532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.865234375
                }
            ],
            "relevance_judgement": 0.865234375,
            "relevance_judgment_input_expanded": "# Title: The Race to Efficiency: A New Perspective on AI Scaling Laws\n# Venue: arXiv.org\n# Authors: Chien-Ping Lu\n## Abstract\nAs large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.\n## Related Work\nThe study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "reference_string": "[275336968 | Lu | 2025 | Citations: 1]"
        },
        {
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34920835",
                    "name": "Ayan Sengupta"
                },
                {
                    "authorId": "2345922770",
                    "name": "Yash Goel"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.",
            "corpus_id": 276421468,
            "sentences": [
                {
                    "corpus_id": "276421468",
                    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                    "text": "While neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility. \n\nInference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications. \n\nCompute-optimal model selection: Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multiobjective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development. \n\nEfficient data scaling and pruning: The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.",
                    "score": 0.625186149794007,
                    "section_title": "Future recommendations",
                    "char_start_offset": 25490,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 353
                        },
                        {
                            "start": 356,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 749
                        },
                        {
                            "start": 752,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1069
                        },
                        {
                            "start": 1072,
                            "end": 1215
                        },
                        {
                            "start": 1216,
                            "end": 1371
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8544921875
                },
                {
                    "corpus_id": "276421468",
                    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                    "text": "Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.",
                    "score": 0.5717058694859517,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76806640625
                },
                {
                    "corpus_id": "276421468",
                    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                    "text": "Understanding the scaling laws of neural models is crucial for optimizing performance across different domains. We predominantly explore the scaling principles for language models, extending to other modalities such as vision and multimodal learning. We also examine scaling behaviors in domain adaptation, inference, efficient model architectures, and data utilization. We highlight the taxonomy tree of scaling laws research in Figure 2. As highlighted in Figure 1, neural scaling laws have been proposed predominantly for pre-training and fine-tuning scaling of large neural models. Among the models studied, as highlighted in Figure 3a, decoder-only Transformers dominate the subject, followed by vision transformers (ViT) and Mixture-of-Experts (MoE). \n\nThe most common neural scaling laws take the form of power laws (Equation 1), where the model's loss (L) or performance metric assumes to follow a predictable relationship with different scaling variables, \n\nwith appropriate scaling parameters \u03b2 i and fitting parameters \u03b1 i for different scaling parameter P i . Figure 3b highlights that the number of model parameters and data size are the most common used scaling factors. The exact forms of all the scaling laws are highlighted in Table 7 of Appendix B. Among all the tasks, Figure 3c suggests that language generation is the most common task used for developing these scaling laws, where the training cross-entropy loss is widely used to fit the laws. Based on the values obtained empirically, the scaling laws are fitted with non-linear optimization, most commonly by running algorithms like least square and BFGS (Broyden-Fletcher-Goldfarb-Shanno). Statistical methods like goodness-of-fit metrics are used to validate the correctness of the fitted curves. We elaborate on the evaluation of neural scaling laws in Appendix A. \n\nIn the following sections, we review the existing literature on neural scaling across various domains. Kaplan et al. (2020) suggested that larger LMs improve performance by reducing loss through power-law scaling. However, this view evolved when studies showed that many large models were undertrained, and data scaling plays an equally crucial role in compute efficiency (Hoffmann et al., 2022). More recent breakthroughs challenged traditional scaling assumptions.",
                    "score": 0.4268795056642635,
                    "section_title": "Taxonomy of neural scaling laws",
                    "char_start_offset": 3460,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 756
                        },
                        {
                            "start": 759,
                            "end": 964
                        },
                        {
                            "start": 967,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1772
                        },
                        {
                            "start": 1773,
                            "end": 1841
                        },
                        {
                            "start": 1844,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2240
                        },
                        {
                            "start": 2241,
                            "end": 2310
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68212890625
                },
                {
                    "corpus_id": "276421468",
                    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                    "text": "Scaling laws have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how * Equal contribution the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization. \n\nThe study of neural scaling laws gained prominence with the foundational work of Kaplan et al. (2020), who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, Hoffmann et al. (2022) refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies (Muennighoff et al., 2023;Caballero et al., 2023;Krajewski et al., 2024) have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrievalaugmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies. \n\nDespite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored.",
                    "score": 0.6289428220497614,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 708
                        },
                        {
                            "start": 711,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1624
                        },
                        {
                            "start": 1625,
                            "end": 1777
                        },
                        {
                            "start": 1780,
                            "end": 1964
                        },
                        {
                            "start": 1965,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2384
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66650390625
                }
            ],
            "relevance_judgement": 0.8544921875,
            "relevance_judgment_input_expanded": "# Title: How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines\n# Venue: arXiv.org\n# Authors: Ayan Sengupta, Yash Goel, Tanmoy Chakraborty\n## Abstract\nNeural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.\n## Introduction\nScaling laws have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how * Equal contribution the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization. \n\nThe study of neural scaling laws gained prominence with the foundational work of Kaplan et al. (2020), who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, Hoffmann et al. (2022) refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies (Muennighoff et al., 2023;Caballero et al., 2023;Krajewski et al., 2024) have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrievalaugmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies. \n\nDespite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored.\n\n## Taxonomy of neural scaling laws\nUnderstanding the scaling laws of neural models is crucial for optimizing performance across different domains. We predominantly explore the scaling principles for language models, extending to other modalities such as vision and multimodal learning. We also examine scaling behaviors in domain adaptation, inference, efficient model architectures, and data utilization. We highlight the taxonomy tree of scaling laws research in Figure 2. As highlighted in Figure 1, neural scaling laws have been proposed predominantly for pre-training and fine-tuning scaling of large neural models. Among the models studied, as highlighted in Figure 3a, decoder-only Transformers dominate the subject, followed by vision transformers (ViT) and Mixture-of-Experts (MoE). \n\nThe most common neural scaling laws take the form of power laws (Equation 1), where the model's loss (L) or performance metric assumes to follow a predictable relationship with different scaling variables, \n\nwith appropriate scaling parameters \u03b2 i and fitting parameters \u03b1 i for different scaling parameter P i . Figure 3b highlights that the number of model parameters and data size are the most common used scaling factors. The exact forms of all the scaling laws are highlighted in Table 7 of Appendix B. Among all the tasks, Figure 3c suggests that language generation is the most common task used for developing these scaling laws, where the training cross-entropy loss is widely used to fit the laws. Based on the values obtained empirically, the scaling laws are fitted with non-linear optimization, most commonly by running algorithms like least square and BFGS (Broyden-Fletcher-Goldfarb-Shanno). Statistical methods like goodness-of-fit metrics are used to validate the correctness of the fitted curves. We elaborate on the evaluation of neural scaling laws in Appendix A. \n\nIn the following sections, we review the existing literature on neural scaling across various domains. Kaplan et al. (2020) suggested that larger LMs improve performance by reducing loss through power-law scaling. However, this view evolved when studies showed that many large models were undertrained, and data scaling plays an equally crucial role in compute efficiency (Hoffmann et al., 2022). More recent breakthroughs challenged traditional scaling assumptions.\n\n## Future recommendations\nWhile neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility. \n\nInference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications. \n\nCompute-optimal model selection: Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multiobjective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development. \n\nEfficient data scaling and pruning: The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.",
            "reference_string": "[276421468 | Sengupta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Parallel Scaling Law for Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2125101083",
                    "name": "Mouxiang Chen"
                },
                {
                    "authorId": "2321578848",
                    "name": "Binyuan Hui"
                },
                {
                    "authorId": "2248072386",
                    "name": "Zeyu Cui"
                },
                {
                    "authorId": "2328943044",
                    "name": "Jiaxin Yang"
                },
                {
                    "authorId": "2248487202",
                    "name": "Dayiheng Liu"
                },
                {
                    "authorId": "2362357192",
                    "name": "Jianling Sun"
                },
                {
                    "authorId": "2326803484",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "2361700209",
                    "name": "Zhongxin Liu"
                }
            ],
            "abstract": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
            "corpus_id": 278636433,
            "sentences": [
                {
                    "corpus_id": "278636433",
                    "title": "Parallel Scaling Law for Language Models",
                    "text": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
                    "score": 0.5202380722863362,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8515625
                },
                {
                    "corpus_id": "278636433",
                    "title": "Parallel Scaling Law for Language Models",
                    "text": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
                    "score": 0.5784099676137021,
                    "section_title": "Discussion and Future Work",
                    "char_start_offset": 32802,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 966
                        },
                        {
                            "start": 969,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1482
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 467,
                            "end": 488,
                            "matchedPaperCorpusId": "266693796"
                        },
                        {
                            "start": 605,
                            "end": 622,
                            "matchedPaperCorpusId": "271601023"
                        },
                        {
                            "start": 622,
                            "end": 641,
                            "matchedPaperCorpusId": "278498044"
                        },
                        {
                            "start": 918,
                            "end": 940,
                            "matchedPaperCorpusId": "266693796"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                },
                {
                    "corpus_id": "278636433",
                    "title": "Parallel Scaling Law for Language Models",
                    "text": "Figure 1(3) illustrates that PARSCALE offers superior inference efficiency. \n\nFurthermore, we show that the high training cost of PARSCALE can be reduced by a two-stage approach: the first stage employs traditional training with most of the training data, and PARSCALE is applied only in the second stage with a small number of tokens. Based on this, we train 1.8B models with various P and scale the training data to 1T tokens. The results of 21 downstream benchmarks indicate the efficacy of this strategy. For example, when scaling to P = 8, it yields a 34% relative improvement for GSM8K and 23% relative improvement for MMLU using exactly the same training data. We also implement PARSCALE on an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024), and demonstrate that PARSCALE is effective in both full and parameter-efficient fine-tuning settings. This also shows the viability of dynamic parallel scaling, which allows flexible adjustment of P during deployment while freezing the backbone weights, to fit different application scenerios. \n\nTable 1 compares PARSCALE with other mainstream scaling strategies. Beyond introducing an efficient scaling approach for language models, our research also tries to address a more fundamental question in machine learning: Is a model's capacity determined by the parameters or by the computation, and what is their individual contribution? Traditional machine learning models typically scale both parameters and computation simultaneously, making it difficult to determine their contribution ratio. The PARSCALE and the fitted parallel scaling law may offer a novel and quantitative perspective on this problem. \n\nTable 1: Comparisons of mainstream LLM scaling strategies. We subdivide parameter scaling into traditional Dense Scaling and Mixture-of-Expert (MoE) Scaling (Fedus et al., 2022) for comparison. Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference.",
                    "score": 0.4563568256182436,
                    "section_title": "Introduction",
                    "char_start_offset": 4059,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 78,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1043
                        },
                        {
                            "start": 1046,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1656
                        },
                        {
                            "start": 1659,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1852
                        },
                        {
                            "start": 1853,
                            "end": 2000
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1816,
                            "end": 1836,
                            "matchedPaperCorpusId": "231573431"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69921875
                }
            ],
            "relevance_judgement": 0.8515625,
            "relevance_judgment_input_expanded": "# Title: Parallel Scaling Law for Language Models\n# Venue: \n# Authors: Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxin Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu\n## Abstract\nIt is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.\n## Introduction\nFigure 1(3) illustrates that PARSCALE offers superior inference efficiency. \n\nFurthermore, we show that the high training cost of PARSCALE can be reduced by a two-stage approach: the first stage employs traditional training with most of the training data, and PARSCALE is applied only in the second stage with a small number of tokens. Based on this, we train 1.8B models with various P and scale the training data to 1T tokens. The results of 21 downstream benchmarks indicate the efficacy of this strategy. For example, when scaling to P = 8, it yields a 34% relative improvement for GSM8K and 23% relative improvement for MMLU using exactly the same training data. We also implement PARSCALE on an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024), and demonstrate that PARSCALE is effective in both full and parameter-efficient fine-tuning settings. This also shows the viability of dynamic parallel scaling, which allows flexible adjustment of P during deployment while freezing the backbone weights, to fit different application scenerios. \n\nTable 1 compares PARSCALE with other mainstream scaling strategies. Beyond introducing an efficient scaling approach for language models, our research also tries to address a more fundamental question in machine learning: Is a model's capacity determined by the parameters or by the computation, and what is their individual contribution? Traditional machine learning models typically scale both parameters and computation simultaneously, making it difficult to determine their contribution ratio. The PARSCALE and the fitted parallel scaling law may offer a novel and quantitative perspective on this problem. \n\nTable 1: Comparisons of mainstream LLM scaling strategies. We subdivide parameter scaling into traditional Dense Scaling and Mixture-of-Expert (MoE) Scaling (Fedus et al., 2022) for comparison. Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference.\n\n## Discussion and Future Work\nTraining Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
            "reference_string": "[278636433 | Chen et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312897913",
                    "name": "Hongru Cai"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2298418808",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2284702952",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "corpus_id": 277940798,
            "sentences": [],
            "relevance_judgement": 0.8349609375,
            "relevance_judgment_input_expanded": "# Title: Exploring Training and Inference Scaling Laws in Generative Retrieval\n# Venue: arXiv.org\n# Authors: Hongru Cai, Yongqi Li, Ruifeng Yuan, Wenjie Wang, Zhen Zhang, Wenjie Li, Tat-Seng Chua\n## Abstract\nGenerative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.\n",
            "reference_string": "[277940798 | Cai et al. | 2025 | Citations: 1]"
        },
        {
            "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 190,
            "citation_count": 77,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "2138301112",
                    "name": "Amanda Bertsch"
                },
                {
                    "authorId": "1580418311",
                    "name": "Matthew Finlayson"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "2253395527",
                    "name": "Alex Xie"
                },
                {
                    "authorId": "2265547593",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2308102420",
                    "name": "Ilia Kulikov"
                },
                {
                    "authorId": "2265540561",
                    "name": "Zaid Harchaoui"
                }
            ],
            "abstract": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
            "corpus_id": 270703266,
            "sentences": [],
            "relevance_judgement": 0.83154296875,
            "relevance_judgment_input_expanded": "# Title: From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models\n# Venue: Trans. Mach. Learn. Res.\n# Authors: S. Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui\n## Abstract\nOne of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.\n",
            "reference_string": "[270703266 | Welleck et al. | 2024 | Citations: 77]"
        },
        {
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.05782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52089350",
                    "name": "Akhilan Boopathy"
                },
                {
                    "authorId": "8657128",
                    "name": "I. Fiete"
                }
            ],
            "abstract": "As neural networks continue to grow in size but datasets might not, it is vital to understand how much performance improvement can be expected: is it more important to scale network size or data volume? Thus, neural network scaling laws, which characterize how test error varies with network size and data volume, have become increasingly important. However, existing scaling laws are often applicable only in limited regimes and often do not incorporate or predict well-known phenomena such as double descent. Here, we present a novel theoretical characterization of how three factors -- model size, training time, and data volume -- interact to determine the performance of deep neural networks. We first establish a theoretical and empirical equivalence between scaling the size of a neural network and increasing its training time proportionally. Scale-time equivalence challenges the current practice, wherein large models are trained for small durations, and suggests that smaller models trained over extended periods could match their efficacy. It also leads to a novel method for predicting the performance of large-scale networks from small-scale networks trained for extended epochs, and vice versa. We next combine scale-time equivalence with a linear model analysis of double descent to obtain a unified theoretical scaling law, which we confirm with experiments across vision benchmarks and network architectures. These laws explain several previously unexplained phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance. Our findings hold significant implications for the practical deployment of neural networks, offering a more accessible and efficient path to training and fine-tuning large models.",
            "corpus_id": 272525306,
            "sentences": [
                {
                    "corpus_id": "272525306",
                    "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
                    "text": "As neural networks continue to grow in size but datasets might not, it is vital to understand how much performance improvement can be expected: is it more important to scale network size or data volume? Thus, neural network scaling laws, which characterize how test error varies with network size and data volume, have become increasingly important. However, existing scaling laws are often applicable only in limited regimes and often do not incorporate or predict well-known phenomena such as double descent. Here, we present a novel theoretical characterization of how three factors -- model size, training time, and data volume -- interact to determine the performance of deep neural networks. We first establish a theoretical and empirical equivalence between scaling the size of a neural network and increasing its training time proportionally. Scale-time equivalence challenges the current practice, wherein large models are trained for small durations, and suggests that smaller models trained over extended periods could match their efficacy. It also leads to a novel method for predicting the performance of large-scale networks from small-scale networks trained for extended epochs, and vice versa. We next combine scale-time equivalence with a linear model analysis of double descent to obtain a unified theoretical scaling law, which we confirm with experiments across vision benchmarks and network architectures. These laws explain several previously unexplained phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance. Our findings hold significant implications for the practical deployment of neural networks, offering a more accessible and efficient path to training and fine-tuning large models.",
                    "score": 0.4268795056642635,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                },
                {
                    "corpus_id": "272525306",
                    "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
                    "text": "Progress in artificial intelligence (AI) has relied heavily on the dramatic growth in the size of models and datasets. An active area of research focuses on understanding how test error decreases with increases in model and data size. This work has led to the development of scaling laws which posit that test error decreases as a power law with both. However, several theoretical aspects remain unclear. One significant gap is understanding how test error and the existing scaling laws change as the training time is varied (Kaplan et al., 2020;Bahri et al., 2021;Rosenfeld et al., 2020;Sharma & Kaplan, 2022). \n\nThe practical relevance of this question is clear: under a fixed compute budget, what is the optimal balance between scaling the model size and dataset volume, and what is the right amount of training for a given data volume? This is particularly relevant in the context of large language models (LLMs), which are often trained for a single epoch, raising questions about the potential efficacy of training smaller models for longer (more epochs). Furthermore, current scaling laws do not account for other well-known phenomena in learning, such as double descent (Belkin et al., 2019), in which model performance exhibits non-monotonic changes with respect to training data volume, model size, and training time. In particular, double descent theory predicts that test error should increase rapidly at the interpolation threshold, the point at which the model interpolates the training set (Nakkiran et al., 2021;Advani & Saxe, 2017). Like scaling laws, current theories of double descent leave several empirical phenomena unexplained: past explanations of double descent require it to occur, but empirically double descent is often not observed; it is unclear whether the interpolation threshold should grow or shrink with model size; prior theory does not explain why models in the infinite-parameter limit sometimes perform worse than their finite-parameter counterparts. \n\nWe seek the simplest possible unified framework in which to understand learning with respect to model size, data volume, and training time. In doing so, we aim to capture the essential scaling properties of learning in deep neural networks.",
                    "score": 0.4483924175418692,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 611
                        },
                        {
                            "start": 614,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1989
                        },
                        {
                            "start": 1992,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 588,
                            "end": 610,
                            "matchedPaperCorpusId": "246559072"
                        },
                        {
                            "start": 1505,
                            "end": 1528,
                            "matchedPaperCorpusId": "207808916"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69677734375
                }
            ],
            "relevance_judgement": 0.8291015625,
            "relevance_judgment_input_expanded": "# Title: Unified Neural Network Scaling Laws and Scale-time Equivalence\n# Venue: arXiv.org\n# Authors: Akhilan Boopathy, I. Fiete\n## Abstract\nAs neural networks continue to grow in size but datasets might not, it is vital to understand how much performance improvement can be expected: is it more important to scale network size or data volume? Thus, neural network scaling laws, which characterize how test error varies with network size and data volume, have become increasingly important. However, existing scaling laws are often applicable only in limited regimes and often do not incorporate or predict well-known phenomena such as double descent. Here, we present a novel theoretical characterization of how three factors -- model size, training time, and data volume -- interact to determine the performance of deep neural networks. We first establish a theoretical and empirical equivalence between scaling the size of a neural network and increasing its training time proportionally. Scale-time equivalence challenges the current practice, wherein large models are trained for small durations, and suggests that smaller models trained over extended periods could match their efficacy. It also leads to a novel method for predicting the performance of large-scale networks from small-scale networks trained for extended epochs, and vice versa. We next combine scale-time equivalence with a linear model analysis of double descent to obtain a unified theoretical scaling law, which we confirm with experiments across vision benchmarks and network architectures. These laws explain several previously unexplained phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance. Our findings hold significant implications for the practical deployment of neural networks, offering a more accessible and efficient path to training and fine-tuning large models.\n## INTRODUCTION\nProgress in artificial intelligence (AI) has relied heavily on the dramatic growth in the size of models and datasets. An active area of research focuses on understanding how test error decreases with increases in model and data size. This work has led to the development of scaling laws which posit that test error decreases as a power law with both. However, several theoretical aspects remain unclear. One significant gap is understanding how test error and the existing scaling laws change as the training time is varied (Kaplan et al., 2020;Bahri et al., 2021;Rosenfeld et al., 2020;Sharma & Kaplan, 2022). \n\nThe practical relevance of this question is clear: under a fixed compute budget, what is the optimal balance between scaling the model size and dataset volume, and what is the right amount of training for a given data volume? This is particularly relevant in the context of large language models (LLMs), which are often trained for a single epoch, raising questions about the potential efficacy of training smaller models for longer (more epochs). Furthermore, current scaling laws do not account for other well-known phenomena in learning, such as double descent (Belkin et al., 2019), in which model performance exhibits non-monotonic changes with respect to training data volume, model size, and training time. In particular, double descent theory predicts that test error should increase rapidly at the interpolation threshold, the point at which the model interpolates the training set (Nakkiran et al., 2021;Advani & Saxe, 2017). Like scaling laws, current theories of double descent leave several empirical phenomena unexplained: past explanations of double descent require it to occur, but empirically double descent is often not observed; it is unclear whether the interpolation threshold should grow or shrink with model size; prior theory does not explain why models in the infinite-parameter limit sometimes perform worse than their finite-parameter counterparts. \n\nWe seek the simplest possible unified framework in which to understand learning with respect to model size, data volume, and training time. In doing so, we aim to capture the essential scaling properties of learning in deep neural networks.",
            "reference_string": "[272525306 | Boopathy et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2922782",
                    "name": "Ibrahim M. Alabdulmohsin"
                },
                {
                    "authorId": "2045380893",
                    "name": "Xiao-Qi Zhai"
                }
            ],
            "abstract": "Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent\"repeat-all-over\"(RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising<1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!",
            "corpus_id": 276259426,
            "sentences": [
                {
                    "corpus_id": "276259426",
                    "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
                    "text": "There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Hestness et al., 2017;Kaplan et al., 2020;Alabdulmohsin et al., 2022;Bansal et al., 2022;Zhai et al., 2022). Generally, the performance of deep neural networks f (x) (such as its error rate or log-perplexity) often follows a power law f (x) \u223c \u03b2x \u2212c + \u03b5 as one varies a dimension x, such as the data size or model parameters. These \"scaling laws,\" as they are known today, have been used, among others, to determine the training data size needed for a specified level of accuracy (Cho et al., 2015;Beleites et al., 2013;Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020;Hoffmann et al., 2022;Alabdulmohsin et al., 2024b), with some theoretical justification (Bahri et al., 2021;Hutter, 2021;Sharma and Kaplan, 2022). \n\nBesides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2024), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such In RINS, the model f : X \u2192 Y is split into two parts: the first block f A : X \u2192 X is applied iteratively to its own output r times before passing the output to the second block. RIGHT: Illustrative examples of models with different signatures and degrees.",
                    "score": 0.45283379289724596,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 899
                        },
                        {
                            "start": 902,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1836
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 220,
                            "matchedPaperCorpusId": "252220884"
                        },
                        {
                            "start": 240,
                            "end": 258,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 648,
                            "end": 670,
                            "matchedPaperCorpusId": "35138335"
                        },
                        {
                            "start": 874,
                            "end": 898,
                            "matchedPaperCorpusId": "246559072"
                        },
                        {
                            "start": 1248,
                            "end": 1266,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1362,
                            "end": 1379,
                            "matchedPaperCorpusId": "246527904"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8095703125
                },
                {
                    "corpus_id": "276259426",
                    "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
                    "text": "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models. \n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law f r (x) = \u03b2 r x \u2212cr + \u03b5 r , then c r increases with r while \u03b5 r decreases. Furthermore, the coefficient \u03b2 r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2 r , its faster convergence (c r ) and lower asymptotic limit (\u03b5 r ) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results. To quantify this more explicitly, we train language models with four signatures A r B: r \u2208 {1, 2, 3, 4}. Then, we plot the optimal value of r against training compute. As shown in Figure 5b, the optimal value of r monotonically increases with training compute, in agreement with earlier results. Also, smaller models benefit more from RINS. \n\nAdding Linear Adapters. Earlier in Section 4, we showed that enabling stochastic RINS during training exhibits a tradeoff between worst-case and best-case performance, depending on whether or not RINS is applied at inference time. Next, we introduce a additional improvement: when a maximum of r recursion rounds are used in stochastic RINS, we add r lightweight, linear adapters (i.e. linear projection layers) to the output before the projection head. The choice of which adapter to apply is a function of how many recursion rounds are used. Specifically, if signature A r B is used, the r-th adapter is applied. Empirically, this introduces < 1% more parameters and has a negligible Figure 6: LEFT 2 PLOTS: y-axis corresponds to performance when RINS is enabled during training but disabled at inference time in 600M-parameter LMs.",
                    "score": 0.44863860833635794,
                    "section_title": "Multimodal Systems",
                    "char_start_offset": 22964,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 248
                        },
                        {
                            "start": 251,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1192
                        },
                        {
                            "start": 1193,
                            "end": 1237
                        },
                        {
                            "start": 1240,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1693
                        },
                        {
                            "start": 1694,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 2074
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77294921875
                },
                {
                    "corpus_id": "276259426",
                    "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
                    "text": "Next, we investigate the effect of stochastically dropping blocks during training, inspired by the regularization technique of stochastic depth (Huang et al., 2016). Our primary goal is to determine whether this approach can further enhance the performance of RINS while simultaneously offering the flexibility of reverting to non-recursive inference without significant degradation in model quality. \n\nTo recall, RINS has the signature A r B for some r > 1. To implement stochastic RINS, we introduce a skip probability p s \u2208 [0, 1) and sample during training the number of recursion rounds in each step to be 1 + \u03b7, where \u03b7 is a binomial random variable with probability of success 1 \u2212 p s and number of trials r \u2212 1. Thus, block A is always executed at least once. During inference, we are free to choose how to scale compute by setting the value of r \u2265 1. See the detailed pseudocode in Appendix A. For this, we train bigger models with 1 billion parameters. We use an embedding dimension 2,048 and MLP dimension 8,192. All models have 18 decoder blocks. We train for 500K steps and compare RINS with signature A 3 B against the non-recursive baseline. For p s = 0, RINS@1x is significantly worse, with perplexity scores > 3. As expected, RINS converges in performance to the baseline as p s \u2192 1. Similar results using C4 are in Appendix F. \n\nFigure 4 summarizes the advantages of stochastic RINS. Notably, we observe that as p s > 0 increases, stochastic RINS mitigates the performance degradation incurred when scaling is not applied at inference time, while still offering big gains when inference time is scaled. Not surprisingly, though, scaling inference time is less effective when p s increases, suggesting a tradeoff between flexibility at inference time and the magnitude of potential gains from scaling. As shown in Figure 3b, similar conclusions hold in the asymptotic (infinite-compute) regime, assuming the loss follows a power law relation (Kaplan et al., 2020). We resolve this apparent tradeoff in Section 6 using linear adapters.",
                    "score": 0.44601790508580524,
                    "section_title": "Stochastic Recursive Inference Scaling",
                    "char_start_offset": 17332,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 400
                        },
                        {
                            "start": 403,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1344
                        },
                        {
                            "start": 1347,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2051
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63818359375
                }
            ],
            "relevance_judgement": 0.8095703125,
            "relevance_judgment_input_expanded": "# Title: Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems\n# Venue: arXiv.org\n# Authors: Ibrahim M. Alabdulmohsin, Xiao-Qi Zhai\n## Abstract\nInspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent\"repeat-all-over\"(RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising<1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!\n## Introduction\nThere has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Hestness et al., 2017;Kaplan et al., 2020;Alabdulmohsin et al., 2022;Bansal et al., 2022;Zhai et al., 2022). Generally, the performance of deep neural networks f (x) (such as its error rate or log-perplexity) often follows a power law f (x) \u223c \u03b2x \u2212c + \u03b5 as one varies a dimension x, such as the data size or model parameters. These \"scaling laws,\" as they are known today, have been used, among others, to determine the training data size needed for a specified level of accuracy (Cho et al., 2015;Beleites et al., 2013;Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020;Hoffmann et al., 2022;Alabdulmohsin et al., 2024b), with some theoretical justification (Bahri et al., 2021;Hutter, 2021;Sharma and Kaplan, 2022). \n\nBesides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2024), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such In RINS, the model f : X \u2192 Y is split into two parts: the first block f A : X \u2192 X is applied iteratively to its own output r times before passing the output to the second block. RIGHT: Illustrative examples of models with different signatures and degrees.\n\n## Stochastic Recursive Inference Scaling\nNext, we investigate the effect of stochastically dropping blocks during training, inspired by the regularization technique of stochastic depth (Huang et al., 2016). Our primary goal is to determine whether this approach can further enhance the performance of RINS while simultaneously offering the flexibility of reverting to non-recursive inference without significant degradation in model quality. \n\nTo recall, RINS has the signature A r B for some r > 1. To implement stochastic RINS, we introduce a skip probability p s \u2208 [0, 1) and sample during training the number of recursion rounds in each step to be 1 + \u03b7, where \u03b7 is a binomial random variable with probability of success 1 \u2212 p s and number of trials r \u2212 1. Thus, block A is always executed at least once. During inference, we are free to choose how to scale compute by setting the value of r \u2265 1. See the detailed pseudocode in Appendix A. For this, we train bigger models with 1 billion parameters. We use an embedding dimension 2,048 and MLP dimension 8,192. All models have 18 decoder blocks. We train for 500K steps and compare RINS with signature A 3 B against the non-recursive baseline. For p s = 0, RINS@1x is significantly worse, with perplexity scores > 3. As expected, RINS converges in performance to the baseline as p s \u2192 1. Similar results using C4 are in Appendix F. \n\nFigure 4 summarizes the advantages of stochastic RINS. Notably, we observe that as p s > 0 increases, stochastic RINS mitigates the performance degradation incurred when scaling is not applied at inference time, while still offering big gains when inference time is scaled. Not surprisingly, though, scaling inference time is less effective when p s increases, suggesting a tradeoff between flexibility at inference time and the magnitude of potential gains from scaling. As shown in Figure 3b, similar conclusions hold in the asymptotic (infinite-compute) regime, assuming the loss follows a power law relation (Kaplan et al., 2020). We resolve this apparent tradeoff in Section 6 using linear adapters.\n\n## Multimodal Systems\nAs shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models. \n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law f r (x) = \u03b2 r x \u2212cr + \u03b5 r , then c r increases with r while \u03b5 r decreases. Furthermore, the coefficient \u03b2 r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2 r , its faster convergence (c r ) and lower asymptotic limit (\u03b5 r ) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results. To quantify this more explicitly, we train language models with four signatures A r B: r \u2208 {1, 2, 3, 4}. Then, we plot the optimal value of r against training compute. As shown in Figure 5b, the optimal value of r monotonically increases with training compute, in agreement with earlier results. Also, smaller models benefit more from RINS. \n\nAdding Linear Adapters. Earlier in Section 4, we showed that enabling stochastic RINS during training exhibits a tradeoff between worst-case and best-case performance, depending on whether or not RINS is applied at inference time. Next, we introduce a additional improvement: when a maximum of r recursion rounds are used in stochastic RINS, we add r lightweight, linear adapters (i.e. linear projection layers) to the output before the projection head. The choice of which adapter to apply is a function of how many recursion rounds are used. Specifically, if signature A r B is used, the r-th adapter is applied. Empirically, this introduces < 1% more parameters and has a negligible Figure 6: LEFT 2 PLOTS: y-axis corresponds to performance when RINS is enabled during training but disabled at inference time in 600M-parameter LMs.",
            "reference_string": "[276259426 | Alabdulmohsin et al. | 2025 | Citations: 0]"
        },
        {
            "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 56,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319420762",
                    "name": "Bingxuan Li"
                },
                {
                    "authorId": "2280103482",
                    "name": "Yiwei Wang"
                },
                {
                    "authorId": "2343700013",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2256996328",
                    "name": "Nanyun Peng"
                }
            ],
            "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.",
            "corpus_id": 276580569,
            "sentences": [
                {
                    "corpus_id": "276580569",
                    "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
                    "text": "Inference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding (Teller, 2000), beam search (Graves, 2012), and Best-of-N. \n\nRecent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored.",
                    "score": 0.45363649026552644,
                    "section_title": "Test-Time Scaling",
                    "char_start_offset": 6996,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 194
                        },
                        {
                            "start": 197,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 847
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78466796875
                }
            ],
            "relevance_judgement": 0.78466796875,
            "relevance_judgment_input_expanded": "# Title: METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling\n# Venue: arXiv.org\n# Authors: Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng\n## Abstract\nChart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.\n## Test-Time Scaling\nInference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding (Teller, 2000), beam search (Graves, 2012), and Best-of-N. \n\nRecent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored.",
            "reference_string": "[276580569 | Li et al. | 2025 | Citations: 5]"
        },
        {
            "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 53,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359897638",
                    "name": "Xinyue Zeng"
                },
                {
                    "authorId": "2155587513",
                    "name": "Haohui Wang"
                },
                {
                    "authorId": "2311427666",
                    "name": "Junhong Lin"
                },
                {
                    "authorId": "2359758871",
                    "name": "Jun Wu"
                },
                {
                    "authorId": "2359450667",
                    "name": "Tyler Cody"
                },
                {
                    "authorId": "2313576252",
                    "name": "Dawei Zhou"
                }
            ],
            "abstract": "The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.",
            "corpus_id": 278367792,
            "sentences": [
                {
                    "corpus_id": "278367792",
                    "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
                    "text": "Here A, B, \u03b1, \u03b1 N , \u03b2 are universal parameters to be fitted. This groundbreaking work revealed that model performance consistently improves as a power law with increases in three critical factors: model size, training data, and computational power. These findings laid a strong foundation for understanding model behavior during pre-training. \n\nBuilding on this, Lin et al. (2024) demonstrated that finetuning performance depends not only on model size N but also on various architectural design choices, including the number of layers, attention heads, and hidden dimensions. This intricate dependency complicates model selection using traditional scaling laws. However, to predict performance for specific models, a simplified version of the scaling law can be employed by excluding architectural considerations. Scaling laws for fixed models, as proposed in (Kaplan et al., 2020;Hernandez et al., 2021;Tay et al., 2022), exhibit the following form: \n\nwhere D represents the training set size, while B, E, \u03b1, and \u03b2 are parameters specific to the model and task. \n\nAs illustrated in Figure 2, two distinct phases emerge in the scaling behavior of fine-tuning test loss (L) with training sample size (D): the pre-power phase and the power phase. \n\nThe pre-power phase occurs in low-data regimes where model behavior is dominated by initialization and early training dynamics. As the training size increases and reaches a transition point, models enter the power phase, characterized by predictable scaling behavior. In this phase, the relationship between training size and test loss follows a nearly linear correlation, as widely studied in prior works (Henighan et al., 2020;Kaplan et al., 2020;Bahri et al., 2024). Understanding the underlying mechanism of this phase transition phenomenon is crucial for effective model selection. \n\nFirst, it enables precise predictions about when additional training data will lead to the power phase, resulting in consistent and predictable performance improvements. Second, once a model transitions into the power phase, scaling laws can guide data-driven decisions about further data investment, effectively balancing the costs of data collection against expected performance gains.",
                    "score": 0.4197385739661104,
                    "section_title": "L(N, D)",
                    "char_start_offset": 5252,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 342
                        },
                        {
                            "start": 345,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 951
                        },
                        {
                            "start": 954,
                            "end": 1063
                        },
                        {
                            "start": 1066,
                            "end": 1245
                        },
                        {
                            "start": 1248,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1515
                        },
                        {
                            "start": 1516,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1834
                        },
                        {
                            "start": 1837,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1697,
                            "end": 1716,
                            "matchedPaperCorpusId": "231918701"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75732421875
                }
            ],
            "relevance_judgement": 0.75732421875,
            "relevance_judgment_input_expanded": "# Title: LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection\n# Venue: arXiv.org\n# Authors: Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou\n## Abstract\nThe proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.\n## L(N, D)\nHere A, B, \u03b1, \u03b1 N , \u03b2 are universal parameters to be fitted. This groundbreaking work revealed that model performance consistently improves as a power law with increases in three critical factors: model size, training data, and computational power. These findings laid a strong foundation for understanding model behavior during pre-training. \n\nBuilding on this, Lin et al. (2024) demonstrated that finetuning performance depends not only on model size N but also on various architectural design choices, including the number of layers, attention heads, and hidden dimensions. This intricate dependency complicates model selection using traditional scaling laws. However, to predict performance for specific models, a simplified version of the scaling law can be employed by excluding architectural considerations. Scaling laws for fixed models, as proposed in (Kaplan et al., 2020;Hernandez et al., 2021;Tay et al., 2022), exhibit the following form: \n\nwhere D represents the training set size, while B, E, \u03b1, and \u03b2 are parameters specific to the model and task. \n\nAs illustrated in Figure 2, two distinct phases emerge in the scaling behavior of fine-tuning test loss (L) with training sample size (D): the pre-power phase and the power phase. \n\nThe pre-power phase occurs in low-data regimes where model behavior is dominated by initialization and early training dynamics. As the training size increases and reaches a transition point, models enter the power phase, characterized by predictable scaling behavior. In this phase, the relationship between training size and test loss follows a nearly linear correlation, as widely studied in prior works (Henighan et al., 2020;Kaplan et al., 2020;Bahri et al., 2024). Understanding the underlying mechanism of this phase transition phenomenon is crucial for effective model selection. \n\nFirst, it enables precise predictions about when additional training data will lead to the power phase, resulting in consistent and predictable performance improvements. Second, once a model transitions into the power phase, scaling laws can guide data-driven decisions about further data investment, effectively balancing the costs of data collection against expected performance gains.",
            "reference_string": "[278367792 | Zeng et al. | 2025 | Citations: 0]"
        },
        {
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1749176844",
                    "name": "Rylan Schaeffer"
                },
                {
                    "authorId": "2327048379",
                    "name": "Joshua Kazdan"
                },
                {
                    "authorId": "2294572631",
                    "name": "John Hughes"
                },
                {
                    "authorId": "50875781",
                    "name": "Jordan Juravsky"
                },
                {
                    "authorId": "2333593482",
                    "name": "Sara Price"
                },
                {
                    "authorId": "2287830769",
                    "name": "Aengus Lynch"
                },
                {
                    "authorId": "2334069920",
                    "name": "Erik Jones"
                },
                {
                    "authorId": "2311693657",
                    "name": "Robert Kirk"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                },
                {
                    "authorId": "143812875",
                    "name": "Oluwasanmi Koyejo"
                }
            ],
            "abstract": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.",
            "corpus_id": 276580891,
            "sentences": [
                {
                    "corpus_id": "276580891",
                    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
                    "text": "This work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
                    "score": 0.5473260482125347,
                    "section_title": "Discussion and Future Directions",
                    "char_start_offset": 19791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1580
                        },
                        {
                            "start": 1583,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1920
                        },
                        {
                            "start": 1923,
                            "end": 2199
                        },
                        {
                            "start": 2200,
                            "end": 2308
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75341796875
                }
            ],
            "relevance_judgement": 0.75341796875,
            "relevance_judgment_input_expanded": "# Title: How Do Large Language Monkeys Get Their Power (Laws)?\n# Venue: arXiv.org\n# Authors: Rylan Schaeffer, Joshua Kazdan, John Hughes, Jordan Juravsky, Sara Price, Aengus Lynch, Erik Jones, Robert Kirk, Azalia Mirhoseini, Oluwasanmi Koyejo\n## Abstract\nRecent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.\n## Discussion and Future Directions\nThis work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
            "reference_string": "[276580891 | Schaeffer et al. | 2025 | Citations: 4]"
        },
        {
            "title": "AGI, Governments, and Free Societies",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281052678",
                    "name": "Justin B. Bullock"
                },
                {
                    "authorId": "2349384137",
                    "name": "Samuel Hammond"
                },
                {
                    "authorId": "2349382249",
                    "name": "Seb Krier"
                }
            ],
            "abstract": "This paper examines how artificial general intelligence (AGI) could fundamentally reshape the delicate balance between state capacity and individual liberty that sustains free societies. Building on Acemoglu and Robinson's 'narrow corridor' framework, we argue that AGI poses distinct risks of pushing societies toward either a 'despotic Leviathan' through enhanced state surveillance and control, or an 'absent Leviathan' through the erosion of state legitimacy relative to AGI-empowered non-state actors. Drawing on public administration theory and recent advances in AI capabilities, we analyze how these dynamics could unfold through three key channels: the automation of discretionary decision-making within agencies, the evolution of bureaucratic structures toward system-level architectures, and the transformation of democratic feedback mechanisms. Our analysis reveals specific failure modes that could destabilize liberal institutions. Enhanced state capacity through AGI could enable unprecedented surveillance and control, potentially entrenching authoritarian practices. Conversely, rapid diffusion of AGI capabilities to non-state actors could undermine state legitimacy and governability. We examine how these risks manifest differently at the micro level of individual bureaucratic decisions, the meso level of organizational structure, and the macro level of democratic processes. To preserve the narrow corridor of liberty, we propose a governance framework emphasizing robust technical safeguards, hybrid institutional designs that maintain meaningful human oversight, and adaptive regulatory mechanisms.",
            "corpus_id": 276903172,
            "sentences": [
                {
                    "corpus_id": "276903172",
                    "title": "AGI, Governments, and Free Societies",
                    "text": "Rapid improvements in models have surpassed the expectations of many observers and academics in the field, largely due to the sheer scale of the data and computing resources available to train deep neural network models. \n\nScaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters. 4  short, bigger is better--at least so far--thanks to a combination of technological advancements and increased resource allocation, leading to rapid progress in AI capabilities. The compute required for language models to reach a set performance threshold has halved approximately every eight months since 2012. In addition to significant algorithmic innovations, such as the Transformer architecture, many recent performance gains stem primarily from the ability to scale up models and datasets to unprecedented sizes. This has been made possible by rapid growth in computing power, which has allowed the rate of improvement to substantially exceed hardware gains from Moore's Law. 5 untless recent advances by labs, academic institutions, and open source communities reflect growing investment in the field and a diversification of approaches and algorithms. For example, new systems from Google DeepMind have recently solved four out of six problems from last year's International Mathematical Olympiad, matching the competition's silver medalist for the first time.6",
                    "score": 0.5509914957545738,
                    "section_title": "II. The Age of Artificial Intelligence",
                    "char_start_offset": 5600,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 223,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1800
                        },
                        {
                            "start": 1801,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2173
                        },
                        {
                            "start": 2174,
                            "end": 2349
                        },
                        {
                            "start": 2350,
                            "end": 2559
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73583984375
                }
            ],
            "relevance_judgement": 0.73583984375,
            "relevance_judgment_input_expanded": "# Title: AGI, Governments, and Free Societies\n# Venue: arXiv.org\n# Authors: Justin B. Bullock, Samuel Hammond, Seb Krier\n## Abstract\nThis paper examines how artificial general intelligence (AGI) could fundamentally reshape the delicate balance between state capacity and individual liberty that sustains free societies. Building on Acemoglu and Robinson's 'narrow corridor' framework, we argue that AGI poses distinct risks of pushing societies toward either a 'despotic Leviathan' through enhanced state surveillance and control, or an 'absent Leviathan' through the erosion of state legitimacy relative to AGI-empowered non-state actors. Drawing on public administration theory and recent advances in AI capabilities, we analyze how these dynamics could unfold through three key channels: the automation of discretionary decision-making within agencies, the evolution of bureaucratic structures toward system-level architectures, and the transformation of democratic feedback mechanisms. Our analysis reveals specific failure modes that could destabilize liberal institutions. Enhanced state capacity through AGI could enable unprecedented surveillance and control, potentially entrenching authoritarian practices. Conversely, rapid diffusion of AGI capabilities to non-state actors could undermine state legitimacy and governability. We examine how these risks manifest differently at the micro level of individual bureaucratic decisions, the meso level of organizational structure, and the macro level of democratic processes. To preserve the narrow corridor of liberty, we propose a governance framework emphasizing robust technical safeguards, hybrid institutional designs that maintain meaningful human oversight, and adaptive regulatory mechanisms.\n## II. The Age of Artificial Intelligence\nRapid improvements in models have surpassed the expectations of many observers and academics in the field, largely due to the sheer scale of the data and computing resources available to train deep neural network models. \n\nScaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters. 4  short, bigger is better--at least so far--thanks to a combination of technological advancements and increased resource allocation, leading to rapid progress in AI capabilities. The compute required for language models to reach a set performance threshold has halved approximately every eight months since 2012. In addition to significant algorithmic innovations, such as the Transformer architecture, many recent performance gains stem primarily from the ability to scale up models and datasets to unprecedented sizes. This has been made possible by rapid growth in computing power, which has allowed the rate of improvement to substantially exceed hardware gains from Moore's Law. 5 untless recent advances by labs, academic institutions, and open source communities reflect growing investment in the field and a diversification of approaches and algorithms. For example, new systems from Google DeepMind have recently solved four out of six problems from last year's International Mathematical Olympiad, matching the competition's silver medalist for the first time.6",
            "reference_string": "[276903172 | Bullock et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Can Test-Time Scaling Improve World Foundation Model?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292160328",
                    "name": "Wenyan Cong"
                },
                {
                    "authorId": "2115315923",
                    "name": "Hanqing Zhu"
                },
                {
                    "authorId": "2118952622",
                    "name": "Peihao Wang"
                },
                {
                    "authorId": "2353216784",
                    "name": "Bangya Liu"
                },
                {
                    "authorId": "1575684088",
                    "name": "Dejia Xu"
                },
                {
                    "authorId": "2268756018",
                    "name": "Kevin Wang"
                },
                {
                    "authorId": "2266751059",
                    "name": "David Z. Pan"
                },
                {
                    "authorId": "2353108236",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "47774591",
                    "name": "Zhiwen Fan"
                },
                {
                    "authorId": "2227945855",
                    "name": "Zhangyang Wang"
                }
            ],
            "abstract": "World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.",
            "corpus_id": 277468441,
            "sentences": [],
            "relevance_judgement": 0.732421875,
            "relevance_judgment_input_expanded": "# Title: Can Test-Time Scaling Improve World Foundation Model?\n# Venue: arXiv.org\n# Authors: Wenyan Cong, Hanqing Zhu, Peihao Wang, Bangya Liu, Dejia Xu, Kevin Wang, David Z. Pan, Yan Wang, Zhiwen Fan, Zhangyang Wang\n## Abstract\nWorld foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.\n",
            "reference_string": "[277468441 | Cong et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Breaking Neural Network Scaling Laws with Modularity",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.05780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52089350",
                    "name": "Akhilan Boopathy"
                },
                {
                    "authorId": "2320292273",
                    "name": "Sunshine Jiang"
                },
                {
                    "authorId": "2297765362",
                    "name": "William Yue"
                },
                {
                    "authorId": "2238396678",
                    "name": "Jaedong Hwang"
                },
                {
                    "authorId": "2297772570",
                    "name": "Abhiram Iyer"
                },
                {
                    "authorId": "8657128",
                    "name": "I. Fiete"
                }
            ],
            "abstract": "Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.",
            "corpus_id": 272524356,
            "sentences": [
                {
                    "corpus_id": "272524356",
                    "title": "Breaking Neural Network Scaling Laws with Modularity",
                    "text": "Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma & Kaplan, 2022;Clark et al., 2022;Tay et al., 2022). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022;Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively. \n\nSome works approach this problem by modeling NN learning as manifold or kernel regression. For example, McRae et al. (2020) considers regression on manifolds and concludes that sample complexity scales based on the intrinsic manifold dimension of the data. In another case, Canatar et al. (2021) draws correlations between the study of kernel regression to how infinite-width deep networks can generalize based on the size of the training dataset and the suitability of a particular kernel for a task. Along these lines, several works use random matrix theory to derive scaling laws for kernel regression (Hastie et al., 2022;Cui et al., 2021;2022;Wei et al., 2022;Jin et al., 2021). \n\nAmong other observations, this body of work shows that in the absence of strong inductive biases, high-dimensional tasks have sample complexity growing roughly exponentially with the intrinsic dimensionality of the data manifold.",
                    "score": 0.4227106823263234,
                    "section_title": "NEURAL NETWORK SCALING LAWS",
                    "char_start_offset": 6138,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 451
                        },
                        {
                            "start": 454,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1838
                        },
                        {
                            "start": 1841,
                            "end": 2070
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 391,
                            "end": 414,
                            "matchedPaperCorpusId": "246559072"
                        },
                        {
                            "start": 414,
                            "end": 433,
                            "matchedPaperCorpusId": "246473179"
                        },
                        {
                            "start": 433,
                            "end": 450,
                            "matchedPaperCorpusId": "237592821"
                        },
                        {
                            "start": 654,
                            "end": 676,
                            "matchedPaperCorpusId": "249947012"
                        },
                        {
                            "start": 1259,
                            "end": 1278,
                            "matchedPaperCorpusId": "219687988"
                        },
                        {
                            "start": 1429,
                            "end": 1450,
                            "matchedPaperCorpusId": "226227332"
                        },
                        {
                            "start": 1760,
                            "end": 1781,
                            "matchedPaperCorpusId": "84186305"
                        },
                        {
                            "start": 1781,
                            "end": 1798,
                            "matchedPaperCorpusId": "235254538"
                        },
                        {
                            "start": 1803,
                            "end": 1820,
                            "matchedPaperCorpusId": "247410980"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73046875
                }
            ],
            "relevance_judgement": 0.73046875,
            "relevance_judgment_input_expanded": "# Title: Breaking Neural Network Scaling Laws with Modularity\n# Venue: arXiv.org\n# Authors: Akhilan Boopathy, Sunshine Jiang, William Yue, Jaedong Hwang, Abhiram Iyer, I. Fiete\n## Abstract\nModular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.\n## NEURAL NETWORK SCALING LAWS\nMany works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma & Kaplan, 2022;Clark et al., 2022;Tay et al., 2022). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022;Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively. \n\nSome works approach this problem by modeling NN learning as manifold or kernel regression. For example, McRae et al. (2020) considers regression on manifolds and concludes that sample complexity scales based on the intrinsic manifold dimension of the data. In another case, Canatar et al. (2021) draws correlations between the study of kernel regression to how infinite-width deep networks can generalize based on the size of the training dataset and the suitability of a particular kernel for a task. Along these lines, several works use random matrix theory to derive scaling laws for kernel regression (Hastie et al., 2022;Cui et al., 2021;2022;Wei et al., 2022;Jin et al., 2021). \n\nAmong other observations, this body of work shows that in the absence of strong inductive biases, high-dimensional tasks have sample complexity growing roughly exponentially with the intrinsic dimensionality of the data manifold.",
            "reference_string": "[272524356 | Boopathy et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308470091",
                    "name": "Tomer Porian"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                },
                {
                    "authorId": "2253541812",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "2444742",
                    "name": "Y. Carmon"
                }
            ],
            "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.",
            "corpus_id": 270764838,
            "sentences": [
                {
                    "corpus_id": "270764838",
                    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                    "text": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
                    "score": 0.5694872285550104,
                    "section_title": "Related work",
                    "char_start_offset": 24085,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 676
                        },
                        {
                            "start": 679,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1917
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 514,
                            "end": 518,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 636,
                            "end": 639,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 1199,
                            "end": 1203,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1299,
                            "end": 1303,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 1453,
                            "end": 1456,
                            "matchedPaperCorpusId": "258832817"
                        },
                        {
                            "start": 1616,
                            "end": 1620,
                            "matchedPaperCorpusId": "269033049"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71484375
                }
            ],
            "relevance_judgement": 0.71484375,
            "relevance_judgment_input_expanded": "# Title: Resolving Discrepancies in Compute-Optimal Scaling of Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Tomer Porian, Mitchell Wortsman, J. Jitsev, Ludwig Schmidt, Y. Carmon\n## Abstract\nKaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.\n## Related work\nRecent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
            "reference_string": "[270764838 | Porian et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 330,
            "influential_citation_count": 28,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283198901",
                    "name": "Bradley Brown"
                },
                {
                    "authorId": "50875781",
                    "name": "Jordan Juravsky"
                },
                {
                    "authorId": "2283134957",
                    "name": "Ryan Ehrlich"
                },
                {
                    "authorId": "2313919316",
                    "name": "Ronald Clark"
                },
                {
                    "authorId": "2151097303",
                    "name": "Quoc V. Le"
                },
                {
                    "authorId": "2313917068",
                    "name": "Christopher R'e"
                },
                {
                    "authorId": "1861312",
                    "name": "Azalia Mirhoseini"
                }
            ],
            "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
            "corpus_id": 271571035,
            "sentences": [
                {
                    "corpus_id": "271571035",
                    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                    "text": "Concurrent with our work, Song et al. [51] finds that using the best available sample improves LLM performance on chat, math, and code tasks, sweeping up to a max of 128 samples. Additionally, Hassid et al. [24] find that when solving coding tasks, it can be more effective to draw more samples from a smaller model than draw fewer samples from a larger one. \n\nScaling Laws: Characterizing how scaling affects model performance can lead to more informed decisions on how to allocate resources. Scaling laws for LLM training find a power law relationship between loss and the amount of training compute and provide estimates for the optimal model and dataset size given a fixed compute budget [27,36,28]. Jones [33] finds scaling laws in the context of the board game Hex, observing that performance scales predictably with model size and the difficulty of the problem. Interestingly, they also show that performance scales with the amount of test-time compute spent while performing tree search. Recently, Shao et al. [49] observe scaling laws when augmenting LLMs with external retrieval datasets, finding that performance on retrieval tasks scales smoothly with the size of the retrieval corpus.",
                    "score": 0.4193009726898843,
                    "section_title": "Related Work",
                    "char_start_offset": 32886,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 358
                        },
                        {
                            "start": 361,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1197
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7138671875
                }
            ],
            "relevance_judgement": 0.7138671875,
            "relevance_judgment_input_expanded": "# Title: Large Language Monkeys: Scaling Inference Compute with Repeated Sampling\n# Venue: arXiv.org\n# Authors: Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R'e, Azalia Mirhoseini\n## Abstract\nScaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.\n## Related Work\nConcurrent with our work, Song et al. [51] finds that using the best available sample improves LLM performance on chat, math, and code tasks, sweeping up to a max of 128 samples. Additionally, Hassid et al. [24] find that when solving coding tasks, it can be more effective to draw more samples from a smaller model than draw fewer samples from a larger one. \n\nScaling Laws: Characterizing how scaling affects model performance can lead to more informed decisions on how to allocate resources. Scaling laws for LLM training find a power law relationship between loss and the amount of training compute and provide estimates for the optimal model and dataset size given a fixed compute budget [27,36,28]. Jones [33] finds scaling laws in the context of the board game Hex, observing that performance scales predictably with model size and the difficulty of the problem. Interestingly, they also show that performance scales with the amount of test-time compute spent while performing tree search. Recently, Shao et al. [49] observe scaling laws when augmenting LLMs with external retrieval datasets, finding that performance on retrieval tasks scales smoothly with the size of the retrieval corpus.",
            "reference_string": "[271571035 | Brown et al. | 2024 | Citations: 330]"
        },
        {
            "title": "Temporal Scaling Law for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.17785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249971338",
                    "name": "Yizhe Xiong"
                },
                {
                    "authorId": "2298904872",
                    "name": "Xiansheng Chen"
                },
                {
                    "authorId": "2299108794",
                    "name": "Xin Ye"
                },
                {
                    "authorId": "2298921971",
                    "name": "Hui Chen"
                },
                {
                    "authorId": "1818920",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2298903058",
                    "name": "Haoran Lian"
                },
                {
                    "authorId": "2293626051",
                    "name": "Jianwei Niu"
                },
                {
                    "authorId": "2242661989",
                    "name": "Guiguang Ding"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.",
            "corpus_id": 269449894,
            "sentences": [
                {
                    "corpus_id": "269449894",
                    "title": "Temporal Scaling Law for Large Language Models",
                    "text": "The concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
                    "score": 0.5039908985968942,
                    "section_title": "Scaling Laws for Language Models",
                    "char_start_offset": 8141,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1795
                        },
                        {
                            "start": 1798,
                            "end": 1820
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 459,
                            "end": 480,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 480,
                            "end": 504,
                            "matchedPaperCorpusId": "255570036"
                        },
                        {
                            "start": 786,
                            "end": 807,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 918,
                            "end": 943,
                            "matchedPaperCorpusId": "255570036"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68359375
                }
            ],
            "relevance_judgement": 0.68359375,
            "relevance_judgment_input_expanded": "# Title: Temporal Scaling Law for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei Niu, Guiguang Ding\n## Abstract\nRecently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.\n## Scaling Laws for Language Models\nThe concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
            "reference_string": "[269449894 | Xiong et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 62,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.10938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "82939895",
                    "name": "Yangjun Ruan"
                },
                {
                    "authorId": "2772217",
                    "name": "Chris J. Maddison"
                },
                {
                    "authorId": "2302156937",
                    "name": "Tatsunori B. Hashimoto"
                }
            ],
            "abstract": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
            "corpus_id": 269899695,
            "sentences": [
                {
                    "corpus_id": "269899695",
                    "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                    "text": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
                    "score": 0.4498742323262134,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68212890625
                },
                {
                    "corpus_id": "269899695",
                    "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                    "text": "We have presented observational scaling laws -an approach that generalizes existing compute scaling laws to handle multiple model families using a shared, low-dimensional capability space. Using this approach, we show that we can build low-cost, high-resolution, and broad-coverage scaling laws that allow us to make accurate predictions for many complex scaling phenomena, such as emergent behaviors, agentic capabilities, and the value of post-training interventions. We provide concrete and practical prescriptions for researchers and practitioners to perform similar forms of scaling analyses for their own benchmarks and post-training methods in the hopes of encouraging more quantitative, scaling-law-based approaches to designing benchmarks and post-training methods. \n\nLimitations and future work Finally, we discuss some limitations of our approach and findings: Firstly, observational scaling laws are primarily applicable to post-training scaling analyses and do not directly translate to pretraining scenarios in the same way as standard compute-based scaling laws. Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12,81], would be valuable. Thirdly, while we have demonstrated that our observational scaling analyses can provide meaningful insights into improving particular models' complex capabilities, a promising direction for future work would be to apply the findings from our approach, such as by deriving surrogate measures for model complex capabilities that can be used to optimize models directly and efficiently. Lastly, our assumptions do not account for potential benchmark contamination (where particular benchmark data leaks into model training) or the heterogeneity within model families (where models within the same family may have varying compute efficiencies and scaling behaviors). Investigating the impact of these assumptions on our approach would be an interesting avenue for future research. \n\nMajor Changelog 07/02/2024 \n\n\u2022 Added clarifications that emphasize the predictions of observational scaling laws are based on standard benchmark metrics instead of training FLOPs. \n\n\u2022 Updated plots to use x-axis with a log10 scale instead of a ln scale for better readability.",
                    "score": 0.46682018331520136,
                    "section_title": "Conclusion, Limitations, and Future Work",
                    "char_start_offset": 44273,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 2118
                        },
                        {
                            "start": 2119,
                            "end": 2232
                        },
                        {
                            "start": 2235,
                            "end": 2261
                        },
                        {
                            "start": 2264,
                            "end": 2414
                        },
                        {
                            "start": 2417,
                            "end": 2511
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67041015625
                }
            ],
            "relevance_judgement": 0.68212890625,
            "relevance_judgment_input_expanded": "# Title: Observational Scaling Laws and the Predictability of Language Model Performance\n# Venue: Neural Information Processing Systems\n# Authors: Yangjun Ruan, Chris J. Maddison, Tatsunori B. Hashimoto\n## Abstract\nUnderstanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.\n## Conclusion, Limitations, and Future Work\nWe have presented observational scaling laws -an approach that generalizes existing compute scaling laws to handle multiple model families using a shared, low-dimensional capability space. Using this approach, we show that we can build low-cost, high-resolution, and broad-coverage scaling laws that allow us to make accurate predictions for many complex scaling phenomena, such as emergent behaviors, agentic capabilities, and the value of post-training interventions. We provide concrete and practical prescriptions for researchers and practitioners to perform similar forms of scaling analyses for their own benchmarks and post-training methods in the hopes of encouraging more quantitative, scaling-law-based approaches to designing benchmarks and post-training methods. \n\nLimitations and future work Finally, we discuss some limitations of our approach and findings: Firstly, observational scaling laws are primarily applicable to post-training scaling analyses and do not directly translate to pretraining scenarios in the same way as standard compute-based scaling laws. Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12,81], would be valuable. Thirdly, while we have demonstrated that our observational scaling analyses can provide meaningful insights into improving particular models' complex capabilities, a promising direction for future work would be to apply the findings from our approach, such as by deriving surrogate measures for model complex capabilities that can be used to optimize models directly and efficiently. Lastly, our assumptions do not account for potential benchmark contamination (where particular benchmark data leaks into model training) or the heterogeneity within model families (where models within the same family may have varying compute efficiencies and scaling behaviors). Investigating the impact of these assumptions on our approach would be an interesting avenue for future research. \n\nMajor Changelog 07/02/2024 \n\n\u2022 Added clarifications that emphasize the predictions of observational scaling laws are based on standard benchmark metrics instead of training FLOPs. \n\n\u2022 Updated plots to use x-axis with a log10 scale instead of a ln scale for better readability.",
            "reference_string": "[269899695 | Ruan et al. | 2024 | Citations: 62]"
        },
        {
            "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
            "venue": "",
            "year": 2025,
            "reference_count": 291,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2124959429",
                    "name": "Qiyuan Zhang"
                },
                {
                    "authorId": "1704274486",
                    "name": "Fuyuan Lyu"
                },
                {
                    "authorId": "2353075323",
                    "name": "Zexu Sun"
                },
                {
                    "authorId": "2353322623",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2353196827",
                    "name": "Weixu Zhang"
                },
                {
                    "authorId": "2301269240",
                    "name": "Zhihan Guo"
                },
                {
                    "authorId": "2268629268",
                    "name": "Yufei Wang"
                },
                {
                    "authorId": "2258549144",
                    "name": "Irwin King"
                },
                {
                    "authorId": "2188246843",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2324831308",
                    "name": "Chen Ma"
                }
            ],
            "abstract": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/",
            "corpus_id": 277467322,
            "sentences": [
                {
                    "corpus_id": "277467322",
                    "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                    "text": "Large language models (LLMs) (Brown et al., 2020;OpenAI, 2024a) have emerged in recent years as a transformative milestone toward artificial general intelligence (AGI) (Goertzel, 2014;Bubeck et al., 2023). These models remarkably learn general intelligence by training-time scaling, where the models ingest more data and parameters (Kaplan et al., 2020;Hoffmann et al., 2022). However, the progress of pretraining scaling has gradually slowed due to its resource-intensive nature and the bounded availability of human data, prompting researchers to shift their focus toward how to fully elicit the intelligence encoded in LLMs at test time to maximize their real-world effectiveness (Wei et al., 2022;Ouyang et al., 2022;Li et al., 2024d)? \n\nHuman cognition may suggest a clue. When faced with complex problems, people tend to engage in deeper, more deliberate thinking, often producing better outcomes (Kahneman, 2011(Kahneman, , 2003;;Evans, 1984). Inspired by this principle, recent research (Wei et al., 2022;Wang et al., 2023b) has introduced methods that allocate additional computation during inference to boost task performance. Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements. This family of methods, referred to as test-time scaling (TTS), progressively elicits the model's intelligence in the test-time, as depicted in Figure 1. The remarkable successes of reasoning models, such as o1 (OpenAI, 2024b) and R1 (DeepSeek-AI, 2025), have further amplified interest in TTS, highlighting its potential as a key driver of LLM reasoning and utility. However, despite this surge in research activity, the field currently lacks a unified and systematic framework to synthesize insights, compare techniques, or identify consistent trends in TTS . To address this gap, we present a comprehensive survey of TTS, offering a hierarchical and extensible framework to analyze methods, map research efforts, and guide future progress.",
                    "score": 0.4213426668299762,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 739
                        },
                        {
                            "start": 742,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2056
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 29,
                            "end": 49,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 168,
                            "end": 184,
                            "matchedPaperCorpusId": "4794432"
                        },
                        {
                            "start": 683,
                            "end": 701,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 701,
                            "end": 721,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 721,
                            "end": 738,
                            "matchedPaperCorpusId": "267760184"
                        },
                        {
                            "start": 918,
                            "end": 937,
                            "matchedPaperCorpusId": "15131441"
                        },
                        {
                            "start": 937,
                            "end": 949,
                            "matchedPaperCorpusId": "145670284"
                        },
                        {
                            "start": 995,
                            "end": 1013,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1013,
                            "end": 1032,
                            "matchedPaperCorpusId": "247595263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6806640625
                }
            ],
            "relevance_judgement": 0.6806640625,
            "relevance_judgment_input_expanded": "# Title: A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?\n# Venue: \n# Authors: Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, Chen Ma\n## Abstract\nAs enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/\n## Introduction\nLarge language models (LLMs) (Brown et al., 2020;OpenAI, 2024a) have emerged in recent years as a transformative milestone toward artificial general intelligence (AGI) (Goertzel, 2014;Bubeck et al., 2023). These models remarkably learn general intelligence by training-time scaling, where the models ingest more data and parameters (Kaplan et al., 2020;Hoffmann et al., 2022). However, the progress of pretraining scaling has gradually slowed due to its resource-intensive nature and the bounded availability of human data, prompting researchers to shift their focus toward how to fully elicit the intelligence encoded in LLMs at test time to maximize their real-world effectiveness (Wei et al., 2022;Ouyang et al., 2022;Li et al., 2024d)? \n\nHuman cognition may suggest a clue. When faced with complex problems, people tend to engage in deeper, more deliberate thinking, often producing better outcomes (Kahneman, 2011(Kahneman, , 2003;;Evans, 1984). Inspired by this principle, recent research (Wei et al., 2022;Wang et al., 2023b) has introduced methods that allocate additional computation during inference to boost task performance. Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements. This family of methods, referred to as test-time scaling (TTS), progressively elicits the model's intelligence in the test-time, as depicted in Figure 1. The remarkable successes of reasoning models, such as o1 (OpenAI, 2024b) and R1 (DeepSeek-AI, 2025), have further amplified interest in TTS, highlighting its potential as a key driver of LLM reasoning and utility. However, despite this surge in research activity, the field currently lacks a unified and systematic framework to synthesize insights, compare techniques, or identify consistent trends in TTS . To address this gap, we present a comprehensive survey of TTS, offering a hierarchical and extensible framework to analyze methods, map research efforts, and guide future progress.",
            "reference_string": "[277467322 | Zhang et al. | 2025 | Citations: 14]"
        },
        {
            "title": "A Dynamical Model of Neural Scaling Laws",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 44,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "77327149",
                    "name": "Blake Bordelon"
                },
                {
                    "authorId": "27755610",
                    "name": "Alexander Atanasov"
                },
                {
                    "authorId": "2577481",
                    "name": "Cengiz Pehlevan"
                }
            ],
            "abstract": "On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate $\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.",
            "corpus_id": 267406160,
            "sentences": [
                {
                    "corpus_id": "267406160",
                    "title": "A Dynamical Model of Neural Scaling Laws",
                    "text": "Large scale language and vision models have been shown to achieve better performance as the number of parameters and number of training steps are increased. Moreover, the scaling of various loss metrics (such as cross entropy or MSE test loss) has been empirically observed to exhibit remarkably regular, often power law behavior across several orders of magnitude (Hestness et al., 2017;Kaplan et al., Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 2020). These findings are termed \"neural scaling laws\". \n\nNeural scaling laws play a central role in modern deep learning practice, and have substantial implications for the optimal trade-off between model size and training time (Hoffmann et al., 2022), as well as architecture selection (Alabdulmohsin et al., 2023). Understanding the origin of such scaling laws, as well as their exponents, has the potential to offer insight into better architectures, the design of better datasets (Sorscher et al., 2022), and the failure modes and limitations of deep learning systems. Yet, many questions about neural scaling laws remain open. \n\nIn this paper, we introduce and analyze a solvable model which captures many important aspects of neural scaling laws. In particular, we are interested in understanding the following empirically observed phenomena: Test Loss Scales as a Power-law in Training Time and Model Size and Compute. In many domains of deep learning, the test loss of a model with N trainable parameters trained for t iterations has been found to scale as L(t, N ) \u2248 L 0 + a t t \u2212rt + a N N \u2212r N (Kaplan et al., 2020;Hoffmann et al., 2022). These scaling law exponents r t , r N generally depend on the dataset and architecture. We demonstrate scaling laws on simple vision and language tasks in Figure 1. The compute is proportional to the number of steps of gradient descent times the model size C \u221d N t. Setting N and t optimally gives that test loss scales as a power law in C. This is the compute optimal scaling law. \n\nCompute-Optimal Training Time and Model Size Scaling Exponents Are Different.",
                    "score": 0.45732318312132936,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 595
                        },
                        {
                            "start": 598,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1855
                        },
                        {
                            "start": 1856,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2072
                        },
                        {
                            "start": 2075,
                            "end": 2152
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 769,
                            "end": 792,
                            "matchedPaperCorpusId": "49321232"
                        },
                        {
                            "start": 1025,
                            "end": 1048,
                            "matchedPaperCorpusId": "250113273"
                        },
                        {
                            "start": 1667,
                            "end": 1689,
                            "matchedPaperCorpusId": "49321232"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: A Dynamical Model of Neural Scaling Laws\n# Venue: International Conference on Machine Learning\n# Authors: Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan\n## Abstract\nOn a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate $\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.\n## Introduction\nLarge scale language and vision models have been shown to achieve better performance as the number of parameters and number of training steps are increased. Moreover, the scaling of various loss metrics (such as cross entropy or MSE test loss) has been empirically observed to exhibit remarkably regular, often power law behavior across several orders of magnitude (Hestness et al., 2017;Kaplan et al., Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 2020). These findings are termed \"neural scaling laws\". \n\nNeural scaling laws play a central role in modern deep learning practice, and have substantial implications for the optimal trade-off between model size and training time (Hoffmann et al., 2022), as well as architecture selection (Alabdulmohsin et al., 2023). Understanding the origin of such scaling laws, as well as their exponents, has the potential to offer insight into better architectures, the design of better datasets (Sorscher et al., 2022), and the failure modes and limitations of deep learning systems. Yet, many questions about neural scaling laws remain open. \n\nIn this paper, we introduce and analyze a solvable model which captures many important aspects of neural scaling laws. In particular, we are interested in understanding the following empirically observed phenomena: Test Loss Scales as a Power-law in Training Time and Model Size and Compute. In many domains of deep learning, the test loss of a model with N trainable parameters trained for t iterations has been found to scale as L(t, N ) \u2248 L 0 + a t t \u2212rt + a N N \u2212r N (Kaplan et al., 2020;Hoffmann et al., 2022). These scaling law exponents r t , r N generally depend on the dataset and architecture. We demonstrate scaling laws on simple vision and language tasks in Figure 1. The compute is proportional to the number of steps of gradient descent times the model size C \u221d N t. Setting N and t optimally gives that test loss scales as a power law in C. This is the compute optimal scaling law. \n\nCompute-Optimal Training Time and Model Size Scaling Exponents Are Different.",
            "reference_string": "[267406160 | Bordelon et al. | 2024 | Citations: 44]"
        },
        {
            "title": "Data Scaling Laws for End-to-End Autonomous Driving",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 83,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354178740",
                    "name": "Alexander Naumann"
                },
                {
                    "authorId": "2293314523",
                    "name": "Xunjiang Gu"
                },
                {
                    "authorId": "2215782082",
                    "name": "Tolga Dimlioglu"
                },
                {
                    "authorId": "2065365958",
                    "name": "Mariusz Bojarski"
                },
                {
                    "authorId": "2899953",
                    "name": "Alperen De\u011firmenci"
                },
                {
                    "authorId": "2322798228",
                    "name": "Alexander Popov"
                },
                {
                    "authorId": "46242907",
                    "name": "Devansh Bisla"
                },
                {
                    "authorId": "2237790577",
                    "name": "Marco Pavone"
                },
                {
                    "authorId": "2322792868",
                    "name": "Urs Muller"
                },
                {
                    "authorId": "145156173",
                    "name": "B. Ivanovic"
                }
            ],
            "abstract": "Autonomous vehicle (AV) stacks have traditionally relied on decomposed approaches, with separate modules handling perception, prediction, and planning. However, this design introduces information loss during inter-module communication, increases computational overhead, and can lead to compounding errors. To address these challenges, recent works have proposed architectures that integrate all components into an end-to-end differentiable model, enabling holistic system optimization. This shift emphasizes data engineering over software integration, offering the potential to enhance system performance by simply scaling up training resources. In this work, we evaluate the performance of a simple end-to-end driving architecture on internal driving datasets ranging in size from 16 to 8192 hours with both open-loop metrics and closed-loop simulations. Specifically, we investigate how much additional training data is needed to achieve a target performance gain, e.g., a 5% improvement in motion prediction accuracy. By understanding the relationship between model performance and training dataset size, we aim to provide insights for data-driven decision-making in autonomous driving development.",
            "corpus_id": 277622207,
            "sentences": [
                {
                    "corpus_id": "277622207",
                    "title": "Data Scaling Laws for End-to-End Autonomous Driving",
                    "text": "Scaling laws in deep learning have demonstrated predictable improvements in model performance as dataset sizes increase, following a power-law relationship [27,28,39,59,64,77]: L val \u221d \u03b2x c , where x is the training data size, \u03b2 and c are problem-specific constants, and L val represents the validation loss. Studies such as [39] established that auto-regressive models like GPT exhibit consistent scaling with respect to dataset size, model capacity, and training iterations, insights later applied to optimize GPT-3's training [50]. Scaling laws have been observed across diverse modalities, from language and vision to multi-modal generative models [27,77]. Recent theoret-ical work further formalizes these observations by linking scaling behaviors to intrinsic data properties [2,4,63]. \n\nA critical aspect of scaling laws is estimating the amount of additional data needed to achieve target performance, especially for costly data domains such as AVs [1,10,25,37,48,49,62]. [48] introduced an active learning framework where estimators iteratively refine dataset size predictions, compensating for overly optimistic projections on smaller datasets. More recent approaches, including probabilistic models [25] and meta-learning [37], aim to reduce extrapolation errors by distinguishing between data regimes. Notably, [1] proposes a generalized estimator to improve data requirement predictions across diverse tasks. Although some prior works in autonomous driving, such as EMMA [34], STR [65], and GUMP [31], briefly mention scaling laws as part of their analyses, they lack a systematic approach and do not derive a formal scaling relationship. \n\nA recent study [81] explores data scaling in end-to-end AV systems but lacks a structured framework, omitting key analyses such as different scaling trends, performance variations across action types, and model capacity effects. In contrast, our work establishes a systematic approach for measuring scaling behavior and data requirement estimates.",
                    "score": 0.46064187727801303,
                    "section_title": "Scaling Laws and Data Estimation",
                    "char_start_offset": 6518,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 791
                        },
                        {
                            "start": 794,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1651
                        },
                        {
                            "start": 1654,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2001
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 169,
                            "end": 172,
                            "matchedPaperCorpusId": "6842201"
                        },
                        {
                            "start": 172,
                            "end": 175,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 656,
                            "end": 659,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 785,
                            "end": 787,
                            "matchedPaperCorpusId": "233739646"
                        },
                        {
                            "start": 787,
                            "end": 790,
                            "matchedPaperCorpusId": "246559072"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "252220884"
                        },
                        {
                            "start": 963,
                            "end": 966,
                            "matchedPaperCorpusId": "265506756"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "257353837"
                        },
                        {
                            "start": 969,
                            "end": 972,
                            "matchedPaperCorpusId": "249947012"
                        },
                        {
                            "start": 972,
                            "end": 975,
                            "matchedPaperCorpusId": "252693192"
                        },
                        {
                            "start": 975,
                            "end": 978,
                            "matchedPaperCorpusId": "272713896"
                        },
                        {
                            "start": 980,
                            "end": 984,
                            "matchedPaperCorpusId": "249947012"
                        },
                        {
                            "start": 1210,
                            "end": 1214,
                            "matchedPaperCorpusId": "265506756"
                        },
                        {
                            "start": 1233,
                            "end": 1237,
                            "matchedPaperCorpusId": "257353837"
                        },
                        {
                            "start": 1323,
                            "end": 1326,
                            "matchedPaperCorpusId": "252220884"
                        },
                        {
                            "start": 1509,
                            "end": 1513,
                            "matchedPaperCorpusId": "270924324"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67041015625
                }
            ],
            "relevance_judgement": 0.67041015625,
            "relevance_judgment_input_expanded": "# Title: Data Scaling Laws for End-to-End Autonomous Driving\n# Venue: arXiv.org\n# Authors: Alexander Naumann, Xunjiang Gu, Tolga Dimlioglu, Mariusz Bojarski, Alperen De\u011firmenci, Alexander Popov, Devansh Bisla, Marco Pavone, Urs Muller, B. Ivanovic\n## Abstract\nAutonomous vehicle (AV) stacks have traditionally relied on decomposed approaches, with separate modules handling perception, prediction, and planning. However, this design introduces information loss during inter-module communication, increases computational overhead, and can lead to compounding errors. To address these challenges, recent works have proposed architectures that integrate all components into an end-to-end differentiable model, enabling holistic system optimization. This shift emphasizes data engineering over software integration, offering the potential to enhance system performance by simply scaling up training resources. In this work, we evaluate the performance of a simple end-to-end driving architecture on internal driving datasets ranging in size from 16 to 8192 hours with both open-loop metrics and closed-loop simulations. Specifically, we investigate how much additional training data is needed to achieve a target performance gain, e.g., a 5% improvement in motion prediction accuracy. By understanding the relationship between model performance and training dataset size, we aim to provide insights for data-driven decision-making in autonomous driving development.\n## Scaling Laws and Data Estimation\nScaling laws in deep learning have demonstrated predictable improvements in model performance as dataset sizes increase, following a power-law relationship [27,28,39,59,64,77]: L val \u221d \u03b2x c , where x is the training data size, \u03b2 and c are problem-specific constants, and L val represents the validation loss. Studies such as [39] established that auto-regressive models like GPT exhibit consistent scaling with respect to dataset size, model capacity, and training iterations, insights later applied to optimize GPT-3's training [50]. Scaling laws have been observed across diverse modalities, from language and vision to multi-modal generative models [27,77]. Recent theoret-ical work further formalizes these observations by linking scaling behaviors to intrinsic data properties [2,4,63]. \n\nA critical aspect of scaling laws is estimating the amount of additional data needed to achieve target performance, especially for costly data domains such as AVs [1,10,25,37,48,49,62]. [48] introduced an active learning framework where estimators iteratively refine dataset size predictions, compensating for overly optimistic projections on smaller datasets. More recent approaches, including probabilistic models [25] and meta-learning [37], aim to reduce extrapolation errors by distinguishing between data regimes. Notably, [1] proposes a generalized estimator to improve data requirement predictions across diverse tasks. Although some prior works in autonomous driving, such as EMMA [34], STR [65], and GUMP [31], briefly mention scaling laws as part of their analyses, they lack a systematic approach and do not derive a formal scaling relationship. \n\nA recent study [81] explores data scaling in end-to-end AV systems but lacks a structured framework, omitting key analyses such as different scaling trends, performance variations across action types, and model capacity effects. In contrast, our work establishes a systematic approach for measuring scaling behavior and data requirement estimates.",
            "reference_string": "[277622207 | Naumann et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Scaling Laws for Floating Point Quantization Training",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277239672",
                    "name": "Xingwu Sun"
                },
                {
                    "authorId": "2303292656",
                    "name": "Shuaipeng Li"
                },
                {
                    "authorId": "2292022322",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "2338983603",
                    "name": "Weidong Han"
                },
                {
                    "authorId": "2327002793",
                    "name": "Kan Wu"
                },
                {
                    "authorId": "2301870117",
                    "name": "Zhen Yang"
                },
                {
                    "authorId": "2309077550",
                    "name": "Yixing Li"
                },
                {
                    "authorId": "2339164541",
                    "name": "An Wang"
                },
                {
                    "authorId": "2303292656",
                    "name": "Shuaipeng Li"
                },
                {
                    "authorId": "2302814808",
                    "name": "Jinbao Xue"
                },
                {
                    "authorId": "2327341631",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "2267016579",
                    "name": "Yang-Dan Tao"
                },
                {
                    "authorId": "2261082002",
                    "name": "Zhanhui Kang"
                },
                {
                    "authorId": "2277225795",
                    "name": "Chengzhong Xu"
                },
                {
                    "authorId": "2277518788",
                    "name": "Di Wang"
                },
                {
                    "authorId": "2228153894",
                    "name": "Jie Jiang"
                }
            ],
            "abstract": "Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models. In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.",
            "corpus_id": 275337234,
            "sentences": [
                {
                    "corpus_id": "275337234",
                    "title": "Scaling Laws for Floating Point Quantization Training",
                    "text": "Classical Scaling Laws. Scaling laws have become a fundamental framework for understanding the relationship between essential factors such as model size (N), data size (D), and the resulting loss (L) in deep learning. Two classical scaling laws have been widely recognized in the industry: Chinchilla scaling law (Hoffmann et al., 2022) and OpenAI scaling law (Kaplan et al., 2020). The Chinchilla scaling law is expressed as: \n\nThe OpenAI scaling law is given by: \n\nwhere n, d, \u03b1, \u03b2, and \u03f5 are positive fitted constants. The balance between N and D emerges as critical for computeoptimal training. \n\nScaling Laws for Precision. Subsequent research extends this framework by incorporating the role of precision in quantized training and inference, so as to provide insights into how precision affects model performance. In Kumar et al. (2024), precision-aware scaling laws were introduced to capture the trade-offs between model size N , data size D, and precision P . For integer quantized training, they proposed the tradeoff between weight N and weight precision P as: \n\nwhere N eff indicates the \"effective parameter count\" of models, and \u03b3 is a constant representing the sensitivity of model weights to precision. Incorporating N eff into the Chinchilla scaling law yields: \n\nThis framework highlights that reducing weight precision P can be compensated by increasing the parameter count N to maintain performance, which is a critical insight for low-precision model optimization. \n\nCurrent Scaling Laws cannot Fit Well in FP Quantization. Note that most previous work focused on integer quantized training. FP quantization is more prevalent in real-world applications due to its hardware compatibility and finer granularity. For instance, formats such as FP16 and BF16 are standard in many large-scale training pipelines, and emerging formats like FP8 and FP4 are gaining traction. Despite this, scaling laws specifically tailored to FP quantization are still largely unexplored. The primary distinction between FP and integer quantization lies in the allocation and usage of bits. FP numbers allocate bits to represent both the exponent and the mantissa, with each set of bits serving distinct purposes: the exponent mainly captures dynamic range, while the mantissa mainly encodes precision within that range.",
                    "score": 0.4799123621305699,
                    "section_title": "Preliminary",
                    "char_start_offset": 5566,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 426
                        },
                        {
                            "start": 429,
                            "end": 464
                        },
                        {
                            "start": 467,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 598
                        },
                        {
                            "start": 601,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1071
                        },
                        {
                            "start": 1074,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1278
                        },
                        {
                            "start": 1281,
                            "end": 1485
                        },
                        {
                            "start": 1488,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 1985
                        },
                        {
                            "start": 1986,
                            "end": 2087
                        },
                        {
                            "start": 2088,
                            "end": 2317
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66943359375
                }
            ],
            "relevance_judgement": 0.66943359375,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Floating Point Quantization Training\n# Venue: arXiv.org\n# Authors: Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuaipeng Li, Jinbao Xue, Yu Cheng, Yang-Dan Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang\n## Abstract\nLow-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models. In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.\n## Preliminary\nClassical Scaling Laws. Scaling laws have become a fundamental framework for understanding the relationship between essential factors such as model size (N), data size (D), and the resulting loss (L) in deep learning. Two classical scaling laws have been widely recognized in the industry: Chinchilla scaling law (Hoffmann et al., 2022) and OpenAI scaling law (Kaplan et al., 2020). The Chinchilla scaling law is expressed as: \n\nThe OpenAI scaling law is given by: \n\nwhere n, d, \u03b1, \u03b2, and \u03f5 are positive fitted constants. The balance between N and D emerges as critical for computeoptimal training. \n\nScaling Laws for Precision. Subsequent research extends this framework by incorporating the role of precision in quantized training and inference, so as to provide insights into how precision affects model performance. In Kumar et al. (2024), precision-aware scaling laws were introduced to capture the trade-offs between model size N , data size D, and precision P . For integer quantized training, they proposed the tradeoff between weight N and weight precision P as: \n\nwhere N eff indicates the \"effective parameter count\" of models, and \u03b3 is a constant representing the sensitivity of model weights to precision. Incorporating N eff into the Chinchilla scaling law yields: \n\nThis framework highlights that reducing weight precision P can be compensated by increasing the parameter count N to maintain performance, which is a critical insight for low-precision model optimization. \n\nCurrent Scaling Laws cannot Fit Well in FP Quantization. Note that most previous work focused on integer quantized training. FP quantization is more prevalent in real-world applications due to its hardware compatibility and finer granularity. For instance, formats such as FP16 and BF16 are standard in many large-scale training pipelines, and emerging formats like FP8 and FP4 are gaining traction. Despite this, scaling laws specifically tailored to FP quantization are still largely unexplored. The primary distinction between FP and integer quantization lies in the allocation and usage of bits. FP numbers allocate bits to represent both the exponent and the mantissa, with each set of bits serving distinct purposes: the exponent mainly captures dynamic range, while the mantissa mainly encodes precision within that range.",
            "reference_string": "[275337234 | Sun et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Scaling Law Phenomena Across Regression Paradigms: Multiple and Kernel Approaches",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 133,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334664374",
                    "name": "Yifang Chen"
                },
                {
                    "authorId": "2348772143",
                    "name": "Xuyang Guo"
                },
                {
                    "authorId": "2316172905",
                    "name": "Xiaoyu Li"
                },
                {
                    "authorId": "2260827689",
                    "name": "Yingyu Liang"
                },
                {
                    "authorId": "113515522",
                    "name": "Zhenmei Shi"
                },
                {
                    "authorId": "2284489474",
                    "name": "Zhao Song"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have achieved remarkable success. A key factor behind this success is the scaling law observed by OpenAI. Specifically, for models with Transformer architecture, the test loss exhibits a power-law relationship with model size, dataset size, and the amount of computation used in training, demonstrating trends that span more than seven orders of magnitude. This scaling law challenges traditional machine learning wisdom, notably the Oscar Scissors principle, which suggests that an overparametrized algorithm will overfit the training datasets, resulting in poor test performance. Recent research has also identified the scaling law in simpler machine learning contexts, such as linear regression. However, fully explaining the scaling law in large practical models remains an elusive goal. In this work, we advance our understanding by demonstrating that the scaling law phenomenon extends to multiple regression and kernel regression settings, which are significantly more expressive and powerful than linear methods. Our analysis provides deeper insights into the scaling law, potentially enhancing our understanding of LLMs.",
            "corpus_id": 276776461,
            "sentences": [
                {
                    "corpus_id": "276776461",
                    "title": "Scaling Law Phenomena Across Regression Paradigms: Multiple and Kernel Approaches",
                    "text": "The scaling behavior of deep learning models concerning compute, sample size, and model size has been a central topic in machine learning research [HNA + 17, RRBS19, BMR + 20, HKK + 20, HBM + 22, ZKHB22, MRB + 23]. A pivotal contribution by [KMH + 20] revealed that generalization error in transformers decays as a power law with respect to these three factors. Their work provided joint formulas predicting how model performance improves with increased compute, data, and parameters. This finding highlighted the importance of scaling models and led to a surge in interest in large-scale models for NLP and other tasks. \n\nFurther research refined these scaling laws. For instance, [HBM + 22] proposed the Chinchilla law, which suggested that under compute constraints, balancing data size with model size yields optimal performance, contrasting with earlier models that focused primarily on increasing parameter counts. Meanwhile, [MRB + 23] introduced methods for data reuse across training passes, showing that efficient data handling can maintain performance while lowering the need for excessive compute resources. These empirical studies have provided clearer guidelines for allocating resources effectively when training large models. \n\nIn parallel, theoretical work has advanced understanding of these scaling laws. [SK20] demonstrated that, in regression, the generalization error scales as n \u22124/d , linking model performance to the intrinsic dimensionality of the data. Additionally, other researchers, such as [BDK + 21] and [BAP24], applied statistical physics to derive scaling laws for linear models, particularly under infinite parameter regimes. Theoretical contributions like these complement empirical findings and offer deeper insights into how models behave as they scale, shaping future directions in machine learning research.",
                    "score": 0.49942297632998583,
                    "section_title": "Scaling Laws",
                    "char_start_offset": 4507,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 620
                        },
                        {
                            "start": 623,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1479
                        },
                        {
                            "start": 1480,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1848
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.662109375
                }
            ],
            "relevance_judgement": 0.662109375,
            "relevance_judgment_input_expanded": "# Title: Scaling Law Phenomena Across Regression Paradigms: Multiple and Kernel Approaches\n# Venue: arXiv.org\n# Authors: Yifang Chen, Xuyang Guo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song\n## Abstract\nRecently, Large Language Models (LLMs) have achieved remarkable success. A key factor behind this success is the scaling law observed by OpenAI. Specifically, for models with Transformer architecture, the test loss exhibits a power-law relationship with model size, dataset size, and the amount of computation used in training, demonstrating trends that span more than seven orders of magnitude. This scaling law challenges traditional machine learning wisdom, notably the Oscar Scissors principle, which suggests that an overparametrized algorithm will overfit the training datasets, resulting in poor test performance. Recent research has also identified the scaling law in simpler machine learning contexts, such as linear regression. However, fully explaining the scaling law in large practical models remains an elusive goal. In this work, we advance our understanding by demonstrating that the scaling law phenomenon extends to multiple regression and kernel regression settings, which are significantly more expressive and powerful than linear methods. Our analysis provides deeper insights into the scaling law, potentially enhancing our understanding of LLMs.\n## Scaling Laws\nThe scaling behavior of deep learning models concerning compute, sample size, and model size has been a central topic in machine learning research [HNA + 17, RRBS19, BMR + 20, HKK + 20, HBM + 22, ZKHB22, MRB + 23]. A pivotal contribution by [KMH + 20] revealed that generalization error in transformers decays as a power law with respect to these three factors. Their work provided joint formulas predicting how model performance improves with increased compute, data, and parameters. This finding highlighted the importance of scaling models and led to a surge in interest in large-scale models for NLP and other tasks. \n\nFurther research refined these scaling laws. For instance, [HBM + 22] proposed the Chinchilla law, which suggested that under compute constraints, balancing data size with model size yields optimal performance, contrasting with earlier models that focused primarily on increasing parameter counts. Meanwhile, [MRB + 23] introduced methods for data reuse across training passes, showing that efficient data handling can maintain performance while lowering the need for excessive compute resources. These empirical studies have provided clearer guidelines for allocating resources effectively when training large models. \n\nIn parallel, theoretical work has advanced understanding of these scaling laws. [SK20] demonstrated that, in regression, the generalization error scales as n \u22124/d , linking model performance to the intrinsic dimensionality of the data. Additionally, other researchers, such as [BDK + 21] and [BAP24], applied statistical physics to derive scaling laws for linear models, particularly under infinite parameter regimes. Theoretical contributions like these complement empirical findings and offer deeper insights into how models behave as they scale, shaping future directions in machine learning research.",
            "reference_string": "[276776461 | Chen et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 10,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.12370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328415570",
                    "name": "Samira Abnar"
                },
                {
                    "authorId": "2341539581",
                    "name": "Harshay Shah"
                },
                {
                    "authorId": "46254693",
                    "name": "Dan Busbridge"
                },
                {
                    "authorId": "2341667138",
                    "name": "Alaaeldin Mohamed Elnouby Ali"
                },
                {
                    "authorId": "2243336902",
                    "name": "Josh Susskind"
                },
                {
                    "authorId": "3042871",
                    "name": "Vimal Thilak"
                }
            ],
            "abstract": "Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.",
            "corpus_id": 275789885,
            "sentences": [
                {
                    "corpus_id": "275789885",
                    "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
                    "text": "-The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see \u00a72.1). \n\n-While the trend of increasing active number of parameters is similar across all training compute budgets; the optimal active number of parameters decrease more rapidly with sparsity as the training compute budget increases (see \u00a73). \u2022 Effect of Sparsity on Downstream Performance: For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2019), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022b;Goyal et al., 2024) may address this gap. We use a \"length-controlled\" variant of few-shot Chain-of-Thought (CoT) prompting to study whether this strategy may help close the gap between MoEs and dense models on reasoning tasks in Appendix E. Empirical evidence suggests that MoEs may benefit more from dynamically increased inference-time compute than dense models under a fixed generated tokens budget. \u2022 Parametric Scaling Law: We propose a parametric form for scaling laws that accounts for sparsity. The model coefficients are estimated using the empirical data obtained by training compute-optimal models. An interesting observation from Appendix F is that the exponent for sparsity term \u03bb is negative which is consistent with our intuition that sparser models lead to a lower perplexity.",
                    "score": 0.4205716913899071,
                    "section_title": "Discussion",
                    "char_start_offset": 22382,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 217,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1338
                        },
                        {
                            "start": 1339,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1728
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 678,
                            "end": 698,
                            "matchedPaperCorpusId": "52055325"
                        },
                        {
                            "start": 706,
                            "end": 730,
                            "matchedPaperCorpusId": "47018994"
                        },
                        {
                            "start": 916,
                            "end": 935,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 935,
                            "end": 954,
                            "matchedPaperCorpusId": "263608983"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65966796875
                }
            ],
            "relevance_judgement": 0.65966796875,
            "relevance_judgment_input_expanded": "# Title: Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models\n# Venue: arXiv.org\n# Authors: Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak\n## Abstract\nScaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increasing both, the precise interplay between these factors and their combined contribution to overall capacity remains not fully understood. We explore this relationship in the context of sparse Mixture-of-Experts (MoEs), which allow scaling the number of parameters without proportionally increasing the FLOPs per example. We investigate how varying the sparsity level, i.e., the fraction of inactive parameters, impacts model's performance during pretraining and downstream few-shot evaluation. We find that under different constraints (e.g., parameter size and total training compute), there is an optimal level of sparsity that improves both training efficiency and model performance. These results provide a better understanding of the impact of sparsity in scaling laws for MoEs and complement existing works in this area, offering insights for designing more efficient architectures.\n## Discussion\n-The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see \u00a72.1). \n\n-While the trend of increasing active number of parameters is similar across all training compute budgets; the optimal active number of parameters decrease more rapidly with sparsity as the training compute budget increases (see \u00a73). \u2022 Effect of Sparsity on Downstream Performance: For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2019), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022b;Goyal et al., 2024) may address this gap. We use a \"length-controlled\" variant of few-shot Chain-of-Thought (CoT) prompting to study whether this strategy may help close the gap between MoEs and dense models on reasoning tasks in Appendix E. Empirical evidence suggests that MoEs may benefit more from dynamically increased inference-time compute than dense models under a fixed generated tokens budget. \u2022 Parametric Scaling Law: We propose a parametric form for scaling laws that accounts for sparsity. The model coefficients are estimated using the empirical data obtained by training compute-optimal models. An interesting observation from Appendix F is that the exponent for sparsity term \u03bb is negative which is consistent with our intuition that sparser models lead to a lower perplexity.",
            "reference_string": "[275789885 | Abnar et al. | 2025 | Citations: 10]"
        },
        {
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 72,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.findings-emnlp.544.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.06387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1962329805",
                    "name": "Maor Ivgi"
                },
                {
                    "authorId": "2444742",
                    "name": "Y. Carmon"
                },
                {
                    "authorId": "1750652",
                    "name": "Jonathan Berant"
                }
            ],
            "abstract": "Neural scaling laws define a predictable relationship between a model's parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.",
            "corpus_id": 246823711,
            "sentences": [
                {
                    "corpus_id": "246823711",
                    "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
                    "text": "Neural scaling laws define a predictable relationship between a model's parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.",
                    "score": 0.5503581067513279,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6572265625
                }
            ],
            "relevance_judgement": 0.6572265625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Maor Ivgi, Y. Carmon, Jonathan Berant\n## Abstract\nNeural scaling laws define a predictable relationship between a model's parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.\n",
            "reference_string": "[246823711 | Ivgi et al. | 2022 | Citations: 17]"
        },
        {
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 162,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109670338",
                    "name": "Zhongzhan Huang"
                },
                {
                    "authorId": "2350347175",
                    "name": "Guoming Ling"
                },
                {
                    "authorId": "2350347092",
                    "name": "Vincent S. Liang"
                },
                {
                    "authorId": "46395826",
                    "name": "Yu-Hsiang Lin"
                },
                {
                    "authorId": "2267017926",
                    "name": "Yandong Chen"
                },
                {
                    "authorId": "2053866242",
                    "name": "Shan Zhong"
                },
                {
                    "authorId": "2264958585",
                    "name": "Hefeng Wu"
                },
                {
                    "authorId": "2237746830",
                    "name": "Liang Lin"
                }
            ],
            "abstract": "Routing large language models (LLMs) is a new paradigm that uses a router to recommend the best LLM from a pool of candidates for a given input. In this paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can significantly enhance the performance of this paradigm as the number of candidates increases. This improvement can even surpass the performance of the best single model in the pool and many existing strong LLMs, confirming it a highly promising paradigm. However, the lack of comprehensive and open-source benchmarks for Routing LLMs has hindered the development of routers. In this paper, we introduce RouterEval, a benchmark tailored for router research, which includes over 200,000,000 performance records for 12 popular LLM evaluations across various areas such as commonsense reasoning, semantic understanding, etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations of existing Routing LLM methods reveal that most still have significant room for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data, code and tutorial.",
            "corpus_id": 277043311,
            "sentences": [
                {
                    "corpus_id": "277043311",
                    "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
                    "text": "Scaling laws (Li et al., 2025) have become a cornerstone for deciphering the behavior of deep learning models across a wide array of domains and tasks. They provide critical insights into how performance correlates with key factors such as dataset size, model capacity, and computational resources. Early work by Banko and Brill (2001) laid the groundwork by identifying a power-law relationship between validation error and training dataset size in tasks like confusion set disambiguation. Their findings revealed that as the dataset grows, the average error decreases predictably, while the model size needed to effectively fit the data scales log-linearly. This seminal observation was later expanded by Amodei et al. (2016), who demonstrated power-law improvements in word error rate with increased training data for the 38M-parameter Deep Speech 2 model, and by Hestness et al. (2017), who extended these exponential trends to diverse fields such as machine translation, language modeling, image processing, and speech recognition. These studies collectively highlighted the robustness of scaling laws, showing that performance gains remain consistent even as models and architectures evolve. \n\nBuilding on this foundation, subsequent research has pushed the boundaries of scale while refining the implications of these laws. For instance, Kaplan et al. (2020) investigated models with up to 1.5B parameters trained on 23B tokens, deriving powerlaw relationships to optimize computational budget allocation. However, later critiques from Hoffmann et al. (2022) andHu et al. (2024c) pointed out an underestimation of required training data, underscoring subtle methodological challenges in scaling studies. Beyond sheer scale, researchers have delved into more nuanced phenomena: Wei et al. (2022) identified emergent abilities in large language models that are absent in smaller ones, while Hernandez et al. (2021) explored scaling laws in transfer learning and finetuning contexts. Architectural diversity has also come under scrutiny, with Tay et al. (2022) showing that not all model designs scale equally, advocating for scaling studies to inform architecture development. The adaptability of scaling laws to emerging paradigms is evident in recent innovations.",
                    "score": 0.5694697026327974,
                    "section_title": "C.3 Scaling Law",
                    "char_start_offset": 34708,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 490
                        },
                        {
                            "start": 491,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1197
                        },
                        {
                            "start": 1200,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2181
                        },
                        {
                            "start": 2182,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6435546875
                }
            ],
            "relevance_judgement": 0.6435546875,
            "relevance_judgment_input_expanded": "# Title: RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs\n# Venue: arXiv.org\n# Authors: Zhongzhan Huang, Guoming Ling, Vincent S. Liang, Yu-Hsiang Lin, Yandong Chen, Shan Zhong, Hefeng Wu, Liang Lin\n## Abstract\nRouting large language models (LLMs) is a new paradigm that uses a router to recommend the best LLM from a pool of candidates for a given input. In this paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can significantly enhance the performance of this paradigm as the number of candidates increases. This improvement can even surpass the performance of the best single model in the pool and many existing strong LLMs, confirming it a highly promising paradigm. However, the lack of comprehensive and open-source benchmarks for Routing LLMs has hindered the development of routers. In this paper, we introduce RouterEval, a benchmark tailored for router research, which includes over 200,000,000 performance records for 12 popular LLM evaluations across various areas such as commonsense reasoning, semantic understanding, etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations of existing Routing LLM methods reveal that most still have significant room for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data, code and tutorial.\n## C.3 Scaling Law\nScaling laws (Li et al., 2025) have become a cornerstone for deciphering the behavior of deep learning models across a wide array of domains and tasks. They provide critical insights into how performance correlates with key factors such as dataset size, model capacity, and computational resources. Early work by Banko and Brill (2001) laid the groundwork by identifying a power-law relationship between validation error and training dataset size in tasks like confusion set disambiguation. Their findings revealed that as the dataset grows, the average error decreases predictably, while the model size needed to effectively fit the data scales log-linearly. This seminal observation was later expanded by Amodei et al. (2016), who demonstrated power-law improvements in word error rate with increased training data for the 38M-parameter Deep Speech 2 model, and by Hestness et al. (2017), who extended these exponential trends to diverse fields such as machine translation, language modeling, image processing, and speech recognition. These studies collectively highlighted the robustness of scaling laws, showing that performance gains remain consistent even as models and architectures evolve. \n\nBuilding on this foundation, subsequent research has pushed the boundaries of scale while refining the implications of these laws. For instance, Kaplan et al. (2020) investigated models with up to 1.5B parameters trained on 23B tokens, deriving powerlaw relationships to optimize computational budget allocation. However, later critiques from Hoffmann et al. (2022) andHu et al. (2024c) pointed out an underestimation of required training data, underscoring subtle methodological challenges in scaling studies. Beyond sheer scale, researchers have delved into more nuanced phenomena: Wei et al. (2022) identified emergent abilities in large language models that are absent in smaller ones, while Hernandez et al. (2021) explored scaling laws in transfer learning and finetuning contexts. Architectural diversity has also come under scrutiny, with Tay et al. (2022) showing that not all model designs scale equally, advocating for scaling studies to inform architecture development. The adaptability of scaling laws to emerging paradigms is evident in recent innovations.",
            "reference_string": "[277043311 | Huang et al. | 2025 | Citations: 7]"
        },
        {
            "title": "Towards Neural Scaling Laws for Time Series Foundation Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12360, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326120888",
                    "name": "Qingren Yao"
                },
                {
                    "authorId": "2326237480",
                    "name": "Chao-Han Huck Yang"
                },
                {
                    "authorId": "2268522268",
                    "name": "Renhe Jiang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2298723734",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                }
            ],
            "abstract": "Scaling laws offer valuable insights into the design of time series foundation models (TSFMs). However, previous research has largely focused on the scaling laws of TSFMs for in-distribution (ID) data, leaving their out-of-distribution (OOD) scaling behavior and the influence of model architectures less explored. In this work, we examine two common TSFM architectures, encoder-only and decoder-only Transformers, and investigate their scaling behavior on both ID and OOD data. These models are trained and evaluated across varying parameter counts, compute budgets, and dataset sizes. Our experiments reveal that the log-likelihood loss of TSFMs exhibits similar scaling behavior in both OOD and ID settings. We further compare the scaling properties across different architectures, incorporating two state-of-the-art TSFMs as case studies, showing that model architecture plays a significant role in scaling. The encoder-only Transformers demonstrate better scalability than the decoder-only Transformers, while the architectural enhancements in the two advanced TSFMs primarily improve ID performance but reduce OOD scalability. While scaling up TSFMs is expected to drive performance breakthroughs, the lack of a comprehensive understanding of TSFM scaling laws has hindered the development of a robust framework to guide model scaling. We fill this gap in this work by synthesizing our findings and providing practical guidelines for designing and scaling larger TSFMs with enhanced model capabilities.",
            "corpus_id": 273375506,
            "sentences": [
                {
                    "corpus_id": "273375506",
                    "title": "Towards Neural Scaling Laws for Time Series Foundation Models",
                    "text": "Neural Scaling Laws. Neural scaling laws seek to provide a predictive framework for optimizing the allocation of computational resources to maximize model performance. In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2024). In vision domains, scaling laws have been explored in areas like discriminative modeling (Hestness et al., 2017) and visual auto-regressive modeling (Henighan et al., 2020). Recently, Edwards et al. ( 2024) introduced scaling laws for large time series models, showing that performance scales according to a power law with model size, compute, and dataset size. Shi et al. (2024a) examined the effect of time-series forecasting horizon on model scaling behavior, offering a theoretical framework to explain its influence. However, both studies have focused on in-distribution scenarios, leaving the investigation of scaling laws in out-of-distribution contexts largely unexplored. \n\nTime Series Foundation Models. Foundation models (Das et al., 2024;Goswami et al., 2024) represent a new paradigm aimed at generalizing across diverse domains and tasks by leveraging knowledge from large-scale data in the pre-training phase. They have significantly advanced time series forecasting, particularly in zero-shot scenarios, where predictions are made on data from previously unseen domains. For instance, Woo et al. (2024) introduced Moirai, an encoder-only transformer architecture that employs an \"any-variate\" attention mechanism to capture dependencies in multivariate time series. Ansari et al. ( 2024) proposed a method that tokenizes time series values through scaling and quantization into a fixed vocabulary, training a series of transformer-based models known as Chronos.",
                    "score": 0.5626390257241433,
                    "section_title": "RELATED WORKS",
                    "char_start_offset": 23184,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 20
                        },
                        {
                            "start": 21,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1195
                        },
                        {
                            "start": 1198,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1992
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 491,
                            "end": 513,
                            "matchedPaperCorpusId": "266693796"
                        },
                        {
                            "start": 1247,
                            "end": 1265,
                            "matchedPaperCorpusId": "264172792"
                        },
                        {
                            "start": 1616,
                            "end": 1633,
                            "matchedPaperCorpusId": "267411817"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63818359375
                }
            ],
            "relevance_judgement": 0.63818359375,
            "relevance_judgment_input_expanded": "# Title: Towards Neural Scaling Laws for Time Series Foundation Models\n# Venue: International Conference on Learning Representations\n# Authors: Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin, Shirui Pan\n## Abstract\nScaling laws offer valuable insights into the design of time series foundation models (TSFMs). However, previous research has largely focused on the scaling laws of TSFMs for in-distribution (ID) data, leaving their out-of-distribution (OOD) scaling behavior and the influence of model architectures less explored. In this work, we examine two common TSFM architectures, encoder-only and decoder-only Transformers, and investigate their scaling behavior on both ID and OOD data. These models are trained and evaluated across varying parameter counts, compute budgets, and dataset sizes. Our experiments reveal that the log-likelihood loss of TSFMs exhibits similar scaling behavior in both OOD and ID settings. We further compare the scaling properties across different architectures, incorporating two state-of-the-art TSFMs as case studies, showing that model architecture plays a significant role in scaling. The encoder-only Transformers demonstrate better scalability than the decoder-only Transformers, while the architectural enhancements in the two advanced TSFMs primarily improve ID performance but reduce OOD scalability. While scaling up TSFMs is expected to drive performance breakthroughs, the lack of a comprehensive understanding of TSFM scaling laws has hindered the development of a robust framework to guide model scaling. We fill this gap in this work by synthesizing our findings and providing practical guidelines for designing and scaling larger TSFMs with enhanced model capabilities.\n## RELATED WORKS\nNeural Scaling Laws. Neural scaling laws seek to provide a predictive framework for optimizing the allocation of computational resources to maximize model performance. In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2024). In vision domains, scaling laws have been explored in areas like discriminative modeling (Hestness et al., 2017) and visual auto-regressive modeling (Henighan et al., 2020). Recently, Edwards et al. ( 2024) introduced scaling laws for large time series models, showing that performance scales according to a power law with model size, compute, and dataset size. Shi et al. (2024a) examined the effect of time-series forecasting horizon on model scaling behavior, offering a theoretical framework to explain its influence. However, both studies have focused on in-distribution scenarios, leaving the investigation of scaling laws in out-of-distribution contexts largely unexplored. \n\nTime Series Foundation Models. Foundation models (Das et al., 2024;Goswami et al., 2024) represent a new paradigm aimed at generalizing across diverse domains and tasks by leveraging knowledge from large-scale data in the pre-training phase. They have significantly advanced time series forecasting, particularly in zero-shot scenarios, where predictions are made on data from previously unseen domains. For instance, Woo et al. (2024) introduced Moirai, an encoder-only transformer architecture that employs an \"any-variate\" attention mechanism to capture dependencies in multivariate time series. Ansari et al. ( 2024) proposed a method that tokenizes time series values through scaling and quantization into a fixed vocabulary, training a series of transformer-based models known as Chronos.",
            "reference_string": "[273375506 | Yao et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Scaling Laws Beyond Backpropagation",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 52,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.14593",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.14593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237875798",
                    "name": "Matthew J. Filipovich"
                },
                {
                    "authorId": "2069293685",
                    "name": "Alessandro Cappelli"
                },
                {
                    "authorId": "80424302",
                    "name": "Daniel Hesslow"
                },
                {
                    "authorId": "143945447",
                    "name": "Julien Launay"
                }
            ],
            "abstract": "Alternatives to backpropagation have long been studied to better understand how biological brains may learn. Recently, they have also garnered interest as a way to train neural networks more efficiently. By relaxing constraints inherent to backpropagation (e.g., symmetric feedforward and feedback weights, sequential updates), these methods enable promising prospects, such as local learning. However, the tradeoffs between different methods in terms of final task performance, convergence speed, and ultimately compute and data requirements are rarely outlined. In this work, we use scaling laws to study the ability of Direct Feedback Alignment~(DFA) to train causal decoder-only Transformers efficiently. Scaling laws provide an overview of the tradeoffs implied by a modeling decision, up to extrapolating how it might transfer to increasingly large models. We find that DFA fails to offer more efficient scaling than backpropagation: there is never a regime for which the degradation in loss incurred by using DFA is worth the potential reduction in compute budget. Our finding comes at variance with previous beliefs in the alternative training methods community, and highlights the need for holistic empirical approaches to better understand modeling decisions.",
            "corpus_id": 253116832,
            "sentences": [
                {
                    "corpus_id": "253116832",
                    "title": "Scaling Laws Beyond Backpropagation",
                    "text": "Can alternative training methods accelerate neural network training? Surveying the current state-of-the-art, one may find numerous claims of alternative training methods achieving competitive performance with BP across a variety of settings and tasks (e.g., [12,18,24]). \n\nWe seek to study this claim, with three restrictions in scope: \n\n1. We focus on Direct Feedback Alignment [25], due its simplicity and wide applicability [12], as well as its broad hardware prospects [14,26,27], and theoretical background [28]. \n\n2. We study compute-efficiency specifically (i.e, best performance achievable for a given compute budget), as this usually a significant bottleneck for scaling-up models. \n\n3. We conduct our study on \"GPT-like\" [29] causal decoder-only Transformers trained on English data. These models are known to possess smooth scaling laws [19,30]. Because of their unique abilities [31], they also command some of the largest training budgets in machine learning [32], making them a prime target for more compute-efficient training. \n\nThese restrictions lead us to test the following hypothesis: \n\nHypothesis. Direct Feedback Alignment can train causal decoder-only models more efficiently than backpropagation, achieving better performance for a given compute budget. \n\nScaling laws as a holistic empirical tool. Scaling laws have been proposed as an empirical approach to connect hyperparameters of neural networks (e.g., parameter count, training dataset size) to their performance. They have been derived both on specific downstream tasks [33,34] and on upstream modeling loss [19]. Scaling laws can characterize the influence of data & modeling decisions [21,22], or even unveil new, more optimal training practices [35,36]. \n\nAs illustrated in Figure 1, it is possible to derive a so-called compute optimal frontier for a class of models: this defines L(C), the best performance L achievable for a compute budget C. We fit a power-law L(C) = (C c C) \u03b1 C over the Pareto front of multiple runs, as proposed in [19]. C c is a constant offsetting the frontier, while \u03b1 C controls the slope.",
                    "score": 0.48905421553514145,
                    "section_title": "Framing",
                    "char_start_offset": 2719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 68
                        },
                        {
                            "start": 69,
                            "end": 270
                        },
                        {
                            "start": 273,
                            "end": 335
                        },
                        {
                            "start": 338,
                            "end": 517
                        },
                        {
                            "start": 520,
                            "end": 690
                        },
                        {
                            "start": 693,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1041
                        },
                        {
                            "start": 1044,
                            "end": 1104
                        },
                        {
                            "start": 1107,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1277
                        },
                        {
                            "start": 1280,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1738
                        },
                        {
                            "start": 1741,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2102
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 258,
                            "end": 262,
                            "matchedPaperCorpusId": "219981351"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "57189514"
                        },
                        {
                            "start": 379,
                            "end": 383,
                            "matchedPaperCorpusId": "2843914"
                        },
                        {
                            "start": 427,
                            "end": 431,
                            "matchedPaperCorpusId": "219981351"
                        },
                        {
                            "start": 477,
                            "end": 480,
                            "matchedPaperCorpusId": "221896106"
                        },
                        {
                            "start": 512,
                            "end": 516,
                            "matchedPaperCorpusId": "235390926"
                        },
                        {
                            "start": 731,
                            "end": 735,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 891,
                            "end": 895,
                            "matchedPaperCorpusId": "248118752"
                        },
                        {
                            "start": 1673,
                            "end": 1676,
                            "matchedPaperCorpusId": "246608156"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6318359375
                }
            ],
            "relevance_judgement": 0.6318359375,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws Beyond Backpropagation\n# Venue: arXiv.org\n# Authors: Matthew J. Filipovich, Alessandro Cappelli, Daniel Hesslow, Julien Launay\n## Abstract\nAlternatives to backpropagation have long been studied to better understand how biological brains may learn. Recently, they have also garnered interest as a way to train neural networks more efficiently. By relaxing constraints inherent to backpropagation (e.g., symmetric feedforward and feedback weights, sequential updates), these methods enable promising prospects, such as local learning. However, the tradeoffs between different methods in terms of final task performance, convergence speed, and ultimately compute and data requirements are rarely outlined. In this work, we use scaling laws to study the ability of Direct Feedback Alignment~(DFA) to train causal decoder-only Transformers efficiently. Scaling laws provide an overview of the tradeoffs implied by a modeling decision, up to extrapolating how it might transfer to increasingly large models. We find that DFA fails to offer more efficient scaling than backpropagation: there is never a regime for which the degradation in loss incurred by using DFA is worth the potential reduction in compute budget. Our finding comes at variance with previous beliefs in the alternative training methods community, and highlights the need for holistic empirical approaches to better understand modeling decisions.\n## Framing\nCan alternative training methods accelerate neural network training? Surveying the current state-of-the-art, one may find numerous claims of alternative training methods achieving competitive performance with BP across a variety of settings and tasks (e.g., [12,18,24]). \n\nWe seek to study this claim, with three restrictions in scope: \n\n1. We focus on Direct Feedback Alignment [25], due its simplicity and wide applicability [12], as well as its broad hardware prospects [14,26,27], and theoretical background [28]. \n\n2. We study compute-efficiency specifically (i.e, best performance achievable for a given compute budget), as this usually a significant bottleneck for scaling-up models. \n\n3. We conduct our study on \"GPT-like\" [29] causal decoder-only Transformers trained on English data. These models are known to possess smooth scaling laws [19,30]. Because of their unique abilities [31], they also command some of the largest training budgets in machine learning [32], making them a prime target for more compute-efficient training. \n\nThese restrictions lead us to test the following hypothesis: \n\nHypothesis. Direct Feedback Alignment can train causal decoder-only models more efficiently than backpropagation, achieving better performance for a given compute budget. \n\nScaling laws as a holistic empirical tool. Scaling laws have been proposed as an empirical approach to connect hyperparameters of neural networks (e.g., parameter count, training dataset size) to their performance. They have been derived both on specific downstream tasks [33,34] and on upstream modeling loss [19]. Scaling laws can characterize the influence of data & modeling decisions [21,22], or even unveil new, more optimal training practices [35,36]. \n\nAs illustrated in Figure 1, it is possible to derive a so-called compute optimal frontier for a class of models: this defines L(C), the best performance L achievable for a compute budget C. We fit a power-law L(C) = (C c C) \u03b1 C over the Pareto front of multiple runs, as proposed in [19]. C c is a constant offsetting the frontier, while \u03b1 C controls the slope.",
            "reference_string": "[253116832 | Filipovich et al. | 2022 | Citations: 3]"
        },
        {
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 55,
            "citation_count": 234,
            "influential_citation_count": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.09720",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3239480",
                    "name": "Tim Dettmers"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.",
            "corpus_id": 254853733,
            "sentences": [
                {
                    "corpus_id": "254853733",
                    "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
                    "text": "For example, there are methods associated with blocking and grouping (Park et al., 2022;Wu et al., 2020;Jain et al., 2020;Nagel et al., 2019;Krishnamoorthi, 2018;Rusci et al., 2020), centering (Krishnamoorthi, 2018;Jacob et al., 2017), learned data types that are found through clustering (Gong et al., 2014;Han et al., 2015;Choi et al., 2016;Park et al., 2017), or direct codebook optimization (Rastegari et al., 2016;Hou et al., 2016;Leng et al., 2018;Zhang et al., 2018). While our work studies grouping and blocking, we only study one data type that groups similar weights through their quantiles of the entire input tensor (Dettmers et al., 2022b). While we do not study learned data types in depth, we are the first work that shows that these are critical for improving bit-level scaling for LLMs. \n\nScaling Laws for Inference. Early work in scaling laws highlighted the importance of studying how variables change with scale since scale is one of the best predictors of model performance (Kaplan et al., 2020;Rosenfeld et al., 2019;Hestness et al., 2017). Particularly, for inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations.",
                    "score": 0.4238115678777957,
                    "section_title": "Related work",
                    "char_start_offset": 23558,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 803
                        },
                        {
                            "start": 806,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1213
                        },
                        {
                            "start": 1216,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1813
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 104,
                            "end": 122,
                            "matchedPaperCorpusId": "202542905"
                        },
                        {
                            "start": 122,
                            "end": 141,
                            "matchedPaperCorpusId": "184487878"
                        },
                        {
                            "start": 162,
                            "end": 181,
                            "matchedPaperCorpusId": "170078885"
                        },
                        {
                            "start": 343,
                            "end": 361,
                            "matchedPaperCorpusId": "29200607"
                        },
                        {
                            "start": 395,
                            "end": 419,
                            "matchedPaperCorpusId": "14925907"
                        },
                        {
                            "start": 436,
                            "end": 454,
                            "matchedPaperCorpusId": "8840788"
                        },
                        {
                            "start": 454,
                            "end": 473,
                            "matchedPaperCorpusId": "50784025"
                        },
                        {
                            "start": 628,
                            "end": 652,
                            "matchedPaperCorpusId": "238408308"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6298828125
                }
            ],
            "relevance_judgement": 0.6298828125,
            "relevance_judgment_input_expanded": "# Title: The case for 4-bit precision: k-bit Inference Scaling Laws\n# Venue: International Conference on Machine Learning\n# Authors: Tim Dettmers, Luke Zettlemoyer\n## Abstract\nQuantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.\n## Related work\nFor example, there are methods associated with blocking and grouping (Park et al., 2022;Wu et al., 2020;Jain et al., 2020;Nagel et al., 2019;Krishnamoorthi, 2018;Rusci et al., 2020), centering (Krishnamoorthi, 2018;Jacob et al., 2017), learned data types that are found through clustering (Gong et al., 2014;Han et al., 2015;Choi et al., 2016;Park et al., 2017), or direct codebook optimization (Rastegari et al., 2016;Hou et al., 2016;Leng et al., 2018;Zhang et al., 2018). While our work studies grouping and blocking, we only study one data type that groups similar weights through their quantiles of the entire input tensor (Dettmers et al., 2022b). While we do not study learned data types in depth, we are the first work that shows that these are critical for improving bit-level scaling for LLMs. \n\nScaling Laws for Inference. Early work in scaling laws highlighted the importance of studying how variables change with scale since scale is one of the best predictors of model performance (Kaplan et al., 2020;Rosenfeld et al., 2019;Hestness et al., 2017). Particularly, for inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations.",
            "reference_string": "[254853733 | Dettmers et al. | 2022 | Citations: 234]"
        },
        {
            "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281963681",
                    "name": "Wenxuan Yang"
                },
                {
                    "authorId": "2357521923",
                    "name": "Qingqu Wei"
                },
                {
                    "authorId": "49679098",
                    "name": "Chenxi Ma"
                },
                {
                    "authorId": "2274926347",
                    "name": "Weimin Tan"
                },
                {
                    "authorId": "2118138243",
                    "name": "Bo Yan"
                }
            ],
            "abstract": "Current scaling laws for visual AI models focus predominantly on large-scale pretraining, leaving a critical gap in understanding how performance scales for data-constrained downstream tasks. To address this limitation, this paper establishes the first practical framework for data-efficient scaling laws in visual transfer learning, addressing two fundamental questions: 1) How do scaling behaviors shift when downstream tasks operate with limited data? 2) What governs the efficacy of knowledge distillation under such constraints? Through systematic analysis of vision tasks across data regimes (1K-1M samples), we propose the distillation boundary theory, revealing a critical turning point in distillation efficiency: 1) Distillation superiority: In data-scarce conditions, distilled models significantly outperform their non-distillation counterparts, efficiently leveraging inherited knowledge to compensate for limited training samples. 2) Pre-training dominance: As pre-training data increases beyond a critical threshold, non-distilled models gradually surpass distilled versions, suggesting diminishing returns from knowledge inheritance when sufficient task-specific data becomes available. Empirical validation across various model scales (2.5M to 38M parameters) and data volumes demonstrate these performance inflection points, with error difference curves transitioning from positive to negative values at critical data thresholds, confirming our theoretical predictions. This work redefines scaling laws for data-limited regimes, bridging the knowledge gap between large-scale pretraining and practical downstream adaptation, addressing a critical barrier to understanding vision model scaling behaviors and optimizing computational resource allocation.",
            "corpus_id": 277940458,
            "sentences": [
                {
                    "corpus_id": "277940458",
                    "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
                    "text": "Previous studies have primarily focused on exploring the relationship between large models and large datasets within upstream tasks without considering their applicability to downstream tasks. Recently, however, several works have investigated scaling laws in the context of downstream applications. \n\nHernandez et al. [16] analyzes scaling laws in both pretraining and fine-tuning, introducing the concept of effective data transferred, which quantifies the efficiency of pretraining data in downstream performance. Additionally, Geiping and Goldstein [11] examines the downstream capabilities of a Transformer-based language model trained entirely from scratch within a single day on a single GPU, leveraging scaling laws as a framework for evaluation. Furthermore, [2,31,45] have identified that in compute-optimal training, model size and the number of training tokens should be scaled proportionally. More recently, studies such as [27,41,44] have aimed to enhance downstream task performance by expanding the size of training datasets. \n\n3 Scaling Laws for Transfer Learning 3.1 Data and Model Size Scaling Methodology Data size variation. To systematically study the impact of data scale on model performance, we employ uniform sampling methods for both upstream and downstream datasets. While maintaining the same number of categories, we sample 5%, 10%, 25%, 33%, 50%, 70%, and 100% of the examples from both pre-training and finetuning datasets. This granular sampling approach allows us to precisely capture the relationships between data volume and model performance at different scales. By systematically varying both upstream and downstream data quantities, we can comprehensively fit the scaling laws specifically tailored for downstream tasks. This methodology enables us to quantify how pre-training data efficiency translates to downstream task performance and identify optimal data allocation strategies. \n\nModel size variation. We maintain a fixed size of 64 for each attention head in the Deit [40] model, and vary the number of heads to adjust the overall model size. Specifically, we implement configurations with 2, 4, 6, and 8 attention heads. This approach is based on the finding from [20] that compared to the total number of parameters, the specific shape of the model has negligible influence on the performance of large transformers. This methodology allows us to systematically investigate how model capacity affects performance across different data regimes.",
                    "score": 0.4958334446999357,
                    "section_title": "Scaling Laws for Downstream Tasks",
                    "char_start_offset": 9028,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 299
                        },
                        {
                            "start": 302,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 754
                        },
                        {
                            "start": 755,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1041
                        },
                        {
                            "start": 1044,
                            "end": 1145
                        },
                        {
                            "start": 1146,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1923
                        },
                        {
                            "start": 1926,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2168
                        },
                        {
                            "start": 2169,
                            "end": 2364
                        },
                        {
                            "start": 2365,
                            "end": 2491
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 553,
                            "end": 557,
                            "matchedPaperCorpusId": "255185900"
                        },
                        {
                            "start": 768,
                            "end": 771,
                            "matchedPaperCorpusId": "255570036"
                        },
                        {
                            "start": 941,
                            "end": 944,
                            "matchedPaperCorpusId": "259145047"
                        },
                        {
                            "start": 944,
                            "end": 947,
                            "matchedPaperCorpusId": "267334725"
                        },
                        {
                            "start": 2015,
                            "end": 2019,
                            "matchedPaperCorpusId": "229363322"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62451171875
                }
            ],
            "relevance_judgement": 0.62451171875,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Data-Efficient Visual Transfer Learning\n# Venue: arXiv.org\n# Authors: Wenxuan Yang, Qingqu Wei, Chenxi Ma, Weimin Tan, Bo Yan\n## Abstract\nCurrent scaling laws for visual AI models focus predominantly on large-scale pretraining, leaving a critical gap in understanding how performance scales for data-constrained downstream tasks. To address this limitation, this paper establishes the first practical framework for data-efficient scaling laws in visual transfer learning, addressing two fundamental questions: 1) How do scaling behaviors shift when downstream tasks operate with limited data? 2) What governs the efficacy of knowledge distillation under such constraints? Through systematic analysis of vision tasks across data regimes (1K-1M samples), we propose the distillation boundary theory, revealing a critical turning point in distillation efficiency: 1) Distillation superiority: In data-scarce conditions, distilled models significantly outperform their non-distillation counterparts, efficiently leveraging inherited knowledge to compensate for limited training samples. 2) Pre-training dominance: As pre-training data increases beyond a critical threshold, non-distilled models gradually surpass distilled versions, suggesting diminishing returns from knowledge inheritance when sufficient task-specific data becomes available. Empirical validation across various model scales (2.5M to 38M parameters) and data volumes demonstrate these performance inflection points, with error difference curves transitioning from positive to negative values at critical data thresholds, confirming our theoretical predictions. This work redefines scaling laws for data-limited regimes, bridging the knowledge gap between large-scale pretraining and practical downstream adaptation, addressing a critical barrier to understanding vision model scaling behaviors and optimizing computational resource allocation.\n## Scaling Laws for Downstream Tasks\nPrevious studies have primarily focused on exploring the relationship between large models and large datasets within upstream tasks without considering their applicability to downstream tasks. Recently, however, several works have investigated scaling laws in the context of downstream applications. \n\nHernandez et al. [16] analyzes scaling laws in both pretraining and fine-tuning, introducing the concept of effective data transferred, which quantifies the efficiency of pretraining data in downstream performance. Additionally, Geiping and Goldstein [11] examines the downstream capabilities of a Transformer-based language model trained entirely from scratch within a single day on a single GPU, leveraging scaling laws as a framework for evaluation. Furthermore, [2,31,45] have identified that in compute-optimal training, model size and the number of training tokens should be scaled proportionally. More recently, studies such as [27,41,44] have aimed to enhance downstream task performance by expanding the size of training datasets. \n\n3 Scaling Laws for Transfer Learning 3.1 Data and Model Size Scaling Methodology Data size variation. To systematically study the impact of data scale on model performance, we employ uniform sampling methods for both upstream and downstream datasets. While maintaining the same number of categories, we sample 5%, 10%, 25%, 33%, 50%, 70%, and 100% of the examples from both pre-training and finetuning datasets. This granular sampling approach allows us to precisely capture the relationships between data volume and model performance at different scales. By systematically varying both upstream and downstream data quantities, we can comprehensively fit the scaling laws specifically tailored for downstream tasks. This methodology enables us to quantify how pre-training data efficiency translates to downstream task performance and identify optimal data allocation strategies. \n\nModel size variation. We maintain a fixed size of 64 for each attention head in the Deit [40] model, and vary the number of heads to adjust the overall model size. Specifically, we implement configurations with 2, 4, 6, and 8 attention heads. This approach is based on the finding from [20] that compared to the total number of parameters, the specific shape of the model has negligible influence on the performance of large transformers. This methodology allows us to systematically investigate how model capacity affects performance across different data regimes.",
            "reference_string": "[277940458 | Yang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.02460",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.02460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2054842299",
                    "name": "Feiyang Kang"
                },
                {
                    "authorId": "1412591482",
                    "name": "H. Just"
                },
                {
                    "authorId": "2894821",
                    "name": "Anit Kumar Sahu"
                },
                {
                    "authorId": "39823639",
                    "name": "R. Jia"
                }
            ],
            "abstract": "Traditionally, data selection has been studied in settings where all samples from prospective sources are fully revealed to a machine learning developer. However, in practical data exchange scenarios, data providers often reveal only a limited subset of samples before an acquisition decision is made. Recently, there have been efforts to fit scaling laws that predict model performance at any size and data source composition using the limited available samples. However, these scaling functions are black-box, computationally expensive to fit, highly susceptible to overfitting, or/and difficult to optimize for data selection. This paper proposes a framework called, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources. Our approach distinguishes itself from existing work by introducing a novel *two-stage* performance inference process. In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes. In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws. We further derive an efficient gradient-based method to select data sources based on the projected model performance. Evaluation over a diverse range of applications demonstrates thatsignificantly improves existing performance scaling approaches in terms of both the accuracy of performance inference and the computation costs associated with constructing the performance predictor. Also,outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.",
            "corpus_id": 259341858,
            "sentences": [
                {
                    "corpus_id": "259341858",
                    "title": "Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources",
                    "text": "Scaling laws, predicting how the model performance changes with the scale of training data, model parameters, and computation budget [26], have seen increasingly successful in a variety of tasks pertaining to vision and text processing [27]. The performance of machine-learning models generally adheres to a power law relationship with the scale of these variables, which allows for predicting the model performance on larger scales with high precision [26] and provides a viable approach to predicting the potential usefulness of target data from only a small proportion of the set. [14] shows that data from different distributions generally scale at different rates. Our work provides a novel approach that materializes this dependency of scaling relationships with data distributions and achieves remarkable empirical results.",
                    "score": 0.4752043857732407,
                    "section_title": "Related Work",
                    "char_start_offset": 8630,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 830
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 236,
                            "end": 240,
                            "matchedPaperCorpusId": "252220884"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.623046875
                }
            ],
            "relevance_judgement": 0.623046875,
            "relevance_judgment_input_expanded": "# Title: Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources\n# Venue: Neural Information Processing Systems\n# Authors: Feiyang Kang, H. Just, Anit Kumar Sahu, R. Jia\n## Abstract\nTraditionally, data selection has been studied in settings where all samples from prospective sources are fully revealed to a machine learning developer. However, in practical data exchange scenarios, data providers often reveal only a limited subset of samples before an acquisition decision is made. Recently, there have been efforts to fit scaling laws that predict model performance at any size and data source composition using the limited available samples. However, these scaling functions are black-box, computationally expensive to fit, highly susceptible to overfitting, or/and difficult to optimize for data selection. This paper proposes a framework called, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources. Our approach distinguishes itself from existing work by introducing a novel *two-stage* performance inference process. In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes. In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws. We further derive an efficient gradient-based method to select data sources based on the projected model performance. Evaluation over a diverse range of applications demonstrates thatsignificantly improves existing performance scaling approaches in terms of both the accuracy of performance inference and the computation costs associated with constructing the performance predictor. Also,outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.\n## Related Work\nScaling laws, predicting how the model performance changes with the scale of training data, model parameters, and computation budget [26], have seen increasingly successful in a variety of tasks pertaining to vision and text processing [27]. The performance of machine-learning models generally adheres to a power law relationship with the scale of these variables, which allows for predicting the model performance on larger scales with high precision [26] and provides a viable approach to predicting the potential usefulness of target data from only a small proportion of the set. [14] shows that data from different distributions generally scale at different rates. Our work provides a novel approach that materializes this dependency of scaling relationships with data distributions and achieves remarkable empirical results.",
            "reference_string": "[259341858 | Kang et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Are Bigger Encoders Always Better in Vision Large Models?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303441380",
                    "name": "Bozhou Li"
                },
                {
                    "authorId": "2303856806",
                    "name": "Hao Liang"
                },
                {
                    "authorId": "2314372736",
                    "name": "Zimo Meng"
                },
                {
                    "authorId": "2309265357",
                    "name": "Wentao Zhang"
                }
            ],
            "abstract": "In recent years, multimodal large language models (MLLMs) have shown strong potential in real-world applications. They are developing rapidly due to their remarkable ability to comprehend multimodal information and their inherent powerful cognitive and reasoning capabilities. Among MLLMs, vision language models (VLM) stand out for their ability to understand vision information. However, the scaling trend of VLMs under the current mainstream paradigm has not been extensively studied. Whether we can achieve better performance by training even larger models is still unclear. To address this issue, we conducted experiments on the pretraining stage of MLLMs. We conduct our experiment using different encoder sizes and large language model (LLM) sizes. Our findings indicate that merely increasing the size of encoders does not necessarily enhance the performance of VLMs. Moreover, we analyzed the effects of LLM backbone parameter size and data quality on the pretraining outcomes. Additionally, we explored the differences in scaling laws between LLMs and VLMs.",
            "corpus_id": 271600832,
            "sentences": [
                {
                    "corpus_id": "271600832",
                    "title": "Are Bigger Encoders Always Better in Vision Large Models?",
                    "text": "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary. \n\nAfter the initial scaling law was proposed, researchers studied the scaling law in different scenarios, such as graph neural networks Liu et al. [2024c], data mixing Ye et al. [2024], data pruning Sorscher et al. [2022], and fine-tuning of large language models Zhang et al. [2024b]. Some of them made certain adjustments to the form of the formula. \n\nWhen it comes to MLLMs, Aghajanyan et al. Aghajanyan et al. [2023] explored the relationship between the loss and the dataset size and the parameter size over seven modalities, including text, image, image-text, speech, speech-text, code, and molecules. They also examined the scaling law when models are pre-trained on two modalities simultaneously and modified the form of the formula. However, the training paradigm they used in their study differs from the connected vision paradigm, the mainstream methods preferred today. They trained LLMs and modality encoders from scratch, similar to the approach chosen by BEiT-3 Wang et al. [2022], to obtain their scaling law. This discrepancy has sparked our research focus. To the best of our knowledge, the scaling law of multimodal large models under the connected vision paradigm is still under-researched. \n\n3 Methods",
                    "score": 0.4389571555724937,
                    "section_title": "Scaling Law",
                    "char_start_offset": 8261,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1424
                        },
                        {
                            "start": 1427,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2283
                        },
                        {
                            "start": 2286,
                            "end": 2295
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1272,
                            "end": 1294,
                            "matchedPaperCorpusId": "250113273"
                        },
                        {
                            "start": 1469,
                            "end": 1493,
                            "matchedPaperCorpusId": "255570036"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61865234375
                }
            ],
            "relevance_judgement": 0.61865234375,
            "relevance_judgment_input_expanded": "# Title: Are Bigger Encoders Always Better in Vision Large Models?\n# Venue: arXiv.org\n# Authors: Bozhou Li, Hao Liang, Zimo Meng, Wentao Zhang\n## Abstract\nIn recent years, multimodal large language models (MLLMs) have shown strong potential in real-world applications. They are developing rapidly due to their remarkable ability to comprehend multimodal information and their inherent powerful cognitive and reasoning capabilities. Among MLLMs, vision language models (VLM) stand out for their ability to understand vision information. However, the scaling trend of VLMs under the current mainstream paradigm has not been extensively studied. Whether we can achieve better performance by training even larger models is still unclear. To address this issue, we conducted experiments on the pretraining stage of MLLMs. We conduct our experiment using different encoder sizes and large language model (LLM) sizes. Our findings indicate that merely increasing the size of encoders does not necessarily enhance the performance of VLMs. Moreover, we analyzed the effects of LLM backbone parameter size and data quality on the pretraining outcomes. Additionally, we explored the differences in scaling laws between LLMs and VLMs.\n## Scaling Law\nIn the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary. \n\nAfter the initial scaling law was proposed, researchers studied the scaling law in different scenarios, such as graph neural networks Liu et al. [2024c], data mixing Ye et al. [2024], data pruning Sorscher et al. [2022], and fine-tuning of large language models Zhang et al. [2024b]. Some of them made certain adjustments to the form of the formula. \n\nWhen it comes to MLLMs, Aghajanyan et al. Aghajanyan et al. [2023] explored the relationship between the loss and the dataset size and the parameter size over seven modalities, including text, image, image-text, speech, speech-text, code, and molecules. They also examined the scaling law when models are pre-trained on two modalities simultaneously and modified the form of the formula. However, the training paradigm they used in their study differs from the connected vision paradigm, the mainstream methods preferred today. They trained LLMs and modality encoders from scratch, similar to the approach chosen by BEiT-3 Wang et al. [2022], to obtain their scaling law. This discrepancy has sparked our research focus. To the best of our knowledge, the scaling law of multimodal large models under the connected vision paradigm is still under-researched. \n\n3 Methods",
            "reference_string": "[271600832 | Li et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Scaling Law with Learning Rate Annealing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.11029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316486693",
                    "name": "Howe Tissue"
                },
                {
                    "authorId": "2316485233",
                    "name": "Venus Wang"
                },
                {
                    "authorId": "2316501479",
                    "name": "Lu Wang"
                }
            ],
            "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step across any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness for training dynamics. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures, and our equation can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical verification and explanation for empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while greatly democratizing scaling laws, and it can guide researchers in refining training strategies (e.g. critical LRS) for further LLMs.",
            "corpus_id": 271909320,
            "sentences": [
                {
                    "corpus_id": "271909320",
                    "title": "Scaling Law with Learning Rate Annealing",
                    "text": "2.1 SCALING LAWS Cross-entropy loss of language models on the validation set is a reliable indicator of LLMs' performance on downstream tasks (Caballero et al., 2022;Du et al., 2024). Kaplan et al. (2020) empirically discovered a power-law relationship between validation loss L and three factors: model size N , dataset size D, and training compute. As an application of scaling law, Hoffmann et al. (2022) developed Chinchilla, a compute-optimal LLM, by balancing model size and dataset size. They used a simplified and intuitive scaling law equation: \n\nwhere L 0 , A, B, \u03b1, \u03b2 are positive constants. Traditional scaling law formulations fit only the loss at the final training step, while ignoring losses from other steps. Note that collecting a new loss value of data size requires launching a another training run with the same LRS, which is resourceintensive. Previous works have conducted some preliminary studies on the impact of the learning rate on the scaling laws. For example, OpenAI and chinchilla scaling laws both report that the choice of learning rate schedule does not influence the power-law format (Kaplan et al., 2020;Hoffmann et al., 2022). Also, OpenAI's experiments suggest that the specific choice of learning rate schedule has minimal impact on the final validation loss, provided that the total summed learning rate is adequately large and the schedule incorporates both a warmup stage and a final annealing stage, reducing the learning rate to nearly zero at the end of training (Kaplan et al., 2020).",
                    "score": 0.4607593877372448,
                    "section_title": "PRELIMINARY",
                    "char_start_offset": 5712,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 553
                        },
                        {
                            "start": 556,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1530
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61083984375
                }
            ],
            "relevance_judgement": 0.61083984375,
            "relevance_judgment_input_expanded": "# Title: Scaling Law with Learning Rate Annealing\n# Venue: arXiv.org\n# Authors: Howe Tissue, Venus Wang, Lu Wang\n## Abstract\nWe find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step across any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness for training dynamics. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures, and our equation can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical verification and explanation for empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while greatly democratizing scaling laws, and it can guide researchers in refining training strategies (e.g. critical LRS) for further LLMs.\n## PRELIMINARY\n2.1 SCALING LAWS Cross-entropy loss of language models on the validation set is a reliable indicator of LLMs' performance on downstream tasks (Caballero et al., 2022;Du et al., 2024). Kaplan et al. (2020) empirically discovered a power-law relationship between validation loss L and three factors: model size N , dataset size D, and training compute. As an application of scaling law, Hoffmann et al. (2022) developed Chinchilla, a compute-optimal LLM, by balancing model size and dataset size. They used a simplified and intuitive scaling law equation: \n\nwhere L 0 , A, B, \u03b1, \u03b2 are positive constants. Traditional scaling law formulations fit only the loss at the final training step, while ignoring losses from other steps. Note that collecting a new loss value of data size requires launching a another training run with the same LRS, which is resourceintensive. Previous works have conducted some preliminary studies on the impact of the learning rate on the scaling laws. For example, OpenAI and chinchilla scaling laws both report that the choice of learning rate schedule does not influence the power-law format (Kaplan et al., 2020;Hoffmann et al., 2022). Also, OpenAI's experiments suggest that the specific choice of learning rate schedule has minimal impact on the final validation loss, provided that the total summed learning rate is adequately large and the schedule incorporates both a warmup stage and a final annealing stage, reducing the learning rate to nearly zero at the end of training (Kaplan et al., 2020).",
            "reference_string": "[271909320 | Tissue et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Scaling Laws for Linear Complexity Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116517206",
                    "name": "Xuyang Shen"
                },
                {
                    "authorId": "2179703418",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2308039623",
                    "name": "Ruitao Leng"
                },
                {
                    "authorId": "2171650015",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2225238340",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "2266275708",
                    "name": "Yiran Zhong"
                }
            ],
            "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for comparison with softmax attention. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.",
            "corpus_id": 270703481,
            "sentences": [
                {
                    "corpus_id": "270703481",
                    "title": "Scaling Laws for Linear Complexity Language Models",
                    "text": "Scaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
                    "score": 0.4353487993929209,
                    "section_title": "Related work",
                    "char_start_offset": 22035,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 286
                        },
                        {
                            "start": 286,
                            "end": 497
                        },
                        {
                            "start": 497,
                            "end": 623
                        },
                        {
                            "start": 623,
                            "end": 918
                        },
                        {
                            "start": 918,
                            "end": 1039
                        },
                        {
                            "start": 1039,
                            "end": 1214
                        },
                        {
                            "start": 1214,
                            "end": 1388
                        },
                        {
                            "start": 1388,
                            "end": 1524
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.609375
                }
            ],
            "relevance_judgement": 0.609375,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Linear Complexity Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong\n## Abstract\nThe interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for comparison with softmax attention. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.\n## Related work\nScaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
            "reference_string": "[270703481 | Shen et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 90,
            "citation_count": 819,
            "influential_citation_count": 98,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.07143",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40063601",
                    "name": "Mehdi Cherti"
                },
                {
                    "authorId": "2125377840",
                    "name": "R. Beaumont"
                },
                {
                    "authorId": "2113839396",
                    "name": "Ross Wightman"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "1387994137",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "2007745319",
                    "name": "Cade Gordon"
                },
                {
                    "authorId": "2137341362",
                    "name": "Christoph Schuhmann"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                }
            ],
            "abstract": "Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https://github.eom/LAION-AI/sealing-laws-openelip.",
            "corpus_id": 254636568,
            "sentences": [
                {
                    "corpus_id": "254636568",
                    "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
                    "text": "Scaling laws for generalization and transfer. Strong empirical evidence that increasing model or data scale is beneficial was initially studied in the context of deep learning and computer vision [69,26]. For instance, in [26], the power law relation between scale and model performance was highlighted. Empirical work stimulated theoretical studies that provided justification for the observed generalization boost with scale, investigating generalization error in overparameterized networks in the interpolation regime [6,9]. \n\nEarly empirical studies focused on the effect of training scale on upstream performance, measuring the test loss from the same distribution used for training. Subsequent studies of large language models such as GPT-3 [8] demonstrated broad generalization capabilities in models with substantially larger scale. Moreover, neural scaling laws of the power law form were derived for language models, connecting model, data, and training compute scale to performance [35,72,28]. This also allowed accurate prediction of model performance at larger scales, and researchers were able to determine the scale parameters for achieving optimal performance given a fixed amount of compute [28,39]. Scaling law studies were then also studied in the vision domain [61,82], also observing a power law dependency of performance on scale. \n\nScaling law studies were also conducted for transfer and out-of-distribution performance [35,72,82]. In these studies, researchers observed that performance on downstream tasks benefits from increasing model, data, and training compute scale [38,8,35,82]. Interestingly, upstream performance does not always correlate with downstream performance [72,71]. Since downstream performance most accurately reflects a practical use cases, examining scaling behavior on downstream tasks is increasingly important. Recent work has also studied the effect of scale on other model characteristics, such as performance after pruning and compression [63,11] and on susceptibility to catastrophic forgetting [58]. \n\nScaling up language-vision learning. Learning from very large amounts of weakly aligned imagetext pairs has led to the development of models with broad generalization capabilities. Notably, work on contrastive language-image pre-training (CLIP [55]) showed dramatic improvement compared to the previous state-of-the-art in zero-shot transfer and unprecendented robustness to distribution shift [70,48,51,18].",
                    "score": 0.4439982908604859,
                    "section_title": "Background and related work",
                    "char_start_offset": 6128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 46,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 527
                        },
                        {
                            "start": 530,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1352
                        },
                        {
                            "start": 1355,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1610
                        },
                        {
                            "start": 1611,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 2054
                        },
                        {
                            "start": 2057,
                            "end": 2093
                        },
                        {
                            "start": 2094,
                            "end": 2237
                        },
                        {
                            "start": 2238,
                            "end": 2465
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 196,
                            "end": 200,
                            "matchedPaperCorpusId": "6842201"
                        },
                        {
                            "start": 524,
                            "end": 526,
                            "matchedPaperCorpusId": "235212537"
                        },
                        {
                            "start": 747,
                            "end": 750,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 997,
                            "end": 1000,
                            "matchedPaperCorpusId": "260498358"
                        },
                        {
                            "start": 1281,
                            "end": 1285,
                            "matchedPaperCorpusId": "235417196"
                        },
                        {
                            "start": 1285,
                            "end": 1288,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1448,
                            "end": 1451,
                            "matchedPaperCorpusId": "260498358"
                        },
                        {
                            "start": 1451,
                            "end": 1454,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1597,
                            "end": 1601,
                            "matchedPaperCorpusId": "214728308"
                        },
                        {
                            "start": 1601,
                            "end": 1603,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1606,
                            "end": 1609,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1701,
                            "end": 1705,
                            "matchedPaperCorpusId": "260498358"
                        },
                        {
                            "start": 1992,
                            "end": 1996,
                            "matchedPaperCorpusId": "219792934"
                        },
                        {
                            "start": 1996,
                            "end": 1999,
                            "matchedPaperCorpusId": "229152261"
                        },
                        {
                            "start": 2049,
                            "end": 2053,
                            "matchedPaperCorpusId": "251648120"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60791015625
                }
            ],
            "relevance_judgement": 0.60791015625,
            "relevance_judgment_input_expanded": "# Title: Reproducible Scaling Laws for Contrastive Language-Image Learning\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Mehdi Cherti, R. Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, J. Jitsev\n## Abstract\nScaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https://github.eom/LAION-AI/sealing-laws-openelip.\n## Background and related work\nScaling laws for generalization and transfer. Strong empirical evidence that increasing model or data scale is beneficial was initially studied in the context of deep learning and computer vision [69,26]. For instance, in [26], the power law relation between scale and model performance was highlighted. Empirical work stimulated theoretical studies that provided justification for the observed generalization boost with scale, investigating generalization error in overparameterized networks in the interpolation regime [6,9]. \n\nEarly empirical studies focused on the effect of training scale on upstream performance, measuring the test loss from the same distribution used for training. Subsequent studies of large language models such as GPT-3 [8] demonstrated broad generalization capabilities in models with substantially larger scale. Moreover, neural scaling laws of the power law form were derived for language models, connecting model, data, and training compute scale to performance [35,72,28]. This also allowed accurate prediction of model performance at larger scales, and researchers were able to determine the scale parameters for achieving optimal performance given a fixed amount of compute [28,39]. Scaling law studies were then also studied in the vision domain [61,82], also observing a power law dependency of performance on scale. \n\nScaling law studies were also conducted for transfer and out-of-distribution performance [35,72,82]. In these studies, researchers observed that performance on downstream tasks benefits from increasing model, data, and training compute scale [38,8,35,82]. Interestingly, upstream performance does not always correlate with downstream performance [72,71]. Since downstream performance most accurately reflects a practical use cases, examining scaling behavior on downstream tasks is increasingly important. Recent work has also studied the effect of scale on other model characteristics, such as performance after pruning and compression [63,11] and on susceptibility to catastrophic forgetting [58]. \n\nScaling up language-vision learning. Learning from very large amounts of weakly aligned imagetext pairs has led to the development of models with broad generalization capabilities. Notably, work on contrastive language-image pre-training (CLIP [55]) showed dramatic improvement compared to the previous state-of-the-art in zero-shot transfer and unprecendented robustness to distribution shift [70,48,51,18].",
            "reference_string": "[254636568 | Cherti et al. | 2022 | Citations: 819]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "This work builds on a large number of recent studies relating scaling law estimation and decisionmaking about model training. Among the aspects studied are total training costs including inference (Sardana et al.), effects of sophisticated data selection (Sorscher et al., 2022;Ge et al., 2024), training time (Inbar & Sernau, 2024), transfer of learned skills (Hernandez et al., 2021), behavior of models in other modalities (Mikami et al., 2022;Abnar et al.;Alabdulmohsin et al., 2024;Hesslow et al., 2022) mixtures of experts (Ludziejewski et al.), data mixing (Ge et al., 2024), downstream performance (Muennighoff et al., 2024), vocabulary size (Tao et al., 2024), and architecture comparisons (Tay et al., 2023;Poli et al., 2024) including small models (Muckatira et al., 2024) or other phenomena like finetuning (Zhang et al.) and the loss in different positions in the training sequences (Xiong et al., 2024). Especially relevant to our context is Ruan et al. (2024) that rely on multiple pretraining settings for creating scaling laws that generalize across models or kinds of losses. \n\nAnother line of works that can be seen as a scaling law discusses the relation between model width and hyperparameter choices (rather than loss) (Yang et al., 2022;2021;Blake et al., 2024;Lingle, 2024).",
            "score": 0.6344895273593659,
            "section_title": "RELATED WORK",
            "char_start_offset": 27172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1298
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 278,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 426,
                    "end": 447,
                    "matchedPaperCorpusId": "238418966"
                },
                {
                    "start": 447,
                    "end": 460,
                    "matchedPaperCorpusId": "238354065"
                },
                {
                    "start": 460,
                    "end": 487,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 606,
                    "end": 632,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 699,
                    "end": 717,
                    "matchedPaperCorpusId": "250920512"
                },
                {
                    "start": 759,
                    "end": 783,
                    "matchedPaperCorpusId": "268876447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Scaling laws have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how * Equal contribution the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization. \n\nThe study of neural scaling laws gained prominence with the foundational work of Kaplan et al. (2020), who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, Hoffmann et al. (2022) refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies (Muennighoff et al., 2023;Caballero et al., 2023;Krajewski et al., 2024) have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrievalaugmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies. \n\nDespite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored.",
            "score": 0.6289428220497614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "While neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility. \n\nInference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications. \n\nCompute-optimal model selection: Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multiobjective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development. \n\nEfficient data scaling and pruning: The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.",
            "score": 0.625186149794007,
            "section_title": "Future recommendations",
            "char_start_offset": 25490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "However, these works have not explored the scaling law of parallel computation from the perspective of model capacity. As we discuss in Appendix A, we find that the specific differentiation technique had a minimal impact, and the key factor is the scaling in parallel computation. \n\nScaling Laws for Language Models Many researchers explore the predictable relationships between LLM training performance and various factors under different settings, such as the number of parameters and data (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022;DeepSeek-AI, 2024;Frantar et al., 2024), data repetition cycles (Muennighoff et al., 2023;Hernandez et al., 2022), data mixing (Ye et al., 2025;Que et al., 2024), and fine-tuning (Zhang et al., 2024a). By extending the predictive empirical scaling laws developed from smaller models to larger models, we can significantly reduce exploration costs. \n\nRecently, some studies have investigated the scaling effects during inference (Sardana et al., 2024), noting a log-linear relationship between sampling number and performance (Brown et al., 2024;Snell et al., 2025). But they are limited to certain application scenarios. Our work extends the Chinchilla scaling law (Hoffmann et al., 2022) by introducing the intrinsic quantitative relationship between parallel scaling and parameter scaling. Existing literature has also identified a power-law relationship between the number of ensembles and loss in model ensemble scaling laws (Lobacheva et al., 2020a), which can be considered a special case of Proposition 1 when \u03c1 = 0.",
            "score": 0.5997266137637469,
            "section_title": "Inference-Time Scaling",
            "char_start_offset": 31191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1580
                }
            ],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 647,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 684,
                    "end": 701,
                    "matchedPaperCorpusId": "268681464"
                },
                {
                    "start": 736,
                    "end": 757,
                    "matchedPaperCorpusId": "268032247"
                },
                {
                    "start": 985,
                    "end": 1007,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1102,
                    "end": 1121,
                    "matchedPaperCorpusId": "278498044"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425048828125
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
            "score": 0.5784099676137021,
            "section_title": "Discussion and Future Work",
            "char_start_offset": 32802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 605,
                    "end": 622,
                    "matchedPaperCorpusId": "271601023"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "278498044"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "270123298",
            "title": "Neural scaling laws from large-N field theory: solvable model beyond the ridgeless limit",
            "text": "The transformative power of modern deep learning hinges upon large models and big data. For example, large language models like GPT-4, which can write and reason like humans do, have trillions of parameters and were trained on trillions of tokens. From the physics point of view, we are talking about systems with a large number of degrees of freedom. Complicated as it sounds, it is perhaps not surprising that certain aspects of the physics simplify in this limit. In particular, it has been observed that many machine learning (ML) models exhibit scaling laws [1][2][3][4][5][6][7][8][9][10][11][12][13], i.e. the test loss L of a fully-trained model scales as power law with the number of parameters N or the number of training samples T when the other quantity is held fixed at a larger value 1 : \n\nUnderstanding the mechanisms of such neural scaling laws will have far-reaching scientific, technological and societal impacts as ML models are deployed to perform more critical tasks. This is because training large models on big data is expensive. Knowing when and how scaling laws arise and being able to calculate the scaling exponents \u03b1 N , \u03b1 T from the underlying task would allow us to make our ML models predictably better by scaling up N and T. See e.g. [15][16][17][18][19][20][21][22] for recent attempts to explain neural scaling laws. \n\nScaling laws are ubiquitous in physics. In the present case, they were observed in systems that are probably too complex for a microscopic treatment. Nevertheless, we may hope to extract the essential properties of the models and data sets that lead to neural scaling laws, and design simple solvable models that exhibit the same properties. In other words, we may hope to find an 'Ising model' for neural scaling laws. \n\nThis task was recently undertaken in [19], where the authors identified a power-law spectrum of the feature representation as the crucial ingredient that eventually leads to power-law scaling of the test loss.",
            "score": 0.5744652182323824,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337158203125
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "A neural network's performance generally increases as more compute is allocated for training.When scaling compute, one must decide whether to increase a model's parameter count or increase the dataset's size-these must trade-off within a fixed compute budget.Scaling laws can tell us what specific allocation (i.e.parameters v.s.data) will maximize performance given a compute budget.Much work has explored scaling laws for neural LM's [Kaplan et al., 2020], generally concluding that parameter & training token count should be scaled 1-to-1 [Hoffmann et al., 2022].\n\nHowever, most prior work on scaling laws for LM's have been estimated from transformers trained on scraped web text.Of course, this is quite a specific data distribution, so we may naturally ask whether the scaling laws extrapolated from such web text datasets generalize to other distributions.Furthermore, it is generally understood that the art of training data mixture is the 'secret sauce' that enables frontier industry labs to continually deliver state-of-the-art LLMs [Penedo et al., 2024, 2023, Xie et al., 2024].Considering that improving data quality can significantly raise LM performance [Gunasekar et al., 2023] and that scaling laws in reinforcement learning have been shown to scale with game difficulty [Jones, 2021], we may hypothesize that current LM scaling laws (e.g.Chinchilla [Hoffmann et al., 2022]) are individual web-text-specific cases of a more general scaling law conditioned on properties of the training data.\n\nThen, what properties of a training dataset of token sequences are neural scaling laws sensitive to?In other words, what can we measure about our data to more accurately predict the optimal compute allocation for training?Furthermore, is data-dependence of scaling laws only of theoretical interest, or can laws be considerably different for real-world datasets?\n\nIn order to study these questions, we seek a textual data setting where we can intuitively control its complexity as well as open avenues for information-theoretic understandings of why scaling laws are data-dependent.",
            "score": 0.5732668418316988,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 566
                },
                {
                    "start": 568,
                    "end": 684
                },
                {
                    "start": 684,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1508
                },
                {
                    "start": 1510,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1872
                },
                {
                    "start": 1874,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 1070,
                    "end": 1089,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1627197265625
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.",
            "score": 0.5717058694859517,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "Neural scaling laws The modern study of neural scaling laws began with Hestness et al. 2017, who observed a power law relationship between data size and error across image, language, and speech domains. Similarly, Rosenfeld et al. 2019 identified a power law relationship involving both model size and data. The work of Kaplan et al. 2020 andHoffmann et al. 2022 furthered this research in language modeling, emphasizing the implications for allocating compute budgets-a tradition we continue in this study. Hernandez et al. 2021 studied scaling laws for transfer in model size and fine-tuning data. Unlike their study, ours differs in three key ways. First, our study examines pre-training steps and fine-tuning data. Second, rather than studying the effective data transferred, our study directly investigates how test loss varies as a function of training inputs. These distinctions are crucial because they allow us to directly estimate the \"transfer gap\" between two distributions, enabling a more fine-grained analysis of the limits to pre-training. Finally, we examine transfer across multiple diverse datasets, not just from English to Python. \n\nAnother similar study is Mikami et al. 2021, which investigated how pre-training data affects transfer properties, similar to our approach. Unlike their study, ours involved training models to convergence on the fine-tuning data but not on the pre-training data, which we believe is a more realistic assumption in the large-data regime. \n\nTheir research also focused on image classification, whereas ours is focused on natural language processing. Moreover, we aim to measure the \"transfer gap\" between various distributions more directly and show how these gaps inform economic trade-offs when training large machine learning models, an aspect not addressed by Mikami et al. 2021. Nonetheless, we borrow the scaling law form from Mikami et al. 2021, which they postulated through theoretical analysis. For convenience and transparency, we detail how this scaling law can be derived in Appendix A. \n\nWhile our study focuses on transfer learning scaling laws between language datasets, Aghajanyan et al. 2023 investigated scaling laws for generative language models trained on multiple modalities. They modeled the individual contributions and interactions between modalities, identifying regimes of competition and synergy, where training on multiple modalities either negatively or positively affected performance, respectively.",
            "score": 0.5701096571801215,
            "section_title": "Related work",
            "char_start_offset": 5224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2483
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
            "score": 0.5694872285550104,
            "section_title": "Related work",
            "char_start_offset": 24085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 636,
                    "end": 639,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "269033049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "Scaling laws (Li et al., 2025) have become a cornerstone for deciphering the behavior of deep learning models across a wide array of domains and tasks. They provide critical insights into how performance correlates with key factors such as dataset size, model capacity, and computational resources. Early work by Banko and Brill (2001) laid the groundwork by identifying a power-law relationship between validation error and training dataset size in tasks like confusion set disambiguation. Their findings revealed that as the dataset grows, the average error decreases predictably, while the model size needed to effectively fit the data scales log-linearly. This seminal observation was later expanded by Amodei et al. (2016), who demonstrated power-law improvements in word error rate with increased training data for the 38M-parameter Deep Speech 2 model, and by Hestness et al. (2017), who extended these exponential trends to diverse fields such as machine translation, language modeling, image processing, and speech recognition. These studies collectively highlighted the robustness of scaling laws, showing that performance gains remain consistent even as models and architectures evolve. \n\nBuilding on this foundation, subsequent research has pushed the boundaries of scale while refining the implications of these laws. For instance, Kaplan et al. (2020) investigated models with up to 1.5B parameters trained on 23B tokens, deriving powerlaw relationships to optimize computational budget allocation. However, later critiques from Hoffmann et al. (2022) andHu et al. (2024c) pointed out an underestimation of required training data, underscoring subtle methodological challenges in scaling studies. Beyond sheer scale, researchers have delved into more nuanced phenomena: Wei et al. (2022) identified emergent abilities in large language models that are absent in smaller ones, while Hernandez et al. (2021) explored scaling laws in transfer learning and finetuning contexts. Architectural diversity has also come under scrutiny, with Tay et al. (2022) showing that not all model designs scale equally, advocating for scaling studies to inform architecture development. The adaptability of scaling laws to emerging paradigms is evident in recent innovations.",
            "score": 0.5694697026327974,
            "section_title": "C.3 Scaling Law",
            "char_start_offset": 34708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that -- all else equal -- estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families.",
            "score": 0.5684164148815222,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "270371652",
            "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
            "text": "Having developed an effective procedure to automatically scale the initialization and learning rates for structured layers, we now aim to understand how various structures compare in performance.\n\nWhen data is not a bottleneck, a neural network's test error or loss on a task follows a power law E \u221d P \u2212\u03b1 P if trained to (near) convergence, where P is the number of parameters and \u03b1 P is a constant (Kaplan et al., 2020;Hoffmann et al., 2022;Henighan et al., 2020).For dense models, compute per forward pass C \u221d P , so E \u221d C \u2212\u03b1 C for some constant \u03b1 C .We explore how different structures change how E scales with C, as P does not consistently relate to training or inference cost when varying the structure (Table 1).\n\nWe train all models for a fixed number of iterations T , so the total training compute C tot \u221d C. Thus, the scaling laws in C can differ from compute-optimal scaling laws, which require carefully optimizing the allocation of C tot \u221d CT between C and T (Kaplan et al., 2020;Hoffmann et al., 2022), which we leave to future work.\n\nTo compare multiple structures across compute scales, we conduct experiments primarily using MLPs and ViTs on CIFAR-10 and CIFAR-100.In Section 5, we present largerscale experiments on ImageNet and language modeling.With limited training data in CIFAR-10 and CIFAR-100, we apply heavy augmentation to alleviate over-fitting.The augmented training set is sufficiently large, resulting in relatively clean power-law scaling of training error with C. We extract these power law parameters, reflecting the expressivity afforded by each structure as a function of C, and visualize the scaling of test error with C, which is not well-described by a power law due to train-test discrepancy.",
            "score": 0.5673347209360613,
            "section_title": "Scaling Laws of Structured Matrices",
            "char_start_offset": 21342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 1047
                },
                {
                    "start": 1049,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "268681464",
            "title": "Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance",
            "text": "Different from existing attempts that rely on intuition or qualitative targets, our study seeks a quantitative solution. As a concurrent work, Liu et al. (2024) also proposes to predict optimal data mixtures through regression but assuming rank invariance across training scales. \n\nScaling laws are functional relationships between the properties of interests (e.g., test loss or other performance metrics) and the scales of controllable factors regarding the optimization process or architecture (e.g., model sizes and numbers of training samples) (Villalobos, 2023). Along with the development of machine learning, characterizing scaling behaviors has garnered great research interest under the context of learning theories, bounding the generalization error given the number of training samples in the form of power laws (Vapnik & Chervonenkis, 1971;Valiant, 1984;Haussler, 1988;Amari et al., 1992). Nevertheless, overly strict assumptions hinder their practical applications. In recent years, statistical estimation on scaling gained fast progress for deep neural networks and spawns the introduction of scaling laws. Hestness et al. (2017) pioneers the trend and demonstrates power-law generalization error scaling across a breadth of factors but the power-law exponents differ from previous theoretical analysis. Kaplan  (2020) conduct more comprehensive investigations on Transformer architecture (Vaswani et al., 2017), further highlighting the power-law relationship on test loss regarding model sizes, the amount of training data and computation across orders of magnitudes. These findings foretell the performance gain with scaling quantitatively and guide the trade-off between larger models and more training data, directing to the later development of large language models (Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023). Lately, progressive investigations propose amendments to existing scaling laws (Caballero et al., 2022;Alabdulmohsin et al., 2022), seeking theoretical explanations on the empirical formulas Bahri et al. (2021); Hutter (2021); Michaud et al. (2024), and exploring the functional relationships in broader scenarios (Hernandez et al., 2021;Frantar et al., 2023;Liu et al., 2023).",
            "score": 0.5655196835846109,
            "section_title": "Related Work",
            "char_start_offset": 28519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 853,
                    "end": 867,
                    "matchedPaperCorpusId": "8142232"
                },
                {
                    "start": 867,
                    "end": 882,
                    "matchedPaperCorpusId": "27204621"
                },
                {
                    "start": 882,
                    "end": 901,
                    "matchedPaperCorpusId": "17577797"
                },
                {
                    "start": 1404,
                    "end": 1426,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1808,
                    "end": 1830,
                    "matchedPaperCorpusId": "2757547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "Neural scaling laws The scaling of neural network performance as a function of model size or training steps has been studied extensively (Ahmad & Tesauro, 1988;Hestness et al., 2017;Rosenfeld et al., 2019;Henighan et al., 2020). Kaplan et al. (2020) first observed that the test loss of autoregressive language models follows a power law over many orders of magnitude. Hoffmann et al. (2022) improved the methodology further and found the \"Chinchilla\" scaling laws, which still serve as a reference point for many language models. In our quantitative analysis of compute scaling, we largely follow their approach. \n\nSeveral works have extended scaling laws from model size and training steps to other dimensions: Muennighoff et al. (2023) studied the effect of the training dataset size, which we also discuss, Alabdulmohsin et al. (2023) analyzed scaling of different architecture hyperparameters separately, and Jones (2021) investigated the scaling with problem complexity.",
            "score": 0.563639166109092,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 1374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 976
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 160,
                    "matchedPaperCorpusId": "15615035"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "273375506",
            "title": "Towards Neural Scaling Laws for Time Series Foundation Models",
            "text": "Neural Scaling Laws. Neural scaling laws seek to provide a predictive framework for optimizing the allocation of computational resources to maximize model performance. In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2024). In vision domains, scaling laws have been explored in areas like discriminative modeling (Hestness et al., 2017) and visual auto-regressive modeling (Henighan et al., 2020). Recently, Edwards et al. ( 2024) introduced scaling laws for large time series models, showing that performance scales according to a power law with model size, compute, and dataset size. Shi et al. (2024a) examined the effect of time-series forecasting horizon on model scaling behavior, offering a theoretical framework to explain its influence. However, both studies have focused on in-distribution scenarios, leaving the investigation of scaling laws in out-of-distribution contexts largely unexplored. \n\nTime Series Foundation Models. Foundation models (Das et al., 2024;Goswami et al., 2024) represent a new paradigm aimed at generalizing across diverse domains and tasks by leveraging knowledge from large-scale data in the pre-training phase. They have significantly advanced time series forecasting, particularly in zero-shot scenarios, where predictions are made on data from previously unseen domains. For instance, Woo et al. (2024) introduced Moirai, an encoder-only transformer architecture that employs an \"any-variate\" attention mechanism to capture dependencies in multivariate time series. Ansari et al. ( 2024) proposed a method that tokenizes time series values through scaling and quantization into a fixed vocabulary, training a series of transformer-based models known as Chronos.",
            "score": 0.5626390257241433,
            "section_title": "RELATED WORKS",
            "char_start_offset": 23184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 513,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1247,
                    "end": 1265,
                    "matchedPaperCorpusId": "264172792"
                },
                {
                    "start": 1616,
                    "end": 1633,
                    "matchedPaperCorpusId": "267411817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "262152780",
            "title": "Neural scaling of deep chemical models",
            "text": "For large language and computer vision models trained to convergence with sufficient model parameters and/or data, performance is characterized by empirical scaling laws where the loss scales as a power law 18 of the form \n\nfor coefficient \u03b1, scaling exponent \u03b2 and resource R. R is the number of model parameters, dataset size or compute. \u03b2 measures the slope of the power law and indicates the scaling efficiency of the model with respect to a scaling factor, R. The power-law trends break down in 'resolution limited' regimes 34 , indicating that the model (dataset) size is insufficient for the given amount of data (model parameters). \n\nNeural scaling presents a best-case scenario for model pre-training loss improvements with increasing resources, and allows for optimal allocation of fixed budgets, for example, to decide whether longer training, more data or larger models will be most efficient for improving pre-training loss. Comparing neural-scaling exponents also provides a fundamental metric for measuring resource efficiency across model architectures. Investigations into neural scaling in the NLP domain have revealed general conclusions about overfitting, sensitivity to architectural choices, transfer learning and sample efficiency 18 . These factors are equally or more important in scientific deep learning applications, where rapid advances are being made in specialized architecture development, and it is often unclear how architectures will perform beyond the small benchmark datasets that are commonly available in scientific settings.",
            "score": 0.5558468709124973,
            "section_title": "Neural scaling",
            "char_start_offset": 24239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294189453125
        },
        {
            "corpus_id": "268681464",
            "title": "Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance",
            "text": "Different from existing attempts that rely on intuition or qualitative targets, our study seeks a quantitative solution. As a concurrent work, Liu et al. (2024) also proposes to predict optimal data mixtures through regression but assuming rank invariance across training scales. \n\nScaling laws are functional relationships between the properties of interests (e.g., test loss or other performance metrics) and the scales of controllable factors regarding the optimization process or architecture (e.g., model sizes and numbers of training samples) (Villalobos, 2023). Along with the development of machine learning, characterizing scaling behaviors has garnered great research interest under the context of learning theories, bounding the generalization error given the number of training samples in the form of power laws (Vapnik & Chervonenkis, 1971;Valiant, 1984;Haussler, 1988;Amari et al., 1992). Nevertheless, overly strict assumptions hinder their practical applications. In recent years, statistical estimation on scaling gained fast progress for deep neural networks and spawns the introduction of scaling laws. Hestness et al. (2017) pioneers the trend and demonstrates power-law generalization error scaling across a breadth of factors but the power-law exponents differ from previous theoretical analysis. Kaplan et al. (2020);Hoffmann et al. (2022); Henighan et al. (2020) conduct more comprehensive investigations on Transformer architecture (Vaswani et al., 2017), further highlighting the power-law relationship on test loss regarding model sizes, the amount of training data and computation across orders of magnitudes. These findings foretell the performance gain with scaling quantitatively and guide the trade-off between larger models and more training data, directing to the later development of large language models (Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2023).",
            "score": 0.5546658471913265,
            "section_title": "A Related Work",
            "char_start_offset": 33202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 853,
                    "end": 867,
                    "matchedPaperCorpusId": "8142232"
                },
                {
                    "start": 867,
                    "end": 882,
                    "matchedPaperCorpusId": "27204621"
                },
                {
                    "start": 882,
                    "end": 901,
                    "matchedPaperCorpusId": "17577797"
                },
                {
                    "start": 1340,
                    "end": 1362,
                    "matchedPaperCorpusId": "2757547"
                },
                {
                    "start": 1457,
                    "end": 1479,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "It is important to note some key differences between their study and ours: they focused on L 2 loss in images, whereas our study deals with cross-entropy loss on mainly natural language data. Additionally, their analysis used the number of fine-tuning data points seen as the data variable, in contrast to our use of fine-tuning data size, as we train our models to convergence on the fine-tuning data. Keeping these distinctions in mind, we now revisit the conditions from Mikami et al. 2021: 1. (Irreducible error) \n\nIn addition to the three conditions from Mikami et al. 2021, we introduce a fourth condition, which enhances our interpretation of the transfer gap: \n\n4. (Power law plus transfer gap with no fine-tuning) \n\nConditions 2, 3, and 4 are natural assumptions given that the power law form is ubiquitous in machine learning scaling laws (Henighan et al. 2020, Sharma and Kaplan 2020, Villalobos et al. 2022). The empirical strength of the power law form is evidenced by the clear power law-like shape observed in the data. This shape is illustrated by Figure 3, which reveals how loss decreases in pre-training steps, for a fixed number of fine-tuning data points. \n\nGiven the first three conditions, Mikami et al. 2021 speculate the following form, which is not the only possible form that satisfies these four conditions: \n\nHere, p refers to the pre-training data steps, and f refers to the fine-tuning data size, with A, G, E, \u03b1, \u03b2 representing the constants in the scaling law, which are determined empirically by fitting the model to data. As previously noted in section 3, this scaling law is technically an approximation of the true scaling law, in which the terms p and f are modified to p \u2212 1 and f \u2212 1 respectively. This adjustment is necessary to ensure that conditions 3 and 4 of the scaling law are satisfied. \n\nMikami et al. 2021 support their speculation with a theoretical analysis of the evolution of loss during training within the Neural Tangent Kernel (NTK) framework, building on the work of Arora et al. 2019 andNitanda, Chinot, andSuzuki 2020.",
            "score": 0.5528315725492721,
            "section_title": "Conclusion",
            "char_start_offset": 30010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 2078
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "276903172",
            "title": "AGI, Governments, and Free Societies",
            "text": "Rapid improvements in models have surpassed the expectations of many observers and academics in the field, largely due to the sheer scale of the data and computing resources available to train deep neural network models. \n\nScaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters. 4  short, bigger is better--at least so far--thanks to a combination of technological advancements and increased resource allocation, leading to rapid progress in AI capabilities. The compute required for language models to reach a set performance threshold has halved approximately every eight months since 2012. In addition to significant algorithmic innovations, such as the Transformer architecture, many recent performance gains stem primarily from the ability to scale up models and datasets to unprecedented sizes. This has been made possible by rapid growth in computing power, which has allowed the rate of improvement to substantially exceed hardware gains from Moore's Law. 5 untless recent advances by labs, academic institutions, and open source communities reflect growing investment in the field and a diversification of approaches and algorithms. For example, new systems from Google DeepMind have recently solved four out of six problems from last year's International Mathematical Olympiad, matching the competition's silver medalist for the first time.6",
            "score": 0.5509914957545738,
            "section_title": "II. The Age of Artificial Intelligence",
            "char_start_offset": 5600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2349
                },
                {
                    "start": 2350,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "246823711",
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "text": "Neural scaling laws define a predictable relationship between a model's parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We find that scaling laws emerge at finetuning time in some NLP tasks, and that they can also be exploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational benefits.",
            "score": 0.5503581067513279,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "We consider the problem of compute-optimal language model training: given a compute budget C, we wish to predict how to best allocate it across model size (in parameters) and dataset size (in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al. [30] proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of C. This scaling law was influential in determining the size of GPT-3 and several subsequent models [12,51,43,32,47,62,52]. However, Hoffmann et al. [25] challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of C, and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget. \n\nWhile Hoffmann et al. [25] and subsequent work [55,56,17,26,15] established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand why the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling. \n\nHoffmann et al. [25] hypothesize that the scaling law discrepancy is due to Kaplan et al. [30] not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. [25]'s hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far. \n\nOur contribution. In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our main  Cosine decay (no tuning) results.",
            "score": 0.5479910217492835,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327392578125
        },
        {
            "corpus_id": "273507997",
            "title": "A Simple Model of Inference Scaling Laws",
            "text": "Advancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022]. \n\nHowever, scaling during inference-the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises: \n\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts? \n\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k. \n\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed",
            "score": 0.5477506554923403,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 243,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 484,
                    "end": 508,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "This work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters. \n\nTwo related questions are why such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community. \n\nDespite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws.",
            "score": 0.5473260482125347,
            "section_title": "Discussion and Future Directions",
            "char_start_offset": 19791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "248986979",
            "title": "Scaling Laws and Interpretability of Learning from Repeated Data",
            "text": "Scaling Laws A scaling law lens consists of finding a small set of hyperparameters that have large, predictable impacts on model performance, and was present throughout this work (at least one of the hyperparameters is generally model size, compute, or dataset size). The predictive nature of scaling laws makes them useful in a broad number of research and engineering settings. The implications of scaling laws are sufficiently broad and understandable that understanding them is relevant to policy makers [Ganguli et al., 2022]. Predictable scaling trends in neural networks were first studied with [Hestness et al., 2017]. [Kaplan et al., 2020] demonstrated that test loss performance on language modeling tasks scales as a predictable function of model size, dataset size, and compute. The scaling law lens has become more popular over time. For instance scaling laws have been shown in many modalities (e.g., images, video, math, etc.) [Henighan et al., 2020], acoustics [Droppo and Elibol, 2021], transfer to code, [Hernandez et al., 2021], and few-shot adaptation of vision models [Prato et al., 2021]. Existing scaling laws have been revisited as training setups change; for instance, [Hoffmann et al., 2022] found that many recent large models have been under-trained. Our work uses the scaling law lens on an aspect of dataset quality and supplements the lens with an interpretability lens, and we believe our work is novel in both these respects.",
            "score": 0.5426299415083742,
            "section_title": "Related Work",
            "char_start_offset": 29124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1458
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 625,
                    "matchedPaperCorpusId": "2222076"
                },
                {
                    "start": 627,
                    "end": 648,
                    "matchedPaperCorpusId": "210861095"
                },
                {
                    "start": 1022,
                    "end": 1046,
                    "matchedPaperCorpusId": "2222076"
                },
                {
                    "start": 1089,
                    "end": 1109,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1194,
                    "end": 1217,
                    "matchedPaperCorpusId": "2222076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "262934950",
            "title": "Uncovering Neural Scaling Laws in Molecular Representation Learning",
            "text": "The study of neural scaling law can be traced back to early theoretical analyses of bounding generalization error [81][82][83][84]. These works, based on assumptions about model capacity and data volume, reveal power-law relationships between the bounds of model generalization error and the amount of data. However, the conclusions drawn from these theoretical studies often yield loose or even vacuous bounds, leading to a disconnection between the theoretical findings and the empirical results of generalization error. \n\nEarly follow-on research have investigated empirical generalization error scaling, which represents an initial attempt at exploring the neural scaling law. Bango and Bill [85] conduct experiments on a language modeling problem called confusion set disambiguation, using subsets of a large-scale text corpus containing billions of words. Their findings suggest a power-law relationship between the average disambiguation validation error and the size of the training data. Similarly, Sun et al. [21] demonstrate that the accuracy of image classification models improves with larger data sizes and conclude that the accuracy increases logarithmically based on the volume of the training data size. \n\nHestness et al. [18] empirically validate that model accuracy improves as a power-law as growing training sets in various domains, which exhibit consistent learning behavior across model architectures, optimizers and loss functions. However, there exists generalization error plateau in small data region and irreducible error region. With a broader coverage, Michael et al. [22] present findings that consistently show the scaling behavior of language model log-likelihood loss in relation to nonembedding parameter count, dataset size, and optimized training computation. They leverage these relationships to derive insights into compute scaling, the extent of overfitting, early stopping step, and data requirements in the training of large language models. \n\nIn recent years, several investigations of neural scaling laws specific to particular tasks have been conducted [86,19,87,60]. Unlike previous research, while power-law relationships hold within specific ranges of data size or model parameter count, certain tasks exhibit unique and uncommon learning behaviors. For instance, only marginal performance gains are expected beyond a few thousand examples in image reconstruction [20].",
            "score": 0.5383642152400367,
            "section_title": "D.2 Neural scaling law",
            "char_start_offset": 42586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "27204621"
                },
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "1925579"
                },
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "1138467"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "1437248"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 2108,
                    "end": 2111,
                    "matchedPaperCorpusId": "250113273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "246608156",
            "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture",
            "text": "Our work builds extensively on the literature on scaling laws Amari et al. (2001); Hestness et al. (2017); Rosenfeld et al. (2019) and in particular Kaplan et al. (2020). \n\nPrior works that have studied scaling laws in NMT include Ghorbani et al. (2021); Hestness et al. (2017);Gordon et al. (2021). Our experimental setup shares various commonalities with Ghorbani et al. (2021) but with the important distinction that Ghorbani et al. study scaling with respect to the number of parameters, while we study scaling with respect to the dataset size. Thus, these works should be considered complementary -they drive experimental decision making in different regimes and can give qualitatively different recommendations as we describe in Section 3. Gordon et al. (2021) also consider data scaling laws for NMT. Our work differs from them in the following fundamental ways (1) While they focus on establishing data and parameter scaling laws for NMT, our main goal is to examine the role of design choices such as model architecture and data collection methods from the perspective of scaling laws (2) Gordon et al. focus on the small data/small model regime (maximum of 50M sentence pairs). In contrast, we focus on the performance of models at web-scale data (ranging up to 1B sentence pairs). Working in the smaller data regime allows Gordon et al. (2021) to make better predictions about the behavior of NMT systems for low-resource languages, while working in the large data regime allows us to make better predictions for high-resource languages and assess the effectiveness of interventions such as filtering (where having a large dataset allows us to throw away data and still observe reasonable scaling laws). \n\nOur scaling law differs from (Hestness et al., 2017) in that they conduct experiments with LSTMs and their law does not scale as O(1/D) when D \u2192 \u221e. Despite these differences, the scaling exponents found in both these papers are in the same range as ours 0.25 \u2212 0.3.",
            "score": 0.5364137246762175,
            "section_title": "Related Works",
            "char_start_offset": 7049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 81,
                    "matchedPaperCorpusId": "17577797"
                },
                {
                    "start": 231,
                    "end": 253,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 357,
                    "end": 379,
                    "matchedPaperCorpusId": "237532682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234619140625
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "In this paper, we revisit the scaling-law formulas proposed by [KMH + 20], confirming that they remain generally applicable when scaling the model size up to 33B.Other works obtain different conclusions primarily due to (1) Many factors such as the data distribution, context length, tokenization affect the constant coefficients in scaling-law formulas, so the constant coefficients, unlike the formulas themselves, are not universal; and (2) The loss value adheres to an analytical power law relationship with the training step under infinite batch size.With a finite batch size, fitting the loss value with an analytical function is problematic.As a result, none of other works have provided compelling evidence to reliably predict the full loss trajectory of larger models by training solely on smaller models.\n\nAfter meticulously identifying influential factors in predicting the loss trajectory, we provide transparent, step-by-step guidelines on how to estimate all constant terms in scaling-law formulas by training on models with only 1M\u223c60M parameters.Using these estimated formulas from small models, we showcase the capability to accurately predict various attributes for models with up to 33B parameters before their training starts.By unravelling the mystery of scaling laws and making them easily accessible to everyone, our objective is to shift the understanding of scaling laws from theoretical concepts to practical implementation, thereby aiding future research in pre-training large language models in a more principled manner.The summary of the key results in this paper is as follows:\n\n\u2022 Hyperparameters such as batch size, learning rate, and learning rate scheduler influence the rate of convergence, yet do not impact the final converged loss provided that (1) their values fall within a reasonable range and (2) the model is trained with sufficient steps on adequate amounts of data.\n\n\u2022 Adjusting the batch size involves a trade-off between time and computation.The critical batch size that strikes an optimal time/computation balance can be determined based solely on the loss value.\n\nTraining with this critical batch size requires twice as many training steps to achieve a specific loss value compared to using an infinite batch size (minimum possible required steps).\n\n\u2022 The context length, tokenization, data distribution and model configurations have big impacts on the constants in scaling law formulas, but do not affect the form of scaling law itself.",
            "score": 0.5358134450533114,
            "section_title": "Introduction",
            "char_start_offset": 1922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 814
                },
                {
                    "start": 816,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1246
                },
                {
                    "start": 1246,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1607
                },
                {
                    "start": 1609,
                    "end": 1909
                },
                {
                    "start": 1911,
                    "end": 1988
                },
                {
                    "start": 1988,
                    "end": 2110
                },
                {
                    "start": 2112,
                    "end": 2297
                },
                {
                    "start": 2299,
                    "end": 2486
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Diaz and Madaio (2024) challenged the generalizability of neural scaling laws, arguing that they fail in diverse real-world AI applications. They argued that scaling laws do not always hold when AI models serve heterogeneous populations with conflicting criteria for model performance. Larger datasets inherently reflect diverse communities, making it difficult to optimize a single model for all users. Similar to issues in multilingual AI, increasing data diversity often leads to performance degradation rather than improvement. Universal evaluation metrics are inadequate for capturing these complexities, potentially reinforcing biases against underrepresented groups. The authors further argued that smaller, localized AI models may be more effective for specific communities, highlighting the need to move beyond one-size-fits-all scaling assumptions. Beyond dataset expansion, data pruning contradicts traditional scaling laws by demonstrating that performance improvements do not always require exponentially more data. Strategic pruning achieves comparable or superior results with significantly fewer training samples (Sorscher et al., 2023). Not all data contributes equally, and selecting the most informative examples enables more efficient learning. Experimental validation on CIFAR-10, SVHN, and ImageNet shows that careful dataset curation can surpass traditional powerlaw improvements, questioning the necessity of brute-force scaling. \n\nDespite their significant impact, many studies on scaling laws suffer from limited reproducibility (see Table 8 in Appendix C) due to proprietary datasets, undisclosed hyperparameters, and undocumented training methodologies. The inability to replicate results across different computing environments raises concerns about their robustness. Large-scale experiments conducted by industry labs often depend on private infrastructure, making independent verification challenging. This lack of transparency undermines the reliability of scaling law claims and highlights the urgent need for open benchmarks and standardized evaluation frameworks to ensure reproducibility.",
            "score": 0.5319816644491167,
            "section_title": "Criticisms of scaling laws",
            "char_start_offset": 23339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2124
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "corpus_id": "275336968",
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "score": 0.5303859718760001,
            "section_title": "Related Work",
            "char_start_offset": 4305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 1803,
                    "end": 1806,
                    "matchedPaperCorpusId": "6519532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "261582515",
            "title": "Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck",
            "text": "Multi-resource scaling laws for deep learning. Many empirical studies (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022;Zhai et al., 2022), motivated by the pressing need to allocate resources effectively in large-scale deep learning, corroborate the presence and regularity of neural scaling laws. Precise statements and hypotheses vary; Kaplan et al. (2020) fit power-law expressions which predict holdout validation log-perplexity of a language model in terms of dataset size, model size, and training iterations (m, r, T in our notation). The present work shows how such a joint dependence on m \u00d7 r \u00d7 T can arise from a single feature learning problem with a computational-statistical gap. Numerous works attempt to demystify neural scaling laws with theoretical models (Bahri et al., 2021;Hutter, 2021;Michaud et al., 2023); ours is unique in that it does not suppose a long-tailed data distribution (the statistical complexity of identifying a sparse parity is benign). We view these accounts to be mutually compatible: we do not purport that statistical query complexity is the unique origin of neural scaling laws, nor that there is a single such mechanism.",
            "score": 0.530319344277036,
            "section_title": "A Additional related work",
            "char_start_offset": 39098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1180
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 153,
                    "matchedPaperCorpusId": "235367962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316162109375
        },
        {
            "corpus_id": "274150498",
            "title": "Scaling Laws for Online Advertisement Retrieval",
            "text": "The neural scaling laws, widely recognized in Natural Language Processing and Computer Vision areas, establish predictable relationships between model performance and key factors such as model size, dataset size, computational cost, and post-training error rates. Hestness et al.(2017) [14] introduced a unified scaling law applicable to machine translation, image classification, and speech recognition, noting that different tasks exhibit distinct scaling coefficients. Kaplan et al.(2020) [19] further elaborated on these laws, defining them through four parameters: model size (number of parameters), dataset size (number of tokens), computational cost (FLOPs), and training loss (perplexity per token). These relationships were empirically validated, including during the training of GPT-3 [3]. Subsequently, Hoffmann et al.(2022) [15] presented the Chinchilla Scaling Law, which differs somewhat from [19] because of their different training setups. In addition to estimating the training loss of the model, Isik et al.(2024) [18] further verified that there also exist scaling laws between the downstream task metrics and the model parameters. Caballero et al.(2023) [4] found that many scaling behaviors of artificial neural networks follow a smoothly broken power law (BNSL), verified on various NLP and CV tasks, covering a wide range of downstream tasks and model structures. \n\nIn the recommendation area, Shin et al.(2023) [23] and Zhang et al.(2024a) [32] conducted studies on whether there exist scaling laws for recommendation models and primarily provided qualitative conclusions. Fang et al.(2024) [8] first proposed a quantitative scaling law in the recommendation area, which describes the relationship between the amount of training data, the size of model parameters, and an offline metric for query-document retrieval. Based on this scaling law, the optimal amount of data and model parameter allocation can be solved under a given total training resource. However, obtaining a multivariate scaling law requires a large number of experiments, and the offline metrics might not be a good indicator for online metrics, these make it somewhat difficult to apply in real-world industrial applications.",
            "score": 0.5299924651833653,
            "section_title": "Related Work 2.1 Neural Scaling Laws",
            "char_start_offset": 6315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1435,
                    "end": 1439,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1597,
                    "end": 1614,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "268723791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6044921875
        },
        {
            "corpus_id": "247292726",
            "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
            "text": "(Learning Rate, Batch Size) Scaling [44] proposed to scale learning rate with batch size while fixing the total epochs of training; [14] proposed to scale learning rate as \u221a batchsize while fixing the total number of steps of training. However, [41] showed that there's no consistent (learning rate, batch size) scaling law across a range of dataset and models. Later, [30] studied the trade-off of training steps vs computation as a result of changing batch size. They proposed an equation of a/(1 + b/batchsize), where a and b are task-and model-specific constants, for the optimal learning rate (see their fig 3 and fig 5). This law suggests that for sufficiently large batch size, the optimal learning rate is roughly constant. 18 This supports our results here as well as the empirical results in [41, fig 8]. \n\nLearning Rate Scaling with Width Assuming that the optimal learning rate should scale with batch size following [44], [34] empirically investigated how the optimal \"noise ratio\" LR/batchsize scales with width for MLP and CNNs in NTK parametrization (NTP) or standard parametrization 18 while the optimal learning is roughly linear in batch size when the latter is small (SP) trained with SGD. They in particular focus on test loss in the regime of small batch size and training to convergence. In this regime, they claimed that in networks without batch normalization, the optimal noise ratio is constant in SP but scales like 1/width for NTP. However, they found this law breaks down for networks with normalization. \n\nIn contrast, here we focus on training loss, without training to convergence and with a range of batch sizes from small to very large (as is typical in large scale pretraining). Additionally, our work applies universally to 1) networks with normalization, along with 2) Adam and other adaptive optimizers; furthermore 3) we empirically validate transfer across depth and sequence length, and 4) explicitly validate tuning via \u00b5Transfer on large models like BERT-large and GPT-3.",
            "score": 0.5271575235457591,
            "section_title": "Previously Proposed Scaling Rules of Hyperparameters",
            "char_start_offset": 31388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 40,
                    "matchedPaperCorpusId": "3516266"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "3516266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "Across different information generation scenarios, scaling has been a siginificant factor to the performance improvement. It is largely attributed to the discovery of scaling laws [24]. Scaling laws describe how loss decreases in a log-linear manner as model size or training data volume increases. It can be formulated as follows: \n\nwhere L is the loss, x is model size or data size, and k and \u03b1 are coefficients. This scaling formula has become a crucial theoretical guide in the era of large models, suggesting that performance can be enhanced at a log-linear rate simply by scaling up the model size or training data. Based on these scaling laws, researchers also derived optimal model sizes given fixed computational resources [25]. Their findings indicate that as computational capacity expands, it is beneficial not only to increase the training step but also the model size. This insight has further facilitated the pursuit of large models. The correctness of scaling laws was first proposed in language modeling field and then validated in many other areas, including data mixture scaling laws [26], multimodal scaling laws [27], and scaling laws specific to information retrieval [28]. Despite wide recognition of scaling laws, there remains disagreement among researchers about whether scaling is the correct path to the future. This stems from two main concerns: the uncertain relationship between loss and practical metrics, and the inference costs associated with large models. \n\n\u2022 Loss vs. Metric Improvement: The first arguing point is whether a linear reduction in loss can translate into super-linear improvements in actual metrics. If metrics could improve super-linearly with linear increases in computational effort, scaling up models would be highly advantageous. However, if the decrease in loss only results in linear or sublinear metric improvements, the diminishing improvements make scaling an inefficient option. The relationship between loss and metric performance remains an open question. Some researchers [29] believe that metrics can improve super-linearly, which is termed emergent abilities. This is further supported by Du et al. [30], who observed a jump in metrics when loss reaches a certain threshold. Additionally, Power et al. [31] introduced the concept of \"grokking\" to explain emergence, showing that models might suddenly exhibit strong generalization capabilities when provided with sufficient computational resources.",
            "score": 0.5257972207049932,
            "section_title": "Scaling",
            "char_start_offset": 10722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2465
                }
            ],
            "ref_mentions": [
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "268723791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "273662300",
            "title": "Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling and Zero-Shot Transfer",
            "text": "Neural Scaling Laws describe empirically-derived power-law relationships between model performance and various scaling factors. These laws have been instrumental in understanding and predicting the behavior of large language models. Kaplan et al. (2020) identified key power-law relationships between the pre-training loss L and several variables: \n\nwhere N is the number of model parameters, D is the dataset size, and C is the amount of compute. These laws have elucidated performance disparities between different neural architectures, such as Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani et al., 2017), particularly in terms of pre-training loss. Subsequent work by Hoffmann et al. (2022) and Muennighoff et al. (2024) further refined these relationships, considering factors like optimal batch size, compute-optimal training, and data constraint. As shown in Figure 5, we scanned combinations of five commonly used functionals (Zhao & Truhlar, 2008;Haoyu et al., 2016;Adamo & Barone, 1999;Lee et al., 1988) and basis sets (Weigend, 2006;Ditchfield et al., 1971;Dunning Jr, 1989), resulting in 25 distinct computational methods using PySCF package (Sun et al., 2020). The standard deviations of the HOMO-LUMO gap predictions were substantial: 12571\u00b11775, 8411\u00b11626, and 9333\u00b11758 meV for gdb1, gdb2, and gdb3, respectively. Such uncertainty from DFT calculations has been widely studied in chemistry community (Zhang & Musgrave, 2007), especially the accuracy of B3LYP functional used in QM9 dataset is non-ideal per current standard (Zhao & Truhlar, 2005;Goerigk & Grimme, 2011;Mardirossian & Head-Gordon, 2017). The plot shows the relationship between the number of layers and the pre-training loss, illustrating the initial rapid improvement followed by diminishing returns as depth increases.",
            "score": 0.5256879324985996,
            "section_title": "C NEURAL SCALING LAW ON LANGUAGE",
            "char_start_offset": 30136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 618,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 750,
                    "end": 775,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 985,
                    "end": 1007,
                    "matchedPaperCorpusId": "98119881"
                },
                {
                    "start": 1007,
                    "end": 1026,
                    "matchedPaperCorpusId": "262116371"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "matchedPaperCorpusId": "96110585"
                },
                {
                    "start": 1047,
                    "end": 1064,
                    "matchedPaperCorpusId": "45348446"
                },
                {
                    "start": 1080,
                    "end": 1095,
                    "matchedPaperCorpusId": "42762306"
                },
                {
                    "start": 1095,
                    "end": 1119,
                    "matchedPaperCorpusId": "96872397"
                },
                {
                    "start": 1119,
                    "end": 1136,
                    "matchedPaperCorpusId": "54183898"
                },
                {
                    "start": 1205,
                    "end": 1223,
                    "matchedPaperCorpusId": "211572993"
                },
                {
                    "start": 1467,
                    "end": 1491,
                    "matchedPaperCorpusId": "1516019"
                },
                {
                    "start": 1591,
                    "end": 1613,
                    "matchedPaperCorpusId": "24061290"
                },
                {
                    "start": 1613,
                    "end": 1636,
                    "matchedPaperCorpusId": "41029213"
                },
                {
                    "start": 1636,
                    "end": 1669,
                    "matchedPaperCorpusId": "103560395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318115234375
        },
        {
            "corpus_id": "274150498",
            "title": "Scaling Laws for Online Advertisement Retrieval",
            "text": "The neural scaling laws, describing how neural network performance changes with key factors (e.g. model size, dataset size, computational cost), have been discovered in various research areas [8,15,18,19,23,32]. Early research [14] shows that the neural network performance is predictable when scaling training data size in various tasks such as neural machine translation and language modeling. Kaplan et al.(2020) [19] further empirically verify the scaling law of Transformer architecture in language modeling, regarding the key factors (model size, data size, training cost) and training performance (PPL). Inspired by the scaling law, researchers extend the size of pre-trained language models and further empirically verify the scaling law by training GPT-3 [3]. This wave of enthusiasm has led to the creation of GPT-3.5 and GPT-4 [1], ushering in a new era of NLP research and applications. \n\nBased on the scaling laws, the optimal key factors of the model can be determined under given constraints, thus guiding us in model design and resource allocation. Recommendation and advertising systems are mature commercial applications that prioritize ROI, making it highly valuable to explore whether there exists scaling laws for recommendation and advertising models. Due to the lack of a thriving community and open data ecosystem similar to NLP, research on model scaling is relatively scarce in the recommendation and advertisement areas. Early studies primarily gave some qualitative conclusions about model scaling [23,32]. Fang et al.(2024) [8] first attempt to give a quantitative scaling law of model performance and the amount of training data and model parameters based on public information retrieval benchmarks, and give the practice to solve the optimal amount of data and model parameters under a given total training resource. \n\nHowever, there is still a lack of literature on scaling law research in real-world online recommendation and advertising systems. We attribute this to two main challenges: 1) For commercial systems, the scaling law should describe the relationship between business revenue and machine costs, rather than the relationship between computing volume, data volume, and offline metrics as seen in traditional research.",
            "score": 0.5254565420601152,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "268723791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380615234375
        },
        {
            "corpus_id": "278310893",
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "text": "The development of neural scaling laws has provided a foundational framework for understanding the performance trajectory of large language models (LLMs). These laws (Kaplan et al., 2020;Hoffmann et al., 2022) describe how model performance improves predictably with increases in parameters, dataset size, and compute resources. Initially, scaling laws seemed to offer a clear roadmap for the continuous and predictable advancement of LLMs. However, as the field has evolved -evidenced by the sharp rise in the number of proposed scaling laws between 2020 and 2024, as shown in Figure 1 -significant limitations and challenges have become apparent. These emerging concerns cast doubt on the long-term viability and effectiveness of scaling laws as the primary strategy for advancing AI.  their reliance on simplified power law relationships to predict model performance. While these laws capture broad trends, they often fail to account for nuanced factors that influence real-world outcomes. For instance, the assumption that increasing dataset size or compute will indefinitely yield proportional performance improvements ignores diminishing returns observed in practice (Diaz & Madaio, 2024). As models scale, the marginal gains from additional data and compute tend to decrease, leading to inefficient resource allocation (Muennighoff et al., 2023). \n\nAnother limitation of neural scaling laws is their emphasis on uniform scaling across model size, data, and compute. Jin et al. (2023) demonstrated that different abilities, such as fact recall and in-context learning, degrade at different rates under pruning or downsizing. This variability suggests that scaling laws may not provide a one-size-fits-all solution and that alternative approaches are needed to address diverse performance requirements. By focusing exclusively on scaling, researchers risk missing opportunities to develop more efficient and specialized models tailored to specific applications. \n\nNeural scaling laws also neglect the broader implications of scaling on energy consumption and environmental sustainability. The computational requirements for training large-scale models are immense, resulting in significant carbon emissions (Faiz et al., 2024). Recent studies showed that the energy efficiency of compute-intensive tasks varies based on factors such as hardware configuration, server location, and training duration (Zhang et al., 2023).",
            "score": 0.5245625103910765,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2232
                },
                {
                    "start": 2233,
                    "end": 2425
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2119140625
        },
        {
            "corpus_id": "276558195",
            "title": "Scaling Laws for Downstream Task Performance in Machine Translation",
            "text": "Scaling laws for transformers. Scaling laws for LLMs have attracted significant attention as they can inform the decisions about key design choices such as model size and the type and size of the pretraining data (Kaplan et al., 2020;Hoffmann et al., 2022;Hernandez et al., 2021). Most of the pioneering work has focused on how upstream cross-entropy loss or perplexity scales with more pretraining data, larger models, or longer training (Kaplan et al., 2020;Hoffmann et al., 2022). Followup works have analyzed scaling behavior of translation models (Ghorbani et al., 2021;Zhuocheng et al., 2023;Gordon et al., 2021;Fernandes et al., 2023;Bansal et al., 2022;Zhang et al., 2022), studied theoretical foundation behind scaling laws (Sharma & Kaplan, 2020;Hutter, 2021;Bahri et al., 2021), or extended the laws to the vision models (Zhai et al., 2022;Jain et al., 2023). Closest to our work, Hernandez et al. (2021) have analyzed transfer learning but with a focus on how the cross-entropy loss behaves as the finetuning data scales. Unlike our work, their scaling law describes the relation between the size of a (finetuning) dataset and the cross-entropy loss on the same dataset -making this closer to the standard scaling laws in the literature since the finetuning loss and the finetuning dataset are computed over samples from the same distribution. On the other hand, we propose scaling laws for the downstream metrics on the finetuning dataset as the pretraining data scales -switching the focus to an \"out-of-distribution\" analysis. The only work we are aware of that proposed scaling laws for the downstream task performance as a function of pretraining dataset size (Sun et al., 2017) has focused on classification tasks in the vision domain and used smaller models. \n\nTransferability metrics and value of pretraining. While it may be commonly suggested that pretraining data improves both upstream and downstream performance, this rule has been challenged in the vision domain.",
            "score": 0.5245461125526303,
            "section_title": "RELATED WORK",
            "char_start_offset": 5628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 575,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 575,
                    "end": 598,
                    "matchedPaperCorpusId": "266177050"
                },
                {
                    "start": 598,
                    "end": 618,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 641,
                    "end": 661,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 661,
                    "end": 680,
                    "matchedPaperCorpusId": "246441975"
                },
                {
                    "start": 832,
                    "end": 851,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 851,
                    "end": 869,
                    "matchedPaperCorpusId": "257353837"
                },
                {
                    "start": 1677,
                    "end": 1695,
                    "matchedPaperCorpusId": "6842201"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3876953125
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.",
            "score": 0.5202380722863362,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "268032247",
            "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
            "text": "Scaling Laws Recent research has shown that the performance of neural models can be predicted by a power-law of model and/or data sizes (Hestness et al., 2017;Kaplan et al., 2020). Such pattern widely exists across different domains and model architectures, such as computer vision (Zhai et al., 2021), autoregressive generative modeling (Henighan et al., 2020), neural machine translation (Gordon et al., 2021;Ghorbani et al., 2021;Bansal et al., 2022;Zhang et al., 2022a), multilingual translation (Fernandes et al., 2023), multi-modal modeling (Aghajanyan et al., 2023) and sparse neural architectures (Frantar et al., 2023). These laws provide a valuable tool for guiding training decisions (Hoffmann et al., 2022) and model development by understanding how model performance evolves with scale, which greatly facilitates the development of LLMs (OpenAI, 2023). Unfortunately, the study of scaling for LLM finetuning lags behind badly, and our study fills this gap. \n\nThe most closely related work to ours is (Hernandez et al., 2021) which explored the scaling for knowledge transfer by comparing finetuning with training from scratch. Our study is orthogonal to theirs with significant difference as our key focus is understanding the scaling of different factors for LLM finetuning, rather than the transfer.",
            "score": 0.5176018894632936,
            "section_title": "RELATED WORK",
            "char_start_offset": 17445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1314
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 433,
                    "end": 453,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 500,
                    "end": 524,
                    "matchedPaperCorpusId": "257038048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349853515625
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Different model families exhibit different scaling behavior, but performance can sometimes be estimated using a single model in a new family. \n\nScaling laws relate performance to scalar training parameters like model or dataset size. For discrete decisions (whether the choice of nonlinearity or data preprocessing scheme), it is not immediately obvious how to pool information across models that differ in these traits (see Ruan et al., 2024, for concurrent work that performs this pooling based on downstream task behavior). Clearly, different pretrained models with the same #params and #toks still show different loss, so these differences can be consequential. But how do discrete choices of architecture, training procedure, or dataset, affect the form of scaling laws? \n\nOne way to answer this question is to look at the parameter estimates for scaling law parameters E, \u03b1, A, \u03b2 and B differ across model families. These results are shown in Fig. 3, where it can be seen that there are often dramatic differences in all five parameters across families. In this sense, even the rate at which additional data or parameters improve model performance depend on underlying architectural details, suggesting that understanding the behavior of a new model family may require a new scaling law. \n\nBut another way to answer this question is to ask how reliably we can predict final model accuracy when borrowing (or pooling) some parameters of scaling laws between families-even if these result in poor parameter estimates, they may predict large-scale model behavior within the range of meaningful differences identified in Section 4. We find that predictions generalize, and a constant #params scaling factor is enough for most models (except the encoder-decoder T5-Pile). However, error rates are larger than in the source family, and predictions for larger models are worse (most conspicuous in OPT's error of 37%, 25% and 15% when extrapolating from 8.7B, 13B and 30B to 175B).",
            "score": 0.5173578212027816,
            "section_title": "WHEN I TRAIN A NEW MODEL, DO I EVEN NEED A NEW SCALING LAW?",
            "char_start_offset": 13436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42138671875
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "Now that we have a statistical model of scaling laws that we understand for jointly largebut-finite model size N , training set size T , and latent space size M , in this section we have a discussion of what we can learn from our statistical model of scaling laws.\n\nIn \u00a74.1, we interpret our results from \u00a73 in the limit that the model size or training set size approaches the size of the latent space, N, T \u223c M , and the neural scaling law phenomenology of \u00a71 breaks down.\n\nIn \u00a74.2, we discuss how the regime of scaling laws, of large training set size and large model size, pushes resource efficient and properly regularized models towards the equiparameterization regime, and how the phenomenon of double descent is not really relevant for such models.\n\nIn \u00a74.3, we try to reconcile the large latent space, M , required for datasets that allow for neural scaling laws with the traditional idea that input datasets are embedded in high-dimensional spaces, N in , and can be compressed to a latent space with much smaller 51 This occurs in the information theory context in the study of von Neumann entropies and in the physics context in the study of quenched disorder. 52 This requires one to instead consider n copies of the original random variable and compute the expectation value of a logarithm by analytically continuing in n using log z = lim n\u21920 (z n \u2212 1) /n. This method only works for certain classes of probability distributions (and quantities being averaged) such that they are so strongly constrained that there is a unique analytic continuation in n. 53 For a useful review of these methods, see [60].\n\nintrinsic dimension, d in . We note that there are a number of notions of dimensionality, and the particular power-law structure of the datasets that give rise to scaling laws makes different notions meaningful for different questions.\n\nFinally, in \u00a74.4, we identify some limitations of our generative data model that could be improved in future analyses.",
            "score": 0.5172829238891311,
            "section_title": "Discussion of Results",
            "char_start_offset": 105969,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1025,
                    "matchedPaperCorpusId": "31589842"
                },
                {
                    "start": 1172,
                    "end": 1174,
                    "matchedPaperCorpusId": "11788069"
                },
                {
                    "start": 1569,
                    "end": 1571,
                    "matchedPaperCorpusId": "119542300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09027099609375
        },
        {
            "corpus_id": "259837466",
            "title": "Large Language Models",
            "text": "Natural language tasks are very complex, and benchmarks constructed from real world data cannot be used directly in theoretical considerations. For this purpose one generally defines \"toy worlds\" and generates synthetic data. The possibilities are endless, but some which have been used are arithmetic problems (decimal arithmetic; modular arithmetic), game play, solving systems of equations, and parsing formal languages. A particularly interesting task is linear regression [48]; since this is the prototypical case of statistical inference, a system which learns to do it can be said to be \"learning how to learn.\" \n\nComing to scaling laws, denote the model size (number of parameters) as P and the dataset size (number of tokens in the corpus) as D, then there are two general regimes. If we hold one of these (say P ) fixed and take the other (say D) to infinity, then a law of large numbers applies and L \u223c 1/D. On the other hand, if we take one parameter very large and study the dependence on the other, nontrivial power law scaling can emerge. In principle one can get different exponents for D and P , suggesting the ansatz \n\nwhere L is test loss Eq. 3 computed in an optimally regularized model. 24 This is a good fit to Figure 2. While in Figure 2 the two exponents appear to differ, there is not really convincing evidence that this is significant. Before working hard on this, one should ask if there is any way to control the many choices involved, so as to define universal exponents. One context in which this can be studied systematically is transfer learning, by distinguishing the dependence on the pretraining and fine tuning datasets [55]. Another relevant and practical question is whether one can prune the dataset to improve the scaling. It is intuitively plausible and can be shown in examples that sets of data items are worth more if they are diverse than if they are similar. The challenge is to find simple ways to quantify this similarity; in [126] many proposals are studied. \n\nScaling laws can arise in many ways, not specific to language models. One hypothesis is that the data lies on a low dimensional submanifold in a higher dimensional space. 25",
            "score": 0.5151799584950875,
            "section_title": "Phenomenology of language models",
            "char_start_offset": 30134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "Large language models with a huge number of parameters, when trained on near internet-sized number of tokens, have been empirically shown to obey neural scaling laws: specifically, their performance behaves predictably as a power law in either parameters or dataset size until bottlenecked by the other resource. To understand this better, we first identify the necessary properties allowing such scaling laws to arise and then propose a statistical model \u2013 a joint generative data model and random feature model \u2013 that captures this neural scaling phenomenology. By solving this model in the dual limit of large training set size and large number of parameters, we gain insight into (i) the statistical structure of datasets and tasks that lead to scaling laws, (ii) the way nonlinear feature maps, such as those provided by neural networks, enable scaling laws when trained on these datasets, (iii) the optimality of the equiparameterization scaling of training sets and parameters, and (iv) whether such scaling laws can break down and how they behave when they do. Key findings are the manner in which the power laws that occur in the statistics of natural datasets are extended by nonlinear random feature maps and then translated into power-law scalings of the test loss and how the finite extent of the data\u2019s spectral power law causes the model\u2019s performance to plateau. ? Equal contribution. ar X iv :2 21 0. 16 85 9v 1 [ cs .L G ] 3 0 O ct 2 02 2",
            "score": 0.5111169730232937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2119140625
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Estimate scaling laws from intermediate checkpoints, not just fully trained models! Most past work on scaling behavior of language models (e.g., Gadre et al., 2024;Muennighoff et al., 2024) has trained a separate model for each value of #toks studied. This is based on the assumption that changes in the learning rate schedule, which depend on the size of the full dataset that will be used for training, render losses from intermediate checkpoints uninformative. \n\nHowever, some recent work has demonstrated the effectiveness of learning schedules that do not require prior access to the size of the training set (Hu et al., 2024), and some work has questioned whether careful choice of the learning rate decay is necessary for reliable scaling laws (Porian et al., 2024). Together, these findings motivate revisiting the assumption that only a single useful datapoint may be obtained from each training run. In the final portion of \u00a75.1, we observed the value of intermediate checkpoints when only a single #params family is used to fit a scaling law. We now test whether this finding extends to larger families-i.e. whether including intermediate checkpoints from all models in a model family reduces ARE. \n\nResults are shown in Fig. 4, which plots ARE for scaling laws estimated from data subsets of the form max #toks (F, q) for varying q. We find that including full training curves in scaling law estimation can predict losses well. In fact, relying merely on the end of training produces significantly worse performance across the board. Our remaining experiments thus fit scaling laws using all these intermediate checkpoints, and not final performance alone.",
            "score": 0.5104679592382955,
            "section_title": "I HAVE SOME DATA, WHAT PORTIONS SHOULD I USE?",
            "char_start_offset": 17498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 189,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "A scaling law estimates the loss of a costly model by training cheaper ones (see Fig. 2) which share a pretraining procedure and differ by some hyperparameters, typically model size (#params) and number of tokens seen during training (#toks). A scaling law is a function that predicts a target model's loss on held-out data when setting the value of one hyperparameter (Kaplan et al., 2020) or both (Rosenfeld et al.;Hoffmann et al., 2022). Comparing laws' predictions about different pretraining choices (e.g. data Ge et al., 2024) allows informed decisions about which large-scale model to train. \n\nMod el #pa ram s \n\nFigure 2: Illustration of a scaled family, an estimated scaling law, and its prediction error for a target model. \n\nA scaling law also enables finding the optimal choice of hyperparameters under computational constraints on pretraining (Hoffmann et al., 2022) or inference (Touvron et al., 2023;Sardana et al.). \n\nFormally, we will call a model f any single concrete neural language model with a specific set of parameters. Different seeds, or even different checkpoints from the same training run, correspond to different models. We define a scaled model family f as a set of models, with each f \u2208 F differing only in size #params(f ) and number of tokens #toks(f ). \n\nThere are two specific subsets of scaled model families that will be useful in our experiments. First, the maximal parameter family max #params (F ) contains only models in F with the largest number of parameters. Formally, define m = max f \u2208F #params(f ); then max # params(F ) =  (3) the size of the largest model trained (expressed as a scale-up factor-the ratio between the target model and the largest model in F train ). In (a), all laws are estimated from four models. In (c) all laws use the full corpus. Orange lines show iso-FLOP contours (sets of scaling laws whose training sets require the same computational cost to produce). \n\nrepresent the most efficient ways to obtain 15%, 10% and 5% ARE.",
            "score": 0.5087431074980545,
            "section_title": "DEFINING A SCALING LAW",
            "char_start_offset": 4633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 417,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 915,
                    "end": 930,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "278367792",
            "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
            "text": "Early model selection relied on feature similarity methods (Vu et al., 2020;Dwivedi & Roig, 2019) to predict transfer performance by comparing source and target tasks. However, these approaches couldn't capture the complex fine-tuning dynamics revealed by our NTK-based analy-sis, particularly the pre-power and power phases. Recent work has introduced training-free transferability metrics, such as LogME (You et al., 2021), which attempt to predict model performance without additional fine-tuning. While these approaches reduce computational overhead, they fail to account for the dynamic nature of fine-tuning that our theoretical framework explicitly models. \n\nThe development of scaling laws has been crucial in understanding LLM performance during pre-training. Kaplan et al. (2020) established foundational power-law relationships between model size, dataset size, and performance. Extensions of these scaling laws (Ghorbani et al., 2021) demonstrated their predictive power across various scales. \n\nRecent work by Isik et al. (2024) has further advanced our understanding by developing scaling laws specifically for downstream task performance, showing how different tasks exhibit distinct scaling behaviors. However, these relationships become insufficient during fine-tuning due to the emergence of distinct phases with different governing dynamics. Recent attempts to adapt scaling laws to finetuning, such as Rectified Scaling Law (Lin et al., 2024), have shown promise by incorporating dataset properties and task-specific factors. Zhang et al. (2024) provides a comprehensive analysis of how data quantity, model size, and fine-tuning methods interact during LLM adaptation, revealing complex trade-offs that traditional scaling laws fail to capture. However, these approaches fall short in lowresource scenarios where the pre-power phase dominates. \n\nOther recent methods, including learning-to-rank and transfer-learning-based approaches (Ji et al., 2024;Hu & Zhang, 2023), have focused on improving model selection efficiency through meta-learning strategies. While these methods show promise in specific contexts, they often assume static model behaviors and fail to account for the dynamic scaling interactions.",
            "score": 0.5084832767158093,
            "section_title": "Related Work",
            "char_start_offset": 20050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 76,
                    "matchedPaperCorpusId": "218487733"
                },
                {
                    "start": 1023,
                    "end": 1041,
                    "matchedPaperCorpusId": "267499809"
                },
                {
                    "start": 1546,
                    "end": 1565,
                    "matchedPaperCorpusId": "268032247"
                },
                {
                    "start": 1972,
                    "end": 1989,
                    "matchedPaperCorpusId": "272596905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41943359375
        },
        {
            "corpus_id": "267411930",
            "title": "Towards Neural Scaling Laws on Graphs",
            "text": "The neural scaling law [1][2][3] serves as an important tool in the development of large models. It quantitatively describes how model performance grows with the scale of training, which have two basic forms: data scaling law (how the performance changes with training dataset size) and model scaling law (how the performance changes with model size). Neural scaling laws guide the construction of large models from the following perspectives: (1) They enable us to predict model performance on large-scale training by extrapolating from small-scale runs with lower computational budgets. For instance, the performance of GPT-4 on coding problems fits accurately with the prediction of the scaling law [4]. (2) They enable us to identify better model architectures through small-scale training, typically selecting those with the largest performance gains for a given computational overhead. (3) They serves as the guidance for resource allocation, i.e., they could tell us how to distribute the cost of data collection and computation for the given model to reach the expected performance. Such laws have played an instrumental role in the recent successes of large models in natural language processing (NLP) and computer vision (CV) domains [5][6][7][8][9]. Despite the success of foundation models in NLP and CV, there is no existing effort investigating the neural scaling law in the graph machine learning domain. As an initial exploration, we found that such an investigation would meet unique challenges given the irregularity and diversity of graphs. Specifically, both the number of nodes and edges of a graph can vary dramatically, different from the fixed-structure figures or texts in CV and NLP. As an example shown in Figure 1, in the OGBG-PPA dataset of 158k graphs [10], the number of nodes in each graph ranges from 25 to 325, and the number of edges from 54 to 18, 096, indicating significant variability across the dataset. The variability in size suggests that the current metric (the number of graph samples) insufficiently represents the data volume, hinting at the need for more proper data metrics of the graph scaling laws. Moreover, the irregularity of graphs requires the deep graph models to introduce the message aggregation mechanisms.",
            "score": 0.507412472229114,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1253,
                    "end": 1256,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1256,
                    "end": 1259,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "53751796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "271571036",
            "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
            "text": "In parallel, empirical scaling laws related to LLM performance have been the subject of extensive investigation in recent research (Hestness et al. 2017;Kaplan et al. 2020). These studies have found a power-law scaling relationships with model size, dataset size, and computational resources, i.e. an increase in either almost always leads to a decrease in loss. Recently, Hoffmann et al. (2022) have also led to a clearer understanding of the tradeoffs between training data size (number of tokens) and model parameters, yielding a unified formula for compute-optimal training, which has already been applied to specific model settings (Clark et al. 2022;Gordon, Duh, and Kaplan 2021;Henighan et al. 2020;Tay et al. 2022). However, it is noteworthy that the scalability of LLMs does not universally translate to improved performance across all downstream tasks, as demonstrated by Ganguli et al. (2022). Similarly, recent work by Wei et al. (2022) highlights emergent abilities unique to larger models not predicted by traditional scaling laws. In response to work in scaling laws, there has also been pushback from critics. Notably, Bender et al. (2021) highlighted the rising environmental and financial costs of model pre-training, and the lack of diversity in training data. Closely related to our study, Birhane et al. (2023) study scaling laws in the context of hateful content present in the LAION family of datasets, popularly used to pre-train text to image diffusion models. They find that as data scale increases, the tendency of models to associate Black faces with categories like \"criminal\" can significantly increase. \n\nOur work lies at the intersection of these research threads. We contribute to the ongoing practice of measuring the adverse outcomes of machine learning systems. Our work also contributes to ongoing work on scaling laws, with a specific focus on bias, and how it is picked up from pre-training data.",
            "score": 0.5057854810248259,
            "section_title": "Related Work",
            "char_start_offset": 12418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 637,
                    "end": 656,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 656,
                    "end": 685,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 882,
                    "end": 903,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 1135,
                    "end": 1155,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2401123046875
        },
        {
            "corpus_id": "273233584",
            "title": "Collective variables of neural networks: empirical time evolution and scaling laws",
            "text": "Previous work has established various scaling laws for neural networks, often relating to the number of parameters and the expected performance [Bahri et al., 2024]. However, these descriptions are predominantly phenomenological and do not fully capture the dynamics of different architectures or the nuances of the learning process. In Bahri et al. [2024], neural scaling laws are studied in a so-called resolution-limited and variance-limited regime, highlighting changes in the network performance within each. While this work highlights a useful relationship between the data manifold and neural scaling, it does not address the fundamental question of whether learning mechanisms within each regime change at all. Leclerc and Madry [2020] identify regimes during training and relate them to a learning-rate dependent mechanism, namely, large-step and small-step regimes. They highlight that the process by which the network learns information differs between these two regimes and can be leveraged more effectively when considered as two separate training phases. In their 2021 paper, Geiger et al. [2021] explored the role of initialization in over-parameterized neural networks and how it changes the training process of the neural network. The problem has also been explored with more practical uses in finding, for example, in Mirzadeh et al. [2020] learning regimes, as impacted by batch size, learning rate, and regularization, is explored in relation to catastrophic forgetting, or the seminal work of Kaplan et al. [2020] where scaling laws are introduced as a means to optimize a cost-performance tradeoff in large language models. The NTK has emerged as a key concept for understanding these dynamics, particularly in the context of wide neural networks where it facilitates a tractable analysis of gradient descent dynamics. Often, this work studies alignment effects in the NTK and what role these play in the learning taking place [Atanasov et al., 2021], or in identifying dynamics regimes [Lewkowycz et al., 2020]. Indeed, a landmark result utilizing theoretical scaling laws studies came in the form of \u00b5-transfer [Yang andHu, 2022, Yang et al., 2022], that can be used to initialize width-scaled networks without losing accuracy. \n\nEach of these studies highlights the importance of scaling laws and network dynamics in the field's current state.",
            "score": 0.5056697463906379,
            "section_title": "State of the Art",
            "char_start_offset": 1538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2251
                },
                {
                    "start": 2254,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 164,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 1090,
                    "end": 1110,
                    "matchedPaperCorpusId": "234848577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36328125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored. To address this gap, we reviewed over 50 research articles (Figure 1 highlights papers on scaling laws on different topics) to comprehensively analyze scal-  Table 1: Key differences between our survey and existing surveys on neural scaling laws (Choshen et al., 2024;Li et al., 2024b). \n\ning laws, examining their validity across different domains and architectures. While prior surveys have made valuable contributions to understanding scaling laws, they have primarily focused on specific aspects of the scaling phenomenon (See Table 1). Choshen et al. (2024) emphasized statistical best practices for estimating and interpreting scaling laws using training data, while Li et al. (2024b) emphasized on methodological inconsistencies in scaling studies and the reproduction crisis in scaling laws. Our survey distinguishes itself by offering comprehensive coverage of architectural considerations, data scaling implications, and inference scaling -areas that previous surveys either overlooked or addressed only partially.",
            "score": 0.5050389877887302,
            "section_title": "Introduction",
            "char_start_offset": 2160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "270068102",
            "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
            "text": "However, they did not investigate the relation of weight averaging to compute optimality and its implications for scaling experiments. \n\nScaling Law Experiments for Neural Language Models. Kaplan et al. (2020) were the first to establish scaling laws for language models by training a suite of models for a fixed token count. Important to our work, Hoffmann et al. (2022) revise their laws and demonstrate specific methods to establish laws, notably training a family of models for different cosine lengths. The subsequent models like LLama and LLama2 (Touvron et al., 2023a,b) further improve performance of smaller models by training beyond the Chinchilla optimal point, motivated by lower inference costs (Gadre et al., 2024;De Vries, 2023;Sardana & Frankle, 2023). Recent works (Muennighoff et al., 2023;Bi et al., 2024;Goyal et al., 2024) highlight how data repetition and quality affect the scaling behavior, which suggests that scaling laws should be updated more frequently. However, these works do not consider efficient experiments for scaling laws, which is the focus of our work.",
            "score": 0.5042944714542136,
            "section_title": "Related work",
            "char_start_offset": 25271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "The concept of scaling laws for language models was proposed by (Kaplan et al., 2020). Their study revealed that the test loss for generative transformer models scales as a power-law with model size, dataset size, and the amount of compute used for training. Building upon this foundational study (Kaplan et al., 2020), further research has expanded the concept of scaling laws across diverse problem settings (Hernandez et al., 2021) and model architectures (Cherti et al., 2023;Aghajanyan et al., 2023). For instance, (Hernandez et al., 2021) has investigated scaling laws for transfer learning, and discovered that the effective data transferred for a model trained on natural language to a model trained on held-out data also follows a power-law. In the scope of multimodal models, (Cherti et al., 2023) found that scaling behavior differs when trained on different task-dependent datasets in different tasks, and (Aghajanyan et al., 2023) organized a scaling law which unified the contributions and interactions of different modalities during multi-modal pre-training. \n\nDespite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training. By studying the loss behavior on different token positions, we introduce a more accurate temporal scaling law for LLM test loss, allowing for precise tracking and prediction of LLM performance during the pre-training stage. \n\n3 Temporal Scaling Law",
            "score": 0.5039908985968942,
            "section_title": "Scaling Laws for Language Models",
            "char_start_offset": 8141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 480,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 480,
                    "end": 504,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 786,
                    "end": 807,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 918,
                    "end": 943,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "272826711",
            "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
            "text": "These properties help explain why Convolutional Neural Networks (CNNs) often generalize better than Multi-Layer Perceptrons (MLPs), and why deeper CNNs often outperform shallower ones. \n\nHowever, when it comes to scaling, the guiding principles become less clear. One foundamental challenge is the existence of the scaling law crossover phenomenon. This phenomenon makes it unwise to optimize performance for any static scale, as such an approach may only be effective up to a certain point and fail to generalize to larger scales, i.e., it risks overfitting to a finite scale. Unfortunately, scaling law crossover is likely unavoidable in practice. Any observed scaling law represents a specific, and likely suboptimal, trajectory through a vast space of possible scaling strategies. \n\nTo illustrate this, consider training a sequence of transformers with increasing flop budgets f \u2208 N. We can approximate the flops as f \u2248 6N D, where N is the number of parameters and D is the number of data tokens. The loss, L , depends on numerous factors including width (D), layers (L), D, batch size (B), learning rate (\u03b7), weight decay (\u03bb), and others. For simplicity, let's focus on these six, recognizing that N \u2248 12D 2 L for transformers with F = 4D. \n\nWith the constraint f = 6 \u2022 12D 2 L \u2022 D, we have five free variables. A scaling rule, \u03d5, dictates how these variables scale with f : \n\nEach \u03d5 produces a scaling law curve within a 5-dimensional surface. Unless we identify the optimal scaling rule(s), \u03d5 * , that minimize L for all f : \n\nany given scaling law can likely be crossed by another. Since practical scaling rules are often heuristic and unlikely optimal, crossover is to be expected. For example: \n\n\u2022 We did not realize that Kaplan's scaling rule D \u221d f 0.27 (Kaplan et al., 2020) was sub-optimal until the discovery of a better scaling rule D \u221d f 0.5 (Hoffmann et al., 2022). But how do we know that the Chinchilla scaling rule is optimal?",
            "score": 0.5026887987613914,
            "section_title": "Guiding Principles for Scaling",
            "char_start_offset": 40293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1947
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311279296875
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "[cs.LG] 15 Oct 2024 arXiv Version \n\nWe begin by collecting diverse model data to perform a large-scale meta-analysis of scaling laws ( \u00a73). Usually, scaling law research relies on a single collection of closely related models, or alters only a minimal aspect of pretraining (e.g. data size; Muennighoff et al., 2024). Instead, we gather data from as diverse a set of scaled families as possible, to allow this and future meta-analysis of scaling laws that generalize across architectures, datasets and settings. \n\nThe rest of the paper uses this data to analyze a number of key questions around scaling law estimation: \n\n1. How reliably may we expect scaling laws to extrapolate? Variation between random parameter initializations can produce changes of up to 4% in loss. Most published improvements in pretraining procedures, when performing minimal controlled experiments, report loss changes between 4% and 50% ( \u00a74). 2. How much does the shape of scaling laws vary across model families? Different model families have scaling laws with a different functional dependence on model size ( \u00a75). However, transformer LMs are similar enough that, with a single model from a target family and a scaling law from a different model family, it is sometimes possible to accurately estimate target model performance ( \u00a75.1). 3. Must scaling laws be estimated only from fully trained models? Even though optimization procedures are typically sensitive to the full size of a training run, estimating scaling laws from intermediate training checkpoints greatly improves scaling law fit ( \u00a76). It is generally possible to estimate a model's final loss beginning roughly 1 /3 of the way through training. 4. How large must models be to produce reliable scaling estimates? All else equal, experimenting with large models is typically more useful than with small models ( \u00a77), but may be outweighed by the benefits of reduced variance from training more, smaller models ( \u00a78). 5. Taken together, cost-effective estimation of a scaling law should consider the number of models, the size of the models, and the number of training tokens for each model. We highlight those size, tokens and number of models effects in Fig. 1.",
            "score": 0.502568190834771,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 36,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 316,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36767578125
        },
        {
            "corpus_id": "270199496",
            "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
            "text": "We verify that the scaling law accurately describes the mean contribution at each dataset size: for example, we find that our scaling law explains >0.9 of the variance in marginal contributions between data points (see Table 1).We also test whether it can extrapolate beyond the range where it is fit, which would enable the scaling law to be learned in a less costly dataset range before being used in a larger regime.Finally, we explore applications of the scaling laws to data subset selection, where we find that they identify useful new data points that depend on the current dataset size; and we demonstrate their application to data valuation, which we show is closely related to our scaling analysis (Ghorbani and Zou, 2019).\n\nOur contributions.The main contributions of this work are the following: (1) we propose and find evidence for individualized data scaling laws, (2) we show how to fit the scaling behavior using a small number of noisy observations per data point, (3) we provide qualitative understanding of factors that influence scaling behavior, and (4) we demonstrate that individualized scaling laws can facilitate data valuation and data subset selection.Overall, the scaling behavior of individual data points provides a new tool for understanding and improving training data for machine learning models.\n\nRelated work.Scaling laws for deep learning have become well known in recent years (Hestness et al., 2017;Rosenfeld et al., 2019;Kaplan et al., 2020;Hoffmann et al., 2022).They serve several purposes, including reasoning about the trade-offs between increasing training data and model parameters (Kaplan et al., 2020;Hoffmann et al., 2022), predicting the performance of large-scale models (Cherti et al., 2023), and comparing the performance of learning algorithms at manageable scales (Dubois et al., 2023).The most similar works to ours are Hashimoto (2021) and Rolf et al. (2021), which study how model performance scales when training with multiple data sources; our work instead takes the perspective of studying individual data points, which offers a more granular tool for analyzing the contents of a training dataset.",
            "score": 0.5024082583860421,
            "section_title": "Introduction",
            "char_start_offset": 3956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 732,
                    "matchedPaperCorpusId": "102350503"
                },
                {
                    "start": 1721,
                    "end": 1742,
                    "matchedPaperCorpusId": "254636568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "One widely used scaling law is the Chinchilla scaling law by Hoffmann et al. (2024). This law models the relationship between the loss L, the number of parameters N , and the number of training tokens D using the following equation: \n\nwhere A, B, \u03b1, \u03b2, and E are free parameters: A and \u03b1 describe how loss decreases with increasing model size N , while B and \u03b2 describe how loss decreases with increasing training tokens D. The constant E represents an irreducible loss. Scaling laws allow practitioners to optimize training configurations to fit within their compute budgets without running costly experiments (Sardana et al., 2024). The laws also hold theoretical value, as they capture the dynamics of how loss changes with data and parameter scaling. We present an overview of existing sparse scaling law in Appendix A. \n\nModels. We pre-train both sparse and dense models with starting parameter count ranging from 58M to 468M. We use the LLaMA 2 (Touvron et al., 2023) base model architecture. For each unique model size, we train two versions: one using over 10x the number of tokens corresponding to Chinchilla optimal, and the other using over 20x Chinchilla optimal. We train substantially past Chinchilla-optimal dataset size, following prevailing practice (Touvron et al., 2023;Grattafiori et al., 2024) which benefits inference efficiency. \n\nTable 1: Training details for models by size and token count. Numbers in parentheses show the token-to-prunable parameter ratio. \n\nTable 1 provides model details. Each model name contains the parameter count and a suffix indicating training duration: \"10x\" means training tokens exceed 10x Chinchilla-optimal, and \"20x\" means they exceed 20x (Hoffmann et al., 2024). We prune only linear layer parameters, leaving embedding and normalization layers dense. For each dense model, we list its total training tokens and the ratio of tokens to prunable parameters (in parentheses). We train four sparse variants for each dense model, matching their training compute while varying sparsity from 20% to 80%. \n\nDataset.",
            "score": 0.5018202438237749,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 9084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 83,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 611,
                    "end": 633,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1696,
                    "end": 1719,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "275471424",
            "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity",
            "text": "The Scaling Laws in LLMs can primarily be categorized into two types: \n\n\u2022 Pre-training Scaling Laws \u2022 Inference Scaling Laws Kaplan et al. (2020) systematically studied the impact of model parameter scale on language model performance and proposed corresponding Scaling Laws. It focuses on resource optimization during the pre-training process, in other words, to improve model performance through increasing key resources such as data volume, model parameter count, and computational power. The proposal of such Scaling Laws also laid an important foundation for training and optimization of LLMs. Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks.",
            "score": 0.4995514562166905,
            "section_title": "Two Types of Scaling Laws",
            "char_start_offset": 13784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 72,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "276776461",
            "title": "Scaling Law Phenomena Across Regression Paradigms: Multiple and Kernel Approaches",
            "text": "The scaling behavior of deep learning models concerning compute, sample size, and model size has been a central topic in machine learning research [HNA + 17, RRBS19, BMR + 20, HKK + 20, HBM + 22, ZKHB22, MRB + 23]. A pivotal contribution by [KMH + 20] revealed that generalization error in transformers decays as a power law with respect to these three factors. Their work provided joint formulas predicting how model performance improves with increased compute, data, and parameters. This finding highlighted the importance of scaling models and led to a surge in interest in large-scale models for NLP and other tasks. \n\nFurther research refined these scaling laws. For instance, [HBM + 22] proposed the Chinchilla law, which suggested that under compute constraints, balancing data size with model size yields optimal performance, contrasting with earlier models that focused primarily on increasing parameter counts. Meanwhile, [MRB + 23] introduced methods for data reuse across training passes, showing that efficient data handling can maintain performance while lowering the need for excessive compute resources. These empirical studies have provided clearer guidelines for allocating resources effectively when training large models. \n\nIn parallel, theoretical work has advanced understanding of these scaling laws. [SK20] demonstrated that, in regression, the generalization error scales as n \u22124/d , linking model performance to the intrinsic dimensionality of the data. Additionally, other researchers, such as [BDK + 21] and [BAP24], applied statistical physics to derive scaling laws for linear models, particularly under infinite parameter regimes. Theoretical contributions like these complement empirical findings and offer deeper insights into how models behave as they scale, shaping future directions in machine learning research.",
            "score": 0.49942297632998583,
            "section_title": "Scaling Laws",
            "char_start_offset": 4507,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "267411718",
            "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
            "text": "Predicting L(FT(M, S)) using a subset S sub is closely related to understanding the scaling behavior in the fine-tuning stage. Indeed, the Scaling Law in the pre-training stage has been widely studied (Henighan et al., 2020;Kaplan et al., 2020;Bahri et al., 2021), and it is commonly believed to have the form below. Definition 2.2 (Power-law in (Kaplan et al., 2020)). The scaling loss L(\u2022, \u2022) is a function of model size N and training set size D, i.e., \n\nHere {A, B, \u03b1, \u03b1 N , \u03b2} are universal parameters to be fitted, and we always use L to indicate that this is an estimated function of true losses. While it is universally observed in many tasks and domains when training models from scratch (Ghorbani et al., 2021;Alabdulmohsin et al., 2022;Fernandes et al., 2023), Tay et al. (2021) finds that the scaling behavior may differ in the fine-tuning phase. As shown in Figure 2, fine-tuning loss is dependent on models not only through their sizes N , but also through other inductive bias like model architecture, the number of layers, attention heads, hidden dimensions and so forth. \n\nThis observation makes it highly non-trivial to select a model using existing Scaling Law. For instance, Equation (4) implies that models with more parameters work better under the same S, which is contradictory to Figure 2. Fortunately, as our goal is to predict performance for each model, a marginal version of the Scaling Law when the model is fixed is sufficient. In this case, the complexity of model architectures can be removed, and the law proposed in (Kaplan et al., 2020;Hernandez et al., 2021;Tay et al., 2021) share the following unified form: \n\nHere D is the number of training data, and B, E, \u03b1, \u03b2 are model/task-dependent parameters. All parameters in Definition 2.2 are non-negative, and so as in Equation (5).",
            "score": 0.4981446782443271,
            "section_title": "Connecting to Scaling Law",
            "char_start_offset": 7885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 747,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 747,
                    "end": 770,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "270045429",
            "title": "AstroPT: Scaling Large Observation Models for Astronomy",
            "text": "We find ourselves in a new era of connectionism.This era is dominated by large language models trained on web-scale data, with a rapid parameter growth fertilised by the discovery of predictable 'neural scaling laws'-power laws that can be used to estimate a model's performance given its parameter count and training set size (Cortes et al., 1993;Kaplan et al., 2020).In the language domain, the law that currently best describes model scaling was introduced in Hoffmann et al. (2022).This 'Chinchilla' scaling law established that for every additional neural parameter added to the network, around twenty additional textual data tokens must also be added to train a model in a training compute optimal regime.It is also often wise to 'overtrain' a model-or train at a token to parameter ratio larger than 20:1-which leads to a more performant model at a lower parameter count and therefore reduces compute cost at inference time.Overtraining is both environmentally and economically beneficial if a model's total lifetime compute costs are inference heavy (Touvron et al., 2023a;b;Zhang et al., 2024).The discovery of the Chinchilla scaling law, and the need to minimise compute cost at inference time means that we have nearly exhausted high-quality publicly available reserves of textual data for training large neural models (Sevilla et al., 2022;Xue et al., 2023).Some potential solutions to this 'token crisis' have already been investigated in the literature: for example, one can repeat training dataset epochs multiple times without significant performance degradation (Xue et al., 2023), or one can also turn to the use of synthetic data to generate tokens at scale (Silver et al., 2018;Gunasekar et al., 2023).We believe that multimodality can provide a further solution.\n\nLarge autoregressive models can process and digest tokens of different modalities (Reed et al., 2022).We can therefore build a model that is capable of learning from modalities that have an abundance of tokens, an abundance that is particularly notable in the observational sciences (Smith & Geach, 2023;Smith et al., 2023a).",
            "score": 0.498082651306807,
            "section_title": "On 'Large Observation Models'",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 48,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1722
                },
                {
                    "start": 1722,
                    "end": 1783
                },
                {
                    "start": 1785,
                    "end": 1887
                },
                {
                    "start": 1887,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 348,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 1677,
                    "end": 1698,
                    "matchedPaperCorpusId": "54457125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "Compute scaling laws In standard scaling laws [6, 34-37, 44, 65], the \"scale\" is defined by the compute resources allocated to training LMs, such as the number of training FLOPs C, model parameters N , and training tokens D. Scaling laws are typically formulated as a power-law relationship between LMs' cross-entropy loss L and their compute scale measures. Common functional forms include L(N, D) = a N \u03b1 + b D \u03b2 + e [37,65] or L(C) = c C \u03b3 + h [34,44], where C \u2248 6N D [44] for the Transformer [93]. The parameters {\u03b1, \u03b2, a, b, e} or {\u03b3, c, h} are fitted by training LMs across different compute scales, varying N and/or D, and measuring their loss. Our work differs from compute scaling laws in our goals -compute scaling aims to understand the scaling properties of pretraining, and thus focuses on a single model family and relates downstream performance to directly controllable quantities such as training compute. In contrast, we are interested in scaling laws for downstream, post-training performance, which leads us to consider scaling laws across model families and use more directly observable capability measures than compute. Downstream scaling laws Scaling laws have been generalized beyond pretraining loss to analyze transfer learning [1, 35,85] and downstream performance [15,30,34] across various domains, see Villalobos [94] for a comprehensive review. In particular, there has been evidence suggesting that the few-shot performance of LMs on downstream benchmarks is closely tied to compute measures like model size [13], but whether this is predictable with scaling laws remains debated. Extensive research has explored the difficulties of predicting benchmark performance due to their appearing rapid \"emergence\" [27,83,98], while recent works argued the discontinuity is due to the metrics used [60,79] or the lack of data points [38] (see Anwar et al. [4] for a survey on this topic).",
            "score": 0.4977412557848032,
            "section_title": "Related Work",
            "char_start_offset": 7079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 423,
                    "end": 426,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "250920512"
                },
                {
                    "start": 1538,
                    "end": 1542,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "269983385",
            "title": "Neural Scaling Laws in Robotics",
            "text": "Secondly, the foundation model approach integrates pre-trained large models like GPT-4 and PaLM-E into robotics, leveraging their semantic reasoning and bridging language with physical action (Driess et al., 2023;Brohan et al., 2023;Chen et al., 2023b;Vemprala et al., 2024;Zeng et al., 2023;Yang et al., 2024).Both strategies face challenges, including data scarcity, real-time performance, and bridging simulation to real-world deployment (Brohan et al., 2022;Firoozi et al., 2023;Hu et al., 2023;Sun et al., 2023).\n\nNeural Scaling Laws Neural scaling laws are empirical principles describing the relationship between a model's quality and its size, the size of its training dataset, and its compute budget.These laws follow a power-law function with often a cross-entropy objective, suggesting that as model size and training resources increase, the quality of the model improves predictably.Scaling laws have been extensively researched across various domains within machine learning, such as LLMs, image and video generative modeling and reinforcement learning (Kaplan et al., 2020;Henighan et al., 2020;Hilton et al., 2023).As models scale, they not only improve quantitatively but also demonstrate new qualitative capabilities, termed emergent capabilities (Wei et al., 2022;Schaeffer et al., 2024).Although these scaling patterns offer high predictability in performance improvement, specific capabilities of these scaled models often remain unpredictable (Ganguli et al., 2022).Data constraints also play a role, limiting performance gains for large models trained on fixed-size datasets.Scaling law research is rapidly growing as seen in figure 1a, and while we have made substantial progress, it keeps to be an intensely researched field with many open questions in both width and depth 1 .\n\nNeural Scaling Laws in Embodied AI While scaling laws have been studied in areas like language models and computer vision, their investigation in the embodied AI domain remains limited.Existing work indicates that scaling phenomena hold true for embodied AI in terms of compute, model size and data input, with larger models exhibiting better performance.",
            "score": 0.4976100067385607,
            "section_title": "Introduction",
            "char_start_offset": 3835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 517
                },
                {
                    "start": 519,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1801
                },
                {
                    "start": 1803,
                    "end": 1988
                },
                {
                    "start": 1988,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 274,
                    "matchedPaperCorpusId": "259141622"
                },
                {
                    "start": 1282,
                    "end": 1305,
                    "matchedPaperCorpusId": "258418299"
                },
                {
                    "start": 1464,
                    "end": 1486,
                    "matchedPaperCorpusId": "246867298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "275336720",
            "title": "The interplay between domain specialization and model size",
            "text": "The work by Kaplan et al. (2020) proposes a powerlaw relationship between performance, training data, and model parameters. By extending the analysis from Kaplan et al. (2020) with a broader range of models, data, and hyperparameter tuning, Hoffmann et al. (2022) found that model size and token quantity scale together to achieve compute optimal training. The Chinchilla-70B, trained on 1.4 trillion tokens, outperforms models with similar compute budgets but less data, supporting their scaling hypothesis. This proposed power-law remains widely accepted in the literature, supporting subsequent studies that explore scaling laws for LMs in different scenarios, including our own. Hernandez et al. ( 2021) explore transfer learning under data-constrained regimes through domain specialization with code data, starting from a model pretrained on text. They propose a power-law for data transfer, demonstrating that pretraining offers compute benefits by leveraging prior knowledge acquired during the pretraining process. However, with less data constraint, transfer learning can hinder performance due to the ossification of prior knowledge. While the work by Hernandez et al. (2021) shares certain limitations with Kaplan et al. (2020), such as the lack of learning rate tuning for each token budget (Hoffmann et al., 2022), the authors observed how different training regimes can lead to changes on classic scaling laws. Our study explores a similar scenario, focusing on the interplay between domain-specific and general training as model size increases, rather than limiting the analysis to transfer learning within a single domain. \n\nScaling laws for Transformer LMs (Hoffmann et al., 2022) have proven replicable across multiple studies (DeepSeek-AI et al., 2024;Muennighoff et al., 2023;Gu et al., 2024;Ye et al., 2024;Que et al., 2024). Variations in training regimes can influence these scaling laws, as observed in dataconstrained settings (Muennighoff et al., 2023) and scenarios involving high-quality data (DeepSeek-AI et al., 2024). Our study focuses on domainspecific continued pretraining.",
            "score": 0.49737239518921583,
            "section_title": "Scaling laws",
            "char_start_offset": 7129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 1771,
                    "end": 1796,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1796,
                    "end": 1812,
                    "matchedPaperCorpusId": "271404362"
                },
                {
                    "start": 1952,
                    "end": 1978,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "277940458",
            "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
            "text": "Previous studies have primarily focused on exploring the relationship between large models and large datasets within upstream tasks without considering their applicability to downstream tasks. Recently, however, several works have investigated scaling laws in the context of downstream applications. \n\nHernandez et al. [16] analyzes scaling laws in both pretraining and fine-tuning, introducing the concept of effective data transferred, which quantifies the efficiency of pretraining data in downstream performance. Additionally, Geiping and Goldstein [11] examines the downstream capabilities of a Transformer-based language model trained entirely from scratch within a single day on a single GPU, leveraging scaling laws as a framework for evaluation. Furthermore, [2,31,45] have identified that in compute-optimal training, model size and the number of training tokens should be scaled proportionally. More recently, studies such as [27,41,44] have aimed to enhance downstream task performance by expanding the size of training datasets. \n\n3 Scaling Laws for Transfer Learning 3.1 Data and Model Size Scaling Methodology Data size variation. To systematically study the impact of data scale on model performance, we employ uniform sampling methods for both upstream and downstream datasets. While maintaining the same number of categories, we sample 5%, 10%, 25%, 33%, 50%, 70%, and 100% of the examples from both pre-training and finetuning datasets. This granular sampling approach allows us to precisely capture the relationships between data volume and model performance at different scales. By systematically varying both upstream and downstream data quantities, we can comprehensively fit the scaling laws specifically tailored for downstream tasks. This methodology enables us to quantify how pre-training data efficiency translates to downstream task performance and identify optimal data allocation strategies. \n\nModel size variation. We maintain a fixed size of 64 for each attention head in the Deit [40] model, and vary the number of heads to adjust the overall model size. Specifically, we implement configurations with 2, 4, 6, and 8 attention heads. This approach is based on the finding from [20] that compared to the total number of parameters, the specific shape of the model has negligible influence on the performance of large transformers. This methodology allows us to systematically investigate how model capacity affects performance across different data regimes.",
            "score": 0.4958334446999357,
            "section_title": "Scaling Laws for Downstream Tasks",
            "char_start_offset": 9028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1923
                },
                {
                    "start": 1926,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2364
                },
                {
                    "start": 2365,
                    "end": 2491
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "255185900"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "259145047"
                },
                {
                    "start": 944,
                    "end": 947,
                    "matchedPaperCorpusId": "267334725"
                },
                {
                    "start": 2015,
                    "end": 2019,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "whether the behavior will continue for especially large scales N and T . In fact, there are a number of details leading to the behavior (1) that are so far implicit in this discussion and should be made explicit. For instance, the fit of the exponents, and the relative scaling, (6), can depend on the details of the learning algorithm; 8 and to find the fit (1), the authors of [18] needed to regularize their models, e.g., by using early stopping. Most importantly, in [18] the authors trained on a specific natural dataset: an extended WebText dataset built from human language [42]. Thus, by identifying the mechanism that leads to such scaling laws, we will see that they arise for far more general dataset-model combinations.\n\nIn the rest of this section we will discover the properties of the data ( \u00a72.1) and model ( \u00a72.2) that must go into a minimal modeling scenario that contains a version of (1), including both the scaling law limit and the plateau limit. In particular, we'll explain how the data distribution must have special statistical properties, which we'll identify in natural datasets, and how a machine learning model must transform those statistical properties in a special manner, which we'll see is generically present in nonlinear DNNs.",
            "score": 0.4954921298570274,
            "section_title": "Prerequisites for Neural Scaling",
            "char_start_offset": 17743,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1177978515625
        },
        {
            "corpus_id": "272525111",
            "title": "Optimization Hyper-parameter Laws for Large Language Models",
            "text": "Large Language Models have driven significant AI advancements, yet their training is resource-intensive and highly sensitive to hyper-parameter selection. While scaling laws provide valuable guidance on model size and data requirements, they fall short in choosing dynamic hyper-parameters, such as learning-rate (LR) schedules, that evolve during training. To bridge this gap, we present Optimization Hyper-parameter Laws (Opt-Laws), a framework that effectively captures the relationship between hyper-parameters and training outcomes, enabling the pre-selection of potential optimal schedules. Grounded in stochastic differential equations, Opt-Laws introduce novel mathematical interpretability and offer a robust theoretical foundation for some popular LR schedules. Our extensive validation across diverse model sizes and data scales demonstrates Opt-Laws' ability to accurately predict training loss and identify optimal LR schedule candidates in pre-training, continual training, and fine-tuning scenarios. This approach significantly reduces computational costs while enhancing overall model performance.",
            "score": 0.49538266511848794,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41943359375
        },
        {
            "corpus_id": "276116987",
            "title": "Scaling laws in wearable human activity recognition",
            "text": "Scaling up the size of Transformers has led to significant improvements in performance in language and vision models. We are particularly interested in the scaling laws that link model capacity, pre-training data volume and performance, as this contributes to more principled design. \n\nIn language foundation models, Kaplan et al. (2020) demonstrated that performance improves as pre-training data and model capacity is increased with a power law relationship. Hoffmann et al. (2022) built on this and demonstrated that data and model capacity should be scaled equally. \n\nVision Transformers with masked pre-training Dosovitskiy et al. (2021) have been shown to perform increasingly better as the model size increased (from 86M to 632M parameters), later even up to 22B parameters Dehghani et al. (2023), also verified in He et al. (2022). A saturating power-law linking performance, data and compute was presented in Zhai et al. (2022), similarly to language. \n\nWork exploring scaling laws in HAR is less established but there is also evidence that more pre-training data is beneficial. Yuan et al. (2024) exploited the 700k hour UK Biobank dataset and showed this on a ResNet encoder with 10M parameter. These benefits are also reported in Haresamudram et al. (2022), but only to a point. Dhekane et al. (2023) tried to identify what is the minimum amount of pre-training data which is required, after which a plateau is reached. The authors clarify their intent is not to identify scaling laws, but rather that identifying \"minimal quantities can be of great importance as they can result in substantial savings during pre-training as well as inform the data collection protocols\". None of these work draw explicit scaling laws. Self PAB Logacjov et al. (2024) Logacjov and Bach (2024) varied data and model capacities and scale Transformers to a size of 60M parameters, similar to this present work. However, our work differs from theirs by establishing scaling laws that link model capacity, data, and performance. \n\nScaling laws have been explored in sensor foundation models Abbaspourazad et al. (2024); Narayanswamy et al. (2024).",
            "score": 0.4945864940084975,
            "section_title": "Scaling laws in language, vision and HAR",
            "char_start_offset": 7800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 483,
                    "matchedPaperCorpusId": "208883577"
                },
                {
                    "start": 781,
                    "end": 803,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 822,
                    "end": 838,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 918,
                    "end": 936,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1088,
                    "end": 1106,
                    "matchedPaperCorpusId": "249431804"
                },
                {
                    "start": 1291,
                    "end": 1312,
                    "matchedPaperCorpusId": "263608227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346923828125
        },
        {
            "corpus_id": "269033049",
            "title": "Scaling Laws for Data Filtering\u2014Data Curation Cannot be Compute Agnostic",
            "text": "Some other works include Yu et al. (2023) which uses a mixture of rules and Xu et al. (2023) which uses similarity with downstream metadata.\n\nIn this work, we highlight why data filtering cannot be agnostic to training compute and how the ordering varies as one changes the training paradigm.In fact, we showcase LAION filtering (used to train state-of-the-art OpenCLIP models ) can even underperform no-filtering or training on the raw common crawl under certain settings.\n\nScaling Laws in Language Modeling One of the most salient trends in recent deep learning research is the observation that neural network performance tends to improve predictably with increases in model size, data size, and computation.In the domain of language modeling, such observations have been systematized into a set of principles known as scaling laws.Kaplan et al. (2020) conducted a comprehensive study on scaling laws for neural language models.They observed that, given fixed computational budgets, there exists an optimal model size, training data size, and training time.Interestingly, the triple (model size, data size, batch size) corresponding to the state of the art tends to scale in lockstep, reinforcing the intuition that larger models require more data and more computation to be trained effectively.This observation is corroborated by Hernandez et al. (2021);Hoffmann et al. (2022) who delve deeper into training compute-optimal language models and highlight the importance of balancing computation with model and data sizes.Sardana and Frankle (2023) propose modifications to incorporate the inference cost into the scaling laws.Bahri et al. (2021); Hutter (2021) theoretically study neural scaling laws.\n\nMost closely related to our work, Muennighoff et al. (2023) show that training on tokens beyond four epochs yields negligible gains compared to training on new language data due to diminishing utility.However, they do not consider the case of different data quality pools.In this work, we how that mixture of data pools cannot be modeled with an effective dataset size formulation of Muennighoff et al. (2023).Crucially, one needs to model a decay in utility factor (the scaling parameter b in y = an b ) as well.",
            "score": 0.4937255328368626,
            "section_title": "Related Work",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 142,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 473
                },
                {
                    "start": 475,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1703
                },
                {
                    "start": 1705,
                    "end": 1906
                },
                {
                    "start": 1906,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2115
                },
                {
                    "start": 2115,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32763671875
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
            "score": 0.49301297287461976,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "266818336",
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "text": "Research on scaling laws (Hestness et al., 2017) predates the emergence of large language models. Scaling laws (Henighan et al., 2020;Hoffmann et al., 2022;Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget , model scale , and data scale . When model scale  is represented by model parameters and data scale  by the number of tokens,  can be approximated as  = 6 . Therefore, how to optimize the allocation between model and data scales when increasing the compute budget is also a crucial research objective in scaling laws. \n\nThe development of LLMs (Dai et al., 2019;Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak. Results in scaling laws demonstrate that expanding the compute budget continues to yield significant benefits, which further encourages the increase in model scales (Brown et al., 2020;Smith et al., 2022). \n\nHowever, as shown in Table 4, early works (Hoffmann et al., 2022;Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance. Therefore, we revisit scaling laws in this section to address these uncertainties and ensure we are on the right path to efficiently scaleup compute, which reflects the long-term perspective and is key to developing continuously improving models. \n\nTo ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters. Empirically, it has been observed that the optimal values of most parameters during training do not change when varying compute budgets. Therefore, these parameters are consistent with those outlined in Section 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.",
            "score": 0.4922265012430942,
            "section_title": "Scaling Laws",
            "char_start_offset": 13043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265625
        },
        {
            "corpus_id": "256868751",
            "title": "Cliff-Learning",
            "text": "The use of pre-trained models as a foundation (Bommasani et al., 2021) for learning has resulted in groundbreaking advances in AI. For example, ChatGPT is a finetuned version of GPT-3 (Brown et al., 2020), and Stable Diffusion (Rombach et al., 2022) is built on top of CLIP-type models (Radford et al., 2021). In addition to enabling new capabilities, transfer learning on top of foundation models enables deep-learning to function in scenarios where training data is limited. \n\nOne way to evaluate the effectiveness of transfer-learning is to look at its data-scaling behavior: how fast does test error decay when we increase the amount of (downstream) training data? Hernandez et al. (2021) finds that when the number of training samples n is large (Appendix A), transferlearning test error decays as a power law in n: (1.1) \n\nThis corroborates a large body of work that finds such power laws are very accurate models of data-scaling for neural networks in the large-n regime (Seung et al., 1992;Hestness et al., 2017;Rosenfeld et al., 2019;Kaplan et al., 2020;Hoffmann et al., 2022). \n\nYet, little is known about the low-data regime. We discover that power laws are not an accurate model of transfer learning data-scaling in the low-data regime; instead, performance often improves at a faster than power law rate. On a log-log plot, this shows up as a concave scaling-law (Appendix B). We dub such regions of concavity cliff-learning regions. These regions are arguably the most interesting parts of the data-scaling domain, since they have the highest \"return on data\". \n\nIn Section 2, we investigate cliff-learning with foundation models. In Section 3, we develop toy models for cliff-learning. We discuss implications and open questions in Section 4.",
            "score": 0.4921055472377451,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 1086
                },
                {
                    "start": 1089,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 227,
                    "end": 249,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 286,
                    "end": 308,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 978,
                    "end": 998,
                    "matchedPaperCorpusId": "7394722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "257687665",
            "title": "The Quantization Model of Neural Scaling",
            "text": "Models of neural scaling: Several models of neural scaling laws have been proposed in prior work. Sharma and Kaplan [31] explain power law scaling w.r.t. model parameters using an argument from approximation theory, which relates neural scaling exponents to the dimension of the data manifold d. Michaud et al. [32] point out that effective dimension d could be generalized to the maximum arity of the target function's computation graph for sparse compositional problems. Bahri et al. [33] generalized the model of Sharma and Kaplan [31] to scaling w.r.t. dataset size, additionally relating scaling exponents to the power law spectrum of certain kernels. Maloney et al. [34] develop an exactly solvable random-feature model of scaling, from which they derive a joint parameter-data scaling law. Bordelon et al. [35] develop a model of data scaling for kernels, decomposing the generalization error into a sum over eigenmodes, whereas we decompose error into a sum over quanta. Arguably the closest prior work to ours is Hutter [36], who develops a model of data scaling wherein a discrete set of \"features\" must be learned. In this model, a feature is learned if it occurs at least once in the training set. If the features are Zipfian distributed, this produces power law scaling in expectation but with high variance. In our model, using a data threshold \u03c4 \u226b 1 lowers the variance in the scaling curve, and we also considered scaling w.r.t. parameters and applied the model to real networks. \n\nUnderstanding emergent abilities: Wei et al. [8] and Srivastava et al. [37] document examples of emergent abilities in large language models, though Schaeffer et al. [38] suggest that these examples are an artifact of the metric used to evaluate performance. Arora and Goyal [39] develop a framework for the emergence of \"skills\", where predicting text requires combining multiple different skills from an underlying set of language skills.",
            "score": 0.49061583732776204,
            "section_title": "Related Work",
            "char_start_offset": 28340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "253097834"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "211066351"
                },
                {
                    "start": 1543,
                    "end": 1546,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417236328125
        },
        {
            "corpus_id": "270620190",
            "title": "Reconciling Kaplan and Chinchilla Scaling Laws",
            "text": "Following early works formalizing how language models improve with parameters, data, and training compute (Rosenfeld et al., 2019;Kaplan et al., 2020;Hoffmann et al., 2022), there has been investigation into whether these scaling laws arise in other domains (Henighan et al., 2020), and to explain their existence from a theoretical standpoint (Hutter, 2021;Maloney et al., 2022;Bahri et al., 2024). \n\nCloser in spirit to our paper are several concurrent works that have investigated the influence of various design decisions on scaling law analyses. Su et al. (2024) revisit the methodology used to find scaling coefficients. H\u00e4gele et al. (2024) found that multiple independent cosine schedules could be reproduced more efficiently through a constant learning rate with multiple short decays, or stochastic weight averaging. \n\nOur finding is subtly different; a simple fixed learning rate will recover very similar compute-parameter scaling coefficients as multiple cosine schedules. Bi et al. (2024) study the effect of various hyperparameters on scaling laws. They observe that different text datasets produce slightly different optimal coefficients, with 'cleaner' data leading to more parameter-hungry scaling behavior, which they speculate could partially explain the difference between Kaplan and Chinchilla coefficients. \n\nPorian et al. ( 2024) provide a concurrent work with the same objective as our paper -explaining the differences between the Kaplan and Chinchilla coefficients. Through a set of large-scale experiments reproducing Kaplan's study, they determine that responsibility for the discrepancy can be attributed, in decreasing order of significance, to; 1) Kaplan counting non-embedding rather than total compute. 2) Kaplan using a fixedlength warmup period that was too long for smaller models, making them appear less efficient. 3) Kaplan not fully tuning optimization hyperparameters. We see these findings as complimentary to our own. We have been able to identify the primary 'first-order' reason using only information that was publicly available in the two papers, with a fully analytical approach. (Tiny-scale experiments were run post-hoc as verification.) This illustrates the promise of applying mathematical approaches to the empirical science of scaling.",
            "score": 0.49018643785109056,
            "section_title": "Related work",
            "char_start_offset": 16226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202392578125
        },
        {
            "corpus_id": "252220884",
            "title": "Revisiting Neural Scaling Laws in Language and Vision",
            "text": "Scale has led to innovative research in both the vision domain [10,14,26,35,40] and the natural language processing (NLP) [8,12] domain. Recent work has found that scaling up the data size [34], the model size [26,35], the training schedule [5,39] or all of them together [8,40] often lead to improved performance. More importantly, scaling up the data size and the model size together can better utilize the compute resources. Scaling laws have been properly studied in several works, e.g. [3,[18][19][20]23], and it has been found that the performance f (x) (e.g. excess loss) often follows a power law f (x) \u223c \u03b2x c for some \u03b2 > 0 and c < 0 as one varies a dimension of interest x, such as the data or the model size. \n\nWhile theoretical arguments alone seldom predict scaling law parameters in modern neural architectures [2,21,32], it has been observed that the benefit of scale could be predicted empirically [3,4,9,17,18,20,22,23,28,30,31]. The general approach is to acquire a learning curve, i.e. a collection of samples (x, f (x)), where x is a dimension of interest such as the training data size while f (x) is a measure of performance, such as the validation loss. After that, parameters are estimated, e.g. by computing the best-fitting values of \u03b2 and c in the model f (x) = \u03b2x c . Given the estimated scaling law parameters, one can then extrapolate by predicting f (x) for large values of x. \n\nSuch learning curve extrapolation has found many applications, of which four seem to be more prominent. First, it offers a tool for understanding deep neural networks; e.g. how the architecture and data distribution impact scaling behaviors [1-3, 18, 20, 23, 30, 32]. Second, it has been used 36th Conference on Neural Information Processing Systems (NeurIPS 2022).",
            "score": 0.490161307119962,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1775
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "235376986"
                },
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "214728308"
                },
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "214728308"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 272,
                    "end": 275,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 917,
                    "end": 919,
                    "matchedPaperCorpusId": "35138335"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "580196"
                },
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "51729732"
                },
                {
                    "start": 936,
                    "end": 939,
                    "matchedPaperCorpusId": "7701356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "272826711",
            "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
            "text": "Through three examples, we provided evidence that regularization, either implicit or explicit, may not be necessary for language model pretraining. It may no longer be the main driving principle for understanding pretraining or making informed decisions during training. This raises a crucial question: what are the emerging guiding principles in the scaling-centric paradigm? See Section 6 for a more in-depth discussion. 5 Scaling Law Crossover, a Curse from Scale? \n\nIn the generalization-centric paradigm, the scales we operate on are significantly smaller than those in the LLM setting. This allows us to test new ideas on smaller datasets like CIFAR10 and, if successful, scale them up to ImageNet. Since hyperparameter tuning is feasible even at ImageNet scales, we can readily test our ideas without excessive concern about hyperparameters. Consequently, when proposing new ideas, we often pay little attention to how hyperparameters should evolve with scale, assuming users can perform the tuning themselves. For instance, when introducing alternative architectures (e.g., skip connections, batch normalization), data augmentation techniques, or new optimizers like AdamW, we don't necessarily need to provide guidance on how the learning rate and other hyperparameters should scale with model and dataset sizes. \n\nHowever, in the \"skydiving\" regime, the sheer scale of data and models presents a significant challenge. \n\nTraditional hyperparameter tuning becomes impractical due to the immense computational costs involved. This makes it incredibly difficult and expensive to verify new ideas or compare different approaches at scale. Consider the seemingly simple question of whether a global gradient clipping norm of 1 or 2 is more effective in a 100B parameter model. Answering this question through direct experimentation would require substantial resources and time, highlighting the unique challenges posed by this regime. \n\nIn what follows, we present a new phenomenon, termed scaling-law crossover, where the effectiveness of different techniques reverses at a certain scale. One idea outperforms another below a critical scale, while the opposite holds above it. \n\nWe present three cases of scaling-law crossover with increasing complexity, offering explanations for the first two but leaving the third as an open question that underscores the complexities of scaling.",
            "score": 0.48983023834337336,
            "section_title": "Discussion",
            "char_start_offset": 20148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2313232421875
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "The prosperity of large language models (LLMs) has necessitated the development of scaling laws (Kaplan et al., 2020) to optimize the tradeoff between increasing model size and expanding training data within finite computational resources.Scaling laws empirically study the correlation between model performance and factors including the number of parameters, training tokens, and FLOPs.Previous works (Kaplan et al., 2020;Henighan et al., 2020;Hoffmann et al., 2022;Clark et al., 2022) have established power laws to describe \u22c6 Equal contribution.The corresponding author (Email: zhongyiran@gmail.com).\n\nthese scaling trends.Experiments are typically conducted on smaller models with relatively low training costs.From these observations, regression models are derived to guide the scaling of parameters, data, and computational resources.Establishing these scaling laws is crucial before expanding language models to the scale of LLMs, ensuring predictable results under controllable training costs.Scaling laws have guided the success of many recent LLMs, such as Chinchilla (Hoffmann et al., 2022) and GPT-4 (OpanAI, 2023).It is noteworthy that existing scaling laws are predominantly established for traditional softmax attention transformers (Vaswani et al., 2017).\n\nLinear complexity language models (Katharopoulos et al., 2020;Qin et al., 2022b;Choromanski et al., 2021;Zheng et al., 2022Zheng et al., , 2023;;Hua et al., 2022;Liu et al., 2022;Qin et al., 2023bQin et al., , 2024c,d;,d;Gu et al., 2021aGu et al., , 2020Gu et al., , 2022;;Fu et al., 2022;Qin et al., 2023a;Fu et al., 2023;Orvieto et al., 2023;Qin et al., 2023dYang et al., 2023;Gu and Dao, 2023;Dao and Gu, 2024;Sun et al., 2023), have emerged as a promising alternative to traditional transformers in causal language modeling.",
            "score": 0.48960041312841035,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 603
                },
                {
                    "start": 605,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1127
                },
                {
                    "start": 1127,
                    "end": 1271
                },
                {
                    "start": 1273,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 1248,
                    "end": 1270,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "253116832",
            "title": "Scaling Laws Beyond Backpropagation",
            "text": "Can alternative training methods accelerate neural network training? Surveying the current state-of-the-art, one may find numerous claims of alternative training methods achieving competitive performance with BP across a variety of settings and tasks (e.g., [12,18,24]). \n\nWe seek to study this claim, with three restrictions in scope: \n\n1. We focus on Direct Feedback Alignment [25], due its simplicity and wide applicability [12], as well as its broad hardware prospects [14,26,27], and theoretical background [28]. \n\n2. We study compute-efficiency specifically (i.e, best performance achievable for a given compute budget), as this usually a significant bottleneck for scaling-up models. \n\n3. We conduct our study on \"GPT-like\" [29] causal decoder-only Transformers trained on English data. These models are known to possess smooth scaling laws [19,30]. Because of their unique abilities [31], they also command some of the largest training budgets in machine learning [32], making them a prime target for more compute-efficient training. \n\nThese restrictions lead us to test the following hypothesis: \n\nHypothesis. Direct Feedback Alignment can train causal decoder-only models more efficiently than backpropagation, achieving better performance for a given compute budget. \n\nScaling laws as a holistic empirical tool. Scaling laws have been proposed as an empirical approach to connect hyperparameters of neural networks (e.g., parameter count, training dataset size) to their performance. They have been derived both on specific downstream tasks [33,34] and on upstream modeling loss [19]. Scaling laws can characterize the influence of data & modeling decisions [21,22], or even unveil new, more optimal training practices [35,36]. \n\nAs illustrated in Figure 1, it is possible to derive a so-called compute optimal frontier for a class of models: this defines L(C), the best performance L achievable for a compute budget C. We fit a power-law L(C) = (C c C) \u03b1 C over the Pareto front of multiple runs, as proposed in [19]. C c is a constant offsetting the frontier, while \u03b1 C controls the slope.",
            "score": 0.48905421553514145,
            "section_title": "Framing",
            "char_start_offset": 2719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "219981351"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "57189514"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "2843914"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "219981351"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "221896106"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "235390926"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 1673,
                    "end": 1676,
                    "matchedPaperCorpusId": "246608156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "273323602",
            "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
            "text": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra. For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.",
            "score": 0.48870009032569084,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "246823711",
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "text": "Scaling laws in transformers Since Kaplan et al. (2020) demonstrated scaling laws for transformerbased language models, researchers have been investigating the extent of this phenomenon. Tay et al. (2022) investigated the effect of inductive bias on scaling laws and showed how different architectures affect the emerging scaling laws. Tay et al. (2021) showed that model shape matters, and that pretraining and finetuning losses are not necessarily correlated. Contrary to Kaplan et al. (2020), they showed that shape also plays a role in finetuning performance, rather than size alone. Hernandez et al. (2021) focused on python code, showing a trade-off between data and compute. Ghorbani et al. (2021)  An important line of work was dedicated to explaining the emergence of scaling laws in neural models (Hashimoto, 2021). Bahri et al. (2021) connect the scaling exponent to the intrinsic dimension of the data-manifold realized by trained networks representations. Bordelon et al. (2020) and Bisla et al. (2021) connect scaling behavior to the spectrum of the kernel in the related NTK model. Theoretical explanations for neural scaling laws include analogy to kernel methods (Spigler et al., 2020;Bordelon et al., 2020), nearest neighbors methods (Sharma and Kaplan, 2022;Bisla et al., 2021), or a combination of the two (Bahri et al., 2021).\n\nHarnessing scaling laws for model design Rosenfeld et al. (2020) performed small-scale experiments to approximate the generalization error of larger models with a functional form accounting for model and dataset sizes. However, they focused on pretraining, while we also investigate the predictive power on downstream language tasks. Hashimoto (2021) used scaling laws to predict the optimal composition of a training set from dif-ferent data sources. Kirstain et al. (2021) investigated the effect of parameter count and data size on improving performance on various language tasks, while Johnson et al. (2018) designed a performance extrapolation task to estimate how much training data is needed to achieve the required performance.\n\nA parallel line of work that tries to extrapolate optimal architectures based on small scale experiments is Neural Architecture Search (Zop",
            "score": 0.48859099359876057,
            "section_title": "Related Work",
            "char_start_offset": 23453,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 824,
                    "matchedPaperCorpusId": "235826265"
                },
                {
                    "start": 1683,
                    "end": 1699,
                    "matchedPaperCorpusId": "235826265"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "246608156",
            "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture",
            "text": "Scaling up the amount of data used for training has emerged as a robust way to make progress on various tasks in deep learning (Krizhevsky et al., 2012;Brown et al., 2020;Kolesnikov et al., 2020;Thoppilan et al., 2022). On the other hand, research on new architectures and data filtering methods that beat fixed dataset benchmarks continues to grow. However, it is unclear how many of these improvements in the architecture & data quality translate to improvements in the sample efficiency, or the rate with which these models learn from increasing amounts of data. \n\nRecent work (Hestness et al., 2017;Rosenfeld et al., 2019;Kaplan et al., 2020) on scaling laws offers a useful tool to answer this question -they show that the test loss of a model scales predictably as a power law in the relevant quantities of interest such as dataset size (D). Thus, the sample efficiency of a training algorithm over many orders of magnitudes of data can be captured succinctly with a scaling law. \n\nWe take these findings a step further -we conduct a largescale study in Neural Machine Translation (NMT) to understand how different interventions to the training setup impact the data scaling laws (See Figure 1 for a summary of our results). In particular, we take the most common and effective tools used to improve performance, architecture and sources/quality of data, and investigate how changing them affects the data scaling law. In particular, we make the following contributions: \n\nOur Contributions: We first establish that the test logperplexity of encoder-decoder transformer models trained on an English \u2192 German translation task evolves as a smooth function of the dataset size, with a dependence on the model size (Figure 1A). We demonstrate that our scaling law predicts experimental results over 3 orders of magnitude of training examples ( (from 500K-512M sentence pairs) 1 . Then, we systematically vary the following aspects of the training setup to understand how they impact scaling laws: \n\nArchitecture: We compare encoder-decoder transformers of different shapes, transformer-LSTM hybrids and decoderonly transformers. We also verify our results for a different language pair (Chinese \u2192 English).",
            "score": 0.48796658354283395,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 152,
                    "matchedPaperCorpusId": "195908774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "Several works have found that this fails to capture the scaling laws of deep networks in the feature learning regime (Fort et al., 2020;Vyas et al., 2022;2023a;Bordelon et al., 2024a). A theory of scaling laws that can capture consistent feature learning even in an infinite parameter N \u2192 \u221e limit is especially pressing given the success of mean field and \u00b5-parameterizations which generate constant scale feature updates across model widths and depths (Mei et al., 2019;Geiger et al., 2020;Yang & Hu, 2021;Bordelon & Pehlevan, 2022;Yang et al., 2022;Bordelon et al., 2023;2024b) Figure 1: Our model changes its scaling law exponents for hard tasks, where the source is sufficiently small \u03b2 < 1. (a) The exponent \u03c7(\u03b2) which appears in the loss scaling L(t) \u223c t \u2212\u03c7(\u03b2) of our model. (b)-(c) Phase plots in the \u03b1, \u03b2 plane of the observed scalings that give rise to the compute-optimal trade-off. Arrows (\u2192) represent a transition from one scaling behavior to another as t \u2192 \u221e, where the balancing of these terms at fixed compute C = N t gives the compute optimal scaling law. In the lazy limit \u03b3 \u2192 0, we recover the phase plot for \u03b1 > 1 of Paquette et al. (2024). At nonzero \u03b3, however, we see that the set of \"hard tasks\", as given by \u03b2 < 1 exhibits an improved scaling exponent. The compute optimal curves for the easy tasks with \u03b2 > 1 are unchanged. \n\ninfinite width/depth limits in such models can significantly differ from the lazy training regime. \n\nInfinite limits which preserve feature learning are better descriptors of practical networks Vyas et al. (2023a). Motivated by this, we ask the following: \n\nQuestion: Under what conditions can feature learning improve the scaling law exponents of neural networks compared to lazy training regime? \n\n1.1 OUR CONTRIBUTIONS \n\nIn this work, we develop a theoretical model of neural scaling laws that allows for improved scaling exponents compared to lazy training under certain settings. Our contributions are 1.",
            "score": 0.4867893561295763,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 136,
                    "matchedPaperCorpusId": "225094694"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "62841577"
                },
                {
                    "start": 471,
                    "end": 491,
                    "matchedPaperCorpusId": "209926574"
                },
                {
                    "start": 533,
                    "end": 551,
                    "matchedPaperCorpusId": "248505757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "261951743",
            "title": "Baichuan 2: Open Large-scale Language Models",
            "text": "Neural scaling laws, where the error decreases as a power function of training set size, model size, or both, have enabled an assuring performance when training became more and more expensive in deep learning and large language models. Before training the large language models of billions of parameters, we first train some small-sized models and fit a scaling law for training larger models. \n\nWe launched a range of model sizes going from 10M to 3B, ranging from 1 1000 to 1 10 the size of the final model, and each of the model is trained for up to 1 trillion tokens, using consistent hyperparameters and the same data set sourced from Baichuan 2. Based on the final loss of different models, we can obtain a mapping from the training flops to the target loss. To fit the scaling law of the model, we employed the formula given by Henighan et al. (2020): \n\nwhere L \u221e is the irreducible loss and the first term is the reducible loss which is formulated as a power-law scaling term. C are training flops and the L C are final loss of the model in that flops. We used the curve_fit function from the SciPy4 library to fit the parameters. The final fitted scaling curve and the predicted 7 billion and 13 billion parameters model's final loss are shown in Figure 4. We can see that the fitted scaling law predicted Baichuan 2's final loss with high accuracy.",
            "score": 0.4843068588895046,
            "section_title": "Scaling Laws",
            "char_start_offset": 11273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "277313756",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
            "text": "Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling [49,51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference. \n\nFor diffusion models [46,48], which are widely used for generation tasks, research on inference-time scaling has been growing in the context of reward-based sampling [21,27,45]. Given a reward function that measures alignment with user preferences [23] or output quality [28,44], the goal is to find the sample from the learned data distribution that best aligns with the reward through repeated sampling. Fig. 1 showcases diverse applications of inference-time scaling, enabling the generation of faithful images that accurately align with complex user descriptions involving objects quantities, logical relationships, and conceptual attributes. Notably, na\u00efve generation from text-to-image models [25,43] often fails to fully meet user specifications, highlighting the effectiveness of inference-time scaling. \n\nOur goal in this work is to extend the inference-time scaling capabilities of diffusion models to flow models. Flow models [29] power state-of-the-art image [14,25] and video generation [7,60], achieving high-quality synthesis with few inference steps, enabled by trajectory stratification techniques during training [32]. Beyond just speed, recent pretrained flow models, equipped with enhanced text-image embeddings [41] and advanced architectures [14], significantly outperform previous pretrained diffusion models in both image and video generation quality. \n\nDespite their advantages in generating high-quality results more efficiently than diffusion models, flow models have an inherent limitation in the context of inference-time scaling. Due to their ODE-based deterministic generative process, they cannot directly incorporate particle sampling at intermediate steps, a key mechanism for effective inference-time scaling in diffusion models.",
            "score": 0.4824396548212185,
            "section_title": "Introduction",
            "char_start_offset": 1224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "271874728"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "276094284"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "258437096"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 1747,
                    "end": 1751,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 1848,
                    "end": 1852,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Training large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
            "score": 0.48062759975164693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1527
                },
                {
                    "start": 1529,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5546875
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "While these studies provide valuable insights into the mechanisms behind transfer learning, it is challenging to directly apply these insights to derive a scaling law for transfer learning in a manner that would be most directly applicable to the present study. Nevertheless, these studies are useful for analyzing the convergence behavior of models in a transfer learning setting. For example, Maurer, Pontil, and Romera-Paredes 2016 proposes a framework for bounding the convergence rate of a learning algorithm depending on the number of samples from the pre-training task (p) and the downstream task (f ), which follows a power law of the form \n\n3 The scaling law for transfer \n\nWe investigate scaling laws for transfer learning using a 2.8 billion parameter transformer model from the Pythia suite of models (Biderman et al. 2023). Specifically, we attempt to fit a scaling law to empirical data that relates cross-entropy loss on the fine-tuning distribution to the number of pre-training tokens seen, denoted p, and the size of fine-tuning data, denoted f . In this article, we refer to this mathematical relationship as a scaling law for transfer. \n\nConsidering a number of potential scaling law forms, we identified one that was both simple and appeared to perform well according to empirical tests. This form was taken directly from Mikami et al. 2021, and serves as the basis of the analysis presented here: \n\nThis scaling law is composed of four distinct terms. The first term, A \u2022 p \u2212\u03b1 , is the pre-training term: a power law in pre-training data steps. The second term is the transfer gap, G. The utility of the transfer gap can be seen by considering lim p\u2192\u221e L(p, f ). In the infinite pre-training regime, the only value remaining inside the parentheses is the transfer gap, implicitly setting the ultimate efficiency of transfer learning from the pre-training distribution to the fine-tuning distribution. The third term, f \u2212\u03b2 , is the fine-tuning term, which is a power law. \n\nFinally, the fourth term, E, represents the intrinsic entropy of the fine-tuning distribution, setting the maximum theoretical limit of training performance.",
            "score": 0.48051836594576036,
            "section_title": "Related work",
            "char_start_offset": 9570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2151
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "275337234",
            "title": "Scaling Laws for Floating Point Quantization Training",
            "text": "Classical Scaling Laws. Scaling laws have become a fundamental framework for understanding the relationship between essential factors such as model size (N), data size (D), and the resulting loss (L) in deep learning. Two classical scaling laws have been widely recognized in the industry: Chinchilla scaling law (Hoffmann et al., 2022) and OpenAI scaling law (Kaplan et al., 2020). The Chinchilla scaling law is expressed as: \n\nThe OpenAI scaling law is given by: \n\nwhere n, d, \u03b1, \u03b2, and \u03f5 are positive fitted constants. The balance between N and D emerges as critical for computeoptimal training. \n\nScaling Laws for Precision. Subsequent research extends this framework by incorporating the role of precision in quantized training and inference, so as to provide insights into how precision affects model performance. In Kumar et al. (2024), precision-aware scaling laws were introduced to capture the trade-offs between model size N , data size D, and precision P . For integer quantized training, they proposed the tradeoff between weight N and weight precision P as: \n\nwhere N eff indicates the \"effective parameter count\" of models, and \u03b3 is a constant representing the sensitivity of model weights to precision. Incorporating N eff into the Chinchilla scaling law yields: \n\nThis framework highlights that reducing weight precision P can be compensated by increasing the parameter count N to maintain performance, which is a critical insight for low-precision model optimization. \n\nCurrent Scaling Laws cannot Fit Well in FP Quantization. Note that most previous work focused on integer quantized training. FP quantization is more prevalent in real-world applications due to its hardware compatibility and finer granularity. For instance, formats such as FP16 and BF16 are standard in many large-scale training pipelines, and emerging formats like FP8 and FP4 are gaining traction. Despite this, scaling laws specifically tailored to FP quantization are still largely unexplored. The primary distinction between FP and integer quantization lies in the allocation and usage of bits. FP numbers allocate bits to represent both the exponent and the mantissa, with each set of bits serving distinct purposes: the exponent mainly captures dynamic range, while the mantissa mainly encodes precision within that range.",
            "score": 0.4799123621305699,
            "section_title": "Preliminary",
            "char_start_offset": 5566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "274150480",
            "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
            "text": "Standard approaches to scaling laws attempt to fit a curve to the optimal number of model parameters N and training tokens D to minimize the pre-training loss under a given budget of FLOPs (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022;Porian et al., 2024;Abnar et al., 2021;Maloney et al., 2022;Bordelon et al., 2024a). \n\nTo fit these curves, it is useful to specify a parametric form of the loss in terms of N and D. Hoffmann et al. (2022) assumes this curve takes the following form: \n\nThis formula is inspired by classical upper bounds on a loss decomposition that attributes error to Bayes risk (entropy), approximation error (from having finite parameters), and estimation error (from having finite data) (Bottou and Bousquet, 2007). \n\nOn the other hand Kaplan et al. (2020) instead assumes that: \n\n(2) \n\nBelow, we will advocate for a slightly different functional form that blends the two of these. \n\nRegardless of the functional form, scaling laws have been an integral part of the success of modern neural language models. Our work builds on the ideas originated in this line of work and extends them to consider how to translate scaling laws across data distributions.",
            "score": 0.47925156748770403,
            "section_title": "Scaling laws",
            "char_start_offset": 5048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393310546875
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "A wide range of studies have shown that the performance of a language model exhibits a notable growth pattern as the number of parameters and data size increase, following a power-law relationship [HNA + 17, KMH + 20, HKK + 20, CDLCG + 22, ZKHB22, GSH23, BSA + 23].This scaling law plays a fundamental role in the development of large language models, enabling us to estimate optimal configurations of large models from the training logs of much smaller models [TDR + 22, HBM + 22].As mentioned in the GPT-4 technical report [AAA + 23], some aspects of GPT-4's performance can be accurately predicted based on models trained with no more than 1/1,000th the compute of GPT-4.By properly utilizing the scaling law, we avoid the need to perform extensive model-specific tuning on large models.\n\nThe original scaling law paper by OpenAI presented the formulas of scaling laws and illustrated how they could aid in determining optimal training configurations [KMH + 20].Nonetheless, the presented formulas are based on static exponents estimated from their specific experiment setup.The full details on how to derive the constant terms in the scaling-law formulas for a new experiment setup (model architecture, tokenization, data distribution, etc) remain undisclosed.Furthermore, [KMH + 20] only conducted experiments with models containing up to 1.5B parameters, a size significantly smaller than that of contemporary large language models.There have been subsequent works that study scaling laws on larger models [CDLCG + 22, IPH + 24].Some have drawn different conclusions from the original scaling-law paper, casting doubt to the general applicability of scaling laws.For example, [HBM + 22] claimed that the training data size should be scaled much more than the recommendation in [KMH + 20].[BCC + 24] suggested that the optimal batch size depends only on the compute budget rather than the loss value.\n\nIn this paper, we revisit the scaling-law formulas proposed by [KMH + 20], confirming that they remain generally applicable when scaling the model size up to 33B.",
            "score": 0.47809364651146147,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 790
                },
                {
                    "start": 792,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1905
                },
                {
                    "start": 1907,
                    "end": 2069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "When researchers propose a new prompting or post-training technique to improve a pretrained model, how can we know whether these gains will persist across models and scales? Scaling analysis could enable more quantitative approaches to the design of post-training interventions, but systematic scaling analyses have been rare due to the small number of models within a single model family. \n\nAdding to these challenges, some recent works have argued that certain interventions, such as Chain-of-Thought [99], behave in an emergent way and their behaviors are not predictable from smaller models [98]. Using observational scaling laws, we show that it is possible to make relatively accurate predictions on the effectiveness of techniques such as Chain-of-Thought (CoT) [99] and Self-Consistency (SC) [97] as model scale increases. We focus on these post-training interventions in particular, as they are sometimes discussed as examples of post-training interventions that require scale to be effective [98,99]. \n\nOur approach to quantifying the scaling properties of post-training is straightforward: we fit one observational scaling law using base model performance on a target benchmark (e.g., GSM8K fewshot), and then fit another on the performance of models with the post-training intervention (e.g.,",
            "score": 0.47701980328436855,
            "section_title": "Predicting the Impact of Post-Training Techniques",
            "char_start_offset": 33437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3876953125
        },
        {
            "corpus_id": "257102896",
            "title": "Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width",
            "text": "The optimization dynamics of deep neural networks (DNNs) is a rich problem that is of great interest. Basic questions about how to choose learning rates and their effect on generalization error and training speed remain intensely studied research problems. Classical intuition from convex optimization has lead to the often made suggestion that in stochastic gradient descent (SGD), the learning rate \u03b7 should satisfy \u03b7 < 2 /\u03bb H , where \u03bb H is the maximum eigenvalue of the Hessian H of the loss, in order to ensure that the network reaches a minimum. However several recent studies have suggested that it is both possible and potentially preferable to have the learning rate early in training reach \u03b7 > 2 /\u03bb H [66,49,72]. The idea is that such a choice will induce a temporary training instability, causing the network to 'catapult' out of a local basin into a flatter one with lower \u03bb H where training stabilizes. Indeed, during the early training phase, the local curvature of the loss landscape changes rapidly [42,1,37,15], and the learning rate plays a crucial role in determining the convergence basin [37]. Flatter basins are believed to be preferable because they potentially lead to lower generalization error [31,32,42,12,39,14] and allow larger learning rates leading to potentially faster training. \n\nFrom a different perspective, the major theme of deep learning is that it is beneficial to increase the model size as much as possible. This has come into sharp focus with the discovery of scaling laws that show power law improvement in generalization error with model and dataset size [40]. This raises the fundamental question of how one can scale DNNs to arbitrarily large sizes while maintaining the ability to learn; in particular, how should initialization and optimization hyperparameters be chosen to maintain a similar quality of learning as the model size is taken to infinity [34,47,48,11,69,58,70,68]?  Motivated by these ideas, we perform a systematic analysis of the training dynamics of SGD for DNNs as learning rate, depth, and width are tuned, across a variety of architectures and datasets.",
            "score": 0.47617763987549006,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "54031709"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "5834589"
                },
                {
                    "start": 1019,
                    "end": 1021,
                    "matchedPaperCorpusId": "108298098"
                },
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "53387011"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "53387011"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "9794487"
                },
                {
                    "start": 1224,
                    "end": 1227,
                    "matchedPaperCorpusId": "733161"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "5834589"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "9636400"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "204893960"
                },
                {
                    "start": 1236,
                    "end": 1239,
                    "matchedPaperCorpusId": "222134093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "259341858",
            "title": "Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources",
            "text": "Scaling laws, predicting how the model performance changes with the scale of training data, model parameters, and computation budget [26], have seen increasingly successful in a variety of tasks pertaining to vision and text processing [27]. The performance of machine-learning models generally adheres to a power law relationship with the scale of these variables, which allows for predicting the model performance on larger scales with high precision [26] and provides a viable approach to predicting the potential usefulness of target data from only a small proportion of the set. [14] shows that data from different distributions generally scale at different rates. Our work provides a novel approach that materializes this dependency of scaling relationships with data distributions and achieves remarkable empirical results.",
            "score": 0.4752043857732407,
            "section_title": "Related Work",
            "char_start_offset": 8630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "252220884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "253097834",
            "title": "Precision Machine Learning",
            "text": "In this work, one property that we focus on is how methods scale as we increase parameters or training data. This builds on a recent body of work on scaling laws in deep learning [8,9,10,11,12,13,14,15,16] which has found that, on many tasks, loss decreases predictably as a power-law in the number of model parameters and amount of training data. Attempting to understand this scaling behavior, [17,18] argue that in some regimes, cross-entropy and MSE loss should scale as N \u2212\u03b1 , where \u03b1 4/d, N is the number of model parameters, and d is the intrinsic dimensionality of the data manifold of the task. \n\nConsider the problem of approximating some analytic function f : [0, 1] d \u2192 R with some function which is a piecewise n-degree polynomial. If one partitions a hypercube in R d into regions of length and approximates f as a n-degree polynomial in each region (requiring N = O(1/ d ) parameters), absolute error in each region will be O( n+1 ) (given by the degree-(n+1) term in the Taylor expansion of f ) and so absolute error scales as N \u2212 n+1 d . If neural networks use ReLU activations, they are piecewise linear, n = 1 and so we may expect rmse (N ) \u221d N \u2212 2 d . However, in line with [17], we find that ReLU NNs often scale as if the problem was lower-dimensional than the input dimension, though we suggest that this is a result of the computational modularity of the problems in our setting, rather than a matter of low intrinsic dimensionality (though these perspectives are related). \n\nIf one desires very low loss, then the exponent \u03b1, the rate at which methods approach their best possible performance 1 matters a great deal. Kaplan et al. [17] note that 4/d is merely a lowerbound on the scaling rate -we consider ways that neural networks can improve on this bound. Understanding model scaling is key to understanding the feasibility of achieving high precision.",
            "score": 0.47500464884277743,
            "section_title": "Importance of Scaling Exponents",
            "char_start_offset": 5443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1880
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 202,
                    "end": 205,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "278636265",
            "title": "Superposition Yields Robust Neural Scaling",
            "text": "Given model size N , m and \u2113 are constrained (roughly, N \u221d m 2 \u2113). There is an optimal m-\u2113 relationship such that the loss f m (m) + f \u2113 (\u2113) can be minimized given N [54]. At this optimal m-\u2113 relationship, f m (m) and f \u2113 (\u2113) should be balanced. One can see that the empirical m-\u2113 relationship is not optimal according to [54] but is close. Therefore, following such m-\u2113 relation, we expect f \u2113 (\u2113) to be similar to f m (m). And if f m (m) \u223c 1/m due to superposition and f \u2113 (\u2113) is similar, we can measure an empirical \u03b1 m \u2248 1 from data, which is true. So, we conclude that superposition is an important mechanism underlying the loss and scaling laws we see. However, since we do not know the function f \u2113 (\u2113), we cannot predict the optimal m-\u2113 relation or the exponent \u03b1 N . \n\nA natural future direction is to study the parsing-limited scaling (i.e, f \u2113 (\u2113) function). We hypothesize that parsing-limited scaling may resemble the scaling behavior observed when fitting functions or manifolds. The open-source LLMs we examined, as well as those studied in prior neural scaling law papers [2,3], span model sizes from 100M to 70B parameters, where width (model dimension) varies. However, in the regime from 70B to 700B [38][39][40][41], model scaling seems to focus on increasing depth. We therefore expect parsing-limited scaling to become important for models larger than 70B. It is also plausible that the observed scaling of inference time [55] is connected to this parsing-limited regime. The study of neural scaling laws can be refined by distinguishing between width-limited and depth-limited regimes. In each regime, there should be loss decay behaviors with model size, dataset size, and training steps, highlighting the need for further investigation. \n\nBeyond explaining existing phenomena, our results may offer guidance for future LLM development and training strategies.",
            "score": 0.47347358909135284,
            "section_title": "Discussion",
            "char_start_offset": 27800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "274762856",
            "title": "Memory Layers at Scale",
            "text": "Language model scaling laws (Kaplan et al., 2020) study the empirical performance of language models as they are scaled in compute, data, and parameter size. Scaling laws are typically formulated in terms of training/test log likelihood, which is generally believed to correlate well with downstream performance. Scaling plots on downstream tasks are also not without precedent (Brown et al., 2020), but have sometimes been shown to exhibit non-linear behaviour and phase transitions (Wei et al., 2022;Ganguli et al., 2022). Nevertheless, given a well behaved metric (such as task likelihood loss), most tasks exhibit smooth improvements with scaling (Schaeffer et al., 2023). Kaplan et al. (2020) showed that performance scales log-linearly with compute and parameter size across a wide range of architecture hyper-parameters, such as model depth and width. It has been difficult to find architectures which substantially deviate from these laws. Mixture-of-experts (MOE) (Shazeer et al., 2017;Lepikhin et al., 2020) is a notable exception. MOE adds extra parameters to the model without increasing the computation budget. While scaling laws for MOE also mostly focus on training perplexity, gains transfer well to downstream applications, as evidenced by the popularity of MOE architectures in recent state-of-the-art model families (Jiang et al., 2024;OpenAI et al., 2024;Team et al., 2024). Nevertheless, scaling laws for specific task families and capabilities like factuality remain understudied. \n\nLike MOE, memory augmented models also aim to augment the parameter space of the model without adding significant computational cost. Memory networks were proposed initially in (Weston et al., 2015), and with end-to-end training in (Sukhbaatar et al., 2015). Neural Turing Machines (Graves et al., 2014(Graves et al., , 2016) ) combine external trainable memory with other components to build a neural trainable computer. Product-key networks (Lample et al., 2019) were introduced to make the memory lookup more efficient and scalable.",
            "score": 0.47085223331716747,
            "section_title": "Related work",
            "char_start_offset": 3364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 973,
                    "end": 995,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 1787,
                    "end": 1807,
                    "matchedPaperCorpusId": "205251479"
                },
                {
                    "start": 1807,
                    "end": 1832,
                    "matchedPaperCorpusId": "205251479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "We plot the results in Figure 2, where each point represents the loss of a training run, detailed in Appendix F. We observe that increasing P yields benefits following a logarithmic trend. Similar gains are seen when raising P from 1 to 2, 2 to 4, and 4 to 8. Thus, we preliminarily try the following form: \n\nwhere we assume that P 1 /\u03b1 \u2022 DIVERSITY = k log P + 1 in Equation ( 4) based on the finding of the logarithmic trend. (A, k, \u03b1, E) are parameters to fit, and we use the natural logarithm (base e). We follow the fitting procedure from Hoffmann et al. (2022); Muennighoff et al. (2023), detailed in Appendix E. \n\nFigure 2 illustrates the parallel scaling law fitted for two training datasets. It shows a high goodness of fit (R 2 up to 0.998), validating the effectiveness of Equation ( 5). Notably, we can observe that the k value for Stack-V2 (0.39) is higher than for Pile (0.33). Recall that k reflects the benefits of increased  parallel computation. Since Stack-V2 emphasizes coding and reasoning abilities while Pile emphasizes memorization capacity, we propose an intuitive conjecture that model parameters mainly impact the memorization skills, while computation mainly impacts the reasoning skills. This aligns with recent findings on inference-time scaling (Geiping et al., 2025). Unlike those studies, we further quantitatively assess the ratio of contribution to model performance between parameters and computation through our proposed scaling laws. \n\nRecall that Equation ( 5) implies scaling P equates to increasing parameters by O(N log P). It suggests that models with more parameters benefit more from PARSCALE. Figure 3 more intuitively displays the influence of computation and parameters on model capacity. As model parameters increase, the loss contours flatten, showing greater benefits from increasing computation.",
            "score": 0.47017796364227626,
            "section_title": "Parametric Fitting",
            "char_start_offset": 15780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1846
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 592,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "276558195",
            "title": "Scaling Laws for Downstream Task Performance in Machine Translation",
            "text": "Scaling laws quantify the relationship between a model's performance and key design factors such as the size of the training data or the model's architecture. In the context of LLMs, these laws offer valuable guidance for model development, resource allocation, and selection of appropriate training data. Extensive research has focused on scaling laws for upstream perplexity or cross-entropy loss (i.e., evaluated on pretraining data), demonstrating that these quantities can be well predicted using power laws (Kaplan et al., 2020;Hoffmann et al., 2022;Gordon et al., 2021;Hernandez et al., 2022;Fernandes et al., 2023;Henighan et al., 2020;Johnson et al., 2018). However, in practice, LLMs often undergo transfer learning-they are first pretrained on unsupervised data and then finetuned for specific downstream1 tasks such as coding or translation. The question of whether scaling laws can be used to predict downstream task performance is critical (OpenAI, 2024), yet remains largely unanswered (Hernandez et al., 2021;Tay et al., 2021). Here, the term task performance refers to metrics that measure task-related quantities such as accuracy and translation scores like BLEU, ROUGE, or COMET, which are different from next-token prediction metrics such as cross-entropy. \n\nIn this work, we study scaling laws for transfer learning and focus on machine translation tasks. Specifically, we look into the relation between the pretraining dataset size and the downstream task performance after finetuning on the task. We find that, in addition to the finetuning data size and the choice of the performance metric, this relation fundamentally depends on the alignment between the pretraining data and the downstream task (based on the translation alignment score provided in Section 3). While similar observations have been made in different contexts in the transfer learning literature (Tamkin et al., 2020;Agostinelli et al., 2022), our work provides new insights and concrete scaling laws for the downstream performance in machine translation. \n\nWe carry out systematic experiments in which we pretrain LLMs on multilingual unsupervised datasets and then finetune them on several machine translation tasks.",
            "score": 0.46972741526057604,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 556,
                    "end": 576,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "51729732"
                },
                {
                    "start": 1025,
                    "end": 1042,
                    "matchedPaperCorpusId": "261076313"
                },
                {
                    "start": 1888,
                    "end": 1909,
                    "matchedPaperCorpusId": "216869396"
                },
                {
                    "start": 1909,
                    "end": 1934,
                    "matchedPaperCorpusId": "244709436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369140625
        },
        {
            "corpus_id": "267547740",
            "title": "A Resource Model For Neural Scaling Law",
            "text": "In previous sections, we demonstrated that \u2113 \u221d N \u22121 , i.e., the loss \u2113 is a scaling law against the number of allocated neurons N . Can we use this result as a starting point to derive the more useful neural scaling laws of large language models (LLMs)? Notice that there are two major gaps between our resource model and the LLM setup: (1) In LLM, the number of parameters N p is measured against instead of the number of allocated neurons N ; (2) In LLM, the depth is usually scaled with width, while our resource model (hypothesis 2) assumes only scaling up width. Below we aim to address these two gaps and derive a meaningful scaling law for LLMs. \n\nIn LLMs the empirical neural scaling law we care about is \u2113 \u221d N \u2212\u03b1 p where N p is the number of model parameters. How is N p related to N ? We aim to show that N p \u221d N 3 and finally derive \u03b1 = 1/3, with a number of reasonable assumptions5 : \n\nAssumption 1: The number of modules is proportional to the network width, i.e., N \u221d N width . This is aligned with the homogeneous growth conjecture. Redundant representations have been empirically observed in Liu & Tegmark (2023); Doimo et al. (2021). \n\nAssumption 2: The depth is scaled as the same factor as the width, i.e., N depth \u221d N width . The theoretical derivation of the optimal depth-width ratio (Levine et al., 2020) has been the foundation for best practices in LLM where the depth and width of the network are scaled by the same factor, e.g., in Hoffmann et al. (2022). \n\nAssumption 3: The number of parameters can be roughly estimated as N = N 2 width N depth . Most neurons of the neural network are in MLPs weights, while the number of other parameters are nearly negligible. The number of MLP weight parameters is simply N 2 width N depth . Combining all three assumptions, we have \n\ntogether with \u2113 \u221d N \u22121 immediately implies that \u2113 \u221d N \u22121/3 p . Pleasantly, this agrees with the empirical scaling law of Chinchilla models (Hoffmann et al., 2022) where they observed \u2113 \u221d N \u22120.34 p .",
            "score": 0.46812841656659704,
            "section_title": "EMERGENT NEURAL SCALING OF MODEL SIZE",
            "char_start_offset": 19688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058349609375
        },
        {
            "corpus_id": "274150480",
            "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
            "text": "For the scaling law setting, we consider the following scenario. There are two pre-training distributions P 0 and P 1 . Assume that we have already fit a set of 88 small models on P 0 so as to fit a scaling law. Then, we fit only 8 small models on a new distribution P 1 . We want to get a scaling law on P 1 . \n\nWe will consider two approaches illustrated in Figure 6 and Figure 7: \u2022 (Ours) Train-to-train translation. We fit a train-to-train curve using the 8 models on P 1 . From this we can translate the scaling law from P 0 to P 1 .2 \n\n\u2022 (Baseline) Independent scaling laws. Here we fit an independent scaling law on P 1 from only the 8 models we have that are trained on that dataset. \n\nThe point of this experiment is to illustrate how train-to-train fits can unlock an efficient way to fit a new scaling law on a new dataset. Note that as we said above, we should caution that under train-to-train translation the size of the compute optimal model is invariant. \n\nWe also consider a skyline of fitting a scaling law on P 1 with access to all 88 models trained on P 1 . Then we compute the R 2 of each of the three scaling law models (skyline, ours, and baseline) on the entire set of 88 models trained on P 1 to assess the goodness of fit. Results are reported in Table 1. For our train-to-train translation we report the mean R 2 averaged over the 5 possible values for P 0 for each target distribution P 1 . With only 8 runs from the new dataset, our method can nearly match the skyline which has access to 88 runs from the target dataset. In contrast, the baseline of fitting an independent scaling law fails badly in this limited data regime since it does not leverage prior computation. \n\nWe find that loss-to-loss prediction yields substantially better scaling law fits than the baseline. In fact, even with only 8 models on P 1 , using train-to-train prediction to tranlate the original scaling law nearly matches the R 2 of the skyline that has access to all 88 models on P 1 , up to about 0.001. In contrast, fitting a new scaling law on only this data is very ineffective.",
            "score": 0.4678655477615718,
            "section_title": "Translating a scaling law",
            "char_start_offset": 23867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2091
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28759765625
        },
        {
            "corpus_id": "267411718",
            "title": "Selecting Large Language Model to Fine-tune via Rectified Scaling Law",
            "text": "To better understand the underlying mechanism of the phase transition phenomenon, we start with the essential difference between pre-training and fine-tuning. Unlike pre-training where we train a model from scratch, fine-tuning starts from a model that has been trained on a large corpus. Consequently, pre-training should have provided models with some amount of information relevant to downstream context (Hernandez et al., 2021). \n\nTo capture this concept, we introduce the term pre-learned data size (represented by D l ) that indicates how much amount of downstream data a model has learned from pretraining. This term could be influenced by multiple factors like the expressivity of models, the pre-training corpus size, as well as the difficulty of this downstream task. Intuitively, D l can be integrated with the scaling term D \u03b2 , which represents the amount of information that fine-tuning on D samples can provide the model with. We propose the following improved Scaling Law by incorporating this term, with an identical amount of parameters to be fitted. \n\nDefinition 3.2 (Rectified Scaling Law). We define the Scaling Law with dataset size D for fine-tuning as \n\nwhere D l is the pre-learned data size, \u03b2 is the power to D denoting the learning difficulty, B adjusts the initial test loss, and E denotes the optimal loss of the model given an  7)) and vanilla law (Equation ( 5)) when fitting fine-tuning test loss versus dataset size in log scale. Under same setting, our law achieves much lower RMSD error. \n\ninfinite amount of data.3 \n\nThis modification of D l essentially improves the mathematical property of Definition 3.2 as the derivative is no longer monotonous. As proved in Theorem 3.3, the first-order derivative decreases before x 0 (corresponding to the prepower phase) and slightly increases afterward (corresponding to the power phase). In other words, the introduction of D l is not only conceptually reasonable, but it also elegantly unifies the co-existence of both phases into one law. Theorem 3.3. For any positive parameters B, E, D l , \u03b2, consider the log-log form of function L(\u2022) in Equation (7):",
            "score": 0.46766637145508244,
            "section_title": "Our Scaling Law with Pre-learned Data",
            "char_start_offset": 14559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38134765625
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "Our scaling law equation allows us to utilize all loss values from a full loss curve during training, while traditional scaling laws can only collect a single data point from the full loss curve. This feature allows us to fit scaling laws with much less cost. For a direct comparison, we compare the computational efficiency of our approach and the Chinchilla scaling law (Hoffmann et al., 2022). Specifically, we assume and evaluate the computational cost of obtaining 100 fitting points for each scaling law equation with an step interval of K: \n\n\u2022 Adopting Chinchilla scaling law, typical cosine LRS requires total steps of at least We present a comparison of the computational costs associated with different laws and LRS in Table 2. The results indicate that our proposed equation uses less than 1% of the computational cost required by the Chinchilla scaling law. Further, our scaling law with LR annealing, can be universally applied to predict full loss curves for unseen LRS, thus conserving even more computational resources. This approach significantly democratizes the study of scaling laws in LLM pre-training, paving the way for a more environmentally friendly and carbon-efficient methodology. The decay factor \u03bb in our equation indicates the information retaining degree in LR annealing. We set \u03bb as 0.999 in our all experiments. We explore the difference from another decay factor \u03bb = 0.99. We fit and get different equations for different \u03bb. We compare them (1) on the predicted loss curves for 1-square and 1-sqrt annealing methods, and (2) on the predicted loss curves in different annealing ratios of WSD LRS (cosine annealing).",
            "score": 0.4670609093817235,
            "section_title": "SCALING LAW FITTING AND PREDICTION DEMOCRATIZATION",
            "char_start_offset": 34386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1649
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "We have presented observational scaling laws -an approach that generalizes existing compute scaling laws to handle multiple model families using a shared, low-dimensional capability space. Using this approach, we show that we can build low-cost, high-resolution, and broad-coverage scaling laws that allow us to make accurate predictions for many complex scaling phenomena, such as emergent behaviors, agentic capabilities, and the value of post-training interventions. We provide concrete and practical prescriptions for researchers and practitioners to perform similar forms of scaling analyses for their own benchmarks and post-training methods in the hopes of encouraging more quantitative, scaling-law-based approaches to designing benchmarks and post-training methods. \n\nLimitations and future work Finally, we discuss some limitations of our approach and findings: Firstly, observational scaling laws are primarily applicable to post-training scaling analyses and do not directly translate to pretraining scenarios in the same way as standard compute-based scaling laws. Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12,81], would be valuable. Thirdly, while we have demonstrated that our observational scaling analyses can provide meaningful insights into improving particular models' complex capabilities, a promising direction for future work would be to apply the findings from our approach, such as by deriving surrogate measures for model complex capabilities that can be used to optimize models directly and efficiently. Lastly, our assumptions do not account for potential benchmark contamination (where particular benchmark data leaks into model training) or the heterogeneity within model families (where models within the same family may have varying compute efficiencies and scaling behaviors). Investigating the impact of these assumptions on our approach would be an interesting avenue for future research. \n\nMajor Changelog 07/02/2024 \n\n\u2022 Added clarifications that emphasize the predictions of observational scaling laws are based on standard benchmark metrics instead of training FLOPs. \n\n\u2022 Updated plots to use x-axis with a log10 scale instead of a ln scale for better readability.",
            "score": 0.46682018331520136,
            "section_title": "Conclusion, Limitations, and Future Work",
            "char_start_offset": 44273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2232
                },
                {
                    "start": 2235,
                    "end": 2261
                },
                {
                    "start": 2264,
                    "end": 2414
                },
                {
                    "start": 2417,
                    "end": 2511
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "We next turn to examine whether scale-time equivalence is present empirically in neural networks. We conduct experiments on MNIST (Deng, 2012), CIFAR-10 (Krizhevsky, 2009), and SVHN (Goodfellow et al., 2013) training a 7-layer convolutional neural network (CNN) and a 6layer multilayer perception (MLP) with stochastic gradient descent (SGD). To assess scale-time equivalence, we measure the minimum amount of training time required to achieve non-zero generalization under various network widths and by varying the dataset size by subsampling. Scale-time equivalence predicts that wider networks will require less time to generalize in a systematically predictable way. See Appendix C for further experimental details. \n\nAs observed in Figure 2, in all settings, we see a clear tradeoff curve between scale and training time: increasing scale by a fixed factor is nearly equivalent to reducing training time by another fixed factor. Importantly, the scale here is set as the effective number of network parameters, defined as the maximum number of training points that can be fit by the network, not the absolute number of parameters. We set the effective parameter count as the cube root of the number of parameters. \n\nAppendix B provides a heuristic argument for this scaling rate. \n\nOn benchmark datasets and architectures, we conduct two experiments: 1) predict performance under varying model scales from a small network trained for long training times, 2) predict performance over long training times by using larger networks trained for just 1 epoch. See Appendix C for experimental details and Figures 10 and 11 for full results. \n\nFigure 3 illustrates that scale-time equivalence can indeed be used to extrapolate performance on large scales and training times. Predictions of test and train performance under large model scales are particularly close to the true performance; notably, we can closely predict the scale at which generalization starts to occur. However, there is a small discrepancy between predicted and actual performance; we believe this can be corrected with dataset and model-specific tuning of the scaletime trade-off curve. Nevertheless, our findings reveal that scale-time equivalence can be used to predict optimal network scale and training time.",
            "score": 0.46644915669159726,
            "section_title": "EMPIRICAL VALIDATION IN NEURAL NETWORKS",
            "char_start_offset": 10472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 142,
                    "matchedPaperCorpusId": "5280072"
                },
                {
                    "start": 182,
                    "end": 207,
                    "matchedPaperCorpusId": "17991431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45068359375
        },
        {
            "corpus_id": "272463550",
            "title": "An overview of domain-specific foundation model: key technologies, applications and challenges",
            "text": "Scaling law refers to the mathematical pattern about how the performance of a system changes as the scale of the system increases. In the field of AI, especially in the research and application of foundation models, scaling law describes a series of rules and phenomena about how model performance changes as the model scale expands, including the number of parameters, dataset size, computational resources. It uses quantitative analysis methods to reveal the intrinsic mechanism of the performance improvement of foundation models. \n\n[54] discussed how the inductive biases of different models affect the relationship between model scale expansion and performance. They found that model architecture is indeed one of the key factors affecting the benefits of model expansion. They also pointed out that although the standard transformer architecture may not always achieve the best performance, it does exhibit the best scalability. In the fields of computer vision [55] and natural language process [56], models based on the transformer architecture have shown an exponential relationship between model scale and model performance. \n\nBesides, [57] examined the impact of the number of downstream tasks and model scale on the performance of instruction fine-tuning. They fine-tuned the models on a wide variety of tasks by a multi-task joint training method. As a result, the language model could learn a broader language representation and knowledge, thereby enhancing its generalization ability on unseen tasks. During the joint training process, knowledge transfer between different tasks is promoted through parameter sharing, significantly improving the model's generalization ability and performance. In addition, joint training also reduces the time and computational resources required to train each task individually, improving training efficiency. This phenomenon of model performance improving as task diversity increases is a manifestation of the scaling law. [58] verified the phenomenon of model performance increasing with the number of tasks on the large-scale benchmark OPT-IML Bench. Additionally, there are also studies that have separately provided the model performance of natural language models [56] and autoregressive generative models of various modalities [59] at different scales. \n\nAlthough there is no unified form for the quantitative representation of the scaling law, it can be generally represented as an exponential relationship between the model loss function and the model's parameters, dataset size, and computational resources.",
            "score": 0.466002845026423,
            "section_title": "Scaling law",
            "char_start_offset": 14422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2309
                },
                {
                    "start": 2312,
                    "end": 2567
                }
            ],
            "ref_mentions": [
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "235367962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424072265625
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "Predicting task performance before training is an aspirational objective for the development of predictable AI systems, and a multitude of studies approach this aim from various perspectives. \n\nLoss Scaling Law. Scaling phenomena have been observed across a broad spectrum of deep learning architectures. The power-law scaling behavior of loss in RNN-based models is investigated in Hestness et al. (2017). Kaplan et al. (2020) delineate the loss scaling trends for Transformer-based language models and explores the scaling behavior of optimal hyper-parameters. They formally established the following scaling law \n\nwhere N is the number of non-embedding parameters of LLM, c, \u03b1 are positive coefficients, and L 0 is the irreducible loss representing the randomness in data. This formulation has catalyzed the proliferation of LLMs. Subsequently, scaling laws are established for various domains and scenarios, including multi-modality (Henighan et al., 2020;Zhai et al., 2022), computation constraint scenario (Hoffmann et al., 2022), data engineering (Muennighoff et al., 2023;Sorscher et al., 2022), and reinforcement learning (Gao et al., 2023). Yao & Wang (2023) extend the scaling law into loss prediction by introducing hyper-parameter scaling methods. The relationship of our work with these existing literature is twofold. First, these works concentrate on training and validation loss metrics, which do not reliably predict task performance. Second, our research builds on these scaling laws and extends the mathematical form of Eq.( 1) to the scaling law of task performance. \n\nScaling Behavior of Task Performance. Despite the predictable decrement in LLM loss, task performance improvements are twisted during scaling. While some tasks, predominantly those relying on memorization of knowledge, have shown progressive improvement, numerous tasks exhibit breakthrough behavior as model size increases (Srivastava et al., 2022;Wei et al., 2022a).",
            "score": 0.4658419686756622,
            "section_title": "RELATED WORK",
            "char_start_offset": 6852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 978,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1080,
                    "end": 1102,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1131,
                    "end": 1149,
                    "matchedPaperCorpusId": "252992904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "276421795",
            "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
            "text": "Compute-to-Train Scaling Laws Scaling laws aim to optimize model size and token allocation within a fixed compute budget (expressed in FLOPs) by modeling the relationship between parameters, training tokens, and training loss (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022). However, these laws are inherently shaped by the data distribution, architecture, and optimization settings (Tay et al., 2022;Wang et al., 2024;Brandfonbrener et al., 2024;Porian et al., 2025), making their application across setups non-trivial. \n\nCompute-to-Downstream Scaling Laws Recent works extend scaling laws to directly predict downstream task per-formance from compute (Gadre et al., 2024;Isik et al., 2024;Du et al., 2025). While some initial works attempt to map compute budgets to accuracy on individual tasks, multiple tasks, or aggregate benchmarks, this mapping is usually noisy due to several transformations in the accuracy computation that degrade the statistical relationship (Schaeffer et al., 2024). More recent efforts instead use the model's average loss on the correct answers of the task as a proxy (Madaan et al., 2024;Brandfonbrener et al., 2024). Such compute-to-downstream scaling laws provide a more practical perspective on scaling but are still specific to a given training setup. \n\nLoss-to-Loss Scaling Laws Loss-to-loss scaling laws aim to improve the transferability of scaling insights between training setups by examining the relationship between training (or validation) and test losses, between different validation losses, or between different test losses (Brandfonbrener et al., 2024). This perspective is crucial for several reasons. First, train-to-train (or validation-to-validation) scaling implies how scaling laws transfer across datasets (Brandfonbrener et al., 2024).",
            "score": 0.465613816156046,
            "section_title": "From Scaling Laws to Interventions",
            "char_start_offset": 4103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "The Scaling Law posits a predictive enhancement in model performance through the augmentation of three critical dimensions: computational budget C, model size N, and data size D. This law articulates that when the model size N is quantified by the count of model parameters, and the data size D is measured in terms of the number of tokens, the computational budget C can be approximated effectively by the formula C = 6ND. Consequently, an essential research focus within the ambit of the Scaling Law involves strategizing an optimal allocation between model size and data size during the amplification of the computational budget, with the ultimate goal of achieving the most efficacious model configuration. \n\nResearch into the optimal allocation strategies for model/data size enhancement (Hoffmann et al., 2022;Kaplan et al., 2020) has led to varied conclusions, prompting skepticism about the universal applicability of the Scaling Law. Furthermore, structural differences among models, disparities in the quality of training data, and variations in hyperparameter configurations are often primary factors that contribute to the discrepancies observed in the Scale Law. Baichuan2 has conducted experiments employing the Scale Law as proposed by Henighan et al. (2020). Meanwhile, the two models of Deepseek have adopted the IsoFLOP profile approach from chinchilla, and have implemented certain degrees of optimization to it. \n\nIn order to extrapolate more universally applicable insights, our investigation is predicated on the foundational principles of the chinchilla law. The formal mathematical representation of this law is encapsulated in the following expression: \n\nwhere P denotes the performance metric of the model. Conventionally, in the ambit of prior scholarly inquiries, this metric is frequently quantified in terms of the model's loss, symbolized by L. And a, \u03b1, \u03b2, \u03b3 are constants that depend on the specific architecture and task. We explain the specific meaning of each parameter in detail in Appendix B. Figure 6 indicates a clear relationship between the size of the training data and the performance scores across the CEval, CMMLU, and MMLU benchmarks. All three subfigures exhibit a monotonically increasing trend, where the scores improve as the number of training checkpoints increases.",
            "score": 0.46346346830976093,
            "section_title": "Scaling law definition",
            "char_start_offset": 15083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1759033203125
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "As a result, optimizing this metric is often a key focus in the development of more efficient LLMs. Recent research in predicting large language model (LLM) performance has concentrated on understanding the scaling law [126]. This law delineates how LLM performance is influenced by factors such as model architecture, neural model size, computing power for training, and available data. The concept of scaling law, rooted in statistical mechanics approaches for predicting model generalization, has a rich history dating back to the early 1990s [10,17,96,240]. Its relevance has been reinvigorated recently in the context of modern deep learning models [9, 25, 102-104, 107, 126, 192, 226, 254, 266, 268]. This section will delve into the latest advancements and insights in the scaling law as applied to LLMs, highlighting how these models evolve and perform under varying conditions.",
            "score": 0.46246258225238585,
            "section_title": "Number of Parameters.",
            "char_start_offset": 14161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 886
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "17577797"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "28307549"
                },
                {
                    "start": 556,
                    "end": 560,
                    "matchedPaperCorpusId": "7394722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34033203125
        },
        {
            "corpus_id": "278310893",
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "text": "Recent studies showed that the energy efficiency of compute-intensive tasks varies based on factors such as hardware configuration, server location, and training duration (Zhang et al., 2023). Despite these insights, scaling laws do not incorporate considerations for energy efficiency, leading to strategies that may be technically effective but environmentally unsustainable. \n\nAs the limitations of neural scaling laws become more apparent, the development of downscaling laws has emerged as a critical area of focus. These laws aim to understand how to reduce model size, dataset requirements, and compute usage while preserving performance for specific tasks. Downscaling laws are essential for promoting efficiency, sustainability, and accessibility in AI research. In this position paper, we enlighten on the importance of focused research on downscaling laws and propose strategies for leveraging the insights from neural scaling laws for downscaling LLMs to efficient, environment-friendly and sustainable foundational models. Smaller models guided by downscaling laws can address resource constraints, enabling broader participation in model development by reducing the financial and computational barriers. Additionally, these laws can provide insights into optimizing task-specific performance, ensuring that models can meet diverse application requirements without excessive scaling. 1",
            "score": 0.4617980041399252,
            "section_title": "Introduction",
            "char_start_offset": 2248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288818359375
        },
        {
            "corpus_id": "254926749",
            "title": "SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning",
            "text": "Scaling law Figure 3 shows the different scaling properties of SPT for multitask-inference full fine-tuning. First, we show the scaling law for different backbone LMs in Figure 3(a). To calculate the score for each of the models, we aggregate the average rank classification score for all the prompts from our selected evaluation tasks. As the model grows bigger, we see overall performance improvement for T5-small/base/large. \n\nIn Figure 3(b) we also explore the influence of data during pre-training. To test how data diversity contributes to model performance, we randomly sample 50, 500 and 5000 examples from each of the templates of the pre-training tasks. In Figure 3(b), although SPT performs well compared to full fine-tuning, we have come to the conclusion that in the absence of an adequate number of samples per template, it is not possible to improve model performance using either fine-tuning (FT) or SPT. \n\nFinally, we explore the effect of sequence length for multitask-inference full fine-tuning. In Figure 3(c), we observe that compared to soft prompts, memory prompts perform reasonably well for all the sequence length. We also observe diminishing return for memory prompt and huge negative return for soft-prompt as the prompt length grows larger (40). This also indicates that memory prompts are more robust.",
            "score": 0.4616907490693171,
            "section_title": "Analysis",
            "char_start_offset": 15567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
            "score": 0.4615211575455042,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46923828125
        },
        {
            "corpus_id": "257353837",
            "title": "A Meta-Learning Approach to Predicting Performance and Data Requirements",
            "text": "More data translates to better performance, on average, and higher cost. As data requirements scale, there is a natural desire to predict the cost to train a model and what performance it may achieve, as a function of cost, without training. Towards this goal, neural scaling laws [3,4,9,19,20,40] have been proposed that suggest that the performance of a model trained on a given dataset size follows a linear fit when plotted on a logarithmic scale (power law in linear scale). \n\nIn practice, however, the power law provides only a family of functions and its parameters must be fitted to each specific case for performance prediction. A common use Figure 1. Performance estimation curves using the powerlaw and piecewise power law (PPL) with estimated confidence. The PPL reduces over-estimation of the power law from 12\u00d7 to 1.9\u00d7 in 1 step, and further to 1.2\u00d7 in 2 steps using the estimated confidence bounds to limit the prediction horizon n (1) in the first step. \n\ncase is one where a small initial dataset is made available and can be used to obtain small-scale performance statistics that are relatively inexpensive to obtain and can be used to fit the power law parameters. Then, the fitted function is used to predict the performance for any dataset size training through extrapolation. This approach is found empirically to generalize across several datasets and deep learning models [40]. However, most applications of power law are done in the high-shot regime. The few-shot regime (e.g., 5 samples/class) is particularly useful given the prevalence of pre-trained initializations available for model training. In the few-shot regime, the performance curve exhibits a nonlinear progression followed by a linear progression, see Figure 1. Thus, data requirements based on the power law can lead to significant errors incurring additional cost for acquiring data. \n\nIn this paper, we propose a piecewise power law (PPL) that models the performance as a quadratic curve in the fewshot regime and a linear curve in the high-shot regime in 1 arXiv:2303.01598v1 [cs.CV] 2 Mar 2023 the log-log domain, while ensuring continuity during the transition.",
            "score": 0.4612170129792565,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 286,
                    "matchedPaperCorpusId": "49744838"
                },
                {
                    "start": 286,
                    "end": 288,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "224820076"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "matchedPaperCorpusId": "203592013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40478515625
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "2.1 SCALING LAWS Cross-entropy loss of language models on the validation set is a reliable indicator of LLMs' performance on downstream tasks (Caballero et al., 2022;Du et al., 2024). Kaplan et al. (2020) empirically discovered a power-law relationship between validation loss L and three factors: model size N , dataset size D, and training compute. As an application of scaling law, Hoffmann et al. (2022) developed Chinchilla, a compute-optimal LLM, by balancing model size and dataset size. They used a simplified and intuitive scaling law equation: \n\nwhere L 0 , A, B, \u03b1, \u03b2 are positive constants. Traditional scaling law formulations fit only the loss at the final training step, while ignoring losses from other steps. Note that collecting a new loss value of data size requires launching a another training run with the same LRS, which is resourceintensive. Previous works have conducted some preliminary studies on the impact of the learning rate on the scaling laws. For example, OpenAI and chinchilla scaling laws both report that the choice of learning rate schedule does not influence the power-law format (Kaplan et al., 2020;Hoffmann et al., 2022). Also, OpenAI's experiments suggest that the specific choice of learning rate schedule has minimal impact on the final validation loss, provided that the total summed learning rate is adequately large and the schedule incorporates both a warmup stage and a final annealing stage, reducing the learning rate to nearly zero at the end of training (Kaplan et al., 2020).",
            "score": 0.4607593877372448,
            "section_title": "PRELIMINARY",
            "char_start_offset": 5712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1530
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "277622207",
            "title": "Data Scaling Laws for End-to-End Autonomous Driving",
            "text": "Scaling laws in deep learning have demonstrated predictable improvements in model performance as dataset sizes increase, following a power-law relationship [27,28,39,59,64,77]: L val \u221d \u03b2x c , where x is the training data size, \u03b2 and c are problem-specific constants, and L val represents the validation loss. Studies such as [39] established that auto-regressive models like GPT exhibit consistent scaling with respect to dataset size, model capacity, and training iterations, insights later applied to optimize GPT-3's training [50]. Scaling laws have been observed across diverse modalities, from language and vision to multi-modal generative models [27,77]. Recent theoret-ical work further formalizes these observations by linking scaling behaviors to intrinsic data properties [2,4,63]. \n\nA critical aspect of scaling laws is estimating the amount of additional data needed to achieve target performance, especially for costly data domains such as AVs [1,10,25,37,48,49,62]. [48] introduced an active learning framework where estimators iteratively refine dataset size predictions, compensating for overly optimistic projections on smaller datasets. More recent approaches, including probabilistic models [25] and meta-learning [37], aim to reduce extrapolation errors by distinguishing between data regimes. Notably, [1] proposes a generalized estimator to improve data requirement predictions across diverse tasks. Although some prior works in autonomous driving, such as EMMA [34], STR [65], and GUMP [31], briefly mention scaling laws as part of their analyses, they lack a systematic approach and do not derive a formal scaling relationship. \n\nA recent study [81] explores data scaling in end-to-end AV systems but lacks a structured framework, omitting key analyses such as different scaling trends, performance variations across action types, and model capacity effects. In contrast, our work establishes a systematic approach for measuring scaling behavior and data requirement estimates.",
            "score": 0.46064187727801303,
            "section_title": "Scaling Laws and Data Estimation",
            "char_start_offset": 6518,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 785,
                    "end": 787,
                    "matchedPaperCorpusId": "233739646"
                },
                {
                    "start": 787,
                    "end": 790,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "265506756"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "257353837"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "252693192"
                },
                {
                    "start": 975,
                    "end": 978,
                    "matchedPaperCorpusId": "272713896"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "265506756"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "257353837"
                },
                {
                    "start": 1323,
                    "end": 1326,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "270924324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "259982648",
            "title": "The semantic landscape paradigm for neural networks",
            "text": "We live in an age in which the readers of this manuscript can never quite be sure whether it was written by a human or a large language model 1 . This sobering fact at once highlights the stunning technological advances in artificial intelligence (AI) in recent years, as well as the urgent need to understand the inner workings of the deep neural networks that made these advances possible. Understanding neural networks requires us to know what the networks are capable of doing, how their performance depends on various factors, and finally why they succeed or fail in different circumstances. Over the last few years, researchers have begun to answer these questions through an impressive set of experimental results as well as theoretical ideas. Several studies have reported empirical observations of power law scaling of the test loss as a function of network size (number of parameters), dataset size (number of training examples), compute budget as well as training time (Hestness et al. [2017], Rosenfeld et al. [2019], Kaplan et al. [2020], Gordon et al. [2021], Zhai et al. [2022], Hoffmann et al. [2022]). Subsequently, a number of distinct theoretical proposals have been put forward to explain the observed power laws (Bahri et al. [2021], Hutter [2021], Sharma and Kaplan [2022], Maloney et al. [2022], Michaud et al. [2023]). \n\nIn addition to these ubiquitous power laws, which provide a precise account of the improvement of model performance with scale, a number of studies have also reported the sudden emergence of new capabilities in neural networks. New capabilities can emerge either as a function of time, a phenomenon known as grokking (Power et al. [2022], Thilak et al.   1 For the record, this manuscript has been written in its entirety by a human named Shreyas Gokhale. arXiv:2307.09550v1 [cs.LG] 18 Jul 2023 [2022], Liu et al. [2022a]), or as a function of the network size, which we will refer to in this paper as 'emergence with scale' (Wei et al. [2022], Ganguli et al. [2022]).",
            "score": 0.46062159431544086,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 1052,
                    "end": 1072,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 1255,
                    "end": 1268,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1281,
                    "end": 1294,
                    "matchedPaperCorpusId": "243865620"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345458984375
        },
        {
            "corpus_id": "269033049",
            "title": "Scaling Laws for Data Filtering\u2014Data Curation Cannot be Compute Agnostic",
            "text": "We begin with our first scaling law formulation that models only the instantaneous system state and not the global state.The intuition behind such a setup is that it allows the mixing of different buckets naturally by calculating the instantaneous change in the loss by training on any sample from any data bucket.Assume that 5 represents the axiom of scaling of loss with data seen.\n\nThe above can be thought of as the derivative of the more generally recognized scaling law y = n b .Then, rearranging we have that,\n\n\u21d2 Assume that 'b' utility term is a constant for a given region of the training curve.Then, between two such points in the state where we have seen n 1 , n 2 samples respectively,\n\nGiven a single data bucket, the assumption that 'b' is constant, should for instance, hold true during a given epoch.This gives us the following closed form relationship between loss values after seeing n 1 , n 2 samples respectively.\n\nThe closed-form solution for the rate of decay or loss, y k , between training epochs is given by:\n\nWhere y 1 is the loss at the end of the first epoch, and\n\nThe scaling term b is assumed to be a constant for a given region of the training curve.For example, during a given epoch, the relationship between y 2 and y 1 is given by y 2 = y 1 n2 n1 b .",
            "score": 0.4590023586968876,
            "section_title": "F.1. Utility Based Decay",
            "char_start_offset": 44923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 383
                },
                {
                    "start": 385,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 516
                },
                {
                    "start": 518,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 697
                },
                {
                    "start": 699,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 933
                },
                {
                    "start": 935,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1091
                },
                {
                    "start": 1093,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1284
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08270263671875
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "In general, a scaling law includes four parameters, which also characterize a language model: (1) Parameters count N . The number of parameters of an LLM is often associated with the number of transformer layers and the hidden size, except for some MoE LLMs. (2) Data size D. In LLM, this refers to the number of tokens for training. (3) Computation cost C. This is typically measured in terms of time and computational resources. (4) Loss L. The performance of training is usually evaluated by the training loss. There are two representative scaling laws for transformer LLMs. The Kaplan scaling law Proposed by Kaplan [48], the law examines the statistical relations between the parameters C, N, D and L over a wide range of values, models and data tokens. The relationships can be expressed through the following equations: \n\nLpDq \" \n\nLpCq \" \n\nwhere the loss L is influenced by parameters N , D, and C, shedding light on decision-making processes when computational resources are limited. The Chinchilla scaling law Proposed by DeepMind [49], the law provides guidelines for compute-optimal training of LLMs, specifically when computational resources are limited. Through rigorous experiments spanning a wide range of model sizes from 70M to 16B and dataset sizes from 5B to 500B tokens, they derived a scaling law with different coefficients compared to Kaplan's, as shown below: \n\nwhere E denotes the loss of an ideal generative process on the test data. Furthermore, claimed by the research, the constants in this formula are \u03b1 \" 0.34, \u03b2 \" 0.28, A \" 406.4, B \" 410.7, L 0 \" 1.69. Moreover, there is a general constraint that model the relationship between C and pN, Dq: C \" 6N D, which means that it costs six FLOPs per parameter to train one token. Thus, the optimal selection of model size and data size can be determined and expressed as: \n\nFrom the equations, scaling laws can guide decisions regarding model size. Given a fixed compute budget (e.g., 100K GPU hours), they enable predictions on whether a smaller model trained for a longer duration or a larger model trained for a shorter time would yield better performance.",
            "score": 0.457884487376788,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 13627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3720703125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.",
            "score": 0.45787045557546346,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443603515625
        },
        {
            "corpus_id": "277940458",
            "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
            "text": "Early studies [5,17,37] explore the relationship between model performance and scale. Although the concept of scaling laws is not initially formalized, these pioneering works provide crucial insights and lay the groundwork for understanding the predictability of model performance as scale increases. \n\nIn NLP, the concept of scaling laws is widely recognized largely due to the works of [15,16,20]. These studies formally establish a quantitative relationship between model size, dataset size, and training compute for transformer models. Their findings have not only provided a foundation for subsequent research, but have also inspired deeper investigations into the fundamental principles governing scaling behavior in language models. \n\nBeyond empirical exploration of neural scaling laws across different models and tasks, many researchers have sought to challenge these patterns by refining model training strategies or data sampling approaches in hopes of breaking conventional scaling laws. [42] proposes a novel approach to *breaking* the neural scaling law by initializing large models with the weights of smaller pretrained models, thus improving data efficiency. [39] has further shown that model shape, rather than model size, plays a more crucial role in downstream NLP tasks and has introduced a scaling strategy called DeepNarrow, which reduces parameter count while improving training speed and performance.",
            "score": 0.45732318312132936,
            "section_title": "Scaling Laws in NLP",
            "char_start_offset": 6545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 17,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "260816728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "267406160",
            "title": "A Dynamical Model of Neural Scaling Laws",
            "text": "Large scale language and vision models have been shown to achieve better performance as the number of parameters and number of training steps are increased. Moreover, the scaling of various loss metrics (such as cross entropy or MSE test loss) has been empirically observed to exhibit remarkably regular, often power law behavior across several orders of magnitude (Hestness et al., 2017;Kaplan et al., Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 2020). These findings are termed \"neural scaling laws\". \n\nNeural scaling laws play a central role in modern deep learning practice, and have substantial implications for the optimal trade-off between model size and training time (Hoffmann et al., 2022), as well as architecture selection (Alabdulmohsin et al., 2023). Understanding the origin of such scaling laws, as well as their exponents, has the potential to offer insight into better architectures, the design of better datasets (Sorscher et al., 2022), and the failure modes and limitations of deep learning systems. Yet, many questions about neural scaling laws remain open. \n\nIn this paper, we introduce and analyze a solvable model which captures many important aspects of neural scaling laws. In particular, we are interested in understanding the following empirically observed phenomena: Test Loss Scales as a Power-law in Training Time and Model Size and Compute. In many domains of deep learning, the test loss of a model with N trainable parameters trained for t iterations has been found to scale as L(t, N ) \u2248 L 0 + a t t \u2212rt + a N N \u2212r N (Kaplan et al., 2020;Hoffmann et al., 2022). These scaling law exponents r t , r N generally depend on the dataset and architecture. We demonstrate scaling laws on simple vision and language tasks in Figure 1. The compute is proportional to the number of steps of gradient descent times the model size C \u221d N t. Setting N and t optimally gives that test loss scales as a power law in C. This is the compute optimal scaling law. \n\nCompute-Optimal Training Time and Model Size Scaling Exponents Are Different.",
            "score": 0.45732318312132936,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 792,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 1025,
                    "end": 1048,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1667,
                    "end": 1689,
                    "matchedPaperCorpusId": "49321232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "270199496",
            "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
            "text": "Our work aims to take initial steps in this direction: motivated by these questions, we propose individualized data scaling laws, which describe how the impact of data scales with the size of the dataset for each training example.Our analysis focuses on a data point's marginal contribution to a trained model's performance, which previous work has noted shrinks with the size of the preceding dataset (Kwon and Zou, 2021); we build on this by showing that the marginal contribution shrinks at a reliable rate, and that this rate varies between data points.The scaling behavior is represented by a simple parametric form, which is inspired by existing aggregate scaling laws (see Section 2).We find support for this phenomenon in certain classical learning settings (e.g., linear regression), and we show that it holds empirically across several model types, including logistic regression, multi-layer perceptrons (MLPs) and support vector machines (SVMs).\n\nAnalyzing individual scaling laws can ultimately help improve the dataset, for example by identifying data points that consistently degrade a model's performance, or by surfacing data points whose contributions remain large even as the dataset size grows.Doing so requires fitting scaling parameters for every example in a dataset, which can be computationally costly; for example, it would be intractable to precisely estimate each point's expected marginal contribution at a range of dataset sizes, even though this is a natural way to observe the trend from the scaling law (see Figure 1).We therefore consider how to make the fitting approach efficient in Section 3, where we develop procedures that let us estimate the scaling parameters for entire datasets.These include (1) a statistical approach to estimate the scaling law given a moderate number of sampled marginal contributions, and (2) a neural estimator that amortizes the fitting process across all examples in a dataset.\n\nWe conduct experiments to test our approach on several datasets (including tabular, text and image data) and multiple model types.We verify that the scaling law accurately describes the mean contribution at each dataset size: for example, we find that our scaling law explains >0.9 of the variance in marginal contributions between data points (see Table 1).",
            "score": 0.4568520212216769,
            "section_title": "Introduction",
            "char_start_offset": 1880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 956
                },
                {
                    "start": 958,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1944
                },
                {
                    "start": 1946,
                    "end": 2076
                },
                {
                    "start": 2076,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.471435546875
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "Figure 1(3) illustrates that PARSCALE offers superior inference efficiency. \n\nFurthermore, we show that the high training cost of PARSCALE can be reduced by a two-stage approach: the first stage employs traditional training with most of the training data, and PARSCALE is applied only in the second stage with a small number of tokens. Based on this, we train 1.8B models with various P and scale the training data to 1T tokens. The results of 21 downstream benchmarks indicate the efficacy of this strategy. For example, when scaling to P = 8, it yields a 34% relative improvement for GSM8K and 23% relative improvement for MMLU using exactly the same training data. We also implement PARSCALE on an off-the-shelf model, Qwen-2.5 (Qwen Team, 2024), and demonstrate that PARSCALE is effective in both full and parameter-efficient fine-tuning settings. This also shows the viability of dynamic parallel scaling, which allows flexible adjustment of P during deployment while freezing the backbone weights, to fit different application scenerios. \n\nTable 1 compares PARSCALE with other mainstream scaling strategies. Beyond introducing an efficient scaling approach for language models, our research also tries to address a more fundamental question in machine learning: Is a model's capacity determined by the parameters or by the computation, and what is their individual contribution? Traditional machine learning models typically scale both parameters and computation simultaneously, making it difficult to determine their contribution ratio. The PARSCALE and the fitted parallel scaling law may offer a novel and quantitative perspective on this problem. \n\nTable 1: Comparisons of mainstream LLM scaling strategies. We subdivide parameter scaling into traditional Dense Scaling and Mixture-of-Expert (MoE) Scaling (Fedus et al., 2022) for comparison. Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference.",
            "score": 0.4563568256182436,
            "section_title": "Introduction",
            "char_start_offset": 4059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 78,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 1816,
                    "end": 1836,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "254636568",
            "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
            "text": "We present a systematic study of scaling laws for contrastive language-image learning, investigating how scale affects performance on several downstream tasks and across adaptation methods. We find-in accord with previous works on uni-modal learning [35,82]-a power law relation between scale (model, data and the number of samples seen) and downstream performance in a broad range of settings, including zero-shot classification, retrieval, few-and full-shot linear probing and fine-tuning. Interestingly, the scaling behavior for OpenCLIP-LAION pre-trained models and for OpenAI-WIT-400M pre-trained models differ, showing distinct benefits of one over another on different downstream tasks. We hypothesize that such task-specific scaling differences originate from the different pre-training datasets. Predictions for model performance on larger scales made on the basis of the scaling laws estimate 81.9% zero-shot top-1 accuracy on ImageNet for a ViT-G/14 CLIP model trained on 68B image-text samples from scratch. \n\nOur study opens many directions for further investigations. Obtaining more data points for smaller and intermediate scales can provide enough sampling density to better understand the optimal configuration of model size, dataset size and number of samples seen given a fixed compute, similar to works such as [28,39]. Scaling laws for robustness benchmarks [70] can be derived when controlling for larger accuracies observed at larger scales. Further, treating vision and text encoder scales separately may lead to modality specific scaling laws. A promising direction is to study the effect of the pre-training dataset on scaling behavior. Our observations so far hint that the data source may strongly influence task-specific scaling. This paves the road for studies on foundation datasets [67]. Having open datasets [65,64] and open source tools [32]  The NVIDIA A100 GPUs reach peak efficiency of 48.75 GFLOP/(s W) when utilizing the FP64 Tensor Cores. This made the employed machine rank highest in the Green500 list as of November 2020 as the most energy efficient supercomputer among the first 100 machines of the Top500 list with 25 GFLOP/(s W).",
            "score": 0.45582116759079405,
            "section_title": "Conclusion",
            "char_start_offset": 31297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "220280805"
                },
                {
                    "start": 1845,
                    "end": 1848,
                    "matchedPaperCorpusId": "252917726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Our first experimental setting considers scaling in a setting where all models have the same data constraint. For these experiments, the unique training data budget D C is fixed at either 100M, 400M or 1.5B tokens. For each data budget, we train a set of language models with increasing amounts of compute that is allocated to either more parameters or more epochs on the unique training data. \n\nFigure 3 (left) shows the main results for scaling with 100M unique tokens2 (see Appendix C for 400M and 1.5B tokens). For 100M tokens, the corresponding one-epoch compute-optimal model  according to scaling laws from [42] has U N of approximately 7M parameters (see Appendix B for the scaling coefficients we use). Results show that more than a 50% reduction in loss can be attained by training for several epochs (R D > 0) and increasing model size beyond what would be compute-optimal for 100M tokens (R N > 0). We find the best loss to be at around 20-60\u00d7 more parameters and epochs, which corresponds to spending around 7000\u00d7 more FLOPs. These results suggest that one-epoch models significantly under-utilize their training data and more signal can be extracted by repeating data and adding parameters at the cost of sub-optimal compute utilization. \n\nFigure 3 (right) shows the predicted contours created by fitting our data-constrained scaling laws on 182 training runs. In the single-epoch case (R D = 0) with near compute-optimal parameters (R N = 0) our scaling equation ( \u00a73.1) reduces to the Chinchilla equation. In this case, both formulas predict the optimal allocation of compute to parameters and data to be the same, resulting in overlapping efficient frontiers. As data is repeated for more than a single epoch, our fit predicts that excess parameters decay faster in value than repeated data (R * N < R * D ). As a result, the dataconstrained efficient frontier suggests allocating most additional compute to more epochs rather than more parameters. This contrasts the Chinchilla scaling laws [42], which suggest equally scaling both. However, note that they do not repeat the entire training data and their parametric fit explicitly relies on the assumption that models are trained for a single epoch only.",
            "score": 0.45575079810387686,
            "section_title": "Results: Resource Allocation for Data-Constrained Scaling",
            "char_start_offset": 11698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271484375
        },
        {
            "corpus_id": "270045432",
            "title": "Scaling Laws for Discriminative Classification in Large Language Models",
            "text": "Given our two-stage training pipeline, our goal in studying scaling laws is to predict how well a model adapted to a domain using a language modeling objective (stage 1) will perform when discriminatively fine-tuned as a classifier (stage 2).We evaluate this scaling behavior using various metrics that offer insights into performance, efficiency, and behavior across different scales, including FLOPs (Floating Point Operations): The number of floating point operations executed during the language modeling stage.It provides insights into the computational complexity of the LLM and how it scales with model and dataset size.\n\nLanguage Modeling Loss: The cross entropy between the model's predicted next token and the actual next token in training data, directly optimized by the language modeling objective.Monitoring the loss ensures that the model is converging toward an optimal language model of the data.Figure 3 depicts properties of the scaling laws observed in our experiments.In Figure 3 (a) we plot classification loss as a function of language modeling FLOPs for each of the model sizes we adapted.We observe overlap across the three model sizes tested such that for a given compute budget such as 10 18 FLOPs, larger models exhibit lower training loss, suggesting their ability to learn complex patterns and structure within the training data.While we do not fit a formal scaling law here, the observation is consistent with compute optimal language modeling [16,18] applied to discriminative finetuning.\n\nIn Figure 3 (b), we plot the classification loss as a function of number of tokens seen during our domain adaptation stage.We observe scaling as a function of number of training tokens with larger models exhibiting lower classification loss for the same number of training tokens.We also observe a linear relationship between supervised classification loss and the number of training tokens across model sizes.These results suggest that even the largest model we trained will still benefit from more data.We find this encouraging because it means we are able to produce better classifiers when discriminatively fine-tuned, despite our practical parameterlimitations [15,18].\n\nWe relate language modeling loss to classification loss in Figure 3  (c).We observe a linear relationship between language modeling loss during domain adaptation and classification loss during discriminative fine-tuning across model sizes.",
            "score": 0.4547558333126729,
            "section_title": "EXPERIMENTS 5.1 Offline Training and Evaluation",
            "char_start_offset": 19942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 627
                },
                {
                    "start": 629,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2195
                },
                {
                    "start": 2197,
                    "end": 2270
                },
                {
                    "start": 2270,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27099609375
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "More recent breakthroughs challenged traditional scaling assumptions. Broken Neural Scaling Law (BNSL) introduced non-monotonic trends, meaning that model performance can sometimes worsen before improving, depending on dataset thresholds and architectural bottlenecks (Caballero et al., 2023). Another exciting development came  from small LMs, where optimized training strategies, such as a higher data-to-parameter ratio and adaptive learning schedules, enable models ranging from 1.2B to 2.4B parameters to rival significantly larger 7B-13B models (Hu et al., 2024). These findings reshape the fundamental assumptions of scaling laws, proving that strategic training can outperform brute-force model expansion.",
            "score": 0.45449433510258413,
            "section_title": "Taxonomy of neural scaling laws",
            "char_start_offset": 5701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "276580569",
            "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
            "text": "Inference strategies have been a long-studied topic in the field of language processing. Traditional approaches include greedy decoding (Teller, 2000), beam search (Graves, 2012), and Best-of-N. \n\nRecent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored.",
            "score": 0.45363649026552644,
            "section_title": "Test-Time Scaling",
            "char_start_offset": 6996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "Prior works of scaling laws in the realm of LLMs have concentrated on predicting the performance of fully-trained LLMs after completing training on given hyperparameters and resource (Kaplan et al., 2020;Henighan et al., 2020;Cherti et al., 2023;Aghajanyan et al., 2023). However, a critical aspect that remains underexplored is the temporal dimension of LLM behaviors during training. Understanding the optimization trajectory during the training process of LLMs enhances our comprehension of their behavioral patterns and holds extensive potential for practical applications. Firstly, the arXiv:2404.17785v1 [cs.CL] 27 Apr 2024 ability to accurately predict the entire training trajectory during the preparation of LLM pre-training facilitates the direct selection of data mixture proportions and training hyperparameters for the target LLM within a short training period. This approach addresses the recognized issue that data mixture proportions (Xie et al., 2023a) and regularization hyperparameters (Yang et al., 2022) are not transferrable as model size scales. Furthermore, from the perspective of the LLM pre-training process, this predictive capability could provide a theoretical basis for implementing precise early stopping mechanisms, thereby optimizing computational resource management. Despite its significance, prior work has not explored predicting LLM training from the temporal perspective. \n\nIn this paper, we propose the novel concept of Temporal Scaling Law for Large Language Models. Specifically, in decoder-based generative models, the generation loss attributed to tokens in different positions is imbalanced. By carefully investigating this phenomenon, we discovered that the loss patterns associated with each token conform to a reciprocal-law across different model scales and training stages. We further examine the evolution of the curve parameters for the reciprocal-law and identify the temporal patterns for how these curve parameters change. We term this phenomenon as the temporal scaling law. To further demonstrate the practical values of our proposed temporal scaling law, we employ the temporal patterns to predict future test loss using only the test loss from the initial training periods. Results on both the in-distribution (IID) validation set and the out-ofdistribution (OOD) validation set show that our methodology marks a significant improvement over baseline approaches.",
            "score": 0.45348599191354927,
            "section_title": "Introduction",
            "char_start_offset": 1828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 246,
                    "end": 270,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 950,
                    "end": 969,
                    "matchedPaperCorpusId": "258741043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10687255859375
        },
        {
            "corpus_id": "257038048",
            "title": "Scaling Laws for Multilingual Neural Machine Translation",
            "text": "Over the past few years, scaling has emerged as a popular and effective way to improve the performance of neural networks (Brown et al., 2020;Chowdhery et al., 2022;Lepikhin et al., 2020). Given the costs associated with training large neural models, much work has gone into understanding their scaling properties and predicting the 1 Google Research 2 Carnegie Mellon University 3 Instituto Superior T\u00e9cnico. Correspondence to: Patrick Fernandes <pfernand@cs.cmu.edu>. evolution of their performance with scale through scaling laws. Such scaling laws have been instrumental in guiding the model development efforts across a variety of domains such as computer vision (Zhai et al., 2022), language modeling (Kaplan et al., 2020;Hoffmann et al., 2022), and neural machine translation (Ghorbani et al., 2022). Despite these impressive developments, most of the scaling laws studies available in the literature only focus on single-task, single-language models. On the contrary, current massive neural models are often trained to solve more than one task across one or more modalities & languages (Chowdhery et al., 2022;Sanh et al., 2022;Reed et al., 2022). This disconnect from the current research frontier limits the applicability of scaling laws in guiding model development decisions. In particular, currently available scaling laws studies are unable to inform the decision process on balancing the different tasks effectively at training time. Without such guidance, practitioners often have to rely on cumbersome and costly approaches such as approximate grid search to inform their decision-making; such approaches quickly become infeasible as the problem scale grows. \n\nIn this paper, we take the initial step towards developing a quantitative understanding of the scaling behavior for multitask models. We choose multilingual neural machine translation (MNMT) as the setup for this initial study.",
            "score": 0.45283379289724596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 142,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 142,
                    "end": 165,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 668,
                    "end": 687,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1094,
                    "end": 1118,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1118,
                    "end": 1136,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280517578125
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Hestness et al., 2017;Kaplan et al., 2020;Alabdulmohsin et al., 2022;Bansal et al., 2022;Zhai et al., 2022). Generally, the performance of deep neural networks f (x) (such as its error rate or log-perplexity) often follows a power law f (x) \u223c \u03b2x \u2212c + \u03b5 as one varies a dimension x, such as the data size or model parameters. These \"scaling laws,\" as they are known today, have been used, among others, to determine the training data size needed for a specified level of accuracy (Cho et al., 2015;Beleites et al., 2013;Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020;Hoffmann et al., 2022;Alabdulmohsin et al., 2024b), with some theoretical justification (Bahri et al., 2021;Hutter, 2021;Sharma and Kaplan, 2022). \n\nBesides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2024), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such In RINS, the model f : X \u2192 Y is split into two parts: the first block f A : X \u2192 X is applied iteratively to its own output r times before passing the output to the second block. RIGHT: Illustrative examples of models with different signatures and degrees.",
            "score": 0.45283379289724596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 220,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "35138335"
                },
                {
                    "start": 874,
                    "end": 898,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1248,
                    "end": 1266,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1362,
                    "end": 1379,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8095703125
        },
        {
            "corpus_id": "273188529",
            "title": "Pioneering new paths: the role of generative modelling in neurological disease research",
            "text": "One of these challenges is the enormous amount of data and computational resources required for training and inference in these models. The relationships between generalisation performance, model complexity, data requirements and computational resources in deep neural networks, in general, are not well understood in theory [127,128]. However, empirical scaling analyses in deep neural networks have been performed to understand these mechanisms [129][130][131][132][133][134] that can be described with a mathematical functional form called scaling laws, a particular form of power law. In particular, generative models seem to smoothly follow scaling laws with a predictable relationship between data set size, model complexity, and generalisation performance for a given computational budget. \n\nKaplan et al. [132] showed the power law relationships between the test loss, computational resources, dataset size and model size, which describe the reduction in generalisation performance based on an increase in the individual factors, given that they are not bottlenecked by the remaining two. In this example, a 10\u00d7 increase in the dataset size results in a \u223c 20% decrease in the test loss, whereas a 10\u00d7 increase in model size results in a \u223c 16% decrease in the generalisation performance within the considered limits. If the computational budget is limited, this type of power analysis can, for instance, be used to determine how this budget should be divided between an increase in the amount of data and the model size. The factors are increased in different proportions, where the proportions depend on different regimes of model and dataset sizes, but not in isolation, as this leads to diminishing returns. While better generative algorithms can improve the performances in a given domain, currently, the field focuses on scaling up models by increasing the dataset sizes in combination with the model complexity [135]. Stateof-the-art generative models are already very complex, with billions of model parameters, and are trained on hundreds of millions or billions of data points, e.g., DALL-E2 uses 3.5 billion parameters and was trained on 650 million textimage pairs [16], GPT-3 has 175 billion parameters trained on 300 billion text tokens [11], Megatron-Turing NLG 530B has 530 billion parameters and a training set of 270 billion tokens [136].",
            "score": 0.45227782779293396,
            "section_title": "Data and computational resources",
            "char_start_offset": 49568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 457,
                    "matchedPaperCorpusId": "59617568"
                },
                {
                    "start": 472,
                    "end": 477,
                    "matchedPaperCorpusId": "235367962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "270199496",
            "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
            "text": "Machine learning models for vision and language have improved dramatically in recent years (Radford et al., 2021;Touvron et al., 2023;OpenAI, 2023;Gemini Team et al., 2023), in part due to increasing model sizes, but also due to 1 Stanford University.Correspondence to: Ian Covert <icovert@stanford.edu>.\n\nProceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).\n\nusing larger amounts of high-quality training data (Gadre et al., 2023).Recent research has found that increasing the amount of training data improves models at a predictable rate, leading to scaling laws that describe the relationship between error and dataset size (Kaplan et al., 2020).For example, Hoffmann et al. (2022) introduced the following scaling law to determine Chinchilla's compute-optimal training budget,\n\nwhere \u03f5 represents irreducible error, p is the number of model parameters, n is the number of training examples, and (a, b, \u03bd, \u03bb) are learned constants.These scaling laws help inform how to trade off model and data size, but they take an aggregate view of the dataset by not differentiating between training examples; this is limiting because certain data points are more useful than others, particularly for noisy web-scraped datasets.\n\nIn the current era of datasets aggregated from heterogeneous sources, it is important to understand how individual data points or data sources affect the behavior of model training.This can give practical guidance on what type of data to prioritize, especially as the dataset grows.It also provides more fundamental insights into how the impact of different data scales with data size-e.g., which points are useful in small datasets but whose value diminishes quickly as the data size increases, and which points are relatively more valuable in large datasets.\n\nOur work aims to take initial steps in this direction: motivated by these questions, we propose individualized data scaling laws, which describe how the impact of data scales with the size of the dataset for each training example.",
            "score": 0.45216235477339617,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 304
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 863
                },
                {
                    "start": 865,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1301
                },
                {
                    "start": 1303,
                    "end": 1484
                },
                {
                    "start": 1484,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1863
                },
                {
                    "start": 1865,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 113,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270263671875
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "Finally, the variable must be fit with an objective and optimization method, which may in turn have their own initialization and hyperparameters to choose. \n\nTo aid scaling laws researchers in reporting details necessary to reproduce their work, we propose a checklist (Figure 1 -an expanded version may be found in Appendix B ). Based on this checklist, we summarize these details for all 51 papers in tabular form in Appendix C. We find that important details are frequently underreported, significantly impacting reproducibility, especially in cases where there is no code -only 19 of 42 papers surveyed have analysis code/code snippets available. Additionally, 23 (a little over half) of surveyed papers do not describe the optimization process, and 15 do not describe how training FLOPs or number of parameters are counted, which has been found to significantly change results (Porian et al., 2024). In addition, we fit our own power laws to further demonstrate how these choices critically impact the final scaling law (Section \u00a77). \n\nThe authors also claimed that the model size required to fit a given dataset grows log linearly. As for larger scale models, Amodei et al. (2016) observe a power-law WER improvement on increasing training data for a 38M parameter Deep Speech 2 model. Hestness et al. (2017) show similar power law relationships across several domains such as machine translation, language modeling, image processing and speech recognition. Moreover, they find that these exponential relationships found hold across model improvements. Kaplan et al. (2020) push the scale of these studies further, studying power laws for models up to 1.5B parameters trained on 23B tokens to determine the optimal allocation of a fixed compute budget. \n\nLater studies (Hoffmann et al., 2022;Hu et al., 2024) revisit this and find that Kaplan et al. (2020) greatly underestimate the amount of data needed to train models optimally, though major procedural differences render it challenging to attribute the source of this discrepancy. Since then, researchers have studied various aspects of scaling up language models.",
            "score": 0.45191417605779527,
            "section_title": "\u2022 Optimization Hyperparameters",
            "char_start_offset": 4399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 1166,
                    "end": 1186,
                    "matchedPaperCorpusId": "11590585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2347412109375
        },
        {
            "corpus_id": "265043448",
            "title": "Into the LAIONs Den: Investigating Hate in Multimodal Datasets",
            "text": "While attempting to unearth what this specific dataset scaling law was that the practitioners were so inspired by, we repeatedly encountered a certain cartoon sketch 'power-law' plot referred to in both personal exchanges as well as in surveys such as [72]. As it turns out, this cartoon sketch power-plot first appeared as Figure 6 in \"Deep learning scaling is predictable, empirically\" [29], a work that emerged out of Baidu research in 2017. The authors that first presented this plot posit that the generalization error associated with an ML model exhibits a three-phase behavior about its training dataset size. In the first phase, they state maps to the 'small data region,' where \"models will struggle to learn from a small number of training samples\" resulting in high generalization errors. The second phase (or the middle portion of learning curves), they claim, is the 'power-law region,' where the generalization error monotonically decreases with training dataset size (linear with application-specific slopes when plotted on a log-log scale). This phase stretches till we hit the point of the 'glass-ceiling' or 'unbreachable error floor' on account of factors such as model mismatch and mislabeled data (constituting the third phase). This, of course, has been further supplanted by the likes of the Chinchilla scaling laws (20 tokens per model parameter) [30] in the specialized context of LLMs.",
            "score": 0.45183086787291477,
            "section_title": "C The origins of the dataset scaling laws: A cartoon sketch emerges",
            "char_start_offset": 40305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12139892578125
        },
        {
            "corpus_id": "278165731",
            "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance",
            "text": "Performance Estimation via Scaling Laws. Early studies on large language models (LLMs) have shown that core metrics like cross-entropy loss exhibit approximate power-law relationships with key design factors such as model size, dataset scale, and overall computational budget (Kaplan et al., 2020;Hoffmann et al., 2022;Hernandez et al., 2022;Gordon et al., 2021). While these classical \"scaling laws\" provide valuable insights into how performance grows with parameters and tokens, their estimates typically assume a single model family or a narrowly defined training setup. Consequently, scaling relationships learned from one architecture (e.g., Transformer-based language models) may not directly generalize to another. \n\nRecent work expands scaling laws beyond a single family. For instance, Ye et al. (2023) investigate whether past performance records from multiple LLM variants can predict new model settings; Owen (2024) show that aggregated benchmark scores (e.g., BIG-Bench) follow smooth scaling trends across diverse model families, yet individual tasks are more difficult to predict. Ruan et al. (2024) explore observational rather than purely experimental scaling, leveraging over a hundred pretrained LLMs to fit parametric curves without training each from scratch. These lines of research challenge the assumption that scaling laws transfer seamlessly across all model families. Indeed, Choshen et al. (2025) highlight large predictive errors when applying scaling trends derived from one family to a different, structurally distinct family. Overall, while scaling laws are fundamental to LLM research, their reliance on homogeneous or near-homogeneous settings can limit predictive accuracy, especially when addressing newly merged or fine-tuned models for which classical scaling metrics (e.g., total FLOPs or training tokens) are not strictly comparable. \n\nPredicting Downstream Task Performance from Observational Data. A growing body of work aims to bypass some limitations of traditional scaling laws by using observational or empirical records from existing models. Rather than modeling only cross-entropy loss, these methods directly estimate how well a new or partially evaluated LLM will perform on downstream benchmarks. One common strategy is to reduce the high cost of comprehensive benchmarking by sampling only a small subset of instances.",
            "score": 0.45105376573764344,
            "section_title": "Related Work",
            "char_start_offset": 4326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 796,
                    "end": 812,
                    "matchedPaperCorpusId": "258865485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "269899695",
            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
            "text": "Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.",
            "score": 0.4498742323262134,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models. \n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law f r (x) = \u03b2 r x \u2212cr + \u03b5 r , then c r increases with r while \u03b5 r decreases. Furthermore, the coefficient \u03b2 r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2 r , its faster convergence (c r ) and lower asymptotic limit (\u03b5 r ) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results. To quantify this more explicitly, we train language models with four signatures A r B: r \u2208 {1, 2, 3, 4}. Then, we plot the optimal value of r against training compute. As shown in Figure 5b, the optimal value of r monotonically increases with training compute, in agreement with earlier results. Also, smaller models benefit more from RINS. \n\nAdding Linear Adapters. Earlier in Section 4, we showed that enabling stochastic RINS during training exhibits a tradeoff between worst-case and best-case performance, depending on whether or not RINS is applied at inference time. Next, we introduce a additional improvement: when a maximum of r recursion rounds are used in stochastic RINS, we add r lightweight, linear adapters (i.e. linear projection layers) to the output before the projection head. The choice of which adapter to apply is a function of how many recursion rounds are used. Specifically, if signature A r B is used, the r-th adapter is applied. Empirically, this introduces < 1% more parameters and has a negligible Figure 6: LEFT 2 PLOTS: y-axis corresponds to performance when RINS is enabled during training but disabled at inference time in 600M-parameter LMs.",
            "score": 0.44863860833635794,
            "section_title": "Multimodal Systems",
            "char_start_offset": 22964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "Scaling law principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of large language models such as GPT-4, Llama and Gemini. However, the original scaling law paper by OpenAI did not disclose the complete details necessary to derive the precise scaling law formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the scaling law formulations proposed in the original OpenAI paper remain valid when scaling the model size up to 33 billion, but the constant coefficients in these formulas vary significantly with the experiment setup. We meticulously identify influential factors and provide transparent, step-by-step instructions to estimate all constant terms in scaling-law formulas by training on models with only 1M~60M parameters. Using these estimated formulas, we showcase the capability to accurately predict various attributes for models with up to 33B parameters before their training, including (1) the minimum possible test loss; (2) the minimum required training steps and processed tokens to achieve a specific loss; (3) the critical batch size with an optimal time/computation trade-off at any loss value; and (4) the complete test loss trajectory with arbitrary batch size.",
            "score": 0.44850058431567036,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "Progress in artificial intelligence (AI) has relied heavily on the dramatic growth in the size of models and datasets. An active area of research focuses on understanding how test error decreases with increases in model and data size. This work has led to the development of scaling laws which posit that test error decreases as a power law with both. However, several theoretical aspects remain unclear. One significant gap is understanding how test error and the existing scaling laws change as the training time is varied (Kaplan et al., 2020;Bahri et al., 2021;Rosenfeld et al., 2020;Sharma & Kaplan, 2022). \n\nThe practical relevance of this question is clear: under a fixed compute budget, what is the optimal balance between scaling the model size and dataset volume, and what is the right amount of training for a given data volume? This is particularly relevant in the context of large language models (LLMs), which are often trained for a single epoch, raising questions about the potential efficacy of training smaller models for longer (more epochs). Furthermore, current scaling laws do not account for other well-known phenomena in learning, such as double descent (Belkin et al., 2019), in which model performance exhibits non-monotonic changes with respect to training data volume, model size, and training time. In particular, double descent theory predicts that test error should increase rapidly at the interpolation threshold, the point at which the model interpolates the training set (Nakkiran et al., 2021;Advani & Saxe, 2017). Like scaling laws, current theories of double descent leave several empirical phenomena unexplained: past explanations of double descent require it to occur, but empirically double descent is often not observed; it is unclear whether the interpolation threshold should grow or shrink with model size; prior theory does not explain why models in the infinite-parameter limit sometimes perform worse than their finite-parameter counterparts. \n\nWe seek the simplest possible unified framework in which to understand learning with respect to model size, data volume, and training time. In doing so, we aim to capture the essential scaling properties of learning in deep neural networks.",
            "score": 0.4483924175418692,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 610,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1505,
                    "end": 1528,
                    "matchedPaperCorpusId": "207808916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "274436016",
            "title": "Optimizing Sequential Recommendation Models with Scaling Laws and Approximate Entropy",
            "text": "Scaling laws was first explored in the context of Large Language Models (Kaplan et al., 2020;Yin et al., 2024b;He et al., 2023). Specifically, since the introduction of the Chinchilla scaling law, which models the final pre-training loss L(N, D) as a function of the number of model parameters N and the number of training tokens D, models such as LLaMA2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), and Gemma (Team et al., 2024) have applied this principle. Empirical evidence indicates that model performance consistently improves with increased model size and training data volume (Kaplan et al., 2020;Khandelwal et al., 2019;Rae et al., 2021;Chowdhery et al., 2023). Extensive experiments have explored neural scaling laws under various conditions, including constraints on computational budget (Hoffmann et al., 2022b), data limitations (Muennighoff et al., 2023) and regeneration (Yin et al., 2024a), and instances of over-training (Gadre et al., 2024). These analyses employ a decomposition of expected risk, resulting in the fit \n\n, where \u03b1 N and \u03b1 D are parameters. However, increasing the model size does not necessarily lead to better performance. Some studies have observed a decline in performance due to overfitting (Belkin et al., 2019;Nakkiran et al., 2021). Following theoretical analysis, (Power et al., 2022) and (Niu et al., 2024) empirically validated this point, underscoring the necessity for an enhanced understanding of scaling laws.",
            "score": 0.4483924175418692,
            "section_title": "Scaling Law on Large Sequential Models",
            "char_start_offset": 6220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 655,
                    "end": 678,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 808,
                    "end": 832,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 851,
                    "end": 877,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 895,
                    "end": 914,
                    "matchedPaperCorpusId": "270067614"
                },
                {
                    "start": 1260,
                    "end": 1282,
                    "matchedPaperCorpusId": "207808916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "The concept of scaling laws involves four key factors: loss L, specifically the cross-entropy loss in a causal language modeling setting; model size N , which is determined by the number of model parameters; dataset size D, calculated as the number of training tokens; and computation budget C, represented by the total FLOPs used for training.N opt and D opt are the optimal model size and dataset size given a fixed computation budget.\n\nInitially, we establish power law equations between L and C. In this analysis, we adopt the approach introduced by (Hoffmann et al., 2022), treating the training loss as an unbiased estimate of the test loss.Subsequently, based on the fitted curve, we ascertain the optimal loss for specific FLOPs, enabling us to obtain coefficients for N opt \u221d C a and D opt \u221d C b .When modeling the scaling trend of loss against factors such as N , D, and C, the original scaling laws (Kaplan et al., 2020) utilize the power function L(X) = (X 0 /X) \u03b1 X , where X represents the factor of interest.Subsequent studies (Henighan et al., 2020;Clark et al., 2022;Hoffmann et al., 2022;Gao et al., 2024) employ a more general power-law plus constant form, L(X) = \u03f5 + (X 0 /X) \u03b1 X , to achieve improved fitting.Here, the constant \u03f5 is interpreted as irreducible loss or the entropy of natural text (Hoffmann et al., 2022).In our case, we have simplified all forms of the power law and unified them into L(X) = \u03b2 X X \u03b1 X , which allows for a more intuitive comparison of the scaling capabilities of different models based on coefficients \u03b1 X and \u03b2 X .",
            "score": 0.4483924175418692,
            "section_title": "Scaling laws",
            "char_start_offset": 13844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 437
                },
                {
                    "start": 439,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "In recent years, a number of papers have uncovered machine learning scaling laws-defined as empirical regularities that describe how the performance of a model increases as a function of scale, usually in parameter count and data (Hestness et al. 2017, Kaplan et al. 2020, Hoffmann et al. 2022). Hernandez et al. 2021 described scaling laws for transfer learning, showing how the transfer learning properties of models change as a function of model size. The primary result was that the degree of transfer-as measured by the amount of effective data transferred from one distribution to another-follows a simple power law in parameter count and fine-tuning data size. However, their analysis left much room for further exploration, as it only considered transfer learning from English to Python, and did not explore the relationship between the pre-training data size and the degree of downstream transfer learning. \n\nScaling laws for transfer are important to study because they inform the degree to which progress in machine learning is bottlenecked by data for specific tasks. Consider that to achieve high performance on some tasks, one standard approach in the foundation model paradigm is to pre-train a model on a large, diverse distribution and then fine-tune it on a particular downstream task (Bommasani et al. 2022). An alternative approach to fine-tuning for adapting pre-trained language models to downstream tasks is in-context learning, where a model is given a sequence of demonstrations within a prompt (Brown et al. 2020, Mosbach et al. 2023). The effectiveness of these approaches depends on the degree of knowledge transfer from the pre-trained model to the downstream task or distribution. If the degree of transfer is high, then the cost of fine-tuning is typically low, allowing foundation models to efficiently learn to perform well on downstream tasks with minimal fine-tuning data. Conversely, if the degree of transfer is low, then performance is more heavily dependant on the amount of high-quality fine-tuning data available, motivating the curation of large fine-tuning datasets. \n\nTo help study whether high-quality fine-tuning data might bottleneck performance on important tasks, we devised a simple framework for measuring the transfer gap between two distributions using scaling laws for transfer.",
            "score": 0.4477862732992191,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "Our contributions are 1. We propose a simple two-layer linear neural network model trained with a form of projected gradient descent. We show that this model reproduces power law scalings in training time, model size and training set size. The predicted scaling law exponents are summarized in terms of two parameters related to the data and architecture (\u03b1, \u03b2). 2. We identify a condition on the difficulty of the learning task, measured by the source exponent \u03b2, under which feature learning can improve the scaling of the loss with time and with compute. For easy tasks, which we define as tasks with \u03b2 > 1 where the RKHS norm of the target is finite, there is no improvement in the power-law exponent while for hard tasks (\u03b2 < 1) that are outside the RKHS of the initial limiting kernel, there can be an improvement. For super-easy tasks \u03b2 > 2 \u2212 1 \u03b1 , which have very low RKHS norm, variance from stochastic gradient descent (SGD) can alter the scaling law at large time. Figure 1 summarizes these results. 3. We provide an approximate prediction of the compute optimal scaling laws for hard, easy tasks and super-easy tasks. Each of these regimes has a different exponent for the compute optimal neural scaling law. Table 1 summarizes these results. 4. We test our predicted feature learning scalings by training deep nonlinear neural networks fitting nonlinear functions. In many cases, our predictions from the initial kernel spectra accurately capture the test loss of the network in the feature learning regime. \n\nOverall our results suggest that feature learning may improve scaling law exponents by changing the optimization trajectory for tasks that are hard for the initial kernel.",
            "score": 0.4471682886170783,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1694
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45556640625
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "We refer to such models as \"overtrained\" in this paper, with the token/parameter ratio D/N being a key quantity throughout. Work on inference-time compute [Snell et al., 2024, Brown et al., 2024] and on synthetic and multimodal data [Yang et al., 2024, Fan et al., 2024, Bauer et al., 2024] suggests future models may be even more overtrained. Therefore, modern work on scale must consider ratios much larger than Chinchilla-optimal, and in this work we perform experiments up to D/N \u2248 10 3 and analyze the predictions found by our scaling law for up to D/N \u2248 10 5 . See Appendix B for additional related work.",
            "score": 0.44686244080870535,
            "section_title": "Scaling Laws and Parametric Fits",
            "char_start_offset": 9285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59814453125
        },
        {
            "corpus_id": "264820451",
            "title": "Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions",
            "text": "The ever-evolving landscape of AI introduces novel types of models, such as generative models or concept-based learning algorithms, each with its unique set of properties. Against this backdrop, this category of challenges describes the intricacies of creating explanations for these new types of AIs. Generative AI models, such as those employed for diffusion denoising [98,99] or the family of GPT models for large-scale language generation [100], are disrupting many sectors. These models deliver exceptional performance due to their immense scale. With billions, and in some cases, nearly trillions of parameters, however, their sheer size poses a significant challenge to existing XAI methods [14]. In particular, these methods grapple with the high-dimensional nature of such models, both in terms of computational complexity and in extracting learned concepts. For instance, one obstacle related to the latter point lies in the polysemantic nature of the neurons in generative models, which is thought to arise from a superposition of multiple independent features [101]. XAI methods have so far been mostly limited to classification and regression problems. Accordingly, completely new approaches have to be developed for generative models. In particular, self-supervised or neural generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are becoming more popular. For instance, examining the latent spaces they learn and synthesizing explanations for them is very challenging. Another challenge, particularly for LLMs, concerns scaling laws. Neural scaling laws are functional relationships that relate variables associated to a neural network, such as the number of layers in its architecture and its achieved accuracy after training. Such functional relation of two x and y variable is of the general form y = a \u2022 x \u03b1 , where a and \u03b1 are constants of the scaling law. Such laws govern the aggregate capabilities of LLMs, yet a precise understanding of individual task-level implications of these laws remains elusive, as they appear to manifest unpredictably. It is an open issue whether scaling laws can be used to infer the quality of the artefacts or concepts learned by LLMs. Even if this were possible, such laws might reveal plateaus of XAI scaling relative to general ability scaling or indicate a gap between the two.",
            "score": 0.4465849060036486,
            "section_title": "Creating Explanations for New Types of AI",
            "char_start_offset": 19803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "252199918"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "252070859"
                },
                {
                    "start": 698,
                    "end": 702,
                    "matchedPaperCorpusId": "250624746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.078369140625
        },
        {
            "corpus_id": "253117181",
            "title": "Broken Neural Scaling Laws",
            "text": "The amount of compute used for training, number of model parameters, and training dataset size of the most capable artificial neural networks keeps increasing and will probably keep rapidly increasing for the foreseeable future. However, no organization currently has direct access to these larger resources of the future; and it has been empirically verified many times that methods which perform best at smaller scales often are no longer the best performing methods at larger scales (e.g., one of such examples can be seen in Figure 2 (right) of Tolstikhin et al. ( 2021)). To work on, identify, and steer the methods that are most probable to stand the test-of-time as these larger resources come online, one needs a way to predict how all relevant performance evaluation metrics of artificial neural networks vary in all relevant settings as scale increases. \n\nNeural scaling laws (Cortes et al., 1994;Hestness et al., 2017;Rosenfeld et al., 2019;Kaplan et al., 2020;Zhai et al., 2021;Abnar et al., 2021;Alabdulmohsin et al., 2022;Brown et al., 2020;Bahri et al., 2021) aim to predict the behavior of large-scale models from smaller, cheaper experiments, allowing to focus on the best-scaling architectures, algorithms, datasets, and so on. The upstream/indistribution test loss typically (but not always!) falls off as a power law with increasing data, model size and compute. However, the downstream/out-of-distribution performance, and other evaluation metrics of interest (even upstream/in-distribution evaluation metrics) are often harder to predict, sometimes exhibiting inflection points (on a linear-linear plot) and non-monotonic behaviors. Discovering universal scaling laws that accurately model and extrapolate a wide range of potentially unexpected behaviors is clearly important not only for identifying that which scales best, but also for AI safety, as predicting the emergence of novel capabilities at scale could prove crucial to responsibly developing and deploying increasingly advanced AI systems. The functional forms of scaling laws evaluated in previous work are not up to this challenge. \n\nOne salient defect is that they can only represent monotonic functions.",
            "score": 0.44624523430764745,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2117
                },
                {
                    "start": 2120,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 1009,
                    "end": 1036,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1036,
                    "end": 1055,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2958984375
        },
        {
            "corpus_id": "273549927",
            "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
            "text": "We next explore whether our experimental results follow power-law scaling laws, as seen in other domains. Specifically, if two variables Y and X satisfy the relation Y = \u03b2 \u2022 X \u03b11 , they exhibit a power-law relationship. Applying a logarithmic transformation to both Y and X reveals a linear relationship: log(Y ) = \u03b1 log(X) + log(\u03b2). In our context, Y represents the optimality gap, defined as the deviation from the maximum score (i.e., 1 \u2212 Normalized Score), while X can denote the number of environments, objects, or demonstrations. Using data from our previous experiments with a 100% fraction of demonstrations, we fit a linear model to the log-transformed data, as shown in Fig. 5. Based on all the results, we summarize the following data scaling laws: \n\n\u2022 The policy's generalization ability to new objects, new environments, or both scales approximately as a power law with the number of training objects, training environments, or training environment-object pairs, respectively. This is evidenced by the correlation coefficient r in Fig. 5. \n\n\u2022 When the number of environments and objects is fixed, there is no clear power-law relationship between the number of demonstrations and the policy's generalization performance. While performance initially increases rapidly with more demonstrations, it eventually plateaus, as most clearly shown in the leftmost plot of Fig. 7 (see caption for details). \n\nThese power laws regarding environments and objects can serve as predictive tools for larger-scale data. For example, according to the equation in Fig. 5, we predict that for Mouse Arrangement, achieving a normalized score of 0.99 on novel environments and objects would require 1,191 training environment-object pairs. We leave the verification of this prediction for future work.",
            "score": 0.4460774632206653,
            "section_title": "POWER-LAW FITTING AND QUANTITATIVE ANALYSIS",
            "char_start_offset": 22203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1792
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "Next, we investigate the effect of stochastically dropping blocks during training, inspired by the regularization technique of stochastic depth (Huang et al., 2016). Our primary goal is to determine whether this approach can further enhance the performance of RINS while simultaneously offering the flexibility of reverting to non-recursive inference without significant degradation in model quality. \n\nTo recall, RINS has the signature A r B for some r > 1. To implement stochastic RINS, we introduce a skip probability p s \u2208 [0, 1) and sample during training the number of recursion rounds in each step to be 1 + \u03b7, where \u03b7 is a binomial random variable with probability of success 1 \u2212 p s and number of trials r \u2212 1. Thus, block A is always executed at least once. During inference, we are free to choose how to scale compute by setting the value of r \u2265 1. See the detailed pseudocode in Appendix A. For this, we train bigger models with 1 billion parameters. We use an embedding dimension 2,048 and MLP dimension 8,192. All models have 18 decoder blocks. We train for 500K steps and compare RINS with signature A 3 B against the non-recursive baseline. For p s = 0, RINS@1x is significantly worse, with perplexity scores > 3. As expected, RINS converges in performance to the baseline as p s \u2192 1. Similar results using C4 are in Appendix F. \n\nFigure 4 summarizes the advantages of stochastic RINS. Notably, we observe that as p s > 0 increases, stochastic RINS mitigates the performance degradation incurred when scaling is not applied at inference time, while still offering big gains when inference time is scaled. Not surprisingly, though, scaling inference time is less effective when p s increases, suggesting a tradeoff between flexibility at inference time and the magnitude of potential gains from scaling. As shown in Figure 3b, similar conclusions hold in the asymptotic (infinite-compute) regime, assuming the loss follows a power law relation (Kaplan et al., 2020). We resolve this apparent tradeoff in Section 6 using linear adapters.",
            "score": 0.44601790508580524,
            "section_title": "Stochastic Recursive Inference Scaling",
            "char_start_offset": 17332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "Mikami et al. 2021 support their speculation with a theoretical analysis of the evolution of loss during training within the Neural Tangent Kernel (NTK) framework, building on the work of Arora et al. 2019 andNitanda, Chinot, andSuzuki 2020. However, their theoretical analysis is not directly applicable to the present study, as our fine-tuning data term, f , represents the number of fine-tuning data points, rather than the number of steps during training. Consequently, the NTK framework has limited applicability to our analysis. \n\nInstead of relying on theory to justify the scaling law form represented by Equation 5, we utilize empirical analysis. Specifically, we explored many alternative plausible forms for the scaling law that satisfy at least some of these four conditions, and rigorously tested their ability to predict our experimental data via extensive cross-validation, as described in Appendix D. \n\nDespite the various forms we considered, none substantially improved upon the simple form proposed by Mikami et al. 2021, lending credence to the idea that this form is relatively robust. Given the form's simplicity and good performance, it is the primary form we employed to demonstrate our results. Figure 3: This plot illustrates a cross-section of the fitted scaling law to the data for the fictional encyclopedia dataset, illustrating both clear transfer learning, and that the power law form provides a good fit in pre-training data steps. This empirical observation confirms our intuitions that the scaling law for transfer should reduce to a power law under various conditions. These conditions are detailed in Appendix A . \n\nA.1 Comparing to Hernandez et al. 2021 It is useful to contrast our scaling law form with that in Hernandez et al. 2021. In their study, they define D T , for effective data transferred, as the amount of additional fine-tuning data that a model of the same size trained only on fine-tuning data would have needed to achieve the same loss on fine-tuning data as a model trained purely on pre-training data. This concept is given by the following equation: \n\nwhere N is the number of non-embedding parameters, and D F is the size of the fine-tuning data.",
            "score": 0.44560522065629526,
            "section_title": "Conclusion",
            "char_start_offset": 31847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4208984375
        },
        {
            "corpus_id": "266818336",
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "text": "Nonetheless, research on scaling laws is of utmost importance, considering that the current open-source models are merely at the initial stage of Artificial General Intelligence (AGI) development. In addition, early works (Hoffmann et al., 2022;Kaplan et al., 2020) reached varying conclusions on the scaling of model and data with increased compute budgets and inadequately addressed hyperparameter discussions. In this paper, we extensively investigate the scaling behavior of language models and apply our findings in two widely used large-scale model configurations, namely 7B and 67B. Our study aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further advancements in this domain. Specifically, we first examined the scaling laws of batch size and learning rate, and found their trends with model size. Building on this, we conducted a comprehensive study of the scaling laws of the data and model scale, successfully revealing the optimal model/data scaling-up allocation strategy and predicting the expected performance of our large-scale models. Additionally, during development, we discovered that the scaling laws derived from different datasets show significant differences. This suggests that choice of dataset remarkably affects the scaling behavior, indicating that caution should be exercised when generalizing scaling laws across datasets. \n\nUnder the guidance of our scaling laws, we build from scratch open-source large language models, and release as much information as possible for community reference. We collect 2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we generally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler with a multi-step learning rate scheduler, maintaining performance while facilitating continual training. We collected over 1 million instances for supervised fine-tuning (SFT) (Ouyang et al., 2022) from diverse sources. This paper shares our experiences with different SFT strategies and findings in data ablation techniques. Additionally, we have utilized direct preference optimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the model. \n\nWe conduct extensive evaluations using our base and chat models. The evaluation results demonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particularly in the fields of code, mathematics, and reasoning.",
            "score": 0.4452606506250079,
            "section_title": "Introduction",
            "char_start_offset": 1814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2466
                }
            ],
            "ref_mentions": [
                {
                    "start": 1936,
                    "end": 1957,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1553955078125
        },
        {
            "corpus_id": "273323602",
            "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
            "text": "Theory of Neural Scaling Laws for Linear Activation Functions. Previous studies on neural scaling laws have primarily focused on random feature models or linear (ridge) regression with power-law features Wei et al. (2022). In particular, Maloney et al. (2022); Paquette et al. (2024) and Atanasov et al. (2024) analyzed random feature models for linear features and ridge regression, employing techniques from random matrix theory. Bahri et al. (2024) examined random feature models for kernel ridge regression within a student-teacher framework using techniques from statistical mechanics. In their analysis, either the number of parameters or the training dataset size was considered infinite, leading to scaling laws in the test loss with respect to the remaining finite quantity. Bordelon et al. (2024b) studied random feature models with randomly projected features and momentum, trained using gradient flow. Using a dynamical mean field theory approach, they derived a \"bottleneck scaling\" where only one of time, dataset size, or model size is finite while the other two quantities approach infinity. Additionally, Hutter (2021) investigated a binary toy model and found non-trivial scaling laws with respect to the number of training examples. \n\nBordelon & Pehlevan (2022) studied one-pass stochastic gradient descent for random feature models, deriving a scaling law for the test error over time in the small learning rate regime. Similarly, Lin et al. (2024) investigated infinite-dimensional linear regression under one-pass stochastic gradient descent, providing insights through a statistical learning theory framework. They derived upper and lower bounds for the test error, demonstrating scaling laws with respect to the number of parameters and dataset size under different scaling exponents. \n\nBuilding upon the work of Lin et al. (2024) and Bordelon & Pehlevan (2022), we also consider one-pass stochastic gradient descent. However, our study extends to both linear and non-linear neural networks where we train the weights used in the pre-activations (i.e., feature learning), and use fixed hidden-to-output connections.",
            "score": 0.4444395806979888,
            "section_title": "RELATED WORK",
            "char_start_offset": 4738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 451,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 784,
                    "end": 807,
                    "matchedPaperCorpusId": "267406160"
                },
                {
                    "start": 1859,
                    "end": 1885,
                    "matchedPaperCorpusId": "235358226"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "Neural scaling laws. Neural scaling laws provide a framework for understanding how neural networks' performance scales with parameters, data, and compute (Banko & Brill, 2001;Goodman, 2001;Ghorbani et al., 2022;Kaplan et al., 2020;Hoffmann et al., 2024;Bansal et al., 2022;Gordon et al., 2021). Kaplan et al. (2020) showed that for modern transformer-based language models, model loss decreases predictably with increasing model size, dataset size, and compute, following a power-law relationship. Hoffmann et al. (2024) later refined these insights by optimizing hyperparameter configurations, such as learning rate schedules, and proposed a new scaling law that emphasizes scaling training data more aggressively than Kaplan et al. (2020)'s original recommendations. Most relevant to our work are Rosenfeld et al. (2021) and Frantar et al. (2024). Rosenfeld et al. (2021) focused on small-scale CNNs for image classification, while Frantar et al. (2024) focused on transformer-based vision and language models; both modeled their respective performance as a function of model size and pruning configurations. Our work is different in that we unify the functional forms of scaling laws for both dense and sparse pre-training. This unification is partly enabled by our novel exploration of optimal hyperparameter configurations for sparse pre-training. \n\nAdditionally, our study is the largest-scale investigation of sparsely pretrained LLMs to date, with our largest model using over 5 times the compute of the largest model examined in prior work (Frantar et al., 2024). The largest model we investigate requires 4.5\u00d710 20 FLOPs training compute. \n\nPruning. Sparse pre-training involves pruning parameters in an LLM during the pre-training process.",
            "score": 0.4439982908604859,
            "section_title": "RELATED WORK",
            "char_start_offset": 3226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 189,
                    "end": 211,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 231,
                    "end": 253,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 253,
                    "end": 273,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 273,
                    "end": 293,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 498,
                    "end": 520,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 799,
                    "end": 822,
                    "matchedPaperCorpusId": "219792934"
                },
                {
                    "start": 827,
                    "end": 848,
                    "matchedPaperCorpusId": "260611140"
                },
                {
                    "start": 850,
                    "end": 873,
                    "matchedPaperCorpusId": "219792934"
                },
                {
                    "start": 934,
                    "end": 955,
                    "matchedPaperCorpusId": "260611140"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "260611140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "Optimal learning rate is another hyperparameter shown to change with scale, though techniques such as those proposed in Tensor Programs series of papers (Yang et al., 2022) can keep this factor constant with simple changes to initialization. More specifically, changing the learning rate schedule from a cosine decay to a constant learning rate with a cooldown (or even changing the learning rate hyperparameters) has been found to greatly affect the results of scaling laws studies (Hu et al., 2024;Porian et al., 2024;H\u00e4gele et al., 2024;Hoffmann et al., 2022). \n\nOne common motivation for fitting a scaling laws is extrapolation to higher compute budgets. However, there is no consensus on the orders of magnitude up that one can project a scaling law and still find it accurate, nor on the breadth of compute budgets that should be covered by the data. We find that the range of model size N and dataset size D greatly varies, with the maximum value of N in each paper ranging from 10M parameters to around 7B and that of D being as large as 400B tokens. For most papers we survey, the scales are relatively modest: 13 of 51 papers train models beyond 2B parameters; most only train models smaller than 1B parameters. It has been shown, with some controversy Schaeffer et al. (2023), that scaling to significantly larger scales can result in new abilities that did not appear in smaller models (Wei et al., 2022). Forecasting limits to extrapolation and the appearance of new abilities at new scales is an open question.",
            "score": 0.4439982908604859,
            "section_title": "HOW DO WE train models?",
            "char_start_offset": 17844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1451416015625
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "Neural network scaling laws describe how generalization error scales with data and model size. A number of works have observed power-law scaling with respect to data and model size (Kaplan et al., 2020;Rosenfeld et al., 2020;Clark et al., 2022), which has been explained theoretically (Bahri et al., 2021;Sharma & Kaplan, 2022;Hutter, 2021). \n\nHowever, other work demonstrates that model scale may not be sufficient to predict model performance (Tay et al., 2022), and casts doubt on power laws as the best model of error rate scaling (Alabdulmohsin et al., 2022;Bansal et al., 2022;Mahmood et al., 2022). Moreover, scaling with respect to training time, holding data volume fixed, remains poorly understood. These observations highlight the need for a more general framework that can predict model performance under many settings.",
            "score": 0.4439982908604859,
            "section_title": "SCALING LAWS",
            "char_start_offset": 3720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 831
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 305,
                    "end": 327,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "237592821"
                },
                {
                    "start": 563,
                    "end": 583,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 583,
                    "end": 604,
                    "matchedPaperCorpusId": "249947012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398193359375
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "Research into scaling laws of deep neural networks has a rich history spanning theoretical foundations, empirical validations, and diverse applications. The earliest investigations discovered power law scaling in simple machine learning settings (Barkai et al., 1993;Mhaskar, 1996;Pinkus, 1999). However, the modern era of scaling laws began with breakthrough studies in neural language models (Hestness et al., 2017;Kaplan et al., 2020;Brown et al., 2020b), catalyzing extensive research across multiple directions. The theoretical understanding of scaling laws has advanced significantly (Spigler et al., 2020;Bousquet et al., 2020;Hutter, 2021;Sharma & Kaplan, 2022;Maloney et al., 2022;Roberts et al., 2022;Bahri et al., 2024;Michaud et al., 2024;Paquette et al., 2024;Atanasov et al., 2024;Bordelon et al., 2024a;b;Lin et al., 2024;Brill, 2024), complemented by comprehensive empirical studies (Rosenfeld et al., 2020;Henighan et al., 2020;Gordon et al., 2021;Tay et al., 2021;Ghorbani et al., 2021;Tay et al., 2022b;Zhai et al., 2022;Alabdulmohsin et al., 2022b;Dehghani et al., 2023;Bachmann et al., 2023). In the context of language models, researchers have explored scaling behaviors in various aspects: context length (Xiong et al., 2023), in-context learning (Chan et al., 2022;Agarwal et al., 2024;Arora et al., 2024), vocabulary size (Tao et al., 2024), and jailbreaking attempts (Anil et al., 2024;Hughes et al., 2024).",
            "score": 0.4439982908604859,
            "section_title": "Related Work",
            "char_start_offset": 16136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1433
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 267,
                    "matchedPaperCorpusId": "28307549"
                },
                {
                    "start": 267,
                    "end": 281,
                    "matchedPaperCorpusId": "9891530"
                },
                {
                    "start": 281,
                    "end": 294,
                    "matchedPaperCorpusId": "16800260"
                },
                {
                    "start": 590,
                    "end": 612,
                    "matchedPaperCorpusId": "260540101"
                },
                {
                    "start": 647,
                    "end": 669,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 711,
                    "end": 730,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 899,
                    "end": 923,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 945,
                    "end": 965,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 982,
                    "end": 1004,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 1040,
                    "end": 1068,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1068,
                    "end": 1090,
                    "matchedPaperCorpusId": "256808367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "256416224",
            "title": "Scaling laws for single-agent reinforcement learning",
            "text": "Although our power law for intrinsic performance holds across environments and model sizes, we only obtain a good fit by excluding an initial transient period of training. Put another way, the scaling constants vary over the course of training. \n\nThis phenomenon is clearest with with our MNIST environment, since we were able to use many random seeds to reduce variance. Recall that in this environment, the agent observes a randomly sampled MNIST training set digit each timestep, and the horizon length of the task is artificially controlled using the GAE discount rate \u03b3, as explained in Section 3.3. We fitted our power law to three different periods of training for this environment: an early period (2 16 -2 19 interactions), a middle period (2 19 -2 22 interactions), and a late period (2 22 -2 25 interactions). \n\nFigure 6 shows the fitted values of \u03b1 N and \u03b1 E for these different periods of training. We found \u03b1 E to be significantly lower during the early and middle periods of training, especially for the shorter horizon lengths. \n\nIn order to accurately measure the scaling constants for optimal model size vs compute, it is best to use a period of training during which the learning curves reach the compute-efficient frontier, since otherwise the measurement is an extrapolation. As shown in Figure 7, this is always in the late period  of training, if at all. For this reason, we use the late period of training for all of our results on MNIST outside of this section. \n\nFigure 7 also shows that, for the longer horizon lengths, the learning curves of the larger models did not reach the compute-efficient frontier even during the late period of training. Hence our measurements of 1 1+\u03b1 N /\u03b1 E , the exponent for the scaling of optimal model size with compute, are likely underestimates for these longer horizon lengths. \n\nFor our other environments, we found that it was enough to exclude only the first 1 64 of training in order for our power law for intrinsic performance to be a good fit around the compute-efficient frontier. This is similar to what is needed for the corresponding law for language [Kaplan et al., 2020, Figure 4, right]. Nevertheless, it is possible that the measurement problem identified in this section affects some of our other results.",
            "score": 0.4439982908604859,
            "section_title": "Variability of exponents over training",
            "char_start_offset": 23333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2282
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2325439453125
        },
        {
            "corpus_id": "254636568",
            "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
            "text": "Scaling laws for generalization and transfer. Strong empirical evidence that increasing model or data scale is beneficial was initially studied in the context of deep learning and computer vision [69,26]. For instance, in [26], the power law relation between scale and model performance was highlighted. Empirical work stimulated theoretical studies that provided justification for the observed generalization boost with scale, investigating generalization error in overparameterized networks in the interpolation regime [6,9]. \n\nEarly empirical studies focused on the effect of training scale on upstream performance, measuring the test loss from the same distribution used for training. Subsequent studies of large language models such as GPT-3 [8] demonstrated broad generalization capabilities in models with substantially larger scale. Moreover, neural scaling laws of the power law form were derived for language models, connecting model, data, and training compute scale to performance [35,72,28]. This also allowed accurate prediction of model performance at larger scales, and researchers were able to determine the scale parameters for achieving optimal performance given a fixed amount of compute [28,39]. Scaling law studies were then also studied in the vision domain [61,82], also observing a power law dependency of performance on scale. \n\nScaling law studies were also conducted for transfer and out-of-distribution performance [35,72,82]. In these studies, researchers observed that performance on downstream tasks benefits from increasing model, data, and training compute scale [38,8,35,82]. Interestingly, upstream performance does not always correlate with downstream performance [72,71]. Since downstream performance most accurately reflects a practical use cases, examining scaling behavior on downstream tasks is increasingly important. Recent work has also studied the effect of scale on other model characteristics, such as performance after pruning and compression [63,11] and on susceptibility to catastrophic forgetting [58]. \n\nScaling up language-vision learning. Learning from very large amounts of weakly aligned imagetext pairs has led to the development of models with broad generalization capabilities. Notably, work on contrastive language-image pre-training (CLIP [55]) showed dramatic improvement compared to the previous state-of-the-art in zero-shot transfer and unprecendented robustness to distribution shift [70,48,51,18].",
            "score": 0.4439982908604859,
            "section_title": "Background and related work",
            "char_start_offset": 6128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2465
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 200,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 524,
                    "end": 526,
                    "matchedPaperCorpusId": "235212537"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "235417196"
                },
                {
                    "start": 1285,
                    "end": 1288,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1448,
                    "end": 1451,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 1451,
                    "end": 1454,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "214728308"
                },
                {
                    "start": 1601,
                    "end": 1603,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1606,
                    "end": 1609,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 1992,
                    "end": 1996,
                    "matchedPaperCorpusId": "219792934"
                },
                {
                    "start": 1996,
                    "end": 1999,
                    "matchedPaperCorpusId": "229152261"
                },
                {
                    "start": 2049,
                    "end": 2053,
                    "matchedPaperCorpusId": "251648120"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "252545361",
            "title": "Scaling Laws For Deep Learning Based Image Reconstruction",
            "text": "Once the error induced by learning the signal model, d/N , is small relative to the error floor, more training examples N are not beneficial. \n\nTogether, our empirical results show that neural networks for denoising, compressive sensing, and super-resolution applied in typical setups (i.e, Gaussian denoising with 20.17dB PSNR, and multi-coil accelerated MRI with 4x acceleration) already operate in a regime where the scaling laws for the training set size are slowing significantly and thus even very large increases of the training data are not expected to improve performance substantially. Even relatively modest model improvements such as those obtained by transformers over convolutional networks are larger than what we expect from scaling the number of training examples from tens of thousands to millions. \n\nFor deep neural networks used in practice, a recent line of work has empirically characterized the performance as a function of training set size and/or network size for classification and NLP [Hes+17; Ros+19; Kap+20; Bah+21; Zha+21a; Gho+21; Ban+22]. In those domains, the scaling laws persist even for very large datasets, as described in more detail below. In contrast, for image reconstruction, we find that the power-law behavior already slows considerably at relatively small numbers of training examples. \n\nRosenfeld et al. [Ros+19] find power-law scaling of performance with training set and network size across models and datasets for language modeling and classification. However, because the work fixes either training set or network size, while scaling the other, the scaling laws span only moderate ranges before saturating at a level determined by the fixed quantity and not the problem specific error floor. The papers [Kap+20; Bah+21; Zha+21a; Gho+21; Ban+22] including ours scale the dataset size and model size simultaneously, resulting in an improved predictive power of the obtained scaling laws. \n\nHestness et al. [Hes+17] study models for language and image classification, and attribute deviations from a power-law curve to a lack of fine-tuning the hyperparameters of very large networks.",
            "score": 0.4433476659076647,
            "section_title": "Introduction",
            "char_start_offset": 5781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1358,
                    "matchedPaperCorpusId": "203592013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "253397775",
            "title": "Will we run out of data? Limits of LLM scaling based on human-generated data",
            "text": "In this appendix we sketch a model of optimal scaling decisions under data scarcity. In particular, we examine how the decision to overtrain models might be affected by data scarcity. \n\nOur starting point is the parametric scaling law of Hoffmann et al. (2022), which predicts the reducible loss of a model L given its number of parameters N and the size of its training dataset D (see Equation 9). Hoffmann et al. (2022) derive from this scaling law a relation between the sizes of the model and the dataset that minimize the reducible loss of their model given a fixed training compute budget. In particular, in compute-optimal models the ratio D/N is around 20. We call this the Chinchilla scaling law, and we call models that follow it Chinchilla-optimal. \n\nModels for which the ratio D/N is above the Chinchillaoptimal ratio are commonly called overtrained, while models that are below that ratio are called undertrained. At a fixed training compute budget, overtrained models require less compute during inference but more data during training. This is currently attractive for developers, as compute is relatively scarce compared to data. As a consequence, some well-known models, like Llama 3 (Meta, 2024), are overtrained. 36 rofit maximization problem maximize \n\nwhere \n\nHere N is the number of parameters of the model, D is the size of the training dataset in tokens and P is the price of each inference token in (some multiple of) dollars. C 0 is the total computational budget for training and inference and I is the number of tokens produced during inference. A, a, B and b are fitted parameters of the scaling law, and I 0 , r and h are parameters of the inference demand function. All parameters are positive. \n\nWe now examine the relationship between overtraining and undertraining in the context of a data bottleneck. To simplify the analysis, here we ignore the cost of gathering data and focus on the computational cost of the model during training and inference. We assume that developers want to achieve the maximum possible profit within their computational budget, and that this computational budget includes both training and inference.",
            "score": 0.44274429051884895,
            "section_title": "F. Overtraining in the context of data scarcity",
            "char_start_offset": 53221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286865234375
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "In expanding the training datasets, Yi-34B appears to utilize its parameters more efficiently, potentially indicating a better generalization capacity or optimization that allows for superior learning from additional data. This could encourage the development of more data-centric approaches in the design of algorithms, focusing on how to better utilize the information available to enhance learning outcomes, even with a comparatively smaller set of parameters. \n\nUpon analyzing the graphs, it is evident that while the trend of increasing performance with larger datasets is present, the actual scores for each model at various training checkpoints do not precisely align with the expected trajectory of the scaling law. This discrepancy may stem from the fact that prior research often used loss as a measure of model performance. \n\nIn contrast, our experiments employ specific downstream task performance metrics. Given that scoring methods vary and dataset distributions differ, there will be a certain degree of divergence from the original scaling law.",
            "score": 0.4417463279034455,
            "section_title": "Scaling law evaluation",
            "char_start_offset": 20104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2427978515625
        },
        {
            "corpus_id": "253117181",
            "title": "Broken Neural Scaling Laws",
            "text": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
            "score": 0.4404571244923938,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "258967177",
            "title": "Likelihood-Based Diffusion Language Models",
            "text": "Our next goal is to understand how to optimally use a given compute budget C to maximize the held-out likelihood of a model. Specifically, we must choose between training a large model for fewer iterations or training a small model for longer. For this, we leverage our parameter scaling law N * (C) which predicts the optimal model size given a compute budget. \n\nWe plot both of our parameter scaling laws in Figure 3 and again find that the trends have nearly the same slope but differ by a constant factor. Specifically, compute-optimal Plaid models should be about 4\u00d7 smaller (and therefore trained for 4\u00d7 longer) than compute-optimal autoregressive models. The large gap in compute-optimal settings suggests that selecting model sizes based on existing scaling laws [15,13], which were developed for autoregressive models, could incur a substantial loss in the effective compute budget.",
            "score": 0.440281801963202,
            "section_title": "Compute-optimal training recipe",
            "char_start_offset": 18093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 891
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "The significance of scaling laws extends beyond mere prediction of the loss trajectory.More importantly, they can aid in pinpointing the optimal experimental configuration without requiring extensive tuning on very large models, thereby transforming the training of large language models from an alchemy-like trialand-error process into a principled methodology.In this section, we highlight main benefits of scaling laws and discuss ways to further advance beyond them.\n\nDetermining B As long as all hyperparameters are well-tuned (especially the learning rate and regularization hyperparameters) and the number of training steps is sufficient, it is believed that the same final performance should be attainable using any batch size [SLA + 19], so the batch size mainly influences the training speed of language models.Often, when training large language models, the ideal batch size is suggested to be set as the largest batch size supported by the available hardware [GDG + 23], so as to maximize the training speed without considering the computational cost.In Eq 3.12, we show that the critical batch size with the optimal speed/computation trade-off can be analytically computed from the loss value.Under the guidance of this formula, we would be able to estimate the preferred batch size under any loss trajectory.Furthermore, this optimal batch size in Eq 3.12 is determined by equally minimizing the training time and required computation, as shown in Eq 3.9.In practice, if we would like to prioritize one over the other, we can follow the same process to derive the optimal batch size.By this means, we are able to obtain the optimal batch size based on our customized need in a systematic way.\n\nDetermining N and S In practice, we often opt for the largest affordable model size and train the model until convergence.Nevertheless, this simplistic approach can deviate significantly from the optimal configuration and result in substantial resource wastage.Scaling laws provide a principled approach to choosing the optimal model size N and number of training steps S given a fixed computational budget C8 .Given that Eq 3.13 already provides the precise relation between the loss L, batch size B, model size N and training steps S, we could find the model size that minimizes L under the critical batch size (B = B crit ).",
            "score": 0.44021438412586483,
            "section_title": "Discussions",
            "char_start_offset": 19934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 472,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1708,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2119
                },
                {
                    "start": 2119,
                    "end": 2335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "267897458",
            "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy",
            "text": "Instead, they rule out specific ways by which the model explanation can be unfaithful to its prediction (Wiegreffe et al., 2021). Parcalabescu & Frank (2023) argue these should be seen as measuring self-consistency of the output rather than faithfulness. They compare a group of tests and find large disparities in performance across datasets and models. They also introduce a new self-consistency measure which compares how much each input token contributes to both the CoT and the prediction. Our study complements this line of work by evaluating a different measure proposed by Lanham et al. (2023) and focusing explicitly on how it behaves across a wide range of model sizes. \n\nScaling Laws With the training of increasingly large models (measured by number of parameters) has come an interest in deriving scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022). While many tasks exhibit a trend of increased performance with larger model sizes, other trends such as inverse scaling (Lin et al., 2022;McKenzie et al., 2022) and U-shaped scaling (Belkin et al., 2019;Black et al., 2022;Ruis et al., 2023;Wei et al., 2023) also occur. Lanham et al. (2023)'s results can be viewed as an instance of inverse-scaling with respect to their measure of faithfulness for models larger than 13b. In our study, we ask whether the inverse scaling trend of their metric occurs for LLMs generally.",
            "score": 0.44015372528847185,
            "section_title": "Introduction",
            "char_start_offset": 5771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1388
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 128,
                    "matchedPaperCorpusId": "225068329"
                },
                {
                    "start": 988,
                    "end": 1006,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 1050,
                    "end": 1071,
                    "matchedPaperCorpusId": "198496504"
                },
                {
                    "start": 1071,
                    "end": 1090,
                    "matchedPaperCorpusId": "248177957"
                },
                {
                    "start": 1090,
                    "end": 1108,
                    "matchedPaperCorpusId": "253157310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "274436125",
            "title": "Scaling Law for Language Models Training Considering Batch Size",
            "text": "Large language models (LLMs) have made remarkable advances in recent years, with scaling laws playing a critical role in this rapid progress. In this paper, we empirically investigate how a critical hyper-parameter, i.e., the global batch size, influences the LLM training prdocess. We begin by training language models ranging from 125 million to 2.6 billion parameters, using up to 300 billion high-quality tokens. Through these experiments, we establish a basic scaling law on model size and training data amount. We then examine how varying batch sizes and learning rates affect the convergence and generalization of these models. Our analysis yields batch size scaling laws under two different cases: with a fixed compute budget, and with a fixed amount of training data. Extrapolation experiments on models of increasing sizes validate our predicted laws, which provides guidance for optimizing LLM training strategies under specific resource constraints.",
            "score": 0.43981010193025316,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "273507997",
            "title": "A Simple Model of Inference Scaling Laws",
            "text": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.",
            "score": 0.4396506628741437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "271600832",
            "title": "Are Bigger Encoders Always Better in Vision Large Models?",
            "text": "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary. \n\nAfter the initial scaling law was proposed, researchers studied the scaling law in different scenarios, such as graph neural networks Liu et al. [2024c], data mixing Ye et al. [2024], data pruning Sorscher et al. [2022], and fine-tuning of large language models Zhang et al. [2024b]. Some of them made certain adjustments to the form of the formula. \n\nWhen it comes to MLLMs, Aghajanyan et al. Aghajanyan et al. [2023] explored the relationship between the loss and the dataset size and the parameter size over seven modalities, including text, image, image-text, speech, speech-text, code, and molecules. They also examined the scaling law when models are pre-trained on two modalities simultaneously and modified the form of the formula. However, the training paradigm they used in their study differs from the connected vision paradigm, the mainstream methods preferred today. They trained LLMs and modality encoders from scratch, similar to the approach chosen by BEiT-3 Wang et al. [2022], to obtain their scaling law. This discrepancy has sparked our research focus. To the best of our knowledge, the scaling law of multimodal large models under the connected vision paradigm is still under-researched. \n\n3 Methods",
            "score": 0.4389571555724937,
            "section_title": "Scaling Law",
            "char_start_offset": 8261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2283
                },
                {
                    "start": 2286,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 1272,
                    "end": 1294,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1469,
                    "end": 1493,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "278368201",
            "title": "Quiet Feature Learning in Algorithmic Tasks",
            "text": "Understanding how and when large language models acquire new capabilities has become an important question in the study of deep learning. While language models have demonstrated remarkable performance across a broad range of tasks, the precise mechanisms driving their improvements remain unknown. Recent discussions of \"emergent abilities\"where larger-scale models outperform baselines abruptly, even though smaller-scale counterparts exhibit little improvement -have led to debate over whether such phenomena are genuine or artifacts of measurement (Wei et al., 2022;Ganguli et al., 2022;Schaeffer et al., 2023). \n\nQuestions about emergent abilities are closely tied to the observation of scaling laws in model training (Kaplan et al., 2020;Ruan et al., 2024;Henighan et al., 2020;Dubey et al., 2024;OpenAI, 2023). These scaling laws typically show a smooth, power-law relationship between compute and model performance. However, most empirical demonstrations of these laws derive from heterogeneous data and tasks, leaving open the possibility that \"averaging out\" many distinct learning behaviors masks more abrupt transitions that occur for individual skills or subtasks. \n\nWe focus on ten foundational algorithmic problems spanning various input types. These algorithmic tasks have precisely defined solutions, making it straightforward to identify clear success criteria, isolate the specific features or subtasks the model must learn, and ensure that improvements cannot be attributed to memorization or partial heuristics. These tasks allow us to investigate fine-grained learning phenomena which might otherwise be obscured by heterogeneous data. \n\nOur key findings include: \n\n1. Phase transitions in scaling laws: We observe two distinct phases in scaling laws across tasks and task lengths. In the \"slow phase,\" loss improves minimally or remains flat. Then, loss drops rapidly (\"fast phase\"). We refer to the change between these two phases as phase transition. We observe this phase transition when we vary model size & training dataset size, fix model size & vary training dataset size and within individual training runs. \n\n2. Quiet features precede phase transitions: Models learn meaningful internal representations during the slow phase, but these features do not yet yield noticeable gains in the output loss (we call these \"quiet features\").",
            "score": 0.4373412343132519,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 569,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 569,
                    "end": 590,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 590,
                    "end": 613,
                    "matchedPaperCorpusId": "258418299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.300537109375
        },
        {
            "corpus_id": "269033049",
            "title": "Scaling Laws for Data Filtering\u2014Data Curation Cannot be Compute Agnostic",
            "text": "Crucially, one needs to model a decay in utility factor (the scaling parameter b in y = an b ) as well.\n\nFinally, Hashimoto (2021) study scaling laws for various mixture proportions, but their study is limited to small-scale supervised learning tasks.In this work, we focus on scaling laws for large-scale contrastive training of visual language models like CLIP.\n\nScaling laws for downstream performance Although traditionally the scaling laws have focused on modeling the training loss, recent works have started directly modeling the downstream performance (Gadre et al. 2024;Isik et al. 2024).Alabdulmohsin et al. (2022); Caballero et al. (2023) propose some amendments to estimate downstream performance on image classification and machine transalation tasks respectively.In this work, we model ImageNet zeroshot accuracy and an average performance over 18 tasks from DataComp (Gadre et al. 2023a) to fit the scaling curves for data filtering.\n\nScaling Laws in CLIP Application of scaling laws to models like CLIP is still an area of active research.As with the scaling laws observed in pure language models, there's an indication that as the model and data sizes for CLIP grow, its performance on downstream vision tasks improves, albeit with diminishing returns (Gadre et al. 2023b;Schuhmann et al. 2022).Cherti et al. (2023) try to fit standard scaling curves similar to Kaplan et al. (2020) on CLIP models of varying size and architecture.However, note that contrary to language models which are rarely trained with more than 3-4 epochs, CLIP training invovles upto 30-40 epochs even at the largest data scale.As we highlight in this work, one needs to model the diminishing gains of data with repeated epochs, in order to accurately estimate scaling curves for visual-language model training.",
            "score": 0.4370604564862974,
            "section_title": "Related Work",
            "char_start_offset": 10882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 105,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 363
                },
                {
                    "start": 365,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 948
                },
                {
                    "start": 950,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 1289,
                    "end": 1311,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 1312,
                    "end": 1332,
                    "matchedPaperCorpusId": "254636568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341064453125
        },
        {
            "corpus_id": "252545361",
            "title": "Scaling Laws For Deep Learning Based Image Reconstruction",
            "text": "Hestness et al. [Hes+17] study models for language and image classification, and attribute deviations from a power-law curve to a lack of fine-tuning the hyperparameters of very large networks. For transformer language models Kaplan et al. [Kap+20] find no deviation from a power-law for up to a training set and network size of 1B images and parameters. Further, Zhai et al. [Zha+21a] find the performance for Vision Transformers [Dos+20] for few-shot image classification to deviate from a power-law curve only at extreme model sizes of 0.3-1B parameters and 3B images. \n\nThe role of training set size in inverse problems. For image reconstruction and inverse problems in general, we are not aware of work studying scaling laws in a principled manner, covering different problems and model architectures. But, when proposing a new method several works study performance as a function of training set and/or network size. However, those studies typically only scale the parameter of interest while fixing all other parameters, unlike our work, which scales dataset size and network size together, which is important for identifying scaling laws. Below, we review the training set sizes, and if available their scaling properties, used by the recent SOTA in image denoising and accelerated MRI. \n\nZhang et al. [Zha+17] report that for DnCNN, a standard CNN, using more than 400 distinct images with data augmentation only yields negligible improvements. Chen et al. [Che+21] pre-train an image processing transformer (IPT) of 115.5M network parameters on ImageNet (1.1M distinct images), and report the performance after fine-tuning to a specific task as a function of the size of the pre-training dataset. IPT's performance for denoising is surpassed by the latest SOTA in form of the CNN based DRUnet [Che+21], the transformer based SwinIR [Lia+21] and the Swin-Conv-Unet [Zha+22] a combination of the two.",
            "score": 0.43661080083302806,
            "section_title": "Introduction",
            "char_start_offset": 7719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 1310,
                    "end": 1318,
                    "matchedPaperCorpusId": "996788"
                },
                {
                    "start": 1466,
                    "end": 1474,
                    "matchedPaperCorpusId": "227239228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "269983385",
            "title": "Neural Scaling Laws in Robotics",
            "text": "In recent years, the field of robotics has witnessed significant advancements, particularly in the development of adaptable, general-purpose robotic systems.However, a comprehensive understanding of how these systems can be scaled effectively remains underexplored.This study aims to address this gap by investigating the scaling laws for embodied AI, specifically focusing on how model size, training data amount, and computing resources impact the performance of Robot Foundation Models (RFMs) and LLMs used in robotics.Our research seeks to determine if the scaling laws observed in other domains, such as language and vision, are applicable to embodied AI and to identify the characteristic power-law coefficients for RFM and LLMs in robotics (Kaplan et al., 2020;Henighan et al., 2020).Additionally, we examine how these coefficients vary between familiar and novel tasks, and whether RFMs and LLMs in robotics exhibit emergent capabilities similar to those seen in other domains.\n\nThe significant advancements in deep learning in recent years has been primarily driven by scaling -training larger neural networks on increasing amounts of data with growing computational resources (Alabdulmohsin et al., 2022).This phenomenon is encapsulated in neural scaling laws, which describe how the performance of neural networks improves as a function of model size, training data size, and compute budget.This foundational concept was first formalized by researchers like Jonathan Rosenfeld (Rosenfeld et al., 2019;Rosenfeld, 2019;Rosenfeld et al., 2021;Rosenfeld, 2021) and OpenAI in 2020 (Henighan et al., 2020;Kaplan et al., 2020).It also builds upon the Bitter Lesson, a This study seeks to address this gap by identifying scaling laws for embodied AI.Specifically, we investigate: 1) Do scaling laws observed in other domains like language and vision hold true for RFMs in terms of compute, model size, and data?2) What are the characteristic power-law coefficients for RFM, and how do they compare to LLMs used in robotics?3) How do these power-law coefficients vary between tasks that the model has seen during training versus completely new (unseen) tasks?and 4) Do RFM and LLMs in robotics exhibit emergent capabilities similar to those observed in other domains?",
            "score": 0.4358199929598827,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1402
                },
                {
                    "start": 1402,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 1186,
                    "end": 1214,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1528,
                    "end": 1551,
                    "matchedPaperCorpusId": "219792934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "Scaling laws in large language models (LLMs) (Kaplan et al., 2020;Hestness et al., 2017;Henighan et al., 2020;Hoffmann et al., 2022) have been widely observed and validated, suggesting that pretraining performance follows a power-law relationship with the compute C. The actual compute could be roughly calculated as C = 6N D, where N is the model size and D is the data quantity. Therefore, determining the scaling law helps us make informed decisions about resource allocation to maximize computational efficiency, namely, figure out the optimal balance between model size and training data (i.e., the optimal model and data scale) given a fixed compute budget. However, scaling laws in diffusion models remain less explored. \n\nThe scalability has already been demonstrated in diffusion models, especially for diffusion transformers (DiT). Specifically, several prior works (Mei et al., 2024;Li et al., 2024) reveal that larger models always result in better visual quality and improved text-image alignment. However, the scaling property of diffusion transformers is clearly observed but not accurately predicted. Besides, the absence of explicit scaling laws also hinders a comprehensive understanding of how training budget relate to model size, data quantity, and loss. As a result, we cannot determine accordingly the optimal model and data sizes for a given compute budget and accurately predict training loss. Instead, heuristic configuration searches of models and data are required, which are costly and challenging to ensure optimal balance. \n\nIn this work, we characterize the scaling behavior of diffusion models for text-to-image synthesis, resulting in the explicit scaling laws of DiT for the first time. To investigate the explicit relationship between pretraining loss and compute, a wide range of compute budgets from 1e17 to 6e18 FLOPs are used. Models ranging from 1M to 1B are pretrained under given compute budgets. As shown in Fig. 1, for each compute budget, we can fit a parabola and extract an optimal point that corresponds to the optimal model size and consumed data under that specific compute constraint. Using these optimal configurations, we derive scaling laws by fitting a power-law relationship between compute budgets, model size, consumed data, and training loss.",
            "score": 0.43545255989569065,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 910,
                    "matchedPaperCorpusId": "268875775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "267547740",
            "title": "A Resource Model For Neural Scaling Law",
            "text": "This conjecture might be equivalent to the optimal allocation of neurons subject to the finite model capacity, specifically the number of neurons in each layer. Combining the single task scaling law and the homogeneous growth conjecture, we can derive (and indeed observe) that the scaling law of the overall composite task can emerge, i.e., the total loss \u2113 total is inversely proportional to the total number of neurons N total , i.e., \u2113 total \u221d N \u22121 total . \u2022 The implication for neural scaling laws in large language models (Section 4): For a finitewidth neural network trained for complex tasks which contains many subtasks, the optimal loss scales with the total resource is \u2113 \u221d N \u22121 . Under a few reasonable assumptions we have N p \u221d N 3 , so \u2113 \u221d N \u22121/3 p (N p is the number of model parameters), agreeing with the compute-optimal scaling law of Chinchilla models where they observed \u2113 \u221d N \u22120.34 p (Hoffmann et al., 2022).",
            "score": 0.4353487993929209,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 929
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11297607421875
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "Scaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
            "score": 0.4353487993929209,
            "section_title": "Related work",
            "char_start_offset": 22035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "275757345",
            "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling",
            "text": "Recently, post-training reinforcement learning as well as test-time scaling for LLMs to boost reasoning capabilities has attracted much attention since the emerge of OpenAI o1 (OpenAI, 2024), but are still under-explored in the open community. Gao et al. (2023); Cobbe et al. (2021) explore the scaling laws in reward modeling under a synthetic setting and Rafailov et al. (2024a) studies the scaling of direct policy optimization (Rafailov et al., 2024b). Hou et al. (2024) systematically investigate the impact of scaling on the performance of LLMs and all of them show that traditional methods are not scalable and are far from effective as shown in o1 in boosting the reasoning abilities of LLMs. Beyond scaling RL training, inference scaling is also a crucial yet under-explored. Existing works (Brown et al., 2024;Snell et al., 2024) measure the inference cost by repeated sampling, which heavily relies on external supervision as a verifier and is not as scalable as proposed in (OpenAI, 2024). \n\n3 Building T1 for Understanding Inference Scaling",
            "score": 0.4347762029885176,
            "section_title": "Related Work",
            "char_start_offset": 7018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1053
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 261,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 431,
                    "end": 455,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "278501333",
            "title": "Learning curves theory for hierarchically compositional data with power-law distributed features",
            "text": "We studied how the scaling laws of deep networks trained in a feature-learning and data-limited regime are affected by two ubiquitous properties of natural data: hierarchical compositionality and Zipfian distribution of features. Remarkably, the effects of these two structural properties on learning greatly differ between classification and next token prediction tasks. \n\nFor classification, we have shown that the learning curve of simple context-free grammars becomes a power law due to the broad distribution of production rules. Specifically, if production rules are power-law distributed with some exponent a, then the learning curve decays as P \u2212a/(a+1) . The exponent a/(a + 1) is also found in elementary toy models of memorisation of Zipf-distributed data (Hutter, 2021;Michaud et al., 2023). Yet, in our case, as in real data, this behaviour is not simply caused by memorisation, as the probability of each single datum decays exponentially with the dimension of the input. Interestingly, the pre-asymptotic phase of the learning curve can be very large and depends on the hierarchical structure of the problem. \n\nIn next-token prediction, as in classification, our analysis predicts that production rules leading to rare features require more data to be learnt and cannot be deduced from more frequent rules. Nevertheless, this effect only changes the asymptotic test loss L \u221e and not the scaling exponent describing how this limit is approached. The slow decay of the curve rather stems from the following effect: as the training set grows, correlations at increasingly longer distances become resolvable, enabling deep networks to progressively reconstruct deeper levels of the latent hierarchical structure in the data.",
            "score": 0.43457386477143806,
            "section_title": "Conclusions",
            "char_start_offset": 27013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1735
                }
            ],
            "ref_mentions": [
                {
                    "start": 781,
                    "end": 801,
                    "matchedPaperCorpusId": "257687665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405029296875
        },
        {
            "corpus_id": "246823711",
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "text": "We pretrain all models from scratch on the Datasets (Lhoest et al., 2021) Wikipedia dump, see experimental details in App. A.1. We use early stopping to declare convergence, but note that determining convergence is non-trivial, as we discuss in \u00a73.1. Fig. 2 shows the evaluation loss of each configuration as a function of the parameter count for both MLM and PMI. As can be seen from the clean linear relationships (in log-log scale), both aspect ratios and both objectives present a power law, with R 2 exceeding 0.99 in all cases. This is consistent with the language modeling results of Kaplan et al. (2020), and shows that scaling laws exist at pretraining time for the MLM and PMI objectives.\n\nPast work (Kaplan et al., 2020;Tay et al., 2022) examined scaling laws of different architectures (e.g., Transformer (Vaswani et al., 2017), Reformer (Kitaev et al., 2020), Performer (Zhao and Deng, 2019)). They showed that while different architectures affect scaling laws, architectural hyperparameters, such as aspect ratio (AR), make little difference as long as they are within a reasonable range. Interestingly, in both MLM and PMI the slope of AR 64 is slightly better than the slope of AR 32, even when taking into account the slope's 95% confidence intervals. The intersection of the two AR lines illustrates the potential of using scaling laws for model selection: choosing the AR based on small scale experiments would lead to choosing AR 32, while the performance of models with AR 64 seems comparable and perhaps better in the larger scale. However, the confidence intervals of the fit, depicted as sleeves in Fig. 2, intersect at the larger scales, meaning we cannot predict a performance difference with confidence.\n\nWe note that our largest MLM model, which uses AR 64, 12 layers, and has 85M trainable parameters performs slightly worse than predicted by the power law, which might hint that it is undertrained. We leave verifying this and training larger models with different ARs to future work.",
            "score": 0.43425394912680776,
            "section_title": "Pretraining",
            "char_start_offset": 9534,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 817,
                    "end": 839,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 850,
                    "end": 871,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 883,
                    "end": 904,
                    "matchedPaperCorpusId": "191539069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "Irreducible loss \n\n(1) \n\nIn this scaling law, taken from Mikami et al. 2021, the transfer gap determines the efficiency of fine-tuning in the limit of infinite pre-training data. By measuring the transfer gap from inexpensive and abundant pretraining distributions to costly downstream distributions, we can predict how much pre-training might improve performance on a given downstream distribution. Thus, measuring the transfer gap across various tasks can meaningfully inform the relative difficulty of automating downstream tasks, although drawing firm conclusions about the difficulty of automation is premature given the limited data in this study. \n\nOur study relies heavily on the Pythia model suite (Biderman et al. 2023), providing a large set of transformer models trained on the Pile at various model sizes and checkpoints during training. We fine-tune a 2.8 billion parameter model on various downstream language datasets, including a mathematics dataset, a genetic sequence dataset, a statistics textbook, and a synthetic dataset of fictional biographies within a procedurally generated universe. After fitting the scaling law to empirical data, we demonstrate how this scaling law can be used to answer concrete questions, such as \"How valuable is it to collect more fine-tuning data for a given task, compared to scaling up pre-training data?\" and \"What is the transfer gap from one distribution to another?\". \n\nOur analysis is incomplete for several reasons. Firstly, we do not derive a scaling law in model size. 1 Secondly, our results are largely limited to language datasets and were obtained with a relatively modest compute budget. \n\nFor these reasons, we cannot necessarily extrapolate these results to much larger model sizes, of the type that exist at the current commercial frontier (e.g. OpenAI et al. 2024, Team, Anil, Borgeaud, Alayrac, et al. 2024). Nonetheless, we hope our study can serve as a limited foundation for further research, stimulating more investigations into empirical scaling laws for transfer learning, enhancing our understanding of the role of data in advancing AI progress, and providing deeper insights into the difficulty of automating various economically important tasks.",
            "score": 0.43412081552743775,
            "section_title": "+ E",
            "char_start_offset": 2982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372314453125
        },
        {
            "corpus_id": "277621577",
            "title": "Compression Laws for Large Language Models",
            "text": "Neural scaling laws. The study of scaling laws dates back several decades, with Cortes et al. (1993) (Vaswani, 2017). While most of these scaling laws manage to capture the expected test-time behaviors of pre-trained LLMs, they fail to explain the scaling behaviors of models in parameter and data-efficient settings. \n\nModel compression for parameter efficiency. Despite the remarkable performance of LLMs such as LLaMA (Dubey et al., 2024) and Deepseek (DeepSeek-AI et al., 2024) on a wide range of tasks, including natural language inference, complex reasoning, summarization, translation, and code generation, large-scale utilization of these models remains challenging due to high computational resource requirements. Model compression (aka pruning) is a common technique to reduce the parameter count in pre-trained models, improving their computational efficiency and speed. It generally falls into two main categories: unstructured and structured pruning. Unstructured pruning focuses on removing individual weights (Frantar & Alistarh, 2023;Sun et al., 2023) from pre-trained models. Despite their ability to retain performance post-compression, unstructured pruning often demands hardware-specific optimizations and may not always lead to substantial computational benefits. Conversely, structured pruning eliminates entire channels or components, making it more suitable for a broader range of hardware configurations. Contemporary structure pruning methods like SliceGPT (Ashkboos et al., 2024), layer collapse (Yang et al., 2024) use a small calibration dataset to assess the importance of different components of a pretrained model and removes them subsequently, if found unimportant. Sengupta et al. (2025) proposed a policydriven calibration-free model compression method and argued that LLMs can withstand even when pruned by a ran-dom subset of the pre-trained components. \n\nScaling laws for parameter-efficient LLMs. Kumar et al. (2024) introduced precision-aware scaling laws, demonstrating that training in lower precision effectively reduces the parameter count of LLMs. Their findings suggest that larger LLMs scale more efficiently when trained at lower precision.",
            "score": 0.4335545968404361,
            "section_title": "Related Work",
            "char_start_offset": 4244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1890
                },
                {
                    "start": 1893,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 100,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 101,
                    "end": 116,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "273549927",
            "title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation",
            "text": "In this section, we first explore how increasing the number of training objects affects object generalization. Next, we analyze how the number of training environments impacts environment generalization. Finally, we study generalization across both dimensions simultaneously. Throughout all experiments, we also analyze the effect of demonstration quantity (Sec. 4.1). From these results, we derive the power-law data scaling laws (Sec. 4.2). Based on these laws, we further demonstrate an efficient data collection strategy to achieve a generalizable policy (Sec. 4.3).",
            "score": 0.43352625384226434,
            "section_title": "UNVEILING OF DATA SCALING LAWS",
            "char_start_offset": 16047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 570
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.245849609375
        },
        {
            "corpus_id": "267411930",
            "title": "Towards Neural Scaling Laws on Graphs",
            "text": "Despite the overall success, the scaling law could fail in certain situations. In particular, we find that the model scaling law could fail when the model size is too large relative to the training set size, causing the phenomenon called model scaling collapse. To illustrate this phenomenon, we consider an example of scaling up 5-layer GIN on OGBG-MOLPCBA dataset. Similar to the setting in Section 4, we vary the model size by changing the model width while fixing the training set size. Figure 13(a) displays the empirical accuracy of models across sizes alongside the model scaling curve. At small model sizes, the accuracy values fit with the model scaling curve well. However, when the number of parameters is around 10 7 , the empirical values start dropping. Thus, the collapse of performance would make the predictions of the scaling law fail, which is referred to as model scaling collapse. \n\nThe model scaling collapse poses difficulty in building large graph models following the model scaling law. Thus, it is essential to explore why the collapse happens and how to mitigate it. In this lo g 1 0 (# P a ra m e te rs )   For PCQM4MV2, the color corresponds to the value of mean square error; for OGBG-PPA, the color corresponds to accuracy. The surfaces predicted by the neural scaling law fit well with the empirical values on both datasets, as indicated by the high R 2 values. The larger the model is, the smaller the corresponding training loss will be. Nonetheless, the validation losses of models with more than 10 7 parameters start increasing after several epochs as shown in Figure 13(c), which indicates overfitting. Moreover, the models whose performance falls behind the scaling law predictions suffer more serious overfitting. For instance, models of 7.4M, 13M, and 20M parameters overfit the most in Figure 13(c), and they are the three models on which model scaling law fails in Figure 13(a). Based on these observations, overfitting can be one potential reason for the model scaling collapse. For more results between the model size and overfitting, please refer to Appendix D. \n\nTo eliminate the effect of model scaling collapse, it is crucial to overcome the overfitting. [2]",
            "score": 0.4334574606971771,
            "section_title": "C Model Scaling Law on Limited Graph Data",
            "char_start_offset": 25715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1524658203125
        },
        {
            "corpus_id": "278502310",
            "title": "Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures",
            "text": "In this work, we studied the learning dynamics of deep neural networks trained on hierarchically structured data. Leveraging synthetic datasets generated by the Random Hierarchy Model (RHM)-an analytically tractable ensemble of probabilistic context-free grammars-we analysed how the test loss evolves as a function of training steps under online stochastic gradient descent. Our theoretical framework elucidates how the observed power-law scaling of performance with the number of training steps (or, equivalently, training samples) arises naturally through the progressive acquisition of increasingly deeper layers of the underlying hierarchical structure. \n\nBy comparing convolutional networks, transformers, and locally connected networks, we demonstrated that different architectural priors lead to systematic differences in learning dynamics and scaling laws. In par- T \u2261 symbol resampling  T \u2261 symbol resampling ticular, convolutional networks tailored to the hierarchical structure of the data achieve a faster scaling of performance. This finding seems to contradict the common intuition that Transformer-based models are better suited for sequence-modelling tasks. However, this contrast highlights the opportunity to use controlled synthetic datasets as probes to uncover the conditions under which different inductive biases achieve better performances. In our case, the strict hierarchical structure of the data aligns naturally with the design of convolutional networks, giving them the edge. One natural extension is to consider variable tree topologies, introducing spatial heterogeneities that might be better exploited by the flexibility of attention mechanisms. Another is to incorporate context-sensitive dependencies, pushing beyond the context-free structure. Exploring such modifications offers a systematic path to understanding when and how Transformer architectures gain their empirical edge. \n\nSeveral extensions are also possible within the framework of the current data model. On the theoretical side, a formal characterisation of the training dynamics of deep networks on hierarchical data could sharpen our understanding of the mechanisms driving representation learning and the ensuing scaling behaviour. The empirical analysis of internal representations presented in section VIII provides a promising starting point, as it identifies the information encoded at different layers of trained networks and how this evolves with training.",
            "score": 0.4334563963580918,
            "section_title": "X. CONCLUSIONS",
            "char_start_offset": 36526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "270212893",
            "title": "LLMs Could Autonomously Learn Without External Supervision",
            "text": "As training in deep learning and large language models becomes increasingly expensive, neural scaling laws can ensure performance.Before training large language models with hundreds of millions of parameters on massive corpora, we initially train models on smaller-scale corpora and fit scaling laws for training on larger corpora.\n\nUnlike previous work [23,24], which typically fix the size of the corpus and vary the scale of model parameters to observe the effects on error, this paper's scaling laws focus more on the corpus.The aim is to demonstrate through experiments on scaling laws of corpora size that our method is universally effective across various scales of corpora.As shown in Table 2 and Table 3, the benchmark results demonstrate that the Autonomous Learning outperforms all the currently most popular knowledge learning paradigms across various document scales.In specific domains such as Medical, the method described in this paper still shows significant improvements.",
            "score": 0.4319107096053666,
            "section_title": "Scaling Laws Across Multi-Magnitude Corpora",
            "char_start_offset": 15714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 331
                },
                {
                    "start": 333,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.217529296875
        },
        {
            "corpus_id": "246867298",
            "title": "Predictability and Surprise in Large Generative Models",
            "text": "Scaling up the amount of data, compute power, and model parameters of neural networks has recently led to the arrival (and real world deployment) of capable generative models such as CLIP [61], Ernie 3.0 Titan [81], FLAN [82], \n\nGopher [62], GPT-3 [11], HyperClova [46], Jurassic-1-Jumbo [51], Megatron Turing NLG [72], LaMDA [78], Pan Gu [89], Yuan 1.0 [87], and more. For this class of models 1 the relationship between scale and model performance is often so predictable that it can be described in a lawful relationship -a scaling law. In most cases, these scaling laws predict a continued increase in certain capabilities as models get larger. At the same time, larger generative models represent an increasing proportion of the eye-catching results in machine learning. As a result, many institutions have started producing large models over the past few years, in response to the predictability afforded by scaling laws, and the fact these models can be plugged into systems that generate economic value, like search engines. 2 It has also become clear that these models present novel risks of harmful behavior, which are difficult to predict and may become more severe as the models increase in capability. Attempts to study these harms with smaller models may not accurately reflect what occurs in larger ones. \n\nIn this paper, we attempt to better understand the influence of scaling laws on the dynamics of large-scale model development and deployment, with a focus on large language models. Our basic thesis is that large generative models have a paradoxical combination of high predictability -model loss improves in relation to resources expended on training, and tends to correlate loosely with improved performance on many tasks -and high unpredictability -specific model capabilities, inputs, and outputs can't be predicted ahead of time. \n\nThe former drives rapid development of such models while the latter makes it difficult to anticipate the consequences of their development and deployment. We go through examples of how this combination can lead to socially harmful behavior, while also analyzing the motivations and challenges that developers of such models will face.",
            "score": 0.431885802599438,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.082275390625
        },
        {
            "corpus_id": "255185900",
            "title": "Cramming: Training a Language Model on a Single GPU in One Day",
            "text": "The most obvious way to efficiently scale down training is by modifying the model architecture; intuitively, it seems likely that smaller/lower capacity models will be optimal in the cramming regime. \n\nIn this section, we study the relationship between model type and training efficiency. We see that scaling laws create a strong barrier to scaling down. Per-token efficiency of training depends strongly on model size, but not transformer type. Furthermore, smaller models learn less efficiently, and this largely mitigates any throughput gains. Fortunately, the fact that training efficiency is nearly constant across models of the same size means that we can boost performance by finding architecture modifications that speed up gradient computation while keeping the parameter count nearly constant. This makes architecture selection fairly straightforward as we can make design choices based primarily on how they affect computation time for a single gradient step. \n\nScaling laws hold in the low-resource regime A large corpus of research in recent years has developed architectural improvements to speed up the original transformer. Many of these methods have not been found to improve training for the large-scale T5 architecture Narang et al. (2021); Tay et al. (2022a). But, in the low compute setting where data throughput is of utmost importance, maybe this is the way forward? Scaling laws have been observed by Kaplan et al. (2020) in the highresource regime, and seem to hold strongly in the limit as resources grow. Surprisingly, these laws also hold in the limit of extreme compute down-scaling, and they create a barrier to low-cost training. \n\nWe exemplify the effect of scaling laws for many transformer variants from the literature in Figure 1, where we train each architecture variant with optimized training hyperparameters as described below in Section 4.3. We apply these architecture variants to a shared baseline model that incorporates",
            "score": 0.43181111863152705,
            "section_title": "MODIFYING THE ARCHITECTURE",
            "char_start_offset": 15180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 1238,
                    "end": 1258,
                    "matchedPaperCorpusId": "232035936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37060546875
        },
        {
            "corpus_id": "278033756",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "text": "Scaling laws research aims to model the relationship between model performance and various factors such as model size, training data, and inference budget. As Li et al. (2025a) comprehensively discuss, these studies typically involve regressing performance metrics onto those factors using data from systematic experiments. We briefly outline key aspects of this field to situate our contribution within the broader scaling laws landscape. \n\nPerformance metrics The choice of target metric depends on the controlled factors. For studies varying model size or data quantity, language modelling metrics like perplexity are common. For supervised models or capability-specific studies (e.g., RAG performance in Yue et al. ( 2025)), task accuracy is typically used. In our work, we measure performance using downstream accuracy on long-context benchmarks (QA, RULER, Story), which directly assess the capabilities we aim to model.",
            "score": 0.43115846493034715,
            "section_title": "B Overview of Scaling Laws Studies",
            "char_start_offset": 51601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 926
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 176,
                    "matchedPaperCorpusId": "276618147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "253237527",
            "title": "A Solvable Model of Neural Scaling Laws",
            "text": "most directly relevant of these, [38], considers a student-teacher model with the right phenomenology though studies it only in certain limits: in particular, the authors assume a power law of infinite extent for the data and then import a result from [36] to find a power-law scaling of the test loss; our work leaves the relative ratios of the size of the latent space, the size of the training set, and the number of features all finite. Additionally, the calculation of [36] gives the averaged test loss of a non-generalized linear regression without any random feature map to extend the power law. Our work is more closely related to [34], which was not focused on scaling laws but finds a general expression for the test loss averaged over nonlinear random feature maps with fixed training data and labels; we extend this further to student-teacher models, finding a very simple expression for the test error and then are able to average it over our random data model as well. In the process, we also find much simpler derivations of the central results in [34] and [36] using our diagrammatic techniques.\n\nThe plan of this paper is as follows:\n\nIn \u00a72, we provide a non-technical overview of the data and feature map settings that lead to neural scaling laws: to begin, we review the phenomenology of the joint parameter and dataset test loss behavior discovered by [18]; then, in \u00a72.1 and \u00a72.2, we use examples from natural datasets to explain the specific spectral properties of the input data and nonlinear feature maps, respectively, needed to have both power law scaling and bottleneck behavior.\n\nIn \u00a73, we present ( \u00a73.1) and then solve ( \u00a73.2) our statistical model of neural scaling laws, consisting jointly of a generative model for the data and random feature map, deriving a formula for the test loss as a function of the size of the dataset and number of features of the model and then showing that it precisely matches experiment. We also outline ( \u00a73.3) how we could use our same RMT tools to model spectral power-law extension in nonlinear feature maps and comment ( \u00a73.4) on the relationship of our work to other RMT results in the machine learning literature.\n\nIn \u00a74, we",
            "score": 0.43115814139229397,
            "section_title": "Introduction",
            "char_start_offset": 7933,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "226227332"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "226227332"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "9593000"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "9593000"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "matchedPaperCorpusId": "226227332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369873046875
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "The adaptability of scaling laws to emerging paradigms is evident in recent innovations. Hybrid models like Mamba (Gu and Dao, 2023), analyzed by Poli et al. (2024), alongside specialized scaling laws for mixture-ofexperts models (Clark et al., 2022;Fedus et al., 2022a;Shazeer et al., 2017) and sparse architectures (Frantar et al., 2023;Zhu and Gupta, 2017), demonstrate their versatility. The scope of scaling laws extends far beyond language tasks, encompassing vision-language models (Cherti et al., 2023;Henighan et al., 2020), reinforcement learning (Hilton et al., 2023;Gao et al., 2023), and recommendation systems (Ardalani et al., 2022), underscoring their wide-ranging applicability. \n\nWhile Transformer-based models (Vaswani, 2017)-exemplified by behemoths like Llama 3 with 405B parameters-dominate scaling law research due to their exceptional scalability, alternative architectures have not been overlooked. For comparison, ResNet101 boasts a modest 44M parameters (Sorscher et al., 2022), where Sorscher et al. ( 2022) investigated data pruning laws. Smaller-scale studies have also employed MLPs or SVMs (Hashimoto, 2021) to probe scaling behavior. Additional dimensions, such as the impact of data quality and language-specific effects on scaling coefficients (Bansal et al., 2022;Zhang et al., 2022), as well as multimodal scaling in foundation models (Aghajanyan et al., 2023), further enrich this research landscape. \n\nTogether, these efforts illustrate that scaling laws do more than predict performance-they serve as a guiding framework for resource allocation, architecture design, and generalization across domains. As such, they have become an indispensable tool in advancing artificial intelligence research and its real-world deployment, offering a lens through which we can better understand and harness the potential of ever-growing models and datasets.",
            "score": 0.4310919816026011,
            "section_title": "C.3 Scaling Law",
            "char_start_offset": 36890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 270,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 578,
                    "end": 595,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 729,
                    "end": 744,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1122,
                    "end": 1139,
                    "matchedPaperCorpusId": "58068920"
                },
                {
                    "start": 1300,
                    "end": 1319,
                    "matchedPaperCorpusId": "246441975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2359619140625
        },
        {
            "corpus_id": "277667736",
            "title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition",
            "text": "Scaling laws have been observed in statistical learning since Cortes et al. [10] proposed power laws for model performance as a function of data size. However, scaling behaviour has only recently been extensively studied, with the availability of compute and internet-scale datasets. Notably, scaling laws have been extensively explored in large language model training [26,25,4,24,34,52], establishing a power law relationship between the number of tokens in the dataset and number of model parameters. Hoffmann et al., [25] formulated scaling laws for compute-optimal training, identifying over-training and under-training regimes for language models, given a compute budget. Furthermore, Hernandez et al. [24] formulated scaling laws for transfer learning, in which there is a predictable downstream performance in terms of the ratio between the amount of pretraining data and amount of fine-tuning data. In general, scaling laws fall under two broad categories of data scaling laws and model scaling laws. Some works [4,33] identified problems in which scaling up does not improve upon downstream performance. Caballero et al. [4] formulated a generalization of scaling laws and showed that a piecewise linear modelling of scaling is more appropriate when analyzing scaling across a wide range of orders of magnitude. \n\nAside from language modelling, scaling laws have been established for other domains as well, for example, for machine translation [2], masked image modelling [48], contrastive language-image learning [9] and recommendation models [41]. For computer vision [37], the need for scaling informed further architectural developments in vision transformers [16,1] for efficient distributed training. \n\nRegarding gait processing, scaling analysis has not been extensively performed to date. Several large-scale datasets have been proposed [58,57,11,18], but lack the magnitude and diversity for properly studying data scaling in realistic environments. Cosma et al., [12] combined multiple existing gait datasets into a larger set of 800k sequences for training an autoencoder-type model. However, they did not further explore the effect of the data scale. Previously, works in self-supervised learning have noticed improved downstream performance with increase in data scale [14,11], but the amount of data is relatively small to provide insights into scaling behaviour.",
            "score": 0.4310919816026011,
            "section_title": "Related Work",
            "char_start_offset": 5675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 377,
                    "end": 379,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "259188012"
                },
                {
                    "start": 1131,
                    "end": 1134,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1454,
                    "end": 1457,
                    "matchedPaperCorpusId": "246608156"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "249538409"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "244477730"
                },
                {
                    "start": 1674,
                    "end": 1678,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1678,
                    "end": 1680,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "266520895"
                },
                {
                    "start": 1859,
                    "end": 1862,
                    "matchedPaperCorpusId": "244906176"
                },
                {
                    "start": 1862,
                    "end": 1865,
                    "matchedPaperCorpusId": "252263212"
                },
                {
                    "start": 1865,
                    "end": 1868,
                    "matchedPaperCorpusId": "250089142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "Training at the scale seen in recent large foundation models (Dubey et al., 2024;OpenAI, 2023;Reid et al., 2024) is an expensive and uncertain process. Given the infeasibility of hyperparameter tuning multi-billion parameter models, researchers extrapolate the optimal training setup from smaller training runs. More precisely, scaling laws (Kaplan et al., 2020) are used to study many different aspects of model scaling. Scaling laws can guide targets for increasing dataset size and model size in pursuit of desired accuracy and latency for a specific deployment scenario, study architectural improvements, determine optimal hyperparameters and assist in model debugging. \n\nTable 1: We provide a summary of the papers surveyed, highlighting the reproducibility challenges endemic to scaling law papers. Scaling laws are often characterized as power laws between the loss and size of the model and dataset, and are seen in several variations (Section 2). These laws are found empirically by training models across a few orders of magnitude in model size and dataset size, and fitting the loss of these models to a proposed scaling law. Each component of this process varies in the reported literature, from the specific equation being fit, to the training setup, and the optimization method, as well as specific details for selecting checkpoints, counting parameters and the objective loss optimized during fitting. \n\nChanges to this setup can lead to significant changes to the results, and therefore completely different conclusion to the study. For example, Kaplan et al. (2020) studied the optimal allocation of compute budget, and found that dataset size should be scaled more slowly than model size (D \u221d N 0.74 , D is dataset size, N is model size). Later, Hoffmann et al. (2022) contradicted this finding, showing that model size and dataset size should be scaled roughly equally for optimal scaling. They highlight the differences in setup which lead to them showing that large models should be SCALING LAW REPRODUCILIBILITY CHECKLIST Scaling Law Hypothesis ( \u00a73) Fitting Algorithm ( \u00a76) \n\n\u2022 Objective (Loss) \n\n\u2022 Algorithm",
            "score": 0.430416852453785,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2117
                },
                {
                    "start": 2120,
                    "end": 2131
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404541015625
        },
        {
            "corpus_id": "257687665",
            "title": "The Quantization Model of Neural Scaling",
            "text": "In Figure 3, we show how the distribution over losses scales with model size. First, we find that for the first six models in the Pythia sequence, the mean loss scales as a power law with exponent \u03b1 N = 0.083, roughly in line with the parameter scaling exponent of 0.076 measured in [3]. The 6.4b model does not fit the scaling curve well, so we excluded its loss when measuring the scaling exponent. Next, we plot the probability distribution over per-token losses p(L). We find that losses close to zero are by far the most common, and that scaling increases the portion of approximately-zero losses. We also plot Lp(L), the probability density over losses weighted by loss. The mean loss is the area under this curve. We see that despite approximately-zero-loss tokens being by far the most common, they do not contribute much mass to the mean loss. See Figure 11 for how these distributions change over training steps rather than across model size. We note that neural scaling in the wild is much more complicated than for multitask sparse parity -notably, the distribution over   where scaling (of loss on predicting the highlighted token) is abrupt versus smooth. If the Quantization Hypothesis describes language modeling, then samples with sharp scaling would be monogenic, displaying sharp emergence at a particular model scale when the relevant quantum is learned. Samples with gradual scaling would be polygenic, where many quanta, emerging at different scales, marginally improve the loss. We show additional examples in Figure 12. \n\nlosses is not bimodal. We leave a detailed study of whether the statistics of neural scaling in LLMs are compatible with prior models of neural scaling to future work.",
            "score": 0.42999649141071544,
            "section_title": "The distribution over per-token losses",
            "char_start_offset": 17108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "The first key observation is that the shapes of LR curve, gradient norm curve, and validation loss curve are quite similar across various LRS when training LLMs (Fig. 4). This suggests an implicit connection between learning rate and loss, where gradient norm could be the bridge. \n\nScaling Laws for Constant LRS. A constant LRS is a special LRS, where every training step can be viewed as an endpoint of the LRS. Notably, the Chinchilla scaling law (Hoffmann et al., 2022) exactly fits losses of last steps, i.e., LRS endpoints. Therefore, the expectation of validation loss of all steps under in constant LRS adheres to a power-law over training step s. \n\nExtra Loss Changes in LR Annealing. Unlike a constant LRS, LR annealing (or re-warmup) brings significant local changes in the loss (see Fig. 4), causing the full loss curve to deviate from the traditional power-law formulation that consider only the training steps s. We hypothesis that such loss changes can be captured by an additional LR (\u03b7) related term, i.e.,  where the first two terms (blue part) follow traditional scaling laws, while the last term (red part) denotes the extra loss change brought by LR annealing. Recall the similarity between learning rate and loss curves, we can form a naive guess for f (\u03b7) as \n\nTraining Discount in Annealing. The form of Eq. 3 is still imperfect. Note that the gradient norm \u2225g\u2225 decreases along with LR during the annealing process (shown in Fig. 4). Thus, the amount of parameter movement (approximately \u03b7\u2022\u2225g\u2225 per step) in the LR annealing stage declines at an almost quadratic rate compared to stages before annealing. As the parameter movement become smaller, the change in loss also slows down accordingly. Therefore, the loss drop brought by the power law term (i.e., the first two terms in Eq. 3) should also diminish during LR annealing. This consideration leads to an improved equation: \n\nwhere S 1 is the forward area, i.e., the area under the LR curve (as visualized in Fig. 1), which could be approximately interpreted as the total amount of parameter updates.",
            "score": 0.4299405536716426,
            "section_title": "SIMILARITY BETWEEN LEARNING RATE, GRADIENT NORM, AND LOSS",
            "char_start_offset": 8695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2078
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "257913333",
            "title": "Eight Things to Know about Large Language Models",
            "text": "Scaling law results (Kaplan et al., 2020;Brown et al., 2020;Hoffmann et al., 2022) have been a major driving factor in the recent surge of research and investment into LLMs (Ganguli et al., 2022a). Scaling laws allow us to precisely predict some coarse-but-useful measures of how capable future models will be as we scale them up along three dimensions: the amount of data they are fed, their size (measured in parameters), and the amount of computation used to train them (measured in FLOPs). These results thereby allow us to make some key design decisions, such as the optimal size of a model given some fixed resource budget, without extremely expensive trial and error. \n\nOur ability to make this kind of precise prediction is unusual in the history of software and unusual even in the history of modern AI research. It is also a powerful tool for driving investment since it allows R&D teams to propose model-training projects costing many millions of dollars, with reasonable confidence that these projects will succeed A scaling law result for one measure of language model performance, showing a consistent trend as the amount of computation used to train a model is scaled up 10,000,000,000\u00d7 times from a small prototype system to GPT-4. \n\nat producing economically valuable systems. \n\nConcretely, consider these three superficially very different systems: OpenAI's original GPT can perform simple text-labeling tasks but cannot generally produce coherent text (Radford et al., 2018). GPT-2 adds the ability to produce text of reasonably high quality, as well as a limited ability to follow simple instructions (Radford et al., 2019). GPT-3 is the first modern general-purpose LLM, and is practically useful across a wide range of language tasks. The designs of these three models hardly differ at all. Instead, the qualitative differences between them stem from vast differences in scale: Training GPT-3 used roughly 20,000\u00d7 more computation than training the original GPT (Sevilla et al., 2022), as well as significantly more data and parameters.",
            "score": 0.42972197237248294,
            "section_title": "LLMs predictably get more capable with increasing investment, even without targeted innovation",
            "char_start_offset": 1569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 60,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 60,
                    "end": 82,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 173,
                    "end": 196,
                    "matchedPaperCorpusId": "246867298"
                },
                {
                    "start": 1471,
                    "end": 1493,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1621,
                    "end": 1643,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32763671875
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "In this case, the dynamics smoothly transition between following the gradient descent trajectory at early time to an asymptote that depends on N as t \u2192 \u221e. In Figure 2 (c)-(d) we illustrate these learning curves from our theory and from finite N simulations, showing a good match of the theory to experiment. \n\nWe derive the asymptotic scaling of N \u2212\u03b1 min{2,\u03b2} in Appendix E.3. Intuitively, at finite N , the dynamics only depend on the filtered signal 1 N A(0) \u22a4 A(0) w \u22c6 . Thus the algorithm can only estimate, at best, the top N components of w \u22c6 , resulting in the following t \u2192 \u221e loss \n\nSGD Noise Effects The variance in the learned model predictions due to random sampling of minibatches during SGD also alters the mean field prediction of the loss. In Figure 3, we show SGD noise effects from finite batch size B for hard \u03b2 < 1 and super easy \u03b2 > 2 \u2212 1/\u03b1 tasks. \n\nCompute Optimal Scaling Laws in Feature Learning Regime At a fixed compute budget C = N t, one can determine how to allocate compute towards training time t and model size N \n\nTable 1: Compute optimal scaling exponents r C for the loss L \u22c6 (C) \u223c C \u2212r C for tasks of varying difficulty in the feature learning regime. For \u03b2 > 1, the exponents coincide with the lazy model analyzed by Bordelon et al. (2024a); Paquette et al. (2024), while for hard tasks they are improved. using our derived exponents from the previous sections. Choosing N, t optimally, we derive the following compute optimal scaling laws L \u22c6 (C) in the feature learning regime \u03b3 > 0. These are also summarized in Figure 1. 5 . Hard task regime (\u03b2 < 1): the compute optimum balances the population gradient flow term t \u2212 2\u03b2 1+\u03b2 and the model bottleneck \n\nWe work out the complete compute optimal scaling laws for these three settings by imposing the constraint C = N t, identifying the optimal choice of N and t at fixed t and verifying the assumed dominant balance. We summarize the three possible compute scaling exponents in Table 1.",
            "score": 0.4295511300201643,
            "section_title": "Accelerated Training in Rich Regime",
            "char_start_offset": 20444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.261962890625
        },
        {
            "corpus_id": "273346307",
            "title": "Universal Scaling Laws in Quantum-Probabilistic Machine Learning by Tensor Network: Toward Interpreting Representation and Generalization Powers",
            "text": "( 7), one immediately obtains the linear scaling law of NLL as given by Eq. ( 1). Fig. 2(a) demonstrates such a scaling law on the Fashion-MNIST dataset [35] for the inter-class NLL, where we take the categories of the samples to differ from those of the (trained) GTN's. By linear fitting, we have k = 0.1837, 0.1764, and 0.1758 for the training set (k = 0.1835, 0.1762, and 0.1756 for the testing set) for \u03c7 = 32, 64, and 128, respectively. This indicates the scaling law of NLL does not change by varying \u03c7. or O(10 \u22121 ). These indicate the contributions from all three terms cannot be ignored. The universality of the scaling laws is further demonstrated on various datasets [36]. \n\nBelow, we focus on the intra-class NLL, if not mentioned specifically. In Fig. 3 12) and those computed using Eq. ( 13). \n\n\u03b1 in Eq. ( 2) for the intra-class NLL. Our results suggest the logarithmic scaling behavior of \u03b1 against \u03c7 as \n\nSimilar logarithmic behaviors are observed for the coefficients \u03b2 and \u03b3, as shown in Fig. 3(b) and (c). Substituting such logarithmic relations of \u03b1, \u03b2, and \u03b3 [one may refer to Eq. ( 10)] to the corrected scaling law, we have \n\n(11) This suggests the logarithmic scaling of L against \u03c7 as \n\nwhich is verified in Fig. 3(d). It means logarithmic improvement of the ML powers versus the number of quantum channels (which will be discussed below). Another important observation in Fig. 3(a)-(c) is the deviation between the curves for the training and testing sets. For about \u03c7 > 30, the coefficients in the corrected scaling law for the training set maintain the logarithmic scaling behavior given by Eq. (12). But those for the testing set start to deviate, leading to higher NLL as shown in Fig. 3(d).",
            "score": 0.42923520344494187,
            "section_title": "body",
            "char_start_offset": 8732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1722
                }
            ],
            "ref_mentions": [
                {
                    "start": 1624,
                    "end": 1628,
                    "matchedPaperCorpusId": "258987622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120849609375
        },
        {
            "corpus_id": "270371652",
            "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
            "text": "To study how structured layers compare against dense layers as a function of compute, we will compare their scaling laws: how compute translates to performance as the models scale up.Across domains such as language, image, and video modeling, the loss or error rate E of a well-trained neural network has shown to be highly predictable as a function of the compute C required by the model, often well-described by a power law E \u221d C \u2212\u03b1 when data is not a bottleneck (Kaplan et al., 2020;Sharma & Kaplan, 2022;Hoffmann et al., 2022).If structured layers can achieve better scaling laws, they will outperform dense layers at scale, delivering exponentially better performance per unit compute if they can improve the scaling exponent \u03b1.\n\nIn this work, we systematically study whether structured Controlling for compute, replacing dense layers with structured matrices enables wider models and can lead to better performance.(a) A neural network with structured matrices can be made much wider, but its learning rate needs to be scaled differently as a function of width since not all connections are present (Section 3).The width d of a dense layer scales as C 1/2 where C is the compute per forward pass, while the width d of a block diagonal layer is exponentially larger, scaling as C 2/3 .The optimal learning rate \u03b7 of the dense layer and \u03b7 of the block diagonal layer scales differently as d \u22121 and d\u22121/2 respectively.(b) Structured matrices can improve the training error scaling laws of MLPs on CIFAR-10 with data augmentation (Section 4).(c) Scaling the learning rate in a structure-aware fashion (\u2022) is crucial for performance (Section 3), without which the benefit of structured layers does not emerge (\u25bc).\n\nmatrices can have better scaling laws than dense matrices, without relying on domain-specific algebraic structures so that our findings can apply to training foundation models broadly.\n\n\u2022 We show that structured layers often require drastically different learning rates and initialization scales compared to their dense counterparts, because their underlying trainable parameter matrices tend to be much smaller in size than the width of the layer (Figure 1a).Naively using dense layer learning rates, structured layers often significantly underperform dense layers, as shown in Figure 1c.",
            "score": 0.42909420824768385,
            "section_title": "Introduction",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1544
                },
                {
                    "start": 1544,
                    "end": 1714
                },
                {
                    "start": 1716,
                    "end": 1900
                },
                {
                    "start": 1902,
                    "end": 2176
                },
                {
                    "start": 2176,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "246559072"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "252693192",
            "title": "Optimizing Data Collection for Machine Learning",
            "text": "Neural Scaling Laws. According to the neural scaling law literature, the performance of a model on a validation set scales with the size of the training data set q via a power law V \u221d \u03b8 0 q \u03b81 [5,6,[8][9][10][15][16][17][18][19]. Hestness et al. [5] observe this property over vision, language, and audio tasks, Bahri et al. [9] develop a theoretical relationship under assumptions on over-parametrization and the Lipschitz continuity of the loss, model, and data, and Rosenfeld et al. [6] estimate power laws using smaller data sets and models to extrapolate future performance. Multi-variate scaling laws have also been considered for some specific tasks, for example in transfer learning from synthetic to real data sets [11]. Finally, Mahmood et al. [2] explore data collection by estimating the minimum amount of data needed to meet a given target performance over multiple rounds. Our paper extends these prior studies by developing an optimization problem to minimize the expected total cost of data collected. Specifically, we incorporate the uncertainty in any regression estimate of data requirements and further generalize to multiple data sources with different costs. \n\nActive Learning. In active learning, a model sequentially collects data by selecting new subsets of an unlabeled data pool to label under a pre-determined labeling budget that replenishes after each round [20][21][22][23][24]. In contrast, our work focuses on systematically determining an optimal collection budget. After determining how much data to collect, we can use active learning techniques to collect the desired amount of data. \n\nStatistical Learning Theory. Theoretical analysis of the sample complexity of machine learning models is typically only tight asymptotically, but some recent work have empirically analyzed these relationships [25,26]. Particularly, Bisla et al. [10] study generalization bounds for deep neural networks, provide empirical validation, and suggest using them to estimate data requirements. In contrast, our paper formally explores the consequences of collection costs on data requirements. \n\nOptimal Experiment Design.",
            "score": 0.428786011294501,
            "section_title": "Related work",
            "char_start_offset": 4569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2110
                },
                {
                    "start": 2113,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 198,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "224820076"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "233739646"
                },
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "1515785"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "6842201"
                },
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "580196"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "3383786"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "matchedPaperCorpusId": "148571749"
                },
                {
                    "start": 1400,
                    "end": 1404,
                    "matchedPaperCorpusId": "90258881"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "235358868"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "237156135"
                },
                {
                    "start": 1868,
                    "end": 1872,
                    "matchedPaperCorpusId": "233739646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "256868751",
            "title": "Cliff-Learning",
            "text": "Our toy models in Section 3 suggest stronger cliffs arise from better prior-task compatibility. This corroborates our findings in Section 2 -we find that the quality of a foundation model (aka the quality of a learned prior) primarily manifests in data-scaling via the initial cliff-learning regime. In the large-n regime, foundation models of different qualities all scale as power laws with similar \u03b1s c.f. Equation (1.1), but different quality models have different As and Es due to different amounts of cliff-learning. \n\nFinally, we present some open questions: \n\n1. In Section 3, we conjectured that when two different types of training methods share the same data-scaling asymptotics, they share similar \"mechanisms\" of learning. For example, if a learning algorithm A has the same asymptotics as a nearest-neighbors method, does this imply A is \"memorizing\" the training data? What other \"mechanisms\" are there? \n\n2. We showed that cliff-learning can be achieved by finetuning foundation models. However, these large foundation models generally follow power law scaling in compute, parameters, and data. Is it possible for large foundation models to exhibit strong cliff-learning when trained from scratch? Or is there some sort of conservation principle that says a large amount of data is required somewhere in the learning pipeline? APPENDIX A RELATED WORK Hernandez et al. (2021) studies scaling laws for transfer learning and concludes that power law data-scaling holds for transfer learning. However the experiments in Hernandez et al. (2021) are conducted exclusively in the large-n regime (even the smallest experiments use > 3 \u00d7 105 characters of downstream text data). In our experiments, we observed that cliff-learning is a transient phenomenon and that data-scaling reverts to a power law in the large-n regime. We believe the absence of cliff-like behavior in Hernandez et al. (2021) is caused by working in a large-n regime where cliff-learning does not apply. Arora et al. (2019) proposes a theoretical model that explains why contrastive-learning produces representations that support well-performing linear probes.",
            "score": 0.4287216925525057,
            "section_title": "DISCUSSION AND OPEN QUESTIONS",
            "char_start_offset": 6619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2430419921875
        },
        {
            "corpus_id": "268819884",
            "title": "The Larger the Better? Improved LLM Code-Generation via Budget Reallocation",
            "text": "Model scaling was found to be one of the key elements in the success of LLMs (Dehghani et al., 2023;Gu et al., 2023;Hassid et al., 2023;Rae et al., 2021;Chowdhery et al., 2023;Touvron et al., 2023), with Wei et al. (2022) demonstrating how specific abilities emerge mainly after reaching a specific scale.The way language models behave when they are scaled up and their ability to adjust have been a significant factor in the creation of LLMs (Hernandez et al., 2021).Kaplan et al. (2020) investigated the optimal model size to train for a given compute budget, while Hoffmann et al. (2022) demonstrated how scaling both model and dataset sizes improves performance across various tasks.Clark et al. (2022) analyzed the scaling properties of mixture-of-experts models, showing that scaling with the number of experts diminishes as model size increases.Recently, Gadre et al. ( 2024) provided a scaling law analysis considering downstream tasks rather than next-token prediction loss.They related the perplexity of a language model to its downstream task performance via a power law and used it to predict the top-1 error averaged over the evaluated downstream tasks.\n\nOur work differs from all of the above, as we do not claim to provide new scaling laws but rather suggest that when fixing the budget, smaller models can provide comparable or superior results to larger ones.\n\nRecent studies by Shi et al. (2024) and Mei et al. (2024) have demonstrated that under constrained compute budgets, smaller vision models can surpass their larger counterparts.Specifically, Shi et al. (2024) found advantages in using multiple image scales, whereas Mei et al. (2024) observed that smaller diffusion models perform better than larger ones when the compute budget is fixed.Our approach, which generates multiple text outputs from a small model, aligns with these findings.",
            "score": 0.42851847889755723,
            "section_title": "Model Scaling",
            "char_start_offset": 17468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1166
                },
                {
                    "start": 1168,
                    "end": 1376
                },
                {
                    "start": 1378,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 100,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "258832977"
                },
                {
                    "start": 153,
                    "end": 176,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 204,
                    "end": 221,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 687,
                    "end": 706,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "272753679",
            "title": "In-Context Learning of Linear Systems: Generalization Theory and Applications to Operator Learning",
            "text": "While empirical scaling laws have been identified for Large Language Models (LLMs) Kaplan et al. (2020) ;Hoffmann et al. (2022) and more recently attempted in a SciFM for PDEs Subramanian et al. (2024); McCabe et al. (2023), a rigorous characterization of scaling laws remains open. Additionally, the generalization performance of SciFMs encounters significant hurdles due to prevalent distribution shifts between tasks and data used in pre-training and those in adaptation Subramanian et al. (2024); McCabe et al. (2023); Ye et al. (2024); Yang et al. (2023). For instance, the behavior and fine structures (e.g. multiple spatial-temporal scales) of a PDE solution can change dramatically in response to the changes in the physical parameters and conditions. Therefore a rigorous quantification of the generalization performance of SciFMs on downstream tasks due to the various domain shifts is an important step for understanding their capabilities and limitations.",
            "score": 0.4280961444463211,
            "section_title": "Introduction",
            "char_start_offset": 3079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 967
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 201,
                    "matchedPaperCorpusId": "259000160"
                },
                {
                    "start": 474,
                    "end": 499,
                    "matchedPaperCorpusId": "259000160"
                },
                {
                    "start": 541,
                    "end": 559,
                    "matchedPaperCorpusId": "258959561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2548828125
        },
        {
            "corpus_id": "257038400",
            "title": "Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension",
            "text": "A number of works including but not limited to (Pope et al., 2021;Sharma & Kaplan, 2020;Bahri et al., 2021), have studied (both theoretically and empirically) the relationship between the intrinsic dimension of a dataset and the loss of a neural network trained on that dataset. Recently works trying There is another way one might relate eq. (1) to our experiments: if high perplexity prompts truly are more rare, we might imagine they effectively decrease the training data size D (since the model has seen them fewer times). This seems incompatible with the above analysis: if for some reason the loss stays constant, d would have to decrease rather than increase as it does in Figure 3. \n\nWhile quite simple, even simplistic, this discussion points to a number of avenues for future work: first, it is not obvious how to attempt something like the above back-of-the-envelope calculation using the multi-modal scaling laws of (Aghajanyan et al., 2023). In a different direction, we were unable to find much on neural scaling laws for diffusion models (the closest we came across was (Caballero et al., 2023)).",
            "score": 0.42792178884894183,
            "section_title": "D Intrinsic dimension of data and scaling laws",
            "char_start_offset": 17114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1112
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 66,
                    "matchedPaperCorpusId": "233296562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0775146484375
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Substantial effort and cost are required to train even a single large language model (LLM). 1 There is thus an acute need for efficient decision-making aids that can evaluate the effectiveness of proposed changes to language models' architecture or training data without full-scale training runs. While there is a large body of work that motivates or evaluates these changes using small models (Warstadt et al., 2023;Hillier et al., 2024), synthetic tasks (Aky\u00fcrek et al., 2024;Wortsman et al., 2023) or theory (Jelassi et al., 2024), one of the most important tools for current practitioners is the estimation of scaling laws for LLMs (Ivgi et al., 2022;Dubey et al., 2024). \n\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. Typically, this extrapolation requires models to belong to the same model family, differing only in parameter count and training set size, but using the same architecture and training distribution. A high-quality scaling law accurately predicts the target model's test performance (Rosenfeld et al.;Kaplan et al., 2020;Hoffmann et al., 2022). \n\nMost past work describing and characterizing scaling laws has begun by exhaustively training models in a family across a full range of dataset sizes and parameter counts. One question that has received comparatively little attention is how, when training a new LLM, a practitioner with limited computational resources should choose which small-scale models to train in order to best estimate a target model's final performance. This paper offers a practical guide to when, and how, to use small models to efficiently obtain meaningful predictions about large models' behaviormaximizing prediction reliability while minimizing the budget for preliminary experimentation, which necessarily involves tradeoffs between the number of preliminary models trained, the size of the largest preliminary model, and size of the dataset used to train it. 1 Code, data and full numbers are found in our repository 1 arXiv:2410.11840v1 [cs.LG] 15 Oct 2024 arXiv Version",
            "score": 0.42771961411024106,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 478,
                    "end": 500,
                    "matchedPaperCorpusId": "262824542"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "203592013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254638671875
        },
        {
            "corpus_id": "273811967",
            "title": "Unlocking the Theory Behind Scaling 1-Bit Neural Networks",
            "text": "Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an impressive combination of efficiency and performance that rivals traditional LLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the performance of these 1-bit LLMs progressively improves as the number of parameters increases, hinting at the potential existence of a Scaling Law for 1-bit Neural Networks. In this paper, we present the first theoretical result that rigorously establishes this scaling law for 1-bit models. We prove that, despite the constraint of weights restricted to $\\{-1, +1\\}$, the dynamics of model training inevitably align with kernel behavior as the network width grows. This theoretical breakthrough guarantees convergence of the 1-bit model to an arbitrarily small loss as width increases. Furthermore, we introduce the concept of the generalization difference, defined as the gap between the outputs of 1-bit networks and their full-precision counterparts, and demonstrate that this difference maintains a negligible level as network width scales. Building on the work of Kaplan et al. (2020), we conclude by examining how the training loss scales as a power-law function of the model size, dataset size, and computational resources utilized for training. Our findings underscore the promising potential of scaling 1-bit neural networks, suggesting that int1 could become the standard in future neural network precision.",
            "score": 0.42737390249773666,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "273877502",
            "title": "Scaling Laws for Pre-training Agents and World Models",
            "text": "Scaling laws origin. The term scaling laws is used throughout the engineering and physical sciences to denote power law relationships between two quantities, e.g. duration of a volcanic eruption and the probability of it continuing (Cannav\u00f2 and Nunnari, 2016). The name derives from the scaleinvariant1 property of power laws. While early work suggested that power laws could be good empirical descriptors of important variables in deep learning (Hestness et al., 2017;Rosenfeld et al., 2019), it was Kaplan et al. (2020) who provided a comprehensive study of power laws in transformer LLMs, and popularized the usage of scaling laws in this context. \n\nScaling laws in LLMs. (Hutter, 2021;Maloney et al., 2022;Bahri et al., 2024). \n\nScaling in embodied AI. Compared to LLMs, our understanding of scale in embodied settings is less advanced. Early successes in competitive games showed that running reinforcement learning (RL) at scale could surpass human performance, e.g. (Silver et al., 2017;Berner et al., 2019). In self-play RL, power laws were observed between certain quantities by Neumann and Gros (2022). Meanwhile, Hilton et al. (2023) noted that, in general, reward signals do not follow power laws, and defined a transformation of reward (intrinsic performance) that created self-consistent scaling laws. \n\nInspired by the effectiveness of scaling in LLMs, embodied AI research has recently begun to explore the effectiveness of generative pre-training objectives on offline datasets, when executed at scale. This includes behavior cloning objectives in video games (Baker et al., 2022;Raad et al., 2024), robotics (Brohan et al., 2022;2023;Padalkar et al., 2023;Bousmalis et al., 2023), or multiple domains (Reed et al., 2022), as well as world modeling objectives (Hu et al., 2023;Yang et al., 2023;Bruce et al., 2024). In these studies, the benefit of scale is generally shown through increasing model size on a specific downstream task of interest (e.g.",
            "score": 0.4272908133955624,
            "section_title": "RELATED WORK",
            "char_start_offset": 4349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 259,
                    "matchedPaperCorpusId": "205689420"
                },
                {
                    "start": 710,
                    "end": 729,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 973,
                    "end": 994,
                    "matchedPaperCorpusId": "205261034"
                },
                {
                    "start": 1577,
                    "end": 1597,
                    "matchedPaperCorpusId": "249953673"
                },
                {
                    "start": 1812,
                    "end": 1831,
                    "matchedPaperCorpusId": "267897982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.155517578125
        },
        {
            "corpus_id": "262230153",
            "title": "The argument for near-term human disempowerment through AI",
            "text": "The first reason is that, in LLMs, scaling has led to massive increases in pre-existing capacities and the emergence of qualitatively new capacities (Wei et al. 2022). This development is often captured in terms of so-called scaling laws: It has been shown that language model performance improves smoothly as the scale of the model increases, where performance has a power-law relationship with each individual factor (model size, size of the training dataset or compute) when not bottlenecked by the other two (Kaplan et al. 2020). These scaling laws pertain to the task the model is directly trained to perform, i.e. predicting the next token in a sequence as accurately as possible. \n\nImprovements in other related capacities cannot be predicted. Nevertheless, in the last five years, performance in virtually all interesting benchmark tasks has improved massively with increased scale (OpenAI 2023; Suzgun et al. 2022). This has been accompanied by the emergence of qualitatively new abilities, e.g. few-shot learning (Brown et al. 2020).3 So, increases in scale have led to improvements in virtually all interesting abilities in LLMs, including the emergence of abilities which some thought were not available to them. Given this, it seems wise to suspect that further increases in scale will lead to further progress. \n\nIt could be that there are limits to what scaling can achieve and that progress will eventually stop. It could be that this limit comes before AGI. However, given that no one has identified an important capacity which does not improve with scaling, the currently best supported hypothesis is arguably that further scaling will bring about AGI. Further support for this idea is that, in terms of real-world usefulness and performance on benchmark tests, the difference in performance between GPT-4 and GPT-1 seems to be a lot higher than the difference between GPT-4 and human-level reasoning. 4 In the light of this, it would be quite coincidental if improvements from increasing scale would level of shortly before LLMs reach human-level intelligence.5 I hold that, if scaling transformer models suffices for AGI, sufficiently powerful models can be built before 2100. This conditional is supported by Cotra's Biological Anchors Framework (Cotra 2020).",
            "score": 0.427283861314801,
            "section_title": "Motivating premise 1",
            "char_start_offset": 15456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1378173828125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Understanding the scaling laws of neural models is crucial for optimizing performance across different domains. We predominantly explore the scaling principles for language models, extending to other modalities such as vision and multimodal learning. We also examine scaling behaviors in domain adaptation, inference, efficient model architectures, and data utilization. We highlight the taxonomy tree of scaling laws research in Figure 2. As highlighted in Figure 1, neural scaling laws have been proposed predominantly for pre-training and fine-tuning scaling of large neural models. Among the models studied, as highlighted in Figure 3a, decoder-only Transformers dominate the subject, followed by vision transformers (ViT) and Mixture-of-Experts (MoE). \n\nThe most common neural scaling laws take the form of power laws (Equation 1), where the model's loss (L) or performance metric assumes to follow a predictable relationship with different scaling variables, \n\nwith appropriate scaling parameters \u03b2 i and fitting parameters \u03b1 i for different scaling parameter P i . Figure 3b highlights that the number of model parameters and data size are the most common used scaling factors. The exact forms of all the scaling laws are highlighted in Table 7 of Appendix B. Among all the tasks, Figure 3c suggests that language generation is the most common task used for developing these scaling laws, where the training cross-entropy loss is widely used to fit the laws. Based on the values obtained empirically, the scaling laws are fitted with non-linear optimization, most commonly by running algorithms like least square and BFGS (Broyden-Fletcher-Goldfarb-Shanno). Statistical methods like goodness-of-fit metrics are used to validate the correctness of the fitted curves. We elaborate on the evaluation of neural scaling laws in Appendix A. \n\nIn the following sections, we review the existing literature on neural scaling across various domains. Kaplan et al. (2020) suggested that larger LMs improve performance by reducing loss through power-law scaling. However, this view evolved when studies showed that many large models were undertrained, and data scaling plays an equally crucial role in compute efficiency (Hoffmann et al., 2022). More recent breakthroughs challenged traditional scaling assumptions.",
            "score": 0.4268795056642635,
            "section_title": "Taxonomy of neural scaling laws",
            "char_start_offset": 3460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "270870707",
            "title": "A Closer Look at Deep Learning Methods on Tabular Datasets",
            "text": "We compare our method with five different scaling laws, whose parameters could be estimated based on the first five points of the validation curve.\n\n\u2022 M1 describes the basic form of the scaling law: y = ax b .This has been used, for example, to estimate the required sample size in language models [32].\u2022 M2 [15,25,42,1] models the performance of multi-layer neural networks as a power law of the form y = ax b + c.\n\n\u2022 M3 [43] introduces the functional form y = a(x + d) b + c, where d represents the scale at which the performance starts to improve beyond the random guess loss and transitions to a power law scaling regime.\u2022 M4 [3] proposes the functional form (y \u2212 \u03f5 \u221e )/((\u03f5 0 \u2212 y) a ) = bx c , where \u03f5 \u221e is the irreducible entropy of the data distribution and \u03f5 0 is the random guess performance.\u2022 Our Scaling Law represents the form y = A log x + B \u221a x + C + D/x, without training an MLP.\n\nIt should be noted that we used the official code to fit the scaling law parameters mentioned above, available at revisiting_neural_scaling_laws.",
            "score": 0.4268795056642635,
            "section_title": "B.4.2 Comparison Methods",
            "char_start_offset": 39354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 149,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 415
                },
                {
                    "start": 417,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 893
                },
                {
                    "start": 895,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 318,
                    "end": 320,
                    "matchedPaperCorpusId": "238354065"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "252220884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39208984375
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "As neural networks continue to grow in size but datasets might not, it is vital to understand how much performance improvement can be expected: is it more important to scale network size or data volume? Thus, neural network scaling laws, which characterize how test error varies with network size and data volume, have become increasingly important. However, existing scaling laws are often applicable only in limited regimes and often do not incorporate or predict well-known phenomena such as double descent. Here, we present a novel theoretical characterization of how three factors -- model size, training time, and data volume -- interact to determine the performance of deep neural networks. We first establish a theoretical and empirical equivalence between scaling the size of a neural network and increasing its training time proportionally. Scale-time equivalence challenges the current practice, wherein large models are trained for small durations, and suggests that smaller models trained over extended periods could match their efficacy. It also leads to a novel method for predicting the performance of large-scale networks from small-scale networks trained for extended epochs, and vice versa. We next combine scale-time equivalence with a linear model analysis of double descent to obtain a unified theoretical scaling law, which we confirm with experiments across vision benchmarks and network architectures. These laws explain several previously unexplained phenomena: reduced data requirements for generalization in larger models, heightened sensitivity to label noise in overparameterized models, and instances where increasing model scale does not necessarily enhance performance. Our findings hold significant implications for the practical deployment of neural networks, offering a more accessible and efficient path to training and fine-tuning large models.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "277065723",
            "title": "A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules",
            "text": "Later studies refined these scaling laws by improving fitting and evaluation pipelines, refining parameter size metrics, ensuring consistent hyperparameter configurations across scales, and extending their applicability to broader training scenarios (Hoffmann et al., 2022;Henighan et al., 2020;Bi et al., 2024;Caballero et al., 2022;Alabdulmohsin et al., 2022). \n\nScaling laws have been used to predict the model performance in various settings, hyperparameter optimization (Kadra et al., 2023), multi-epoch training (Muennighoff et al., 2023), training sparselyconnected models (Frantar et al., 2023;Ludziejewski et al., 2024), length extrapolation (Liu et al., 2023), transfer learning (Hernandez et al., 2021), and data mixing (Hashimoto, 2021;Ge et al., 2024;Ye et al., 2024;Jain et al., 2024). However, most existing work neglects the impact of the learning rate, making their predictions unreliable for assessing model performance throughout training (Hoffmann et al., 2022). As a result, deriving scaling laws for final losses under specific schedules (e.g., the cosine schedule (Hoffmann et al., 2022;Muennighoff et al., 2023)) requires more than 10 full training runs. Another line of works extrapolate loss curves with Bayesian methods (Klein et al., 2017;Domhan et al., 2015;Choi et al., 2018;Adriaensen et al., 2023) for hyperparameter optimization, but they also need a large number of training runs. In comparison, our work introduces a multi-power law that extends the existing power-law form of specific LR schedules by incorporating terms to capture LR schedule effects. Unlike prior work, our approach is LR-dependent and can predict full loss curves across various schedules using fewer than three training curves. \n\nConcurrent to our work, Tissue et al. ( 2024) proposed a momentum law that also incorporates LR into scaling laws.",
            "score": 0.4268795056642635,
            "section_title": "RESULTS",
            "char_start_offset": 39211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 361,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 475,
                    "end": 495,
                    "matchedPaperCorpusId": "258887933"
                },
                {
                    "start": 602,
                    "end": 628,
                    "matchedPaperCorpusId": "267626982"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "267522769"
                },
                {
                    "start": 1247,
                    "end": 1267,
                    "matchedPaperCorpusId": "91185890"
                },
                {
                    "start": 1267,
                    "end": 1287,
                    "matchedPaperCorpusId": "369457"
                },
                {
                    "start": 1287,
                    "end": 1305,
                    "matchedPaperCorpusId": "67876314"
                },
                {
                    "start": 1305,
                    "end": 1329,
                    "matchedPaperCorpusId": "254240628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "270870624",
            "title": "Information-Theoretic Foundations for Neural Scaling Laws",
            "text": "Neural scaling laws aim to characterize how out-of-sample error behaves as a function of model and training dataset size. Such scaling laws guide allocation of a computational resources between model and data processing to minimize error. However, existing theoretical support for neural scaling laws lacks rigor and clarity, entangling the roles of information and optimization. In this work, we develop rigorous information-theoretic foundations for neural scaling laws. This allows us to characterize scaling laws for data generated by a two-layer neural network of infinite width. We observe that the optimal relation between data and model size is linear, up to logarithmic factors, corroborating large-scale empirical investigations. Concise yet general results of the kind we establish may bring clarity to this topic and inform future investigations.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "267406160",
            "title": "A Dynamical Model of Neural Scaling Laws",
            "text": "On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-width dynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate $\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture and task. We show that our model exhibits this behavior. Lastly, our theory shows how the gap between training and test loss can gradually build up over time due to repeated reuse of data.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "274776083",
            "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning",
            "text": "Following prior work (Que et al., 2024), we utilize both R 2 (Fisher, 1922) and Huber loss (Huber, 1992) to evaluate different parameterizations of scaling law. The R 2 value, or coefficient of determination, measures the fitting quality, with values closer to 1 indicating a better fit. Huber loss, a robust loss function, blends the characteristics of mean squared error and mean absolute error, making it less sensitive to outliers. The Huber loss is a positive number, and a lower Huber loss suggests a better fit. \n\nScaling laws are often used to determine the optimal amount of training data by balancing computational cost and model performance. For instance, as shown in Figure 5, there is one actual loss curve and two predicted loss curves. Traditional metrics like R 2 and Huber loss indicate that predicted curve 1 better matches the actual curve. However, when predicted curve 1 is used to predict the point where the actual curve starts to flatten, it results in a large error. On the other hand, while predicted curve 2 deviates more from the actual curve in terms of absolute values, its slope is consistently closer to the actual curve. This makes its prediction of the flattening point much more accurate. To address this issue, we propose a new metric called Average Slope Difference (ASD), which measures the difference between the slope of the loss curve predicted by the scaling law and the slope of the actual loss curve. ASD is formally defined as: \n\nwhere   used during the pre-training process. It is formally defined as follows: \n\nwhere N C , D C , E, \u03b1, and \u03b2 are constants, N represents the model size, D denotes the number of pretraining tokens and L represents the model's loss. \n\nCompared to the OpenAI scaling law (Kaplan et al., 2020), the Chinchilla scaling law demonstrates superior performance (detailed in Appendix D). Therefore, we adopt the Chinchilla scaling law as the foundational parameterization for our P 2 Law. \n\nCombining the pruning rate \u03c1 and the model's loss L 0 before pruning, we define the following three candidate parameterizations:",
            "score": 0.4261392065000828,
            "section_title": "Metric for Accessing Law Fitting",
            "char_start_offset": 13149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 104,
                    "matchedPaperCorpusId": "61846277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394287109375
        },
        {
            "corpus_id": "276117030",
            "title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization",
            "text": "As deep learning continues to scale toward larger models and datasets, significant attention has been devoted to studying the scaling laws that trade-off between model and dataset size to optimize performance and computational efficiency (Hoffmann et al., 2022;Kumar et al., 2024;Dettmers & Zettlemoyer, 2023). In the meantime, the field is shifting toward lower-precision computation, particularly in large language models, driven by the substantial benefits of mem- ory savings and computational efficiency (Liu et al., 2023a;Ma et al., 2024). This shift necessitates a rethinking of scaling laws to account for the effects of quantization on resulting quantized model performance. \n\nWhen allowing for lower-bit quantization, we can freely trade off the bit-width and the number of parameters. Keeping the amount of memory used the same, we could have an 8-bit model, or a 4-bit model twice the size. This begs the question: What is the optimal trade-off between bit-width and model size? Recent papers (Dettmers & Zettlemoyer, 2023;Kumar et al., 2024) on scaling laws for low-precision conclude that 4 or 6-bit quantization often resides on the Pareto frontier to balance accuracy and efficiency. Other studies (Ma et al., 2024;Kaushal et al., 2024a) suggest that bit-widths as low as 1.58-bit per parameter hold significant promise for the optimal scaling law trade-off. These opposing conclusions highlight the challenges of studying scaling laws in the low-precision domain. \n\nIn this paper, we demonstrate that previous conclusions on the low-bit scaling laws can be significantly sharpened by better quantization scheme design and training improvements. While previous works define the search space of the QAT scaling laws solely as a function of model parameters (N ), token count (D), and quantization precision (P), we emphasize the critical role that the training scheme (S train ) and the bit-specific quantization function (F) play in the equation. We formalize the search space as ParetoQ L(N , D, P, S train , F), comprising five dimensions.",
            "score": 0.4261034519068143,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 309,
                    "matchedPaperCorpusId": "254853733"
                },
                {
                    "start": 1005,
                    "end": 1035,
                    "matchedPaperCorpusId": "254853733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "273507537",
            "title": "Bayesian scaling laws for in-context learning",
            "text": "In-context learning, like most of the noteworthy properties of large language models, is something that we don't quite understand. This paper emerged from our attempt to reconcile the existing literature that attempts to ascribe a Bayesian basis for the emergence of ICL with the empirical science of scaling laws. We did find that Bayesian scaling laws are competitive with non-theoretical (and relatively unconstrained) scaling laws at modelling ICL behaviour in both toy and real settings. \n\nReal-world applications. The Bayesian approach seems to perform better at extrapolating model behaviour from a few shots. This can be useful for predicting multi-turn safety failures before they happen or whether additional inference-time computation will deliver worthwhile gains. \n\nInterpretability. An additional advantage of our approach is that the parameters of the scaling laws mean something and so can shed light on the internal workings of LLMs without needing to fully open the black box. E.g. studying both the prior over tasks and how ICL affects their posterior is valuable for interpreting the effects of alignment on real-world LLMs. Future work could also mechanistically interpret how Bayesian ICL is performed (e.g. localise the prior in activation space). \n\nAre LLMs Bayesian? In this work we attempt to elucidate model behaviour without reference to model internals. We believe that our results strengthen the claim that LLMs perform Bayesian inference, but do not necessarily prove it. We note that previous works claiming that LLMs are theoretically Bayesian prove their claims on toy models that vastly simplify the complexity of natural language and web-scale pretraining data;2 it's possible that actual web-scale Bayesian reasoning is beyond the capacity of current LLMs, but they still may behave approximately Bayesian, explaining the success of our scaling law. \u2022 \u03a3 is a finite alphabet of symbols \u03c3.",
            "score": 0.4260532395926211,
            "section_title": "DISCUSSION",
            "char_start_offset": 18105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1925
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30859375
        },
        {
            "corpus_id": "268032123",
            "title": "Principled Architecture-aware Scaling of Hyperparameters",
            "text": "Recent works also tried to propose principles of hyperparameter scaling and targeted on better stability for training deep neural networks (Glorot & Bengio, 2010;Schoenholz et al., 2016;Yang & Schoenholz, 2017;Zhang et al., 2019;Bachlechner et al., 2021;Huang et al., 2020;Liu et al., 2020). Some even explored transfer learning of hyperparameters (Yogatama & Mann, 2014;Perrone et al., 2018;Stoll et al., 2020;Horv\u00e1th et al., 2021). (Smith et al., 2017;Hoffer et al., 2017) proposed to scale the learning rate with batch size while fixing the total epochs of training. (Shallue et al., 2018) demonstrated the failure of finding a universal scaling law of learning rate and batch size across a range of datasets and models. Assuming that the optimal learning rate should scale with batch size, (Park et al., 2019) empirically studied how the optimal ratio of learning rate over batch size scales for MLP and CNNs trained with SGD. \n\nMore recently, Yang & Hu (2020) found that standard parameterization (SP) and NTK parameterization (NTP) lead to bad infinite-width limits and hence are suboptimal for wide neural networks. Yang et al. (2022) proposed \u00b5P initialization, which enabled the tuning of hyperparameters indirectly on a smaller model, and zero-shot transfer them to the full-sized model without any further tuning. In (Iyer et al., 2022), an empirical observation was found that, for fully-connected deep ReLU networks, the maximal initial learning rate follows a power law of the product of width and depth. Jelassi et al. (2023) recently studied the depth dependence of \u00b5P learning rates vanilla ReLU-based MLP networks. Two core differences between \u00b5P and our method: 1) \u00b5P mainly gives width-dependent learning rates in the first and last layer, and is width-independent in other layers (see Table 3 in (Yang et al., 2022)).",
            "score": 0.4258922339802021,
            "section_title": "HYPERPARAMETER TRANSFER",
            "char_start_offset": 7327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 162,
                    "matchedPaperCorpusId": "5575601"
                },
                {
                    "start": 186,
                    "end": 210,
                    "matchedPaperCorpusId": "1680430"
                },
                {
                    "start": 229,
                    "end": 254,
                    "matchedPaperCorpusId": "212644626"
                },
                {
                    "start": 254,
                    "end": 273,
                    "matchedPaperCorpusId": "221505538"
                },
                {
                    "start": 371,
                    "end": 392,
                    "matchedPaperCorpusId": "54035096"
                },
                {
                    "start": 411,
                    "end": 432,
                    "matchedPaperCorpusId": "232046268"
                },
                {
                    "start": 454,
                    "end": 474,
                    "matchedPaperCorpusId": "7967806"
                },
                {
                    "start": 794,
                    "end": 813,
                    "matchedPaperCorpusId": "148571743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1259765625
        },
        {
            "corpus_id": "270869927",
            "title": "Collaborative Performance Prediction for Large Language Models",
            "text": "In this section, we detail the setup of each experiment in 5. \n\nDifferent Prediction Methods. Due to the 44% sparsity of the collected collaboration matrix, we used 5% of the known data as the validation set, with the remaining data serving as the observed training set. We trained each model five times through random splitting, deriving an average performance and variance. We configured our models with latent factors = 10, learning rate = 0.01, and iteration = 250, 000. The Figure 4 is the results when random_seed = 1. \n\nPredicting from Small to Large LMs. The focus here is on deriving the scaling law applicable to specific task metrics. Undeniably, traditional methods do not provide a directly usable scaling law across all downstream tasks for comparative analysis. However, we observed in the literature (Ruan et al., 2024) that a sigmoidal curve with a single coefficient and a single bias value represents the scaling law for downstream tasks. Moreover, this curve's coefficients and bias values have a general range across all tasks, w = \n\nConsequently, we set this range of coefficients and bias for this curve. Then we used the normalized scores of smaller models within the same model family and their corresponding parameter sizes to fit the scaling law curve for each task. This approach generally follows a \"pretrain-finetune\" methodology. Additionally, CPP-2 refers to randomly selecting two scores from the observed performances of the model to be included in the training data. In this experiment, we use factorenhanced NCF (setting is same as above).",
            "score": 0.4252983800525785,
            "section_title": "C.2 Detailed Setting of Validation Prediction Accuracy Experiments",
            "char_start_offset": 27316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 64,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20947265625
        },
        {
            "corpus_id": "269043201",
            "title": "Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping",
            "text": "Our choice of number of parameters versus hours of dataset builds on the work of [2] to optimize the tradeoff between inference time, training time, and accuracy. 7his work encouraged us to explore the relationship between parameter count and data size for the task of Speech Recognition, motivating a focus on determining benefits of the data-only scaling strategy.We experimentally show that scaling the dataset in accordance with the scaling laws proposed by [2] (Appendix C) while keeping the model size fixed provides sizeable improvements in terms of accuracy and robustness.",
            "score": 0.424563142514115,
            "section_title": "Related Work",
            "char_start_offset": 30821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 581
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "A selection of in-depth results from the experiments is presented in this section. A basic presentation of the data is shown in Figure 5. Model performance across varying pre-training steps and fine-tuning data sizes demonstrates clear transfer learning across all datasets, as evidenced by the smoothly decreasing loss with increasing pre-training data for all datasets, with the notable exception of the house cat genome dataset. This dataset exhibits a more erratic pattern compared to the others, likely reflecting its significant dissimilarity to the pile. 5) fits the experimental data from the fictional encyclopedia dataset, in a 3D plot. Through close visual inspection, it is clear that the underlying structure of the data is well-modeled by the fitted scaling law. In particular, the power law-like relationship in both pre-training steps and fine-tuning data points is well-captured by the scaling law. Given the simple nature of the scaling law, it is unlikely to be overfitting the data significantly. The goodness of fit of this particular scaling law is further validated by the results from cross-validation, outlined in Appendix D. \n\nFinally, Figure 6 illustrates the trade-offs between expanding pre-training and collecting more fine-tuning data across all of the datasets. Comparing these different plots reveals how optimal strategies for allocating data can change depending on the scaling law for transfer. For example, for the math arXiv dataset, pre-training remains a relatively effective strategy for reducing loss for even high levels of pre-training, as indicated by the nearly-vertical iso-lines. By contrast, for the fictional encyclopedia dataset, pre-training is generally highly beneficial in a regime of low pre-training, but becomes relatively less helpful in a regime of high pre-training, as the isolines become closer to being horizontal. This demonstrates how scaling laws for transfer can be used to inform strategies for allocating data and compute when training models. Figure 6: These iso-loss curves illustrate the various ways of achieving identical cross entropy loss using different combinations of training inputs, for each dataset. It is clear that the math arXiv dataset benefits greatly from pre-training relative to fine-tuning, as the iso-loss curves are very steep vertically.",
            "score": 0.423864489222515,
            "section_title": "B A presentation and analysis of the empirical data",
            "char_start_offset": 34699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "254853733",
            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
            "text": "For example, there are methods associated with blocking and grouping (Park et al., 2022;Wu et al., 2020;Jain et al., 2020;Nagel et al., 2019;Krishnamoorthi, 2018;Rusci et al., 2020), centering (Krishnamoorthi, 2018;Jacob et al., 2017), learned data types that are found through clustering (Gong et al., 2014;Han et al., 2015;Choi et al., 2016;Park et al., 2017), or direct codebook optimization (Rastegari et al., 2016;Hou et al., 2016;Leng et al., 2018;Zhang et al., 2018). While our work studies grouping and blocking, we only study one data type that groups similar weights through their quantiles of the entire input tensor (Dettmers et al., 2022b). While we do not study learned data types in depth, we are the first work that shows that these are critical for improving bit-level scaling for LLMs. \n\nScaling Laws for Inference. Early work in scaling laws highlighted the importance of studying how variables change with scale since scale is one of the best predictors of model performance (Kaplan et al., 2020;Rosenfeld et al., 2019;Hestness et al., 2017). Particularly, for inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations.",
            "score": 0.4238115678777957,
            "section_title": "Related work",
            "char_start_offset": 23558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 122,
                    "matchedPaperCorpusId": "202542905"
                },
                {
                    "start": 122,
                    "end": 141,
                    "matchedPaperCorpusId": "184487878"
                },
                {
                    "start": 162,
                    "end": 181,
                    "matchedPaperCorpusId": "170078885"
                },
                {
                    "start": 343,
                    "end": 361,
                    "matchedPaperCorpusId": "29200607"
                },
                {
                    "start": 395,
                    "end": 419,
                    "matchedPaperCorpusId": "14925907"
                },
                {
                    "start": 436,
                    "end": 454,
                    "matchedPaperCorpusId": "8840788"
                },
                {
                    "start": 454,
                    "end": 473,
                    "matchedPaperCorpusId": "50784025"
                },
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "238408308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "273323177",
            "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
            "text": "Large language models (LLMs) form the basis for numerous real-world applications (Brown et al., 2020;Jiang et al., 2023;Xu et al., 2024;Hadi et al., 2023) and scaling laws analysis serves as the foundation for LLMs development (Kaplan et al., 2020;Bahri et al., 2024). The key idea of scaling laws involves training a sequence of language models (LMs) to gather data (e.g., expended compute and corresponding model performance). This data is then used to build a predictive model that estimates the performance of a substantially larger target LLM (Su et al., 2024;Hoffmann et al., 2022). \n\nPrevious efforts focus on predicting the target LLM's pre-training loss and establish a power-law relation between the computational resource expended (e.g., floating-point operations per second (FLOPs)) and the final loss achieved (Kaplan et al., 2020;Muennighoff et al., 2024;Henighan et al., 2020). Further, we aim to predict the downstream performance in pre-trained LLMs (i.e., zero-or few-shot evaluation) to more accurately reflect the primary concerns regarding their capabilities. The critical challenge is the emergent abilities in LLMs, which states that LLMs only exceed random performance when the FLOPs expended during training surpass task-specific thresholds (Wei et al., 2022a). Supposing a task threshold of F c , typical methods require training N LMs, expending total FLOPs F t = N i=1 FLOPs i > N \u00d7 F c , to obtain N effective data points, thereby necessitating significant computational resources. Fig. 1 demonstrates that the sampling LMs require more than 5 \u00d7 10 20 FLOPs to perform better than random on most benchmarks, with only three data points available to fit the predictive curve across these benchmarks. Hu et al. (2023) address this challenge by significantly increasing the sampling times to compute the PassUntil of a task, basically increasing the \"metric resolution\" to enable the abilities to emerge earlier (i.e., reducing F c ).",
            "score": 0.42343029401862353,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 844,
                    "end": 869,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "Assuming that as the network capacity goes to \u221e, the training error goes to 0, the mean value of the predicted output on a training point must be the true value; thus, the predicted output differs from the true output by O( 1m ). This corresponds to a mean squared error on the training points scaling as O( 1 m 2 ). Finally, to get the dependence on n, we use the observation that the error must depend on only the fraction n m . Thus, the mean squared training error scales as O( n 2 m 2 ). Finally, we know by the argument above that power law error rate scaling of the form n \u03b1 p \u03b1+1 are consistent with the interpolation threshold property, where p is the effective number of parameters. Equating this with the neural network scaling result of O( n 2 m 3 ), we must have that \u03b1 = 2, yielding: \n\nEquating the denominators, we have p 3 = m 2 , or p = m 2/3 . In other words, the effective number of parameters is m 2/3 . Since the absolute number of parameters in the network is m 2 , the effective number of parameters is the cube root of the absolute parameter count.",
            "score": 0.42316793886800813,
            "section_title": "DISCUSSION",
            "char_start_offset": 27937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "252220884",
            "title": "Revisiting Neural Scaling Laws in Language and Vision",
            "text": "Nevertheless, prior to reaching peak performance, training examples behave as if they are fresh samples, which is consistent with the earlier observations reported in [29]. [ 29], where it showed that training examples behave as fresh samples prior to convergence, which would be equivalent to our observation if no overfitting occurs. Throughout the sequel, we refer to the examples seen during training prior to peak performance as \"bootstrapped examples\" and use their number as the independent variable x when evaluating the scaling law estimators in this section. \n\nFigure 5 illustrates how well each of the the four scaling law estimators M 1 , M 2 , M 3 , and M 4 can extrapolate from a given learning curve. The complete set of figures is provided in Appendix A.3. \n\nWe observe that M 4 extrapolates better than other methods and produces learning curves that approximate the empirical results more faithfully. As shown in Figure 1, M 4 outperforms the other methods in more than 70% of the tasks in this domain. \n\nImpact of the Architecture. Figure 6 plots the scaling exponent c in each architecture when the downstream task is n-shot accuracy on ImageNet. We observe that within each family of models, larger models have more favorable scaling exponents. In addition, M 4 yields estimates of the scaling exponents that are larger in absolute magnitude than in other methods. Figure 7 shows that such differences in scaling exponents show up indeed in the slopes of the learning curves as expected.",
            "score": 0.42298880176428544,
            "section_title": "Image Classification",
            "char_start_offset": 17221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "272524356",
            "title": "Breaking Neural Network Scaling Laws with Modularity",
            "text": "Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma & Kaplan, 2022;Clark et al., 2022;Tay et al., 2022). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022;Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively. \n\nSome works approach this problem by modeling NN learning as manifold or kernel regression. For example, McRae et al. (2020) considers regression on manifolds and concludes that sample complexity scales based on the intrinsic manifold dimension of the data. In another case, Canatar et al. (2021) draws correlations between the study of kernel regression to how infinite-width deep networks can generalize based on the size of the training dataset and the suitability of a particular kernel for a task. Along these lines, several works use random matrix theory to derive scaling laws for kernel regression (Hastie et al., 2022;Cui et al., 2021;2022;Wei et al., 2022;Jin et al., 2021). \n\nAmong other observations, this body of work shows that in the absence of strong inductive biases, high-dimensional tasks have sample complexity growing roughly exponentially with the intrinsic dimensionality of the data manifold.",
            "score": 0.4227106823263234,
            "section_title": "NEURAL NETWORK SCALING LAWS",
            "char_start_offset": 6138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 414,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 414,
                    "end": 433,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 433,
                    "end": 450,
                    "matchedPaperCorpusId": "237592821"
                },
                {
                    "start": 654,
                    "end": 676,
                    "matchedPaperCorpusId": "249947012"
                },
                {
                    "start": 1259,
                    "end": 1278,
                    "matchedPaperCorpusId": "219687988"
                },
                {
                    "start": 1429,
                    "end": 1450,
                    "matchedPaperCorpusId": "226227332"
                },
                {
                    "start": 1760,
                    "end": 1781,
                    "matchedPaperCorpusId": "84186305"
                },
                {
                    "start": 1781,
                    "end": 1798,
                    "matchedPaperCorpusId": "235254538"
                },
                {
                    "start": 1803,
                    "end": 1820,
                    "matchedPaperCorpusId": "247410980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73046875
        },
        {
            "corpus_id": "276776035",
            "title": "Measurement noise scaling laws for cellular representation learning",
            "text": "Scaling laws in machine learning describe how model performance improves with increases in key resources like data, compute, and model parameters, often following power-law relationships (Hestness et al., 2017;Rosenfeld et al., 2019;Kaplan et al., 2020;Bahri et al., 2024). These empirical relationships have become crucial for understanding the fundamental limits of neural networks and for making principled decisions about resource allocation in training large models (Hoffmann et al., 2022). Another axis in which 1 Wyss Institute for Biologically Inspired Engineering 2 Department of Systems Biology, Harvard University. Correspondence to: Gokul Gowri <ggowri@g.harvard.edu>, Allon Klein <allon klein@hms.harvard.edu>.",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 723
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 272,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40576171875
        },
        {
            "corpus_id": "277104404",
            "title": "\u03d5-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation",
            "text": "Inference-Time Optimization. To alleviate the post-training workload (Zelikman et al., 2024;Liu et al., 2024;Team, 2024;Guo et al., 2025), inference-time optimization methods arouse wide concerns, showcasing a notable performance boost in reasoning scenarios (Snell et al., 2024;Sun et al., 2023;Zhao et al., 2024). Mainstream methods can be categorized into searching-based (Yao et al., 2024;Hao et al., 2023;Xie et al., 2024;Wu et al., 2024) and sampling-based (Ma et al., 2024;Chen et al., 2023;Zhang et al., 2024). Although these works achieve the globally-optimal inference, they either induce large computation costs or yield inadequate step value estimation. Other classical methods, such as Best-of-N, usually involve delegating the step selection to the external reward model (Wang et al., 2024;Guan et al., 2025), and self-reflection strategies (Cheng et al., 2024;Xu et al., 2024) usually involve extra training. \u03d5-Decoding stands out as an optimal and efficient decoding choice without reliance on external auxiliary. \n\nAdaptive Inference-time Scaling. Though scaling of inference-time computations has proved to be effective (Snell et al., 2024), the issue of over-thinking is widely observed and remains to be addressed (Chen et al., 2024). One line of works (Team et al., 2025;Han et al., 2024) stress on the control of the generation length, while another line of methods (Manvi et al., 2024;Sun et al., 2024) leverage the idea of early-stopping. In contrast, the adaptive scaling technique presented in our work is training-free and independent of external models. Based on the self-evaluation of stepwise value, \u03d5-Decoding introduces the comprehensive pruning strategy from the dimensions of width and depth. It stands out as a light-weight solution to alleviate the inference-time over-thinking.",
            "score": 0.4227106823263234,
            "section_title": "Related Works",
            "char_start_offset": 19863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 410,
                    "end": 427,
                    "matchedPaperCorpusId": "258426922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "246823711",
            "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments",
            "text": "This work is motivated by a practical question: can scaling laws provide a principled method for developing models at very small scale and extrapolating to larger ones? We perform a thorough empirical analysis on the emergence of scaling laws on a wide range of language understanding tasks. We find that scaling laws emerge for some tasks, potentially as a function of the proximity between the downstream task and the pretraining objective, but revealing them incurs the overhead of hyperparameter tuning across multiple scales. Our results show that scaling laws are beneficial for debugging model convergence when training large models, and to predict model behavior when they emerge at small scale.",
            "score": 0.4227106823263234,
            "section_title": "Conclusion",
            "char_start_offset": 26639,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "272310219",
            "title": "An Empirical Study of Scaling Laws for Transfer",
            "text": "We present a limited empirical study of scaling laws for transfer learning in transformer models. More specifically, we examine a scaling law that incorporates a\"transfer gap\"term, indicating the effectiveness of pre-training on one distribution when optimizing for downstream performance on another distribution. When the transfer gap is low, pre-training is a cost-effective strategy for improving downstream performance. Conversely, when the gap is high, collecting high-quality fine-tuning data becomes relatively more cost effective. Fitting the scaling law to experiments from diverse datasets reveals significant variations in the transfer gap across distributions. In theory, the scaling law can inform optimal data allocation strategies and highlights how the scarcity of downstream data can bottleneck performance. Our findings contribute to a principled way to measure transfer learning efficiency and understand how data availability affects capabilities.",
            "score": 0.4223539291797309,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50244140625
        },
        {
            "corpus_id": "277667587",
            "title": "Scaling Laws for Native Multimodal Models",
            "text": "We investigate how the scaling laws change when modifying the training mixtures. Specifically, we vary the ratio of image caption, interleaved, and text-only data and report the results in Figure 26. Overall, we observe similar scaling trends, with only minor changes in the scaling coefficients. \n\nUpon closer analysis, we find that increasing the ratio of a particular data type in the training mixture, leads to a corresponding increase in its scaling exponent. For instance, increasing the ratio of image captions from 30% to 40% raises the absolute value of the exponent from 0.056 to 0.061. However, for text-only data, we do not observe significant changes in the scaling coefficients when varying its proportion in the training mixture. \n\nC.6 Scaling laws evaluation and sensitivity  For each model size and number of training tokens, we compute the loss based on the estimated functional form in Equation ( 1) and compare it with the actual loss achieved by our runs. We visualize these points in Figure 22, demonstrating that our estimation is highly accurate, particularly for lower loss values, and hence for larger FLOPs. Additionally, we perform a sensitivity analysis using bootstrapping. Specifically, we sample with replacement P points (P being equal to the total number of trained models) and re-estimate the scaling law coefficients. This process is repeated 100 times, and we report the average and standard deviation of each coefficient. Table 12 shows that our estimation is more precise for \u03b2 compared to \u03b1, primarily due to the smaller number of model sizes relative to the number of different token counts used to derive the scaling laws. C.7 Scaling laws for sparse NMMs. \n\nSimilar to dense models, we fit a parametric loss function (Equation ( 1)) to predict the loss of sparse NMMs based on the number of parameters and training tokens, replacing the total parameter count with the number of active parameters. While incorporating sparsity is standard when deriving scaling laws for MoEs [Wang et al., 2024b;Krajewski et al., 2024;Abnar et al., 2025], we focus on deriving scaling laws specific to the sparsity level used in our MoE setup. This yields coefficients that are implicitly conditioned on the sparsity configuration.",
            "score": 0.42230531076291067,
            "section_title": "C.5 Scaling laws for different training mixtures",
            "char_start_offset": 42286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 2017,
                    "end": 2037,
                    "matchedPaperCorpusId": "273228196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "270045836",
            "title": "SMART: Scalable Multi-agent Real-time Simulation via Next-token Prediction",
            "text": "Prior research [22,42] have established that scaling up large language models (LLMs) leads to a predictable decrease in test loss L. This trend correlates with parameter counts N , training tokens T , following a power-law: \n\nwhere X can be any of N , T . The exponent \u03b1 reflects the smoothness of power-law, and L denotes the reducible loss normalized by irreducible loss. The data sources for validating scaling laws are detailed in the A.3. Overall, we trained models across four sizes, ranging from 1M to 100M parameters, on a training set containing 2.2M scenarios (or 1B motion tokens under 0.5s agent motion tokenization). \n\nScaling laws with model parameters We investigate the test loss trend as the model size increases. \n\nWe assessed the final test cross-entropy loss L on the validation set of 100,000 traffic scenarios. The results are plotted in Figure 4, where we observed a clear power-law scaling trend for Loss L as a function of model size N . The power-law scaling laws can be expressed as: \n\nThese results verify the strong scalability of SMART, providing valuable insights into how model performance scales with dataset size.",
            "score": 0.42215969382072055,
            "section_title": "Scalability",
            "char_start_offset": 23088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47998046875
        },
        {
            "corpus_id": "277467322",
            "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
            "text": "Large language models (LLMs) (Brown et al., 2020;OpenAI, 2024a) have emerged in recent years as a transformative milestone toward artificial general intelligence (AGI) (Goertzel, 2014;Bubeck et al., 2023). These models remarkably learn general intelligence by training-time scaling, where the models ingest more data and parameters (Kaplan et al., 2020;Hoffmann et al., 2022). However, the progress of pretraining scaling has gradually slowed due to its resource-intensive nature and the bounded availability of human data, prompting researchers to shift their focus toward how to fully elicit the intelligence encoded in LLMs at test time to maximize their real-world effectiveness (Wei et al., 2022;Ouyang et al., 2022;Li et al., 2024d)? \n\nHuman cognition may suggest a clue. When faced with complex problems, people tend to engage in deeper, more deliberate thinking, often producing better outcomes (Kahneman, 2011(Kahneman, , 2003;;Evans, 1984). Inspired by this principle, recent research (Wei et al., 2022;Wang et al., 2023b) has introduced methods that allocate additional computation during inference to boost task performance. Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements. This family of methods, referred to as test-time scaling (TTS), progressively elicits the model's intelligence in the test-time, as depicted in Figure 1. The remarkable successes of reasoning models, such as o1 (OpenAI, 2024b) and R1 (DeepSeek-AI, 2025), have further amplified interest in TTS, highlighting its potential as a key driver of LLM reasoning and utility. However, despite this surge in research activity, the field currently lacks a unified and systematic framework to synthesize insights, compare techniques, or identify consistent trends in TTS . To address this gap, we present a comprehensive survey of TTS, offering a hierarchical and extensible framework to analyze methods, map research efforts, and guide future progress.",
            "score": 0.4213426668299762,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 168,
                    "end": 184,
                    "matchedPaperCorpusId": "4794432"
                },
                {
                    "start": 683,
                    "end": 701,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 701,
                    "end": 721,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 721,
                    "end": 738,
                    "matchedPaperCorpusId": "267760184"
                },
                {
                    "start": 918,
                    "end": 937,
                    "matchedPaperCorpusId": "15131441"
                },
                {
                    "start": 937,
                    "end": 949,
                    "matchedPaperCorpusId": "145670284"
                },
                {
                    "start": 995,
                    "end": 1013,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1013,
                    "end": 1032,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "275907043",
            "title": "Top Ten Challenges Towards Agentic Neural Graph Databases",
            "text": "In deep learning, neural scaling law is an empirical law that describes the performance of neural models improves with the number of parameters, training dataset size, and training cost [52,53]. During the development of the NGDB model, scaling is also a major thread, primarily encompassing the scaling of parameter number, query data size, and training costs. The query embedding methods and sequence models often scale the training costs in the training stage, including the model parameters and queries. In contrast, the neuro-symbolic methods often scale the computation cost over the test stage to improve the performance. We mainly discuss how to scale these models further, particularly when the query structure becomes increasingly complex [25,54] and the magnitude of the knowledge databases becomes very large [55]. Specifically, we introduce the complexity of these models in the training and inference stages and discuss their efficiency and scalability challenges. \n\nData Scaling in the Training Stage. Both query embedding and sequence models are trained from scratch, requiring many sampled queries as training data. The quality and size of these training queries are crucial, and they typically encompass various query types. The NGDB models generally use the same dataset, with the basic 1p query type enumerating the entire knowledge graph [20]. To incorporate new features such as negation [22], cyclic queries [25], and multivariable queries [54], it is essential to sample query types that include these features. Materializing training queries becomes infeasible as the knowledge graph grows, and sampling logical queries is incompatible with traditional single-hop frameworks based on graph partitioning. To address this challenge, SMORE [55] proposes a scalable framework that efficiently samples training data on the fly with high throughput. In contrast, neuro-symbolic methods primarily rely on pre-training for the knowledge graph completion task and depend on search algorithms to address general logical queries. \n\nTest Time Scaling in Inference Stage. We first introduce the notion of query complexity and data complexity [56]. Data complexity captures the relation between the time complexity and the database size |E| (number of the edges) when the query is fixed.",
            "score": 0.4208515041469856,
            "section_title": "Challenge 5: Scaling for Higher Complexity",
            "char_start_offset": 27315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "240070688"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "211126567"
                },
                {
                    "start": 1410,
                    "end": 1414,
                    "matchedPaperCorpusId": "225040315"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "240070688"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "As noted in Kaplan et al. (2020), the model design does not significantly impact scaling trends unless the architecture is extremely shallow or slim. We employ a vanilla transformer architecture (Vaswani, 2017) with minimal modifications. The text, image, and timestep tokens are concatenated as input (in-context conditioning (Peebles & Xie, 2023)). A dataset of 108 million image-text pairs is randomly sampled from Laion-Aesthetic (Schuhmann et al., 2022). This subset is then re-captioned using LLAVA 1.5 (Liu et al., 2024). A validation set consisting of 1 million pairs is created from this subset. Further details can be found in Appendix C.1 and C.2. \n\nScaling Metrics A natural question arises when investigating scaling laws during training: What metrics should be selected to observe scaling behavior? In the context of Large Language Models (LLMs), the standard approach is autoregressive training (Radford, 2018;Radford et al., 2019), where the model is trained to predict the next token in a sequence, directly optimizing the likelihood of the training data. This has proven to be a reliable method for measuring model performance as the compute budget scales up. Inspired by this approach, we extend the concept of scaling laws to diffusion models, using loss and likelihood as our key metrics. \n\nLoss is the primary metric chosen to observe scaling behavior during training. Unlike autoregressive models, diffusion models do not directly optimize likelihood. Instead, the objective is to match a time-conditioned velocity field (Ma et al., 2024;Liu et al., 2022;Lipman et al., 2022;Albergo et al., 2023). Specifically, the velocity v(x t , t) at timestep t is defined as: \n\nwhere x 0 represents the original data and \u03f5 denotes the noise. Here, the prime symbol \u2032 indicates the derivative with respect to time t. In the rectified flow framework, the coefficients \u03b1 t and \u03b2 t are defined as \u03b1 t = 1 \u2212 t, \u03b2 t = t. Thus, the velocity v can be further simplified as:",
            "score": 0.42080258422385886,
            "section_title": "Models & Dataset",
            "char_start_offset": 8430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 210,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 327,
                    "end": 348,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 434,
                    "end": 458,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 509,
                    "end": 527,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 925,
                    "end": 946,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12359619140625
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "-The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see \u00a72.1). \n\n-While the trend of increasing active number of parameters is similar across all training compute budgets; the optimal active number of parameters decrease more rapidly with sparsity as the training compute budget increases (see \u00a73). \u2022 Effect of Sparsity on Downstream Performance: For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2019), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022b;Goyal et al., 2024) may address this gap. We use a \"length-controlled\" variant of few-shot Chain-of-Thought (CoT) prompting to study whether this strategy may help close the gap between MoEs and dense models on reasoning tasks in Appendix E. Empirical evidence suggests that MoEs may benefit more from dynamically increased inference-time compute than dense models under a fixed generated tokens budget. \u2022 Parametric Scaling Law: We propose a parametric form for scaling laws that accounts for sparsity. The model coefficients are estimated using the empirical data obtained by training compute-optimal models. An interesting observation from Appendix F is that the exponent for sparsity term \u03bb is negative which is consistent with our intuition that sparser models lead to a lower perplexity.",
            "score": 0.4205716913899071,
            "section_title": "Discussion",
            "char_start_offset": 22382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 698,
                    "matchedPaperCorpusId": "52055325"
                },
                {
                    "start": 706,
                    "end": 730,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 916,
                    "end": 935,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "263608983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "246608156",
            "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture",
            "text": "In contrast, decoder-heavy models have shown great promise in applications such as conversational AI (Xu et al., 2020). Prior work (Ghorbani et al., 2021) has demonstrated that, in the context of parameter scaling, encoder-heavy models exhibit significantly different behaviors compared to decoder-heavy models. \n\nTo examine how model shape interacts with sample efficiency, we examine the data scaling characteristics of 5 different models with various degrees of encoder-decoder asymmetry: {2L6L, 6L2L, 6L6L, 28L6L, 6L28L}. Our results indicate that the same scaling parameters \u03b1, p are sufficient to capture the data scaling behavior for all of these models; the only parameter that has to change from model to model is C which is due to the difference in the model capacities. See Figure 1A for the scaling law fit and Appendix A.1 for individual fits of Eq. ( 1) for each model. As such, our results indicate that data efficiency characteristics of encoder-decoder NMT models is largely independent of the model shape. \n\nJoint Data & Parameter Scaling Law: In Eq. ( 1), as D \u2192 \u221e, L(D) \u2192 \u03b1C p which corresponds to the achievable test loss when the learning problem is limited only by the model capacity. Earlier work has studied the behavior of \u03b1C p in the context of parameter scaling laws (Ghorbani et al., 2021;Gordon et al., 2021). Here, we leverage these results to derive a joint data & parameter scaling law. To be specific, instead of empirically fitting the constant C, we replace it with the quantity implied by the model scaling law computed by Ghorbani et al.: \n\nHere, N e and N d correspond to the number of parameters in the encoder & decoder respectively. The only fitted parameters in this equation are \u03b1 and p; the parameters (\u03b2, p e , p d , L \u221e ) appearing in (2) are directly borrowed from Ghorbani et al. (2021).",
            "score": 0.41981223684310287,
            "section_title": "Data Scaling Laws",
            "char_start_offset": 11207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 154,
                    "matchedPaperCorpusId": "237532682"
                },
                {
                    "start": 1295,
                    "end": 1318,
                    "matchedPaperCorpusId": "237532682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3291015625
        },
        {
            "corpus_id": "270391350",
            "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
            "text": "We analyze neural scaling laws in infinite-dimensional linear regression. We consider a linear predictor with M trainable parameters on the sketched covariates, which is trained by one-pass stochastic gradient descent with N data. Under a Gaussian prior assumption on the optimal model parameter and a power law (of degree a > 1) assumption on the spectrum of the data covariance, we derive matching upper and lower bounds on the population risk minus the irreducible error, that is, \u0398(M \u2212(a\u22121) + N \u2212(a\u22121)/a ). In particular, we show that the variance error, which increases with M , is of strictly higher order compared to the other errors, thus disappearing from the risk bound. We attribute the nice empirical formula of the neural scaling law to the non-domination of the variance error, which ultimately is an effect of the implicit regularization of SGD. \n\nMany directions remain open for future study. First, our work is limited to the linear model; it would be interesting to see whether similar scaling laws can be derived in more complex models, such as random feature models or two-layer networks. Second, we focus on one-pass SGD training, and it is unclear if similar results hold for other optimization methods like accelerated SGD or Adam. Additionally, from a technical perspective, many results in our work depend on the Gaussian assumption and the source condition of the data. Investigating how these assumptions can be relaxed would also be valuable.",
            "score": 0.41975517348375324,
            "section_title": "Conclusion",
            "char_start_offset": 28566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "278367792",
            "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
            "text": "Here A, B, \u03b1, \u03b1 N , \u03b2 are universal parameters to be fitted. This groundbreaking work revealed that model performance consistently improves as a power law with increases in three critical factors: model size, training data, and computational power. These findings laid a strong foundation for understanding model behavior during pre-training. \n\nBuilding on this, Lin et al. (2024) demonstrated that finetuning performance depends not only on model size N but also on various architectural design choices, including the number of layers, attention heads, and hidden dimensions. This intricate dependency complicates model selection using traditional scaling laws. However, to predict performance for specific models, a simplified version of the scaling law can be employed by excluding architectural considerations. Scaling laws for fixed models, as proposed in (Kaplan et al., 2020;Hernandez et al., 2021;Tay et al., 2022), exhibit the following form: \n\nwhere D represents the training set size, while B, E, \u03b1, and \u03b2 are parameters specific to the model and task. \n\nAs illustrated in Figure 2, two distinct phases emerge in the scaling behavior of fine-tuning test loss (L) with training sample size (D): the pre-power phase and the power phase. \n\nThe pre-power phase occurs in low-data regimes where model behavior is dominated by initialization and early training dynamics. As the training size increases and reaches a transition point, models enter the power phase, characterized by predictable scaling behavior. In this phase, the relationship between training size and test loss follows a nearly linear correlation, as widely studied in prior works (Henighan et al., 2020;Kaplan et al., 2020;Bahri et al., 2024). Understanding the underlying mechanism of this phase transition phenomenon is crucial for effective model selection. \n\nFirst, it enables precise predictions about when additional training data will lead to the power phase, resulting in consistent and predictable performance improvements. Second, once a model transitions into the power phase, scaling laws can guide data-driven decisions about further data investment, effectively balancing the costs of data collection against expected performance gains.",
            "score": 0.4197385739661104,
            "section_title": "L(N, D)",
            "char_start_offset": 5252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1697,
                    "end": 1716,
                    "matchedPaperCorpusId": "231918701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "S 1 is the forward area, i.e., sum of red grid areas, which can be approximately regarded as the total amount of movement for neural network parameters; S 2 is the decayed annealing area, i.e., weighted sum of blue grid areas, where lighter shades indicate smaller weights. Both S 1 and S 2 contribute to loss reduction, and balancing their values is crucial for achieving the lowest possible final loss. \n\nIn this study, we propose a scaling law that models the full loss curve within a complete LLM training run. Specifically, we dive deeper into the training dynamics during LR annealing, and incorporate a LR annealing factor into the traditional scaling law formula to formulate the process. This design is motivated by the observed correlation between LRS and loss curves, where loss gradually decreases as we consume more training steps2 and then sharply declines when the LR undergoes significant annealing (Loshchilov & Hutter, 2016;Smith et al., 2018;Ibrahim et al., 2024;Hu et al., 2024). Fig. 4 depicts how loss curves change over different learning rate schedules. Overall, we discover that the validation loss of a language model at any step is determined by two factors: the forward area S 1 under the LR curve and the degree of LR annealing S 2 at that step. Formally, the expectation of loss L at step s of a language model follows:: \n\nwhere \u03b7 i is the learning rate at step i, and \u03bb is a hyper-parameter representing the decay factor for LR annealing momentum (see Sec. 3 in detail),which typically ranges from 0.99 to 0.999. L 0 , A, C, \u03b1 are undetermined positive constants. S 1 is also known as the summed learning rate (Kaplan et al., 2020), and S 2 represents the LR annealing area. A visualization of S 1 and S 2 is provided as Fig. 1. \n\nEq. 1 describes how loss changes for each step in a full loss curve during training. In Eq. 1, the term L 0 + A \u2022 S \u2212\u03b1 1 represents a rectified scaling law that captures the expected loss decreases as a power-law function of the number of training steps.",
            "score": 0.4197140953872268,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 942,
                    "end": 961,
                    "matchedPaperCorpusId": "3516266"
                },
                {
                    "start": 961,
                    "end": 982,
                    "matchedPaperCorpusId": "268379604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134521484375
        },
        {
            "corpus_id": "253116832",
            "title": "Scaling Laws Beyond Backpropagation",
            "text": "Alternatives to backpropagation have long been studied to better understand how biological brains may learn. Recently, they have also garnered interest as a way to train neural networks more efficiently. By relaxing constraints inherent to backpropagation (e.g., symmetric feedforward and feedback weights, sequential updates), these methods enable promising prospects, such as local learning. However, the tradeoffs between different methods in terms of final task performance, convergence speed, and ultimately compute and data requirements are rarely outlined. In this work, we use scaling laws to study the ability of Direct Feedback Alignment~(DFA) to train causal decoder-only Transformers efficiently. Scaling laws provide an overview of the tradeoffs implied by a modeling decision, up to extrapolating how it might transfer to increasingly large models. We find that DFA fails to offer more efficient scaling than backpropagation: there is never a regime for which the degradation in loss incurred by using DFA is worth the potential reduction in compute budget. Our finding comes at variance with previous beliefs in the alternative training methods community, and highlights the need for holistic empirical approaches to better understand modeling decisions.",
            "score": 0.41947854158341624,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.472900390625
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "Concurrent with our work, Song et al. [51] finds that using the best available sample improves LLM performance on chat, math, and code tasks, sweeping up to a max of 128 samples. Additionally, Hassid et al. [24] find that when solving coding tasks, it can be more effective to draw more samples from a smaller model than draw fewer samples from a larger one. \n\nScaling Laws: Characterizing how scaling affects model performance can lead to more informed decisions on how to allocate resources. Scaling laws for LLM training find a power law relationship between loss and the amount of training compute and provide estimates for the optimal model and dataset size given a fixed compute budget [27,36,28]. Jones [33] finds scaling laws in the context of the board game Hex, observing that performance scales predictably with model size and the difficulty of the problem. Interestingly, they also show that performance scales with the amount of test-time compute spent while performing tree search. Recently, Shao et al. [49] observe scaling laws when augmenting LLMs with external retrieval datasets, finding that performance on retrieval tasks scales smoothly with the size of the retrieval corpus.",
            "score": 0.4193009726898843,
            "section_title": "Related Work",
            "char_start_offset": 32886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "259204097",
            "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication",
            "text": "Previously, conforming to the empirical scaling laws (Kaplan et al., 2020), where the final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time; several works (Li et al., 2021;Jaiswal et al., 2022;Zhou et al., 2021b) have attempted to scale GNNs (up to 1000 layers) assuming that processing larger graphs would likely benefit from more parameters. Unlike conventional deep neural networks, exploiting scale to revamp information absorption is not straight-forward for GNNs, and numerous existing works rely on architectural changes, regularization & normalization, better initialization (Li et al., 2019;Chen et al., 2020;Li et al., 2018;Liu et al., 2020;Rong et al., 2020;Huang et al., 2020;Zhao & Akoglu, 2020;Zhou et al., 2021a;Jaiswal et al., 2022) for improving the trainability and try to overcome astonishingly high memory footprints by mini-batch training, i.e. sampling a smaller set of nodes or partitioning large graphs (Hamilton et al., 2017;Chen et al., 2018;Zou et al., 2019;Chiang et al., 2019;Zeng et al., 2019). While these methods are a step in the right direction, they do not scale well as the models become deeper or wider, since memory consumption is still dependent on the number of layers. We are interested in exploring an orthogonal step: Does there exist a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across small and large graphs?",
            "score": 0.4191940908900554,
            "section_title": "Introduction",
            "char_start_offset": 1219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1481
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "235421912"
                },
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "235742666"
                },
                {
                    "start": 686,
                    "end": 702,
                    "matchedPaperCorpusId": "11118105"
                },
                {
                    "start": 702,
                    "end": 719,
                    "matchedPaperCorpusId": "220647438"
                },
                {
                    "start": 719,
                    "end": 737,
                    "matchedPaperCorpusId": "212859361"
                },
                {
                    "start": 776,
                    "end": 795,
                    "matchedPaperCorpusId": "237497467"
                },
                {
                    "start": 995,
                    "end": 1018,
                    "matchedPaperCorpusId": "4755450"
                },
                {
                    "start": 1036,
                    "end": 1053,
                    "matchedPaperCorpusId": "202779897"
                },
                {
                    "start": 1053,
                    "end": 1073,
                    "matchedPaperCorpusId": "159042192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2093505859375
        },
        {
            "corpus_id": "246430659",
            "title": "Error scaling laws for kernel classification under source and capacity conditions",
            "text": "a. Neural scaling laws -A number of works [1][2][3][4] have provided empirical evidence of scaling laws in neural networks, with the number of parameters, training samples, compute, or other observables. These findings motivated theoretical investigations of the underlying mechanisms. Authors of [5] show how the scaling of the test loss with the number of parameters is related to the intrinsic dimension of the data. This dimension is further tied in with the kernel spectrum by [6], a work that leverages the kernel ridge regression viewpoint to translate, in turn, the decay of the spectrum to test error rates. Authors of [16] similarly study a simple toy model where the power-law data is processed through a random features layer. Finally, [17] investigate a toy model of scalar integer data in the context of classification, and ascertain the corresponding scaling law. Relating in classification settings the rate of decay of the kernel spectrum to the test error, like [6] for regression, is still an open question. \n\nb. Source and capacity conditions -The source and capacity conditions are standard regularity assumptions in the theoretical study of kernel methods, as they allow to subsume a large class of learning setups, c.f. [7,8,12,13,15,18]. c. Kernel ridge regression -The error rates for kernel ridge regression have been extensively and rigorously characterized in terms of the source/capacity coefficients in the seminal work of [7,8], with a sizeable body of work being subsequently devoted thereto [9][10][11][12][13][14]19]. In particular, in [15] it was shown that rates derived under worst-case assumptions [7][8][9][10]20] are identical to the typical rates computed under the standard Gaussian design [21][22][23] assumption. Crucially, it was observed that many real data-sets satisfy the source/capacity conditions, and arXiv:2201.12655v3 [stat.ML] 6 Sep 2023 display learning rates in very good agreement to the theoretical values [15]. d.",
            "score": 0.4186863712309719,
            "section_title": "Related works",
            "char_start_offset": 1835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "207063850"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "44115585"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "59842919"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "207063850"
                },
                {
                    "start": 1524,
                    "end": 1527,
                    "matchedPaperCorpusId": "53222217"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "44115585"
                },
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "231839648"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "7741716"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "207063850"
                },
                {
                    "start": 1642,
                    "end": 1645,
                    "matchedPaperCorpusId": "53222217"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "matchedPaperCorpusId": "195700154"
                },
                {
                    "start": 1732,
                    "end": 1736,
                    "matchedPaperCorpusId": "88513634"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "54660782"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "266602596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27490234375
        },
        {
            "corpus_id": "276116987",
            "title": "Scaling laws in wearable human activity recognition",
            "text": "In Figure 2 we establish scaling laws of the pretraining test loss vs hours of data. To calculate the loss, we use the full test set from each Extrasensory fold. This allows us to compare different training data amounts and distributions within a fold and fit power-law relationships for each fold. We observe roughly the same power-law exponent (or slope on the log-log plot) for a given fold and sampling strategy, giving confidence that this relationship was not due to random chance. Furthermore, in Table 2 we see that the exponent is roughly of 3x greater magnitude (or steeper slope) when data is increased by adding more users, as opposed to uniformly or per-user. This emphasizes that diversity of data is extremely important, and dictates the scaling law. Note that the offset is different for each fold, but that is to be expected, since the test sets are different. Similarly, in Figure 3 we fit a power law between pre-training test loss and model capacity in terms of number of parameters, further demonstrating the existence of a scaling law.",
            "score": 0.4185848365485682,
            "section_title": "Scaling laws",
            "char_start_offset": 18790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296630859375
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28]. In particular, Paquette et al. [38] obtain a power law with exponent a = 0.5 (as in Hoffmann et al. [25]) for a random-feature linear regression setting [35,5], conjecturing that this is a part of a broader, universal phenomenon. Jeon and Van Roy [28] also establish an exponent of 0.5 for data generated by infinite-width two-layer ReLU networks, using information-theoretic arguments. \n\nWe also remark on two themes of our paper that draw from prior work. The first is the importance of hyperparameter tuning: several works [30,27,17,15] make the case that smooth, predictable scaling laws emerge when models on all scales are properly tuned. Our work (and particularly Section 4.1) provides another example of this principle and agrees with previous observations that tuning is particularly important at smaller scales. Second, previous studies [59,8,15,26] as well as the concurrent work [22], propose alternative learning rate schedules that address a key shortcoming of cosine decay: the need to commit to a step budget in advance. We consider a constant learning rate that requires no commitment at all. We show this simple choice suffices to reproduce the Hoffmann et al. law and quantify the computational savings compared to a cosine schedule. However, Section 4.1 (and also [25], among others) show that in terms of loss, the constant schedule clearly underperforms the cosine schedule. \n\nConcurrent and independent work. Pearce and Song [40] [30] (see discussion in Section 3.1, Section 3.5, and Appendix H). In addition, our experiments roughly match the Kaplan et al. [30] compute budget, which is about 3 orders of magnitudes larger than budget in Pearce and Song [40], and we perform careful tuning of both the learning rate and the batch size.",
            "score": 0.4185848365485682,
            "section_title": "Related work",
            "char_start_offset": 25787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "259243822"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "267406160"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 373,
                    "end": 375,
                    "matchedPaperCorpusId": "255096726"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "246823711"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "272826711",
            "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
            "text": "This demonstrates that naively extrapolating scaling laws for model comparison can be unreliable and lacks a solid theoretical foundation. In contrast, the GPT-4 technical report (OpenAI et al., 2023) showcases the potential for accurate 1,000x and even 10,000x extrapolations, although the specific techniques and conditions enabling such accurate extrapolations remain unclear. \n\nOverall, to reliably apply scaling law extrapolation for model comparison at scale, we believe extensive research is necessary to fully comprehend both the macro and micro dynamics at play. This may include a deeper understanding of the intricate relationship between optimization, architectures, data, and scales (Kaplan et al., 2020;Bahri et al., 2024;Hoffmann et al., 2022;DeepSeek-AI et al., 2024;Paquette et al., 2024;Bordelon et al., 2024;Lin et al., 2024), as well as the subtleties within our machine learning systems. These subtleties encompass factors such as learning rate schedules (Hoffmann et al., 2022), parameter counting choices, the number of warmup steps (Porian et al., 2024), curve fitting approaches (Besiroglu et al., 2024), and even the epsilon value in AdamW (Wortsman et al., 2023;Everett et al., 2024).",
            "score": 0.4185848365485682,
            "section_title": "Scaling Law Extrapolation for Model Comparision",
            "char_start_offset": 34670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 1189,
                    "end": 1210,
                    "matchedPaperCorpusId": "271051056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "264814430",
            "title": "Large Trajectory Models are Scalable Motion Predictors and Planners",
            "text": "This research laid the groundwork for understanding how scaling influences the efficacy of language models. Henighan et al. (Henighan et al., 2020) further studied scaling laws in autoregressive generative modeling of images, videos, vision-language, and mathematical problems. These works suggest that scaling laws have important implications for neural network performance.",
            "score": 0.4185848365485682,
            "section_title": "RELATED WORKS",
            "char_start_offset": 9549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362060546875
        },
        {
            "corpus_id": "269761957",
            "title": "Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory",
            "text": "Scaling laws. Empirical evidence suggests that the performance of models increases as both the size of the models and the volume of training data scale up (Kaplan et al., 2020;Khandelwal et al., 2019;Rae et al., 2021;Chowdhery et al., 2023). Intensive experiments on transformer-based large language models have also been conducted to explore neural scaling laws under various conditions, including constraints on computational budget (Hoffmann et al., 2022b), data (Muennighoff et al., 2024), and instances of over-training (Gadre et al., 2024). In these analyses, a decomposition of the expected risk is utilized, leading to the following fit: \n\nwhere N and D denote the number of parameters of the model and the size of the training data respectively. For Chinchilla models, the fitted parameters are (Hoffmann et al., 2022a) \n\nA line of research concerns the generalization of over-parameterized neural networks (Belkin et al., 2019;Nakkiran et al., 2021;Power et al., 2022). Recent experiments show that over-trained Transformers exhibit inverted U-shaped scaling behavior (Murty et al., 2023), which is not explained by the empirical scaling laws. Further discussions on the relationship between our method and the Chinchilla scaling laws are deferred to Section 6. \n\nEnergy-based models. Energy-based models (LeCun et al., 2006), motivated by statistical physics, have become a fundamental modeling tool in various fields of machine learning over the past few decades. The central idea is to model the neural network through a parameterized probability density function p \u03b8 (x) for x \u2208 R n and to express the distribution in terms of a learnable energy function E \u03b8 (x) : R n \u2192 R whose parameters correspond to the model's parameters as \n\n. Here, Z \u03b8 = exp(\u2212E \u03b8 (x)) dx is the normalizing constant known as the partition function. \n\nHopfield models. Classical Hopfield networks (Amari, 1972;Hopfield, 1982) were introduced as paradigmatic examples of associative memory. The network's update dynamics define an energy function, whose fixed points correspond to the stored memories.",
            "score": 0.4185848365485682,
            "section_title": "Related work",
            "char_start_offset": 6350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 240,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 435,
                    "end": 459,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 466,
                    "end": 492,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 937,
                    "end": 959,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 1315,
                    "end": 1335,
                    "matchedPaperCorpusId": "8531544"
                },
                {
                    "start": 1885,
                    "end": 1898,
                    "matchedPaperCorpusId": "3708480"
                },
                {
                    "start": 1898,
                    "end": 1913,
                    "matchedPaperCorpusId": "784288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30859375
        },
        {
            "corpus_id": "272987110",
            "title": "Scaling Optimal LR Across Token Horizons",
            "text": "The study of scaling laws for LLMs originated in Kaplan et al. (2020) and it is still an active area of research. Scaling laws are researched in the context post-training (Lin et al., 2024;Zhang et al., 2024a;Gao et al., 2023), model architecture (Krajewski et al., 2024;Alabdulmohsin et al., 2024;Frantar et al., 2023;Wang et al., 2023), multi-modality (Aghajanyan et al., 2023;Cherti et al., 2023), inference (Sardana & Frankle, 2023), data (Dohmatob et al., 2024;Fernandes et al., 2023) and other domains (Zhang et al., 2024b;Neumann & Gros, Kadra et al., 2024). There are also more theoretical studies (Michaud et al., 2024;Caballero et al., 2022). \n\nLR has long been known as an important hyperparameter, and hence there is ample work on how to select it (Goyal, 2017). Larger models are known to require smaller LR, and e.g. Kaplan et al. (2020) suggests the formula LR(N ) \u2248 0.003239 \u2212 0.0001395 log(N ) for tuning LR as a function of model size. MuP (Yang et al., 2022) is a principled methodology for selecting LR when the model is scaled, and the method is actively researched (Lingle, 2024;Everett et al., 2024;Noci et al., 2024;Blake et al., 2024). Recently Everett et al. (2024) showed that LR transfer across model size is possible both with different parametrizations and optimizers. However, a limitation in both Yang et al. (2022) and Everett et al. (2024) is the fixed training horizon assumption used in the theoretical derivations. Our work directly explores this limitation.",
            "score": 0.41787902689176926,
            "section_title": "RELATED WORK",
            "char_start_offset": 18051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 226,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 271,
                    "end": 298,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 319,
                    "end": 337,
                    "matchedPaperCorpusId": "260816728"
                },
                {
                    "start": 354,
                    "end": 379,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 466,
                    "end": 489,
                    "matchedPaperCorpusId": "257038048"
                },
                {
                    "start": 606,
                    "end": 628,
                    "matchedPaperCorpusId": "257687665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.261474609375
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy with theoretically infinite resolution, through massive sampling in the decoding phase. With PassUntil, we conduct a quantitative investigation into the scaling law of task performance. The investigation contains two parts. Firstly, a strict task scaling law that is not conventionally known to exist, is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\\% deviation before training starts, which is the first systematic attempt to verify predictable scaling proposed by GPT-4's report. Secondly, we are able to study emergent abilities quantitatively. We identify a kind of accelerated emergence whose scaling curve cannot be fitted by standard scaling law function and has a increasing speed. We then examine two hypothesis and imply that the ``multiple circuits hypothesis'' might be responsible for the accelerated emergence.",
            "score": 0.4177351651081944,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43603515625
        },
        {
            "corpus_id": "275133765",
            "title": "Slow Perception: Let's Perceive Geometric Figures Step-by-step",
            "text": "As the length of the perceptual ruler decreases, we can observe a steady improvement in almost all metrics. The shorter the perceptual ruler, the more \"strokes\" are needed to model a line, resulting in the model outputting more intermediate \"gaze\" points. This leads to increased computational complexity during inference, and correspondingly longer inference times, exhibiting to some extent an inference time scaling law.",
            "score": 0.41723056467290004,
            "section_title": "Experimental Settings",
            "char_start_offset": 20307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "Recent advancements in language model (LM) development have been significantly influenced by the exploration of scaling laws, which articulate the relationship between training loss, dataset size, and the number of model parameters (Hestness et al., 2017;Kaplan et al., 2020;Aghajanyan et al., 2023). These scaling laws have been instrumental in predicting the computational resources necessary for training increasingly large models and have provided a framework for understanding how model performance scales with data and parameters (Hoffmann et al., 2022;Kaplan et al., 2020). However, these laws primarily focus on the quantity of data and model size, often underestimating the critical role of data quality in model generalization. \n\nIn this work, we challenge the prevailing focus 1 on merely increasing data volume and model size by emphasizing the importance of data quality, particularly in scenarios constrained by the number of model parameters. We argue that for sub-billion parameter models, the quality of data-or what we term as effective training tokens -plays a more decisive role in model performance than previously recognized. This perspective shifts the paradigm from a quantity-centric view to a quality-centric approach in the development of language models. Further, we provide qualitative measures of standard data refinement techniques including data sampling (Penedo et al., 2023;Wang et al., 2024;Albalak et al., 2024) and text synthesis (Liu et al., 2024), applied to a pretraining corpus such as Re-finedWeb (Penedo et al., 2023). This helps to formulate the relationship between the diversity and syntheticity of pretraining data in order to compute the number of effective training tokens, which evaluate the impact of data quality in terms of model size and the token number. Further, we conduct extensive experiments across eight different benchmarks to evaluate the impact of data refinement techniques which allow us to significantly outperform models trained on randomly selected data samples, across a spectrum of model sizes ranging from 25 million to 1.5 billion parameters. \n\nBy integrating the notion of effective token size into the scaling law formulation, we extend the existing scaling law formulation to better capture the nuances of data quality.",
            "score": 0.4168529912889354,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "263672005",
            "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
            "text": "Large Language Models (LLMs) (Devlin et al., 2018;Raffel et al., 2020;Brown et al., 2020;Chowdhery et al., 2022) have become a center of interest among AI researchers recently. These models, trained on expansive datasets and furnished with an enormous number of parameters, have demonstrated unparalleled proficiency across diverse domains, such as text generation (Dubois et al., 2023), code completion (Chen et al., 2021;Rozi\u00e8re et al., 2023), and academic test (Hendrycks et al., 2020). \n\nThe impressive success of these LLMs depends heavily on scaling up the model parameters and pre-training data volume. It has been consistently observed that, when considering a continuum of models with nearly identical architectures, larger models coupled with increased pre-training corpora consistently yield diminished training loss. This observation has been mathematically formalized as the scaling law of loss (Kaplan et al., 2020;Henighan et al., 2020), which states that the reducible loss achieved by the model in the log scale is linear to the model size in the log scale. Scaling law has provided guidance for the scientific scaling of LLMs, including determining the balance of the model size and pre-training data size (Hoffmann et al., 2022;Muennighoff et al., 2023). This has transformed what was once a somewhat blind scaling process into a methodology underpinned by empirical assurance. Nonetheless, such beneficial scaling law yield predictions solely on the loss, not extending to the real task performance encountered in practice. This divergence establishes a substantial gap in a comprehensive scaling-up methodology (Ganguli et al., 2022). space, which displays strong linearity, supporting the task scaling law (Eq.( 3)). \n\nThe challenge in extending loss caling law to task performance predominantly stems from the discontinuity observed in task performance during scaling. Language models below a certain size yield trivial performance, i.e., random guessing on multiple choices or zero scores on generation tasks. However, when the model size surpasses a certain threshold, a distinct surge in performance appears, which leads to substantially non-trivial performance.",
            "score": 0.416423541963697,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 70,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 70,
                    "end": 89,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1632,
                    "end": 1654,
                    "matchedPaperCorpusId": "246867298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "270869389",
            "title": "Beyond Scaleup: Knowledge-aware Parsimony Learning from Deep Networks",
            "text": "The learning techniques have progressed from manual feature engineering to shallow models, then to deep networks, and now to foundation models, achieving great success in Correspondence is to Q. Yao at qyaoaa@tsinghua.edu.cn the field of computer vision, natural language understanding and speech processing. Specifically, large language models, like Chat-GPT [Ouyang et al., 2022], as representatives of foundation model, has shown strong performance in versatile learning, which can adopted in many different tasks. The belief is that larger models can be more expressive, thus are likely to generalize better given sufficient training data [Jordan andMitchell, 2015, Donoho, 2017]. This gives birth to the current roadmap, i.e., achieving stronger performance by aggressively scaling up the size of data and models, which is also observed as scaling law [Kaplan et al., 2020]. \n\nHowever, such a roadmap potentially leads to serious problems (as shown in the left of Figure 1): Data bottleneck -the scaling law relies on vast amounts of high-quality data, yet all available online corpora are projected to be exhausted by 2028 [Villalobos et al., 2024]; Computational bottleneck -the exponentially growing number of parameters demands substantial high-performance computing power, yet the pace of hardware development struggles to keep up with this rapid increase [Wolters et al., 2024]; and Trust bottleneck -the scaling law follows the data driven path, disregarding internal logical relationships, which leads to opaque reasoning processes and severe hallucination issues [Duan et al., 2024]. These indicate that the current roadmap is not sustainable, and motivates us to ask: \n\nWhere is the way if the scaling law fails? \n\nTo address this question, we look back to the fundamental principles that drive machine learning. Albert Einstein famously stated, \"Everything should be made as simple as possible, but not simpler.\" This insight has inspired many AI researchers to develop learning techniques that embrace parsimony, aiming to achieve \"maximum output with minimal input\" and \"leveraging small inputs for significant effects\". Traditional approaches to achieving parsimony learning have typically followed statistical-driven and data-driven methods (as shown in the right of Figure 1).",
            "score": 0.4162433572017233,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 381,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 643,
                    "end": 654,
                    "matchedPaperCorpusId": "677218"
                },
                {
                    "start": 1129,
                    "end": 1154,
                    "matchedPaperCorpusId": "272330241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12347412109375
        },
        {
            "corpus_id": "276117024",
            "title": "FuXi-\u03b1: Scaling Recommendation Model with Feature Interaction Enhanced Transformer",
            "text": "Recent advancements [1,3,14,24] in scaling laws have revealed that the performance of Large Language Models (LLMs) systematically improves predictably as the number of model parameters, the volume of training data, and computational resources increase. These findings are crucial as they provide researchers and practitioners with a framework for efficiently allocating limited computational resources to optimize model performance. Building on this foundation, we propose to investigate whether recommendation models also conform to scaling laws. By identifying such models, scaling laws can be utilized to guide the training of larger models, thus enhancing their performance. \n\nBesides the scaling laws found in LLMs such as GPTs [1,3], LLa-MAs [10,53], autoregressive sequential models have been shown to adhere to scaling laws across various domains, including generative image modeling, video modeling, etc [19]. The expansion of Vision Transformers (ViT) has also achieved significant success in the field of computer vision [9,73]. This revolutionary innovation has also been extended to recommendation models. Recent studies [47,76] demonstrate that autoregressive sequence recommendation models also follow these scaling laws. The success of projects like HSTU [4,60,71] indicates that scaling up sequential recommendation models in accordance with these laws is an effective strategy for developing large-scale recommendation systems. \n\nSequential recommendation models have been a focal point of research in the field of recommender systems, characterized by a wide array of architectural innovations [36,48,55,57,62,63,68]. \n\nInitially, pooling operations were employed to manage interaction sequences [8]. With the development of deep learning, more sophisticated models emerged, including CNN-based architectures such as Caser [52], GNN-based models like SR-GNN [61], RNN-based frameworks like GRU4Rec [20]. Inspired by the huge success of Transformers in NLP, models based on self-attention mechanisms were proposed, leading to notable architectures such as SASRec [23] and Bert4Rec [51]. \n\nBesides sequential recommendation models, traditional Deep Learning Recommendation Models (DLRMs), such as DCN [58] and xDeepFM [33], also play a crucial role in recommender systems.",
            "score": 0.41596917683276585,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "258987581"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "261823722"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "235433868"
                },
                {
                    "start": 1629,
                    "end": 1632,
                    "matchedPaperCorpusId": "272693948"
                },
                {
                    "start": 1632,
                    "end": 1635,
                    "matchedPaperCorpusId": "270067614"
                },
                {
                    "start": 1715,
                    "end": 1718,
                    "matchedPaperCorpusId": "207240067"
                },
                {
                    "start": 1842,
                    "end": 1846,
                    "matchedPaperCorpusId": "39847715"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "53219431"
                },
                {
                    "start": 2081,
                    "end": 2085,
                    "matchedPaperCorpusId": "52127932"
                },
                {
                    "start": 2099,
                    "end": 2103,
                    "matchedPaperCorpusId": "119181611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "273162494",
            "title": "Scaling Parameter-Constrained Language Models with Quality Data",
            "text": "In this work, we introduce a scaling law for language modeling systems, defined as \u011c(N, \n\nHere, \u011c(N, D) estimates accuracy, with N as model size and D as dataset size. Constants E, A, \u03b1, B, \u03b2, c1, and c2 are parameters to be determined. \n\nThe estimation of this scaling law constants involved analyzing a dataset of 210 data points, each representing different model and dataset sizes with corresponding training losses and accuracy scores. These estimation accounted for the refinement of the training data that incorporate additional factors such as diversity and syntheticity into the dataset size. Further, different transformations of the dataset size were included to determine how these factors could be integrated effectively. The accuracy of the model was then obtained for each of these refinements. This comprehensive dataset allowed for robust parameter estimation. Parameter estimation was achieved through nonlinear curve fitting, aiming to align the scaling law's predictions with observed training losses. The process included: During curve fitting, the goodness of fit was assessed using the R-squared value, which measures the proportion of variance in the observed data that is predictable from the model inputs. This iterative process of refinement and evaluation helped in achieving the best possible fit between the predicted and observed accuracies, enhancing the scaling law's ability to predict training losses across various settings. We stop the iteration at 200. This process refined the estimates of E, A, \u03b1, B, \u03b2, c1, and c2, enhancing the scaling law's ability to predict training losses across various settings, thus supporting efficient resource allocation and model design in language modeling. The refined constants provided a more accurate description of how training loss scales with changes in model size and dataset size, incorporating the effects of diversity and syntheticity through c1 and c2.",
            "score": 0.41585459467037733,
            "section_title": "D Scaling Law Constant Estimation",
            "char_start_offset": 22532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27978515625
        },
        {
            "paperId": "f631306560824a471c971ed12f0822d7879e54a7",
            "corpusId": 277468441,
            "title": "Can Test-Time Scaling Improve World Foundation Model?",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292160328",
                    "name": "Wenyan Cong"
                },
                {
                    "authorId": "2115315923",
                    "name": "Hanqing Zhu"
                },
                {
                    "authorId": "2118952622",
                    "name": "Peihao Wang"
                },
                {
                    "authorId": "2353216784",
                    "name": "Bangya Liu"
                },
                {
                    "authorId": "1575684088",
                    "name": "Dejia Xu"
                },
                {
                    "authorId": "2268756018",
                    "name": "Kevin Wang"
                },
                {
                    "authorId": "2266751059",
                    "name": "David Z. Pan"
                },
                {
                    "authorId": "2353108236",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "47774591",
                    "name": "Zhiwen Fan"
                },
                {
                    "authorId": "2227945855",
                    "name": "Zhangyang Wang"
                }
            ],
            "abstract": "World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.",
            "corpus_id": "277468441",
            "text": "World foundation models, which simulate the physical world by predicting future states from current observations and inputs, have become central to many applications in physical intelligence, including autonomous driving and robotics. However, these models require substantial computational resources for pretraining and are further constrained by available data during post-training. As such, scaling computation at test time emerges as both a critical and practical alternative to traditional model enlargement or re-training. In this work, we introduce SWIFT, a test-time scaling framework tailored for WFMs. SWIFT integrates our extensible WFM evaluation toolkit with process-level inference strategies, including fast tokenization, probability-based Top-K pruning, and efficient beam search. Empirical results on the COSMOS model demonstrate that test-time scaling exists even in a compute-optimal way. Our findings reveal that test-time scaling laws hold for WFMs and that SWIFT provides a scalable and effective pathway for improving WFM inference without retraining or increasing model size. The code is available at https://github.com/Mia-Cong/SWIFT.git.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.732421875
        },
        {
            "paperId": "1e272e0dfd28da85156f1ea43f42efc6becf8b98",
            "corpusId": 277994329,
            "title": "From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 43,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356789927",
                    "name": "Zhe Huang"
                },
                {
                    "authorId": "2258720029",
                    "name": "Simone Rossi"
                },
                {
                    "authorId": "2066287321",
                    "name": "Rui Yuan"
                },
                {
                    "authorId": "2258718352",
                    "name": "T. Hannagan"
                }
            ],
            "abstract": "Transformers have become a standard architecture in machine learning, demonstrating strong in-context learning (ICL) abilities that allow them to learn from the prompt at inference time. However, uncertainty quantification for ICL remains an open challenge, particularly in noisy regression tasks. This paper investigates whether ICL can be leveraged for distribution-free uncertainty estimation, proposing a method based on conformal prediction to construct prediction intervals with guaranteed coverage. While traditional conformal methods are computationally expensive due to repeated model fitting, we exploit ICL to efficiently generate confidence intervals in a single forward pass. Our empirical analysis compares this approach against ridge regression-based conformal methods, showing that conformal prediction with in-context learning (CP with ICL) achieves robust and scalable uncertainty estimates. Additionally, we evaluate its performance under distribution shifts and establish scaling laws to guide model training. These findings bridge ICL and conformal prediction, providing a theoretically grounded and new framework for uncertainty quantification in transformer-based models.",
            "corpus_id": "277994329",
            "text": "Transformers have become a standard architecture in machine learning, demonstrating strong in-context learning (ICL) abilities that allow them to learn from the prompt at inference time. However, uncertainty quantification for ICL remains an open challenge, particularly in noisy regression tasks. This paper investigates whether ICL can be leveraged for distribution-free uncertainty estimation, proposing a method based on conformal prediction to construct prediction intervals with guaranteed coverage. While traditional conformal methods are computationally expensive due to repeated model fitting, we exploit ICL to efficiently generate confidence intervals in a single forward pass. Our empirical analysis compares this approach against ridge regression-based conformal methods, showing that conformal prediction with in-context learning (CP with ICL) achieves robust and scalable uncertainty estimates. Additionally, we evaluate its performance under distribution shifts and establish scaling laws to guide model training. These findings bridge ICL and conformal prediction, providing a theoretically grounded and new framework for uncertainty quantification in transformer-based models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.36376953125
        },
        {
            "paperId": "9c8f548b85af8d3c861c9d97b6172c4ee44673d6",
            "corpusId": 275570556,
            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 69,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279750963",
                    "name": "Nanye Ma"
                },
                {
                    "authorId": "2058178039",
                    "name": "Shangyuan Tong"
                },
                {
                    "authorId": "2340654494",
                    "name": "Haolin Jia"
                },
                {
                    "authorId": "2307548497",
                    "name": "Hexiang Hu"
                },
                {
                    "authorId": "2269866136",
                    "name": "Yu-Chuan Su"
                },
                {
                    "authorId": "2326256475",
                    "name": "Mingda Zhang"
                },
                {
                    "authorId": "2340726220",
                    "name": "Xuan Yang"
                },
                {
                    "authorId": "2324838111",
                    "name": "Yandong Li"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "2325917399",
                    "name": "Xuhui Jia"
                },
                {
                    "authorId": "2324769373",
                    "name": "Saining Xie"
                }
            ],
            "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "corpus_id": "275570556",
            "text": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.919921875
        },
        {
            "paperId": "a8b57843affbd87dab44b1d86d15640c8f5e9376",
            "corpusId": 277999690,
            "title": "Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 60,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2358774933",
                    "name": "Zexi Fan"
                },
                {
                    "authorId": "2290067286",
                    "name": "Yan Sun"
                },
                {
                    "authorId": "2258799878",
                    "name": "Shihao Yang"
                },
                {
                    "authorId": "2357016552",
                    "name": "Yiping Lu"
                }
            ],
            "abstract": "High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.",
            "corpus_id": "277999690",
            "text": "High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.46435546875
        },
        {
            "paperId": "f9ca7ee3daa5c35da0fe6adfcd00d6300dee8231",
            "corpusId": 277940798,
            "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 47,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312897913",
                    "name": "Hongru Cai"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2298418808",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2284702952",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "corpus_id": "277940798",
            "text": "Generative retrieval reformulates retrieval as an autoregressive generation task, where large language models (LLMs) generate target documents directly from a query. As a novel paradigm, the mechanisms that underpin its performance and scalability remain largely unexplored. We systematically investigate training and inference scaling laws in generative retrieval, exploring how model size, training data scale, and inference-time compute jointly influence performance. We propose a novel evaluation metric inspired by contrastive entropy and generation loss, providing a continuous performance signal that enables robust comparisons across diverse generative retrieval methods. Our experiments show that n-gram-based methods align strongly with training and inference scaling laws. We find that increasing model size, training data scale, and inference-time compute all contribute to improved performance, highlighting the complementary roles of these factors in enhancing generative retrieval. Across these settings, LLaMA models consistently outperform T5 models, suggesting a particular advantage for larger decoder-only models in generative retrieval. Our findings underscore that model sizes, data availability, and inference computation interact to unlock the full potential of generative retrieval, offering new insights for designing and optimizing future systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8349609375
        },
        {
            "paperId": "9c47ea2bf1c47dc6054c05cc4d3a09e364b49f7b",
            "corpusId": 276079420,
            "title": "Test-Time Training Scaling Laws for Chemical Exploration in Drug Design",
            "venue": "",
            "year": 2025,
            "referenceCount": 52,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.19153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300442958",
                    "name": "Morgan Thomas"
                },
                {
                    "authorId": "2078357925",
                    "name": "Albert Bou"
                },
                {
                    "authorId": "144186698",
                    "name": "G. D. Fabritiis"
                }
            ],
            "abstract": "Chemical Language Models (CLMs) leveraging reinforcement learning (RL) have shown promise in de novo molecular design, yet often suffer from mode collapse, limiting their exploration capabilities. Inspired by Test-Time Training (TTT) in large language models, we propose scaling TTT for CLMs to enhance chemical space exploration. We introduce MolExp, a novel benchmark emphasizing the discovery of structurally diverse molecules with similar bioactivity, simulating real-world drug design challenges. Our results demonstrate that scaling TTT by increasing the number of independent RL agents follows a log-linear scaling law, significantly improving exploration efficiency as measured by MolExp. In contrast, increasing TTT training time yields diminishing returns, even with exploration bonuses. We further evaluate cooperative RL strategies to enhance exploration efficiency. These findings provide a scalable framework for generative molecular design, offering insights into optimizing AI-driven drug discovery.",
            "corpus_id": "276079420",
            "text": "Chemical Language Models (CLMs) leveraging reinforcement learning (RL) have shown promise in de novo molecular design, yet often suffer from mode collapse, limiting their exploration capabilities. Inspired by Test-Time Training (TTT) in large language models, we propose scaling TTT for CLMs to enhance chemical space exploration. We introduce MolExp, a novel benchmark emphasizing the discovery of structurally diverse molecules with similar bioactivity, simulating real-world drug design challenges. Our results demonstrate that scaling TTT by increasing the number of independent RL agents follows a log-linear scaling law, significantly improving exploration efficiency as measured by MolExp. In contrast, increasing TTT training time yields diminishing returns, even with exploration bonuses. We further evaluate cooperative RL strategies to enhance exploration efficiency. These findings provide a scalable framework for generative molecular design, offering insights into optimizing AI-driven drug discovery.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.243408203125
        },
        {
            "paperId": "8b8555d4b109395d937f8b20deaee76ebaa83d73",
            "corpusId": 279244494,
            "title": "Kinetics: Rethinking Test-Time Scaling Laws",
            "venue": "",
            "year": 2025,
            "referenceCount": 102,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003376054",
                    "name": "Ranajoy Sadhukhan"
                },
                {
                    "authorId": "2300556424",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "2366124482",
                    "name": "Haizhong Zheng"
                },
                {
                    "authorId": "2364951887",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "2301065578",
                    "name": "Beidi Chen"
                }
            ],
            "abstract": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
            "corpus_id": "279244494",
            "text": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9482421875
        },
        {
            "paperId": "82f75d838e92196864131bad25b1abc3b5d40a6f",
            "corpusId": 266693796,
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "referenceCount": 59,
            "citationCount": 88,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277217297",
                    "name": "Nikhil Sardana"
                },
                {
                    "authorId": "2040790531",
                    "name": "Sasha Doubov"
                },
                {
                    "authorId": "2277215716",
                    "name": "Jonathan Frankle"
                }
            ],
            "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
            "corpus_id": "266693796",
            "text": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.493408203125
        },
        {
            "paperId": "99b38b72026775a1e91d83fb71e984b5e8b7b374",
            "corpusId": 270703266,
            "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "referenceCount": 190,
            "citationCount": 77,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "2138301112",
                    "name": "Amanda Bertsch"
                },
                {
                    "authorId": "1580418311",
                    "name": "Matthew Finlayson"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "2253395527",
                    "name": "Alex Xie"
                },
                {
                    "authorId": "2265547593",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2308102420",
                    "name": "Ilia Kulikov"
                },
                {
                    "authorId": "2265540561",
                    "name": "Zaid Harchaoui"
                }
            ],
            "abstract": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
            "corpus_id": "270703266",
            "text": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.83154296875
        },
        {
            "paperId": "c2feda1e804700d0980d71cfc71ce66d369b6b6c",
            "corpusId": 278327499,
            "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 140,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319606952",
                    "name": "Qianjun Pan"
                },
                {
                    "authorId": "2359235537",
                    "name": "Wenkai Ji"
                },
                {
                    "authorId": "2243263723",
                    "name": "Yuyang Ding"
                },
                {
                    "authorId": "2274071511",
                    "name": "Junsong Li"
                },
                {
                    "authorId": "2359173303",
                    "name": "Shilian Chen"
                },
                {
                    "authorId": "2359205193",
                    "name": "Junyi Wang"
                },
                {
                    "authorId": "145558445",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2152518131",
                    "name": "Qin Chen"
                },
                {
                    "authorId": "2344763968",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "2359165491",
                    "name": "Yulan Wu"
                },
                {
                    "authorId": "2268703214",
                    "name": "Liang He"
                }
            ],
            "abstract": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic\"slow thinking\"- a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.",
            "corpus_id": "278327499",
            "text": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic\"slow thinking\"- a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.517578125
        },
        {
            "paperId": "1535c04a6c7f55338e0d8d5f17325176ba5b7b24",
            "corpusId": 269982332,
            "title": "Scaling-laws for Large Time-series Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 53,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266240992",
                    "name": "Thomas D. P. Edwards"
                },
                {
                    "authorId": "2302794109",
                    "name": "James Alvey"
                },
                {
                    "authorId": "81230967",
                    "name": "J. Alsing"
                },
                {
                    "authorId": "2303623210",
                    "name": "Nam H. Nguyen"
                },
                {
                    "authorId": "6315490",
                    "name": "B. Wandelt"
                }
            ],
            "abstract": "Scaling laws for large language models (LLMs) have provided useful guidance in training ever larger models for predictable performance gains. Time series forecasting shares a similar sequential structure to language, and is amenable to large-scale transformer architectures. Here we show that foundational decoder-only time series transformer models exhibit analogous scaling-behavior to LLMs, with architectural details (aspect ratio and number of heads) having a minimal effect over broad ranges. We assemble a large corpus of heterogenous time series data on which to train, and establish for the first time power-law scaling with parameter count, dataset size, and training compute, spanning five orders of magnitude.",
            "corpus_id": "269982332",
            "text": "Scaling laws for large language models (LLMs) have provided useful guidance in training ever larger models for predictable performance gains. Time series forecasting shares a similar sequential structure to language, and is amenable to large-scale transformer architectures. Here we show that foundational decoder-only time series transformer models exhibit analogous scaling-behavior to LLMs, with architectural details (aspect ratio and number of heads) having a minimal effect over broad ranges. We assemble a large corpus of heterogenous time series data on which to train, and establish for the first time power-law scaling with parameter count, dataset size, and training compute, spanning five orders of magnitude.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.40234375
        },
        {
            "paperId": "c341dd4a60c2ab924cdb2d82ddae5703710e830e",
            "corpusId": 270765130,
            "title": "Time Matters: Scaling Laws for Any Budget",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 5,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308469303",
                    "name": "Itay Inbar"
                },
                {
                    "authorId": "2308469085",
                    "name": "Luke Sernau"
                }
            ],
            "abstract": "A primary cost driver for training large models is wall-clock training time. We show that popular time estimates based on FLOPs are poor estimates, and construct a more accurate proxy based on memory copies. This allows us to accurately estimate the training speed of a transformer model from its hyperparameters. Combined with a scaling law curve like Chinchilla, this allows us to accurately predict the final loss of a model from a simple equation. We show that this expression is accurate across a wide range of model hyperparameter values, enabling us to analytically make architectural decisions and train models more efficiently. Crucially, this analysis predicts that in contrast to existing literature, models should be wider rather than deeper, as the benefits of speed outweigh the benefits of depth.",
            "corpus_id": "270765130",
            "text": "A primary cost driver for training large models is wall-clock training time. We show that popular time estimates based on FLOPs are poor estimates, and construct a more accurate proxy based on memory copies. This allows us to accurately estimate the training speed of a transformer model from its hyperparameters. Combined with a scaling law curve like Chinchilla, this allows us to accurately predict the final loss of a model from a simple equation. We show that this expression is accurate across a wide range of model hyperparameter values, enabling us to analytically make architectural decisions and train models more efficiently. Crucially, this analysis predicts that in contrast to existing literature, models should be wider rather than deeper, as the benefits of speed outweigh the benefits of depth.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5498046875
        }
    ],
    "quotes": {
        "cost": 0.180909,
        "quotes": [
            {
                "idx": 0,
                "key": "[254853733 | Dettmers et al. | 2022 | Citations: 234]",
                "snippets": "For inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 1077,
                        "end": 1813,
                        "sentence_offsets": [
                            {
                                "start": 1063,
                                "end": 1213
                            },
                            {
                                "start": 1216,
                                "end": 1302
                            },
                            {
                                "start": 1303,
                                "end": 1416
                            },
                            {
                                "start": 1417,
                                "end": 1540
                            },
                            {
                                "start": 1541,
                                "end": 1670
                            },
                            {
                                "start": 1671,
                                "end": 1813
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[269449894 | Xiong et al. | 2024 | Citations: 10]",
                "snippets": "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scaling Laws for Language Models",
                        "pdf_hash": "",
                        "start": 1076,
                        "end": 1571,
                        "sentence_offsets": [
                            {
                                "start": 1076,
                                "end": 1217
                            },
                            {
                                "start": 1218,
                                "end": 1368
                            },
                            {
                                "start": 1369,
                                "end": 1571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[269899695 | Ruan et al. | 2024 | Citations: 62]",
                "snippets": "Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12]81], would be valuable.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion, Limitations, and Future Work",
                        "pdf_hash": "",
                        "start": 1078,
                        "end": 1455,
                        "sentence_offsets": [
                            {
                                "start": 1078,
                                "end": 1286
                            },
                            {
                                "start": 1287,
                                "end": 1455
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12]81], would be valuable."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270764838 | Porian et al. | 2024 | Citations: 26]",
                "snippets": "Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 212,
                        "end": 362,
                        "sentence_offsets": [
                            {
                                "start": 89,
                                "end": 362
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[271571035 | Brown et al. | 2024 | Citations: 330]",
                "snippets": "Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[271600832 | Li et al. | 2024 | Citations: 3]",
                "snippets": "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scaling Law",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1072,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 300
                            },
                            {
                                "start": 301,
                                "end": 424
                            },
                            {
                                "start": 425,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 1015
                            },
                            {
                                "start": 1016,
                                "end": 1072
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[272524356 | Boopathy et al. | 2024 | Citations: 2]",
                "snippets": "Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma, 2022)(Clark et al., 2022)(Tay et al., 2021). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022)Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237592821 | Tay et al. | 2021 | Citations: 115]": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\\% fewer parameters and training 40\\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.",
                    "[246473179 | Clark et al. | 2022 | Citations: 185]": "The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.",
                    "[249947012 | Mahmood et al. | 2022 | Citations: 28]": "Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs."
                },
                "metadata": [
                    {
                        "section_title": "NEURAL NETWORK SCALING LAWS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1152,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 451
                            },
                            {
                                "start": 454,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 869
                            },
                            {
                                "start": 870,
                                "end": 1152
                            }
                        ],
                        "ref_mentions": [
                            "246559072",
                            "246473179",
                            "237592821",
                            "249947012"
                        ],
                        "quote": "Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma, 2022)(Clark et al., 2022)(Tay et al., 2021). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022)Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[273375506 | Yao et al. | 2024 | Citations: 9]",
                "snippets": "In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[266693796 | Sardana et al. | 2023 | Citations: 88]": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS",
                        "pdf_hash": "",
                        "start": 168,
                        "end": 514,
                        "sentence_offsets": [
                            {
                                "start": 168,
                                "end": 351
                            },
                            {
                                "start": 352,
                                "end": 514
                            }
                        ],
                        "ref_mentions": [
                            "266693796"
                        ],
                        "quote": "In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2023)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[275336968 | Lu | 2025 | Citations: 1]",
                "snippets": "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 852,
                        "end": 1173,
                        "sentence_offsets": [
                            {
                                "start": 852,
                                "end": 1081
                            },
                            {
                                "start": 1082,
                                "end": 1173
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[275471424 | Wan | 2025 | Citations: 1]",
                "snippets": "Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Two Types of Scaling Laws",
                        "pdf_hash": "",
                        "start": 599,
                        "end": 1555,
                        "sentence_offsets": [
                            {
                                "start": 599,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 862
                            },
                            {
                                "start": 863,
                                "end": 1096
                            },
                            {
                                "start": 1097,
                                "end": 1260
                            },
                            {
                                "start": 1261,
                                "end": 1555
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[275570556 | Ma et al. | 2025 | Citations: 69]",
                "snippets": "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[275789885 | Abnar et al. | 2025 | Citations: 10]",
                "snippets": "For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2018), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022)(Goyal et al., 2023) may address this gap.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246411621 | Wei et al. | 2022 | Citations: 9683]": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                    "[263608983 | Goyal et al. | 2023 | Citations: 122]": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                    "[47018994 | Rajpurkar et al. | 2018 | Citations: 2854]": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
                    "[52055325 | Reddy et al. | 2018 | Citations: 1212]": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa."
                },
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 499,
                        "end": 976,
                        "sentence_offsets": [
                            {
                                "start": 217,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 857
                            },
                            {
                                "start": 858,
                                "end": 976
                            }
                        ],
                        "ref_mentions": [
                            "52055325",
                            "47018994",
                            "246411621",
                            "263608983"
                        ],
                        "quote": "For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2018), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022)(Goyal et al., 2023) may address this gap."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[276259426 | Alabdulmohsin et al. | 2025 | Citations: 0]",
                "snippets": "Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such...As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246411621 | Wei et al. | 2022 | Citations: 9683]": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                    "[246527904 | Li et al. | 2022 | Citations: 1419]": "Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 902,
                        "end": 1580,
                        "sentence_offsets": [
                            {
                                "start": 902,
                                "end": 1074
                            },
                            {
                                "start": 1075,
                                "end": 1345
                            },
                            {
                                "start": 1346,
                                "end": 1480
                            },
                            {
                                "start": 1481,
                                "end": 1758
                            }
                        ],
                        "ref_mentions": [
                            "246411621",
                            "246527904"
                        ],
                        "quote": "Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such"
                    },
                    {
                        "section_title": "Multimodal Systems",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 757,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 88
                            },
                            {
                                "start": 89,
                                "end": 248
                            },
                            {
                                "start": 251,
                                "end": 286
                            },
                            {
                                "start": 287,
                                "end": 482
                            },
                            {
                                "start": 483,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 768
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276421468 | Sengupta et al. | 2025 | Citations: 0]",
                "snippets": "Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Future recommendations",
                        "pdf_hash": "",
                        "start": 356,
                        "end": 749,
                        "sentence_offsets": [
                            {
                                "start": 356,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 749
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276580569 | Li et al. | 2025 | Citations: 5]",
                "snippets": "Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Test-Time Scaling",
                        "pdf_hash": "",
                        "start": 197,
                        "end": 847,
                        "sentence_offsets": [
                            {
                                "start": 197,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 592
                            },
                            {
                                "start": 593,
                                "end": 847
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276580891 | Schaeffer et al. | 2025 | Citations: 4]",
                "snippets": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[276903172 | Bullock et al. | 2025 | Citations: 0]",
                "snippets": "Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. The Age of Artificial Intelligence",
                        "pdf_hash": "",
                        "start": 223,
                        "end": 1486,
                        "sentence_offsets": [
                            {
                                "start": 223,
                                "end": 433
                            },
                            {
                                "start": 434,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 1017
                            },
                            {
                                "start": 1018,
                                "end": 1151
                            },
                            {
                                "start": 1152,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1488
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[277313756 | Kim et al. | 2025 | Citations: 3]",
                "snippets": "Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling (Stiennon et al., 2020)[51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221665105 | Stiennon et al. | 2020 | Citations: 2189]": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 613,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 110
                            },
                            {
                                "start": 111,
                                "end": 283
                            },
                            {
                                "start": 286,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 613
                            }
                        ],
                        "ref_mentions": [
                            "221665105"
                        ],
                        "quote": "Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling (Stiennon et al., 2020)[51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[277467322 | Zhang et al. | 2025 | Citations: 14]",
                "snippets": "Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1137,
                        "end": 1313,
                        "sentence_offsets": [
                            {
                                "start": 1137,
                                "end": 1313
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[278367792 | Zeng et al. | 2025 | Citations: 0]",
                "snippets": "Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[278636433 | Chen et al. | 2025 | Citations: 2]",
                "snippets": "Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference. Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference. Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.022851,
        "cot": "For this query about inference-time scaling laws, I need to organize the quotes to clearly distinguish between inference-time scaling and traditional training scaling laws while providing comprehensive information about the specific inference-time scaling laws identified in recent research.\n\nI'll structure the dimensions as follows:\n\n1. \"Introduction to Scaling Laws\" - This should be a synthesis to provide background on what scaling laws are generally and the historical focus on training-time scaling laws. This sets the stage for understanding the newer inference-time scaling laws.\n\n2. \"Inference-Time Scaling Laws Definition\" - This should be a synthesis section explaining what inference-time scaling laws are and how they fundamentally differ from traditional training scaling laws.\n\n3. \"Specific Inference-Time Scaling Laws and Patterns\" - This should be a synthesis section that details the specific mathematical relationships and patterns observed in inference-time scaling across different domains and tasks.\n\n4. \"Applications and Implementations of Inference-Time Scaling\" - This section should be a synthesis discussing how these inference-time scaling laws are being applied in practice, with examples of models and approaches that leverage them.\n\n5. \"Comparison: Inference-Time vs. Traditional Training Scaling Laws\" - This should be a synthesis that directly compares and contrasts the two types of scaling laws, highlighting key differences in mathematical relationships, applications, and implications.\n\n6. \"Future Directions and Research Gaps\" - This should be a synthesis discussing the limitations and future research directions related to inference-time scaling laws.\n\nEach section will be in synthesis format rather than list format because the query asks for an explanation of complex relationships and comparisons, which is better served by coherent paragraphs that can build connections between concepts.",
        "plan": {
            "Introduction to Scaling Laws (synthesis)": [
                5,
                6,
                7,
                16
            ],
            "Inference-Time Scaling Laws Definition (synthesis)": [
                4,
                14,
                16,
                19
            ],
            "Specific Inference-Time Scaling Laws and Patterns (synthesis)": [
                4,
                10,
                12,
                15,
                18
            ],
            "Applications and Implementations of Inference-Time Scaling (synthesis)": [
                0,
                9,
                10,
                11,
                12,
                17,
                20
            ],
            "Comparison: Inference-Time vs. Traditional Training Scaling Laws (synthesis)": [
                1,
                3,
                5,
                8,
                13,
                19
            ],
            "Future Directions and Research Gaps (synthesis)": [
                2,
                13,
                14
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Scaling Laws",
                "tldr": "Scaling laws are mathematical relationships that predict how AI model performance changes with factors like model size, dataset size, and computational resources. These relationships typically follow power-law patterns across various domains and have expanded to include inference-time considerations beyond traditional training-focused scaling. (5 sources)",
                "text": "\nScaling laws represent mathematical relationships that predict how a model's performance\u2014typically measured by test loss\u2014changes with respect to key factors such as model size (parameter count), dataset size, and training compute resources <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. These laws have become fundamental to understanding the development trajectory of deep learning models, particularly as researchers pursue increasingly larger models to achieve better performance <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nEmpirical and theoretical research has consistently demonstrated that neural network performance follows power-law relationships with respect to parameter count and dataset size for well-trained models <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>. This pattern holds across various domains, including language modeling, computer vision, and reinforcement learning, though with different coefficients and exponents for each domain <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. The seminal work by Kaplan et al. (2020) established that language model performance follows these power-law relationships as computational resources, parameters, and data increase <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>.\n\nBeyond simple power-laws, research has shown that generalizations of power-law or non-power-law distributions can sometimes model neural scaling laws more accurately <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>. For instance, some studies demonstrate that exponential scaling works better than power-law scaling under certain data pruning conditions, and countably infinite parameter models may follow non-power-law distributions in unbounded data complexity regimes <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>.\n\nScaling laws have practical applications in optimizing resource allocation. They help determine the optimal distribution of training data and model parameters within computational constraints <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This optimization is especially crucial for large language models (LLMs) that require significant computational resources for training. Even when prioritizing data usage over computational efficiency during training (as in approaches like LLAMA), scaling laws remain necessary to evaluate whether performance gains from larger models justify the increased inference costs <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe scope of scaling laws has expanded beyond traditional training-focused metrics. Recent research has extended scaling laws to predict downstream task performance <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> and\u2014critically for our discussion\u2014inference time considerations <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper>. This emerging focus on \"test-time compute scaling\" explores how models can leverage additional computational resources during inference to improve performance, sometimes more effectively than simply increasing model parameters <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Bullock et al., 2025)",
                        "snippets": [
                            "Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters."
                        ],
                        "paper": {
                            "corpus_id": 276903172,
                            "title": "AGI, Governments, and Free Societies",
                            "authors": [
                                {
                                    "authorId": "2281052678",
                                    "name": "Justin B. Bullock"
                                },
                                {
                                    "authorId": "2349384137",
                                    "name": "Samuel Hammond"
                                },
                                {
                                    "authorId": "2349382249",
                                    "name": "Seb Krier"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73583984375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary."
                        ],
                        "paper": {
                            "corpus_id": 271600832,
                            "title": "Are Bigger Encoders Always Better in Vision Large Models?",
                            "authors": [
                                {
                                    "authorId": "2303441380",
                                    "name": "Bozhou Li"
                                },
                                {
                                    "authorId": "2303856806",
                                    "name": "Hao Liang"
                                },
                                {
                                    "authorId": "2314372736",
                                    "name": "Zimo Meng"
                                },
                                {
                                    "authorId": "2309265357",
                                    "name": "Wentao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.61865234375
                    },
                    {
                        "id": "(Boopathy et al., 2024)",
                        "snippets": [
                            "Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma, 2022)(Clark et al., 2022)(Tay et al., 2021). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022)Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively."
                        ],
                        "paper": {
                            "corpus_id": 272524356,
                            "title": "Breaking Neural Network Scaling Laws with Modularity",
                            "authors": [
                                {
                                    "authorId": "52089350",
                                    "name": "Akhilan Boopathy"
                                },
                                {
                                    "authorId": "2320292273",
                                    "name": "Sunshine Jiang"
                                },
                                {
                                    "authorId": "2297765362",
                                    "name": "William Yue"
                                },
                                {
                                    "authorId": "2238396678",
                                    "name": "Jaedong Hwang"
                                },
                                {
                                    "authorId": "2297772570",
                                    "name": "Abhiram Iyer"
                                },
                                {
                                    "authorId": "8657128",
                                    "name": "I. Fiete"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.73046875
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273375506,
                            "title": "Towards Neural Scaling Laws for Time Series Foundation Models",
                            "authors": [
                                {
                                    "authorId": "2326120888",
                                    "name": "Qingren Yao"
                                },
                                {
                                    "authorId": "2326237480",
                                    "name": "Chao-Han Huck Yang"
                                },
                                {
                                    "authorId": "2268522268",
                                    "name": "Renhe Jiang"
                                },
                                {
                                    "authorId": "2253824408",
                                    "name": "Yuxuan Liang"
                                },
                                {
                                    "authorId": "2298723734",
                                    "name": "Ming Jin"
                                },
                                {
                                    "authorId": "2254047333",
                                    "name": "Shirui Pan"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 9
                        },
                        "score": 0.63818359375
                    },
                    {
                        "id": "(Sardana et al., 2023)",
                        "snippets": [
                            "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges."
                        ],
                        "paper": {
                            "corpus_id": 266693796,
                            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2277217297",
                                    "name": "Nikhil Sardana"
                                },
                                {
                                    "authorId": "2040790531",
                                    "name": "Sasha Doubov"
                                },
                                {
                                    "authorId": "2277215716",
                                    "name": "Jonathan Frankle"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 88
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Inference-Time Scaling Laws Definition",
                "tldr": "Inference-time scaling laws describe mathematical relationships between model performance and computational resources allocated during inference, as opposed to training. These laws often follow log-linear or exponentiated power-law patterns and focus on optimizing how models utilize additional computation when generating outputs. (4 sources)",
                "text": "\nInference-time scaling laws represent a significant extension of traditional scaling laws, focusing specifically on how model performance changes with computational resources allocated during the inference (testing) phase rather than during training. These laws examine the relationship between performance metrics and the amount of computation used when a model is generating outputs or solving problems <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws are particularly relevant for decoder-only language models where generation quality depends on the tokens produced during inference <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>.\n\nA fundamental concept in inference-time scaling is \"coverage,\" defined as the fraction of problems that can be solved by any generated sample. Research has demonstrated that coverage scales with the number of samples over multiple orders of magnitude, often following a log-linear relationship that can be modeled as an exponentiated power law <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>. This consistent mathematical pattern suggests the existence of inference-time scaling laws that are distinct from but complementary to traditional training scaling laws.\n\nThe emergence of these inference-time scaling laws has practical implications for model deployment strategies. While traditional scaling laws have guided the development of increasingly large models, inference scaling laws help optimize how these models are used in practice, balancing performance against operational costs <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. Recent research has shown that optimizing test-time compute allocation can significantly enhance problem-solving performance, and that dynamic adjustments in sample allocation can maximize efficiency under compute constraints <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nThe concept of \"test-time compute scaling\" encompasses techniques that allow models to utilize additional computational resources when processing specific prompts, potentially leading to improved performance that can sometimes be more effective than simply scaling model parameters <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. This strategic allocation of compute during inference represents a promising direction for enhancing AI capabilities beyond the traditional approach of increasing model size or training data.",
                "citations": [
                    {
                        "id": "(Zeng et al., 2025)",
                        "snippets": [
                            "Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs."
                        ],
                        "paper": {
                            "corpus_id": 278367792,
                            "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
                            "authors": [
                                {
                                    "authorId": "2359897638",
                                    "name": "Xinyue Zeng"
                                },
                                {
                                    "authorId": "2155587513",
                                    "name": "Haohui Wang"
                                },
                                {
                                    "authorId": "2311427666",
                                    "name": "Junhong Lin"
                                },
                                {
                                    "authorId": "2359758871",
                                    "name": "Jun Wu"
                                },
                                {
                                    "authorId": "2359450667",
                                    "name": "Tyler Cody"
                                },
                                {
                                    "authorId": "2313576252",
                                    "name": "Dawei Zhou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Brown et al., 2024)",
                        "snippets": [
                            "Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance."
                        ],
                        "paper": {
                            "corpus_id": 271571035,
                            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                            "authors": [
                                {
                                    "authorId": "2283198901",
                                    "name": "Bradley Brown"
                                },
                                {
                                    "authorId": "50875781",
                                    "name": "Jordan Juravsky"
                                },
                                {
                                    "authorId": "2283134957",
                                    "name": "Ryan Ehrlich"
                                },
                                {
                                    "authorId": "2313919316",
                                    "name": "Ronald Clark"
                                },
                                {
                                    "authorId": "2151097303",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2313917068",
                                    "name": "Christopher R'e"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 330
                        },
                        "score": 0.7138671875
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."
                        ],
                        "paper": {
                            "corpus_id": 276580569,
                            "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
                            "authors": [
                                {
                                    "authorId": "2319420762",
                                    "name": "Bingxuan Li"
                                },
                                {
                                    "authorId": "2280103482",
                                    "name": "Yiwei Wang"
                                },
                                {
                                    "authorId": "2343700013",
                                    "name": "Jiuxiang Gu"
                                },
                                {
                                    "authorId": "2257127887",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "2256996328",
                                    "name": "Nanyun Peng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Bullock et al., 2025)",
                        "snippets": [
                            "Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters."
                        ],
                        "paper": {
                            "corpus_id": 276903172,
                            "title": "AGI, Governments, and Free Societies",
                            "authors": [
                                {
                                    "authorId": "2281052678",
                                    "name": "Justin B. Bullock"
                                },
                                {
                                    "authorId": "2349384137",
                                    "name": "Samuel Hammond"
                                },
                                {
                                    "authorId": "2349382249",
                                    "name": "Seb Krier"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73583984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Specific Inference-Time Scaling Laws and Patterns",
                "tldr": "Recent research has identified distinct mathematical patterns in how model performance improves with increased inference computation, including log-linear relationships in coverage scaling and power-law patterns across multiple domains. These inference-time scaling laws reveal that performance gains can be achieved through various strategies such as multiple sampling, recursive techniques, and optimal allocation of inference resources. (7 sources)",
                "text": "\nResearch has revealed several specific mathematical patterns that characterize inference-time scaling laws across different AI domains. A foundational pattern observed by Brown et al. is that coverage\u2014the fraction of problems solved by any generated sample\u2014scales with the number of samples over four orders of magnitude, following a log-linear relationship that can be modeled as an exponentiated power law <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>. This pattern is particularly evident in domains like coding and formal proofs, where solution verification is automated and increased coverage directly translates to improved performance <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>.\n\nSchaeffer et al. identified an apparent puzzle in these scaling patterns: while aggregate success rates across multiple tasks follow power-law scaling with respect to the number of attempts, individual problems show exponential scaling in their failure rates <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>. They reconciled this apparent contradiction by demonstrating that if the distribution of single-attempt success probabilities is heavy-tailed\u2014with a small fraction of extremely difficult problems\u2014this warps the aggregate success trend into a power law even as each individual problem scales exponentially <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>.\n\nBeyond language models, inference-time scaling has been explored in other domains such as diffusion models, which inherently allow adjustment of inference computation through the number of denoising steps <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This research demonstrates that inference-time scaling behaviors extend across different model architectures, suggesting these laws may represent fundamental properties of computational systems.\n\nRecent advances in recursive inference strategies (RINS) have revealed another dimension of inference-time scaling. Alabdulmohsin et al. found that RINS improves both the scaling exponent and asymptotic limit of performance <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>. Their analysis showed that when modeling RINS performance as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, the exponent cr increases with recursion rounds r while the asymptotic limit \u03b5r decreases <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>. This implies that increasing recursion rounds may initially show higher loss due to larger \u03b2r values, but eventually leads to superior performance through faster convergence and lower asymptotic limits <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>.\n\nMultiple studies have confirmed these inference-time scaling patterns in diverse applications. Chain-of-thought prompting demonstrates how longer inference paths through additional token generation can improve reasoning capabilities in LLMs <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>. Similarly, systems like AlphaCode generate millions of diverse programs and then filter them down, achieving competitive performance in programming competitions <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. These approaches show consistent patterns: increasing inference-time compute yields reliable performance improvements that follow predictable scaling laws <Paper corpusId=\"277467322\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Brown et al., 2024)",
                        "snippets": [
                            "Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance."
                        ],
                        "paper": {
                            "corpus_id": 271571035,
                            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                            "authors": [
                                {
                                    "authorId": "2283198901",
                                    "name": "Bradley Brown"
                                },
                                {
                                    "authorId": "50875781",
                                    "name": "Jordan Juravsky"
                                },
                                {
                                    "authorId": "2283134957",
                                    "name": "Ryan Ehrlich"
                                },
                                {
                                    "authorId": "2313919316",
                                    "name": "Ronald Clark"
                                },
                                {
                                    "authorId": "2151097303",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "2313917068",
                                    "name": "Christopher R'e"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 330
                        },
                        "score": 0.7138671875
                    },
                    {
                        "id": "(Schaeffer et al., 2025)",
                        "snippets": [
                            "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own."
                        ],
                        "paper": {
                            "corpus_id": 276580891,
                            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
                            "authors": [
                                {
                                    "authorId": "1749176844",
                                    "name": "Rylan Schaeffer"
                                },
                                {
                                    "authorId": "2327048379",
                                    "name": "Joshua Kazdan"
                                },
                                {
                                    "authorId": "2294572631",
                                    "name": "John Hughes"
                                },
                                {
                                    "authorId": "50875781",
                                    "name": "Jordan Juravsky"
                                },
                                {
                                    "authorId": "2333593482",
                                    "name": "Sara Price"
                                },
                                {
                                    "authorId": "2287830769",
                                    "name": "Aengus Lynch"
                                },
                                {
                                    "authorId": "2334069920",
                                    "name": "Erik Jones"
                                },
                                {
                                    "authorId": "2311693657",
                                    "name": "Robert Kirk"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                },
                                {
                                    "authorId": "143812875",
                                    "name": "Oluwasanmi Koyejo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.75341796875
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation."
                        ],
                        "paper": {
                            "corpus_id": 275570556,
                            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
                            "authors": [
                                {
                                    "authorId": "2279750963",
                                    "name": "Nanye Ma"
                                },
                                {
                                    "authorId": "2058178039",
                                    "name": "Shangyuan Tong"
                                },
                                {
                                    "authorId": "2340654494",
                                    "name": "Haolin Jia"
                                },
                                {
                                    "authorId": "2307548497",
                                    "name": "Hexiang Hu"
                                },
                                {
                                    "authorId": "2269866136",
                                    "name": "Yu-Chuan Su"
                                },
                                {
                                    "authorId": "2326256475",
                                    "name": "Mingda Zhang"
                                },
                                {
                                    "authorId": "2340726220",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2324838111",
                                    "name": "Yandong Li"
                                },
                                {
                                    "authorId": "35132120",
                                    "name": "T. Jaakkola"
                                },
                                {
                                    "authorId": "2325917399",
                                    "name": "Xuhui Jia"
                                },
                                {
                                    "authorId": "2324769373",
                                    "name": "Saining Xie"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 69
                        },
                        "score": 0.919921875
                    },
                    {
                        "id": "(Alabdulmohsin et al., 2025)",
                        "snippets": [
                            "Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such",
                            "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance."
                        ],
                        "paper": {
                            "corpus_id": 276259426,
                            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
                            "authors": [
                                {
                                    "authorId": "2922782",
                                    "name": "Ibrahim M. Alabdulmohsin"
                                },
                                {
                                    "authorId": "2045380893",
                                    "name": "Xiao-Qi Zhai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8095703125
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
                        ],
                        "paper": {
                            "corpus_id": 246411621,
                            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "1714772",
                                    "name": "Dale Schuurmans"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "144956443",
                                    "name": "F. Xia"
                                },
                                {
                                    "authorId": "1998340269",
                                    "name": "Quoc Le"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 9683
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests."
                        ],
                        "paper": {
                            "corpus_id": 246527904,
                            "title": "Competition-level code generation with AlphaCode",
                            "authors": [
                                {
                                    "authorId": "47002813",
                                    "name": "Yujia Li"
                                },
                                {
                                    "authorId": "2114950020",
                                    "name": "David Choi"
                                },
                                {
                                    "authorId": "8270717",
                                    "name": "Junyoung Chung"
                                },
                                {
                                    "authorId": "1684887",
                                    "name": "Nate Kushman"
                                },
                                {
                                    "authorId": "4337102",
                                    "name": "Julian Schrittwieser"
                                },
                                {
                                    "authorId": "37212795",
                                    "name": "R\u00e9mi Leblond"
                                },
                                {
                                    "authorId": "2152472076",
                                    "name": "Tom"
                                },
                                {
                                    "authorId": "2152469120",
                                    "name": "Eccles"
                                },
                                {
                                    "authorId": "2058168486",
                                    "name": "James Keeling"
                                },
                                {
                                    "authorId": "49423009",
                                    "name": "Felix Gimeno"
                                },
                                {
                                    "authorId": "2152469362",
                                    "name": "A. D. Lago"
                                },
                                {
                                    "authorId": "2067208983",
                                    "name": "T. Hubert"
                                },
                                {
                                    "authorId": "2070068655",
                                    "name": "Peter Choy"
                                },
                                {
                                    "authorId": "2154435638",
                                    "name": "Cyprien de"
                                },
                                {
                                    "authorId": "2152471553",
                                    "name": "Masson d\u2019Autume"
                                },
                                {
                                    "authorId": "2256699276",
                                    "name": "Igor Babuschkin"
                                },
                                {
                                    "authorId": "1425082935",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2421691",
                                    "name": "Po-Sen Huang"
                                },
                                {
                                    "authorId": "1851564",
                                    "name": "Johannes Welbl"
                                },
                                {
                                    "authorId": "2071666",
                                    "name": "Sven Gowal"
                                },
                                {
                                    "authorId": "2079024030",
                                    "name": "Alexey"
                                },
                                {
                                    "authorId": "152394142",
                                    "name": "Cherepanov"
                                },
                                {
                                    "authorId": "2065370007",
                                    "name": "James Molloy"
                                },
                                {
                                    "authorId": "3187297",
                                    "name": "D. Mankowitz"
                                },
                                {
                                    "authorId": "2152471960",
                                    "name": "Esme Sutherland Robson"
                                },
                                {
                                    "authorId": "143967473",
                                    "name": "Pushmeet Kohli"
                                },
                                {
                                    "authorId": "2152472162",
                                    "name": "Nando de"
                                },
                                {
                                    "authorId": "2152469135",
                                    "name": "Freitas"
                                },
                                {
                                    "authorId": "2645384",
                                    "name": "K. Kavukcuoglu"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                }
                            ],
                            "year": 2022,
                            "venue": "Science",
                            "n_citations": 1419
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements."
                        ],
                        "paper": {
                            "corpus_id": 277467322,
                            "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                            "authors": [
                                {
                                    "authorId": "2124959429",
                                    "name": "Qiyuan Zhang"
                                },
                                {
                                    "authorId": "1704274486",
                                    "name": "Fuyuan Lyu"
                                },
                                {
                                    "authorId": "2353075323",
                                    "name": "Zexu Sun"
                                },
                                {
                                    "authorId": "2353322623",
                                    "name": "Lei Wang"
                                },
                                {
                                    "authorId": "2353196827",
                                    "name": "Weixu Zhang"
                                },
                                {
                                    "authorId": "2301269240",
                                    "name": "Zhihan Guo"
                                },
                                {
                                    "authorId": "2268629268",
                                    "name": "Yufei Wang"
                                },
                                {
                                    "authorId": "2258549144",
                                    "name": "Irwin King"
                                },
                                {
                                    "authorId": "2188246843",
                                    "name": "Xue Liu"
                                },
                                {
                                    "authorId": "2324831308",
                                    "name": "Chen Ma"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 14
                        },
                        "score": 0.6806640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Implementations of Inference-Time Scaling",
                "tldr": "Inference-time scaling has been implemented in various applications including code generation systems, reasoning frameworks, image generation models, and commercial LLMs like OpenAI's o1 and DeepSeek R1. These implementations leverage strategies such as multiple sampling, chain-of-thought prompting, and recursive techniques to achieve significant performance improvements across different tasks. (12 sources)",
                "text": "\nInference-time scaling approaches have been successfully implemented across diverse applications in the AI landscape. In programming domains, systems like AlphaCode and Codex generate multiple program samples during inference to enhance code generation capabilities. <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> This approach has proven particularly effective for competitive programming problems, with AlphaCode achieving rankings in the top 54.3% in programming competitions, representing a breakthrough in AI performance on such complex tasks. <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n\nFor reasoning tasks, chain-of-thought (CoT) prompting has emerged as a powerful inference-time scaling technique. By generating longer inference paths through additional token generation, CoT significantly improves large language models' reasoning capabilities across arithmetic, commonsense, and symbolic reasoning tasks. <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper> The empirical gains can be striking\u2014for instance, prompting a 540B-parameter language model with just eight chain-of-thought exemplars has achieved state-of-the-art accuracy on mathematical word problems. <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>\n\nBeyond language models, diffusion models represent another domain where inference-time scaling has been applied. These models inherently possess the flexibility to adjust inference-time computation through the number of denoising steps, though performance gains typically plateau after a few dozen steps. Recent research explores extending inference-time scaling behavior in diffusion models beyond simply increasing denoising steps to further improve generation performance with increased computation. <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\nCommercial implementations of inference-time scaling have gained prominence with systems like OpenAI's o1 model and DeepSeek R1, which demonstrate consistent output improvements with increased inference computation. <Paper corpusId=\"277313756\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper> Research shows that significantly increasing computational resources during inference can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. <Paper corpusId=\"275471424\" paperTitle=\"(Wan, 2025)\" isShortName></Paper> This finding validates the importance of computational scaling during inference and provides a new technical pathway for optimizing LLM performance, with o1's outstanding performance confirming the significant positive correlation between inference process resource investment and model performance. <Paper corpusId=\"275471424\" paperTitle=\"(Wan, 2025)\" isShortName></Paper>\n\nRecent research has also explored optimization strategies for inference-time scaling. The concept of \"test-time budget forcing\" aims to achieve high efficiency with limited token sampling during inference. <Paper corpusId=\"277313756\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper> Additionally, quantization research has investigated how reduced bit precision affects inference scaling, with studies examining precisions from 3 to 16-bit to disentangle factors that improve scaling. <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper> This work is particularly relevant for deployment scenarios with small batch sizes common for consumers and small organizations. <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>\n\nFor specific task domains, inference-time scaling implementations show varying effectiveness. In reading comprehension tasks such as CoQA and SQuAD, denser models perform better, potentially due to their higher inference-time compute compared to perplexity-matched sparse models. <Paper corpusId=\"275789885\" paperTitle=\"(Abnar et al., 2025)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper> <Paper corpusId=\"52055325\" paperTitle=\"(Reddy et al., 2018)\" isShortName></Paper> This performance gap can be addressed through strategies that increase inference-time compute dynamically, such as chain-of-thought prompting or the \"pause-training\" technique, which allows models to process extra computation before committing to an answer. <Paper corpusId=\"275789885\" paperTitle=\"(Abnar et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263608983\" paperTitle=\"(Goyal et al., 2023)\" isShortName></Paper>\n\nWhile many inference-time scaling implementations have demonstrated success, most current approaches rely on specific scenarios and datasets. <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> This limitation highlights the need for more generalizable inference-time scaling strategies that can be applied across diverse tasks and domains.",
                "citations": [
                    {
                        "id": "(Alabdulmohsin et al., 2025)",
                        "snippets": [
                            "Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such",
                            "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance."
                        ],
                        "paper": {
                            "corpus_id": 276259426,
                            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
                            "authors": [
                                {
                                    "authorId": "2922782",
                                    "name": "Ibrahim M. Alabdulmohsin"
                                },
                                {
                                    "authorId": "2045380893",
                                    "name": "Xiao-Qi Zhai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8095703125
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests."
                        ],
                        "paper": {
                            "corpus_id": 246527904,
                            "title": "Competition-level code generation with AlphaCode",
                            "authors": [
                                {
                                    "authorId": "47002813",
                                    "name": "Yujia Li"
                                },
                                {
                                    "authorId": "2114950020",
                                    "name": "David Choi"
                                },
                                {
                                    "authorId": "8270717",
                                    "name": "Junyoung Chung"
                                },
                                {
                                    "authorId": "1684887",
                                    "name": "Nate Kushman"
                                },
                                {
                                    "authorId": "4337102",
                                    "name": "Julian Schrittwieser"
                                },
                                {
                                    "authorId": "37212795",
                                    "name": "R\u00e9mi Leblond"
                                },
                                {
                                    "authorId": "2152472076",
                                    "name": "Tom"
                                },
                                {
                                    "authorId": "2152469120",
                                    "name": "Eccles"
                                },
                                {
                                    "authorId": "2058168486",
                                    "name": "James Keeling"
                                },
                                {
                                    "authorId": "49423009",
                                    "name": "Felix Gimeno"
                                },
                                {
                                    "authorId": "2152469362",
                                    "name": "A. D. Lago"
                                },
                                {
                                    "authorId": "2067208983",
                                    "name": "T. Hubert"
                                },
                                {
                                    "authorId": "2070068655",
                                    "name": "Peter Choy"
                                },
                                {
                                    "authorId": "2154435638",
                                    "name": "Cyprien de"
                                },
                                {
                                    "authorId": "2152471553",
                                    "name": "Masson d\u2019Autume"
                                },
                                {
                                    "authorId": "2256699276",
                                    "name": "Igor Babuschkin"
                                },
                                {
                                    "authorId": "1425082935",
                                    "name": "Xinyun Chen"
                                },
                                {
                                    "authorId": "2421691",
                                    "name": "Po-Sen Huang"
                                },
                                {
                                    "authorId": "1851564",
                                    "name": "Johannes Welbl"
                                },
                                {
                                    "authorId": "2071666",
                                    "name": "Sven Gowal"
                                },
                                {
                                    "authorId": "2079024030",
                                    "name": "Alexey"
                                },
                                {
                                    "authorId": "152394142",
                                    "name": "Cherepanov"
                                },
                                {
                                    "authorId": "2065370007",
                                    "name": "James Molloy"
                                },
                                {
                                    "authorId": "3187297",
                                    "name": "D. Mankowitz"
                                },
                                {
                                    "authorId": "2152471960",
                                    "name": "Esme Sutherland Robson"
                                },
                                {
                                    "authorId": "143967473",
                                    "name": "Pushmeet Kohli"
                                },
                                {
                                    "authorId": "2152472162",
                                    "name": "Nando de"
                                },
                                {
                                    "authorId": "2152469135",
                                    "name": "Freitas"
                                },
                                {
                                    "authorId": "2645384",
                                    "name": "K. Kavukcuoglu"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                }
                            ],
                            "year": 2022,
                            "venue": "Science",
                            "n_citations": 1419
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
                        ],
                        "paper": {
                            "corpus_id": 246411621,
                            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "1714772",
                                    "name": "Dale Schuurmans"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "144956443",
                                    "name": "F. Xia"
                                },
                                {
                                    "authorId": "1998340269",
                                    "name": "Quoc Le"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 9683
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2025)",
                        "snippets": [
                            "Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation."
                        ],
                        "paper": {
                            "corpus_id": 275570556,
                            "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
                            "authors": [
                                {
                                    "authorId": "2279750963",
                                    "name": "Nanye Ma"
                                },
                                {
                                    "authorId": "2058178039",
                                    "name": "Shangyuan Tong"
                                },
                                {
                                    "authorId": "2340654494",
                                    "name": "Haolin Jia"
                                },
                                {
                                    "authorId": "2307548497",
                                    "name": "Hexiang Hu"
                                },
                                {
                                    "authorId": "2269866136",
                                    "name": "Yu-Chuan Su"
                                },
                                {
                                    "authorId": "2326256475",
                                    "name": "Mingda Zhang"
                                },
                                {
                                    "authorId": "2340726220",
                                    "name": "Xuan Yang"
                                },
                                {
                                    "authorId": "2324838111",
                                    "name": "Yandong Li"
                                },
                                {
                                    "authorId": "35132120",
                                    "name": "T. Jaakkola"
                                },
                                {
                                    "authorId": "2325917399",
                                    "name": "Xuhui Jia"
                                },
                                {
                                    "authorId": "2324769373",
                                    "name": "Saining Xie"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 69
                        },
                        "score": 0.919921875
                    },
                    {
                        "id": "(Kim et al., 2025)",
                        "snippets": [
                            "Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling (Stiennon et al., 2020)[51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference."
                        ],
                        "paper": {
                            "corpus_id": 277313756,
                            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
                            "authors": [
                                {
                                    "authorId": "2292419803",
                                    "name": "Jaihoon Kim"
                                },
                                {
                                    "authorId": "2328014454",
                                    "name": "Taehoon Yoon"
                                },
                                {
                                    "authorId": "2322353657",
                                    "name": "Jisung Hwang"
                                },
                                {
                                    "authorId": "2322097538",
                                    "name": "Minhyuk Sung"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.92333984375
                    },
                    {
                        "id": "(Wan, 2025)",
                        "snippets": [
                            "Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks."
                        ],
                        "paper": {
                            "corpus_id": 275471424,
                            "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity",
                            "authors": [
                                {
                                    "authorId": "2340178423",
                                    "name": "Jun Wan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(Dettmers et al., 2022)",
                        "snippets": [
                            "For inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations."
                        ],
                        "paper": {
                            "corpus_id": 254853733,
                            "title": "The case for 4-bit precision: k-bit Inference Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 234
                        },
                        "score": 0.6298828125
                    },
                    {
                        "id": "(Abnar et al., 2025)",
                        "snippets": [
                            "For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2018), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022)(Goyal et al., 2023) may address this gap."
                        ],
                        "paper": {
                            "corpus_id": 275789885,
                            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
                            "authors": [
                                {
                                    "authorId": "2328415570",
                                    "name": "Samira Abnar"
                                },
                                {
                                    "authorId": "2341539581",
                                    "name": "Harshay Shah"
                                },
                                {
                                    "authorId": "46254693",
                                    "name": "Dan Busbridge"
                                },
                                {
                                    "authorId": "2341667138",
                                    "name": "Alaaeldin Mohamed Elnouby Ali"
                                },
                                {
                                    "authorId": "2243336902",
                                    "name": "Josh Susskind"
                                },
                                {
                                    "authorId": "3042871",
                                    "name": "Vimal Thilak"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.65966796875
                    },
                    {
                        "id": "(Rajpurkar et al., 2018)",
                        "snippets": [
                            "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
                        ],
                        "paper": {
                            "corpus_id": 47018994,
                            "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
                            "authors": [
                                {
                                    "authorId": "2706258",
                                    "name": "Pranav Rajpurkar"
                                },
                                {
                                    "authorId": "3422908",
                                    "name": "Robin Jia"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2854
                        },
                        "score": 0
                    },
                    {
                        "id": "(Reddy et al., 2018)",
                        "snippets": [
                            "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa."
                        ],
                        "paper": {
                            "corpus_id": 52055325,
                            "title": "CoQA: A Conversational Question Answering Challenge",
                            "authors": [
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                }
                            ],
                            "year": 2018,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 1212
                        },
                        "score": 0
                    },
                    {
                        "id": "(Goyal et al., 2023)",
                        "snippets": [
                            "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm."
                        ],
                        "paper": {
                            "corpus_id": 263608983,
                            "title": "Think before you speak: Training Language Models With Pause Tokens",
                            "authors": [
                                {
                                    "authorId": "2253470854",
                                    "name": "Sachin Goyal"
                                },
                                {
                                    "authorId": "2253445320",
                                    "name": "Ziwei Ji"
                                },
                                {
                                    "authorId": "2241094",
                                    "name": "A. Rawat"
                                },
                                {
                                    "authorId": "2844480",
                                    "name": "A. Menon"
                                },
                                {
                                    "authorId": "2254172047",
                                    "name": "Sanjiv Kumar"
                                },
                                {
                                    "authorId": "34602162",
                                    "name": "Vaishnavh Nagarajan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 122
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference. Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets."
                        ],
                        "paper": {
                            "corpus_id": 278636433,
                            "title": "Parallel Scaling Law for Language Models",
                            "authors": [
                                {
                                    "authorId": "2125101083",
                                    "name": "Mouxiang Chen"
                                },
                                {
                                    "authorId": "2321578848",
                                    "name": "Binyuan Hui"
                                },
                                {
                                    "authorId": "2248072386",
                                    "name": "Zeyu Cui"
                                },
                                {
                                    "authorId": "2328943044",
                                    "name": "Jiaxin Yang"
                                },
                                {
                                    "authorId": "2248487202",
                                    "name": "Dayiheng Liu"
                                },
                                {
                                    "authorId": "2362357192",
                                    "name": "Jianling Sun"
                                },
                                {
                                    "authorId": "2326803484",
                                    "name": "Junyang Lin"
                                },
                                {
                                    "authorId": "2361700209",
                                    "name": "Zhongxin Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.8515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparison: Inference-Time vs. Traditional Training Scaling Laws",
                "tldr": "Traditional training scaling laws focus on how performance improves with increased model size, data, and training compute, typically following power-law relationships. In contrast, inference-time scaling laws examine how performance scales with computation during deployment, often favoring smaller models with optimized inference strategies over simply increasing parameter counts. (6 sources)",
                "text": "\nTraditional scaling laws, as established by seminal works like Kaplan et al. (2020), primarily focus on how model performance improves with increased model size, dataset size, and training compute. These laws typically follow power-law relationships when assuming infinite training resources. However, this assumption \"cannot be fulfilled in real-world, thus the power-law may not be accurate to portrait the temporal behaviors of LLM performance during pre-training\" <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. Traditional scaling laws have driven the trend toward larger models with more parameters to achieve better performance, often without explicitly considering inference costs.\n\nInference-time scaling laws represent a fundamental shift in how we conceptualize model optimization. Unlike traditional training scaling laws that focus on model size, training tokens, and training compute, \"inference scaling laws examine the relationship between model performance and the amount of computation used during inference\" <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. This distinction is particularly important for decoder-only language models where generation quality depends heavily on the computational resources allocated during token generation.\n\nA key difference between these paradigms is their impact on model size optimization. Traditional scaling laws typically push toward larger models, while inference-aware scaling laws may favor smaller models with optimized inference strategies. Research has shown that when \"accounting for the expected inference cost of the model,\" optimal configurations naturally \"skew toward smaller models\" <Paper corpusId=\"270764838\" paperTitle=\"(Porian et al., 2024)\" isShortName></Paper>. This suggests that the performance advantages of larger models may not always justify their increased computational costs during deployment.\n\nThe shift toward inference-aware scaling recognizes that \"using larger models inherently translates to increased computational costs during inference,\" raising a critical question: \"is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference?\" <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This consideration is relevant even when following approaches like LLAMA that prioritize data usage over computational efficiency during training.\n\nRecent research has begun exploring how smaller models\u2014when paired with compute-efficient inference strategies\u2014can sometimes outperform larger ones. Sardana et al. proposed methods where \"smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases\" <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. Similarly, Snell et al. investigated \"strategies for optimizing compute specifically at test time\" <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. These approaches recognize that the traditional focus on training scaling alone provides an incomplete picture of model efficiency.\n\nLooking forward, research suggests that scaling laws should comprehensively \"incorporate test-time computation\" since \"compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones\" <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. The future of AI scaling appears to be moving toward balancing training versus inference compute costs to develop models that perform efficiently in real-world applications rather than simply pursuing ever-larger parameter counts.",
                "citations": [
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."
                        ],
                        "paper": {
                            "corpus_id": 269449894,
                            "title": "Temporal Scaling Law for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2249971338",
                                    "name": "Yizhe Xiong"
                                },
                                {
                                    "authorId": "2298904872",
                                    "name": "Xiansheng Chen"
                                },
                                {
                                    "authorId": "2299108794",
                                    "name": "Xin Ye"
                                },
                                {
                                    "authorId": "2298921971",
                                    "name": "Hui Chen"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2298903058",
                                    "name": "Haoran Lian"
                                },
                                {
                                    "authorId": "2293626051",
                                    "name": "Jianwei Niu"
                                },
                                {
                                    "authorId": "2242661989",
                                    "name": "Guiguang Ding"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.68359375
                    },
                    {
                        "id": "(Zeng et al., 2025)",
                        "snippets": [
                            "Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs."
                        ],
                        "paper": {
                            "corpus_id": 278367792,
                            "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection",
                            "authors": [
                                {
                                    "authorId": "2359897638",
                                    "name": "Xinyue Zeng"
                                },
                                {
                                    "authorId": "2155587513",
                                    "name": "Haohui Wang"
                                },
                                {
                                    "authorId": "2311427666",
                                    "name": "Junhong Lin"
                                },
                                {
                                    "authorId": "2359758871",
                                    "name": "Jun Wu"
                                },
                                {
                                    "authorId": "2359450667",
                                    "name": "Tyler Cody"
                                },
                                {
                                    "authorId": "2313576252",
                                    "name": "Dawei Zhou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Porian et al., 2024)",
                        "snippets": [
                            "Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."
                        ],
                        "paper": {
                            "corpus_id": 270764838,
                            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                            "authors": [
                                {
                                    "authorId": "2308470091",
                                    "name": "Tomer Porian"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "2191688",
                                    "name": "J. Jitsev"
                                },
                                {
                                    "authorId": "2253541812",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.71484375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary."
                        ],
                        "paper": {
                            "corpus_id": 271600832,
                            "title": "Are Bigger Encoders Always Better in Vision Large Models?",
                            "authors": [
                                {
                                    "authorId": "2303441380",
                                    "name": "Bozhou Li"
                                },
                                {
                                    "authorId": "2303856806",
                                    "name": "Hao Liang"
                                },
                                {
                                    "authorId": "2314372736",
                                    "name": "Zimo Meng"
                                },
                                {
                                    "authorId": "2309265357",
                                    "name": "Wentao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.61865234375
                    },
                    {
                        "id": "(Lu, 2025)",
                        "snippets": [
                            "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                        ],
                        "paper": {
                            "corpus_id": 275336968,
                            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2338865687",
                                    "name": "Chien-Ping Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.865234375
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications."
                        ],
                        "paper": {
                            "corpus_id": 276421468,
                            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8544921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Future Directions and Research Gaps",
                "tldr": "While current research has established fundamental inference-time scaling patterns, significant gaps remain in understanding how these laws apply across different model types, tasks, and inference strategies. Future research should focus on balancing training and inference compute costs, extending inference-time scaling to diverse applications, and developing comprehensive frameworks that incorporate both training and inference-time considerations. (3 sources)",
                "text": "\nCurrent research on inference-time scaling laws reveals promising directions but also highlights several critical gaps that require further investigation. One significant research gap is the limited understanding of inference-time scaling behaviors across diverse model architectures and applications. While studies have demonstrated the effectiveness of inference-time scaling in language models, its existence and patterns in other contexts, such as different model types and cross-modal generation, remain under-explored <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Extending research to these areas would provide a more comprehensive understanding of inference-time scaling laws.\n\nAnother important direction involves developing more sophisticated inference-time computation strategies. Current research mainly focuses on basic prompting techniques like chain-of-thought, self-consistency, or simple agent scaffolding. Expanding these approaches to include more intensive inference-time computation scenarios and post-training setups, including fine-tuning, represents a valuable research direction <Paper corpusId=\"269899695\" paperTitle=\"(Ruan et al., 2024)\" isShortName></Paper>. Advanced inference strategies such as iterative refinement, tree search, and retrieval-based augmentation hold significant potential for allowing smaller models to outperform larger ones in specific tasks <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>.\n\nThe optimization of resource allocation between training and inference phases represents a crucial area for future research. Current scaling laws often consider training and inference separately, but developing integrated frameworks that balance these costs would enable more efficient model deployment in real-world applications <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This approach recognizes that the most effective AI systems may not simply be those with the largest parameter counts, but rather those that optimize compute efficiency across their entire lifecycle.\n\nAdditionally, research should focus on dynamic inference-time compute allocation strategies. While some studies have demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance and that dynamic adjustments in sample allocation can maximize efficiency under compute constraints, these approaches need further development and validation across a wider range of applications <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. The ability to dynamically adjust inference resources based on task difficulty or importance could significantly improve the efficiency of deployed AI systems.\n\nFinally, future research should work toward developing unified scaling frameworks that incorporate both training and inference considerations. These comprehensive frameworks would enable researchers and practitioners to make more informed decisions about model architecture, size, and deployment strategies based on their specific application requirements and resource constraints <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. Such integrated approaches would help the field move beyond the current focus on parameter count as the primary metric for model capability and toward more nuanced measures of AI system effectiveness.",
                "citations": [
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."
                        ],
                        "paper": {
                            "corpus_id": 276580569,
                            "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
                            "authors": [
                                {
                                    "authorId": "2319420762",
                                    "name": "Bingxuan Li"
                                },
                                {
                                    "authorId": "2280103482",
                                    "name": "Yiwei Wang"
                                },
                                {
                                    "authorId": "2343700013",
                                    "name": "Jiuxiang Gu"
                                },
                                {
                                    "authorId": "2257127887",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "2256996328",
                                    "name": "Nanyun Peng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.78466796875
                    },
                    {
                        "id": "(Ruan et al., 2024)",
                        "snippets": [
                            "Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12]81], would be valuable."
                        ],
                        "paper": {
                            "corpus_id": 269899695,
                            "title": "Observational Scaling Laws and the Predictability of Language Model Performance",
                            "authors": [
                                {
                                    "authorId": "82939895",
                                    "name": "Yangjun Ruan"
                                },
                                {
                                    "authorId": "2772217",
                                    "name": "Chris J. Maddison"
                                },
                                {
                                    "authorId": "2302156937",
                                    "name": "Tatsunori B. Hashimoto"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 62
                        },
                        "score": 0.68212890625
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications."
                        ],
                        "paper": {
                            "corpus_id": 276421468,
                            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.8544921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.145995
    }
}