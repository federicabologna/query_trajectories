{
    "query": "Please provide a detailed survey of the most recent autoregressive language model architectures for dense passage retrieval (such as FiD-Light and Neural Corpus Indexer), including how their retrieval paradigms and innovations compare with or improve upon dual-encoder bi-directional models like DPR and ColBERT, focusing on both core quantifiable performance metrics and qualitative trade-offs.",
    "user_id": "lib_user",
    "task_id": "5a6fcdce-042d-48ae-a106-b262a7b25cbc",
    "timestamp": "2025-06-23T22:23:56.506918",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 1,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.119172,
    "decomposed_query": {
        "rewritten_query": "Survey of autoregressive language model architectures for dense passage retrieval (such as FiD-Light and Neural Corpus Indexer), including how their retrieval paradigms and innovations compare with or improve upon dual-encoder bi-directional models like DPR and ColBERT, focusing on both core quantifiable performance metrics and qualitative trade-offs.",
        "keyword_query": "autoregressive language model architectures dense passage retrieval FiD-Light Neural Corpus Indexer retrieval paradigms innovations dual-encoder bi-directional models DPR ColBERT performance metrics qualitative trade-offs",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.0,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 53,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3477495.3531772",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.10641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "121875983",
                    "name": "Xinyu Ma"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "7888704",
                    "name": "Yixing Fan"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Dense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.",
            "corpus_id": 248366563,
            "sentences": [
                {
                    "corpus_id": "248366563",
                    "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                    "text": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                    "score": 0.5758476017013487,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 753
                        },
                        {
                            "start": 756,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1523
                        },
                        {
                            "start": 1526,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 177,
                            "end": 180,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 180,
                            "end": 183,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 204,
                            "end": 208,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 208,
                            "end": 211,
                            "matchedPaperCorpusId": "229363636"
                        },
                        {
                            "start": 211,
                            "end": 214,
                            "matchedPaperCorpusId": "237502990"
                        },
                        {
                            "start": 246,
                            "end": 249,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 457,
                            "end": 461,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 461,
                            "end": 464,
                            "matchedPaperCorpusId": "210063976"
                        },
                        {
                            "start": 464,
                            "end": 467,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 470,
                            "end": 473,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 882,
                            "end": 886,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 889,
                            "end": 892,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 1034,
                            "end": 1037,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 1037,
                            "end": 1040,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 1068,
                            "end": 1071,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 1689,
                            "end": 1693,
                            "matchedPaperCorpusId": "5541663"
                        },
                        {
                            "start": 1754,
                            "end": 1758,
                            "matchedPaperCorpusId": "215416349"
                        },
                        {
                            "start": 1758,
                            "end": 1761,
                            "matchedPaperCorpusId": "243865399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5078125
                }
            ],
            "relevance_judgement": 0.5078125,
            "relevance_judgment_input_expanded": "# Title: Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xinyu Ma, J. Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng\n## Abstract\nDense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.\n## INTRODUCTION\nDense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
            "reference_string": "[248366563 | Ma et al. | 2022 | Citations: 53]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Baselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
            "score": 0.7481026191409839,
            "section_title": "Retrieval Performance",
            "char_start_offset": 19754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "268031876",
            "title": "Generative Retrieval with Large Language Models",
            "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "score": 0.7397253677019604,
            "section_title": "Related Work",
            "char_start_offset": 21339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "239016601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "Passage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
            "score": 0.7337750564054011,
            "section_title": "RELATED WORK",
            "char_start_offset": 4763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2380
                },
                {
                    "start": 2381,
                    "end": 2538
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1058977"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "1815908"
                },
                {
                    "start": 2199,
                    "end": 2202,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 2239,
                    "end": 2242,
                    "matchedPaperCorpusId": "235293983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341064453125
        },
        {
            "corpus_id": "248366293",
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "text": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
            "score": 0.7140222111606412,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.440673828125
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "score": 0.7101924731880056,
            "section_title": "Related Work",
            "char_start_offset": 3117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 87,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 118,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 625,
                    "end": 642,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 927,
                    "end": 946,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423583984375
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Passage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
            "score": 0.7047196601835178,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 170,
                    "end": 186,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 186,
                    "end": 216,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 332,
                    "end": 347,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 404,
                    "end": 436,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 834,
                    "end": 858,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 895,
                    "end": 911,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1402,
                    "end": 1418,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400634765625
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "Our benchmark addresses this limitation by introducing semantically parallel queries across multiple languages, enabling comprehensive analysis of language fairness in MLIR. \n\nDPR Dense Passage Retrieval (DPR: Karpukhin et al. (2020)) is a neural retrieval framework initially proposed for monolingual supervised finetuning. DPR uses dual encoders: one for encoding queries and another for encoding passages (documents), both based on the BERT architecture (Devlin et al., 2019). The primary advantage of DPR over traditional retrieval models like BM25 is its ability to embed both queries and documents into a shared dense vector space, enabling efficient nearest-neighbor search for retrieval. The relevance of a document to a query is determined by the similarity between their embeddings, typically using the dot product as a similarity measure. \n\nIn our work, we employ mDPR using mBERT and XLM-R to handle multilingual queries and documents. These models are fine-tuned on parallel query-document pairs from multiple languages, allowing the system to generalize across different languages. The use of mDPR allows us to explore how multilingual language models handle language biases, which often favor high-resource languages over low-resource ones. Furthermore, we investigate the performance of these models on the Mul-tiEuP dataset, assessing their ability to ensure fair and equitable retrieval across 24 languages, thus promoting fairness in multilingual IR.",
            "score": 0.7027249589754729,
            "section_title": "Background Knowledge",
            "char_start_offset": 23243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1469
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 233,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 457,
                    "end": 478,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "273185905",
            "title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA",
            "text": "Retrieval Traditional retrieval techniques, such as TF-IDF (Jones, 1972) and BM25 (Robertson et al., 1995;Robertson & Zaragoza, 2009), retrieve information based on word terms. Subsequently, deep learning-based retrieval methods quickly became popular. REALM (Guu et al., 2020) augments the language model pre-training with a latent knowledge retriever using masked language modeling. DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) encodes queries and documents as dense embeddings, with similarity computed between them. ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) produces multi-vector representations at the token level. JPR (Joint Passage Retrieval) (Min et al., 2021) is a joint passage retrieval model with an autoregressive reranker that selects a sequence of passages. DHR (Dense Hierarchical Retrieval) (Liu et al., 2021) leverages both macroscopic document-level semantics and microscopic passage-level semantics. Fusion-in-Decoder (Izacard & Grave, 2021) employs both DPR and BM25 in a knowledge distillation manner, which does not require annotated query-document pairs. CPT-text (Neelakantan et al., 2022) utilizes contrastive pre-training on unsupervised data. NCI (Wang et al., 2022) directly generates relevant document identifiers for a given query. Atlas (Izacard et al., 2022) fine-tunes an encoder-decoder model with a retriever to address knowledge-intensive tasks with minimal training examples. RETRO (Borgeaud et al., 2022;Wang et al., 2023a) conditions on document chunks based on local similarity with preceding tokens. HHR (Hybrid Hierarchical Retrieval) (Arivazhagan et al., 2023) combines sparse and dense retrieval methods across both document and passage retrieval stages. SimLM (Wang et al., 2023b) proposes a new loss function to reduce the mismatch between pre-training and fine-tuning input distributions.",
            "score": 0.7024458244418573,
            "section_title": "RELATED WORK",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 106,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 106,
                    "end": 133,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 259,
                    "end": 277,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 415,
                    "end": 439,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 538,
                    "end": 563,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 563,
                    "end": 586,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 963,
                    "end": 986,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1200,
                    "end": 1219,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "258170263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26025390625
        },
        {
            "corpus_id": "258107796",
            "title": "Generator-Retriever-Generator Approach for Open-Domain Question Answering",
            "text": "The Retriever-Reader approach is based on the idea of combining information retrieval (retriever) and machine reading comprehension (reader) techniques. Previous work in this area includes the use of document retrieval techniques such as TF-IDF, BM25, or neural ranking models [26,33] to select relevant documents from a large corpus. mNotable works include the Stanford Question Answering Dataset (SQuAD) and subsequent advancements in retriever-reader architectures like DrQA and BiDAF [35]. Dense Passage Retrieval (DPR) [13] focuses on dense representations for passage retrieval, utilizing a dual-encoder architecture to retrieve passages and a reader model to extract the answer. T5-RC [29], a variant of the T5 model, follows the Retriever-Reader approach by retrieving relevant passages and applying T5 as a reader for answer extraction.",
            "score": 0.6995843864454667,
            "section_title": "Retriever Reader",
            "char_start_offset": 5205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 845
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "53702508"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2646484375
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Large-scale query-passage retrieval is a core task in search systems, which aims to rank a collection of passages based on their relevance with regard to a query. To balance efficiency and effectiveness, existing work typically adopts a two-stage retrieval pipeline (Ren et al., 2021b;Zhu et al., 2021). The first-stage aims to retrieve a subset of candidate passages by a recall model from the entire corpus and the second stage aims to rerank the retrieved passages. In the first-stage retrieval, traditional approaches (Chen et al., 2017) implemented term-based retriever (e.g. TF-IDF and BM25) by weighting terms based on their frequency, which have limitations on representing semantics of text. Recently, dense passage retrieval is drawing more and more attention in the task of passage retrieval (Karpukhin et al., 2020). The underlying idea is to represent both queries and passages as embeddings, so that the semantic relevance can be measured via embeddings similarity. With the great success of pre-trained language models (PLMs) such as BERT/RoBERTa (Devlin et al., 2019;Liu et al., 2019) in natural language processing tasks, dense retrieval models parameterized by PLMs is emerging as the new state-of-the-art in a variety of passage retrieval tasks (Karpukhin et al., 2020;Xiong et al., 2020). \n\nTwo paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.",
            "score": 0.6993281538851825,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 285,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 803,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1062,
                    "end": 1083,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1264,
                    "end": 1288,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1288,
                    "end": 1307,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380859375
        },
        {
            "corpus_id": "271891920",
            "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
            "text": "Traditional information retrieval (IR) methods are based on exact term matching, like BM25. While it is still widely used due to its efficiency, effectiveness, and robustness, it suffers from the well-known issue of lexical gap [3]. To address this, leveraging neural networks, dense retrieval (DR) employs pre-trained language models like BERT [7] to encode questions and passages into embeddings, and measures the question-passage relevance score by comparing the vectors similarly in the embedding space. Specifically, DR encodes the whole corpus into embeddings and builds the index, such as Faiss [8], on them. When a new question comes in, DR encodes the question into an embedding and performs a nearest neighbor search. DR can be classified into two categories: supervised, like DPR [22], TAS-B [13], and ColBERT [23,44]; unsupervised, like Contriever [15] and ReContriever [27], based on the training method. DPR utilizes a dual-tower architecture, with one BERT model dedicated to encoding questions and another to encoding passages, though in practice sometimes a single encoder is shared between passages and questions. The similarity between the question and passage embeddings is calculated, aiming to maximize the loglikelihood of the positive passage. ColBERT uses the same BERT model for both the question and passage encoders, differentiating them by appending a unique special token after the [CLS] token. Unlike DPR, which directly compares question and passage embeddings, ColBERT introduces a late interaction mechanism. It computes the similarity between each question token and all passage tokens, followed by maximum pooling over these similarities. The final similarity score for a question-passage pair is the sum of the pooled scores. TAS-B groups queries based on their embedding similarities and applies a training data sampling technique along with dual-teacher supervision distillation. Contriever trains a bi-encoder model using contrastive learning, generating positive question-passage pairs from an unlabeled corpus.",
            "score": 0.6990306799853174,
            "section_title": "RELATED WORK 2.1 Dense Retrieval",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "6749682"
                },
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "259089062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27099609375
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "To test the effectiveness of our proposed dense PRF approach, we compare with five families of baseline models, for which we vary the use of a BERT-based reranker (namely BERT or ColBERT). For the BERT reranker, we use OpenNIR [24] and capreolus/ bert-base-msmarco fine-tuned model from [21]. For the ColBERT reranker, unless otherwise noted, we use the existing pre-indexed ColBERT representation of passages for efficient reranking. The five families are: Lexical Retrieval Approaches: These are traditional retrieval models using a sparse inverted index, with and without BERT and ColBERT rerankers, namely: (i) BM25 (ii) BM25+BERT (iii) BM25+ColBERT, (iv) BM25+RM3, (v) BM25+RM3+BERT and (vi) BM25+RM3+ColBERT. Neural Augmentation Approaches: These use neural components to augment the (sparse) inverted index: (i) BM25+DeepCT and (ii) BM25+docT5query, both without and with BERT and ColBERT rerankers. For BM25+docT5query+ColBERT, the ColBERT reranker is applied on expanded passage texts encoded at querying time, rather than the indexed ColBERT representation. The response time for BM25+docT5query+ColBERT reflects this difference. Dense Retrieval Models: This family consists of the dense retrieval approaches: (i) ANCE: The ANCE [46] model is a single representation dense retrieval model. We use the trained models provided by the authors trained on MSMARCO training data. (ii) ANCE-PRF: The ANCE-PRF [48] is a PRF variant of ANCE model -we use the results released by the authors.",
            "score": 0.6977861799248684,
            "section_title": "Baselines.",
            "char_start_offset": 28415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "210883962"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "Dense retrieval models. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and utilize embedding similarities as estimated relevance scores for effective retrieval [9]. Karpukhin et al. [12] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [32,35], late interaction [14]. Recently, researchers have also investigated pre-training tasks for dense retrieval [8,19]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually need considerable memory consumption and computational overhead. Generative retrieval models. Generative retrieval has recently garnered increasing interest [1,4,20,30], which retrieves documents by directly generating their identifiers based on the given query. It offers an end-to-end solution for document retrieval tasks [3,20] and allows for better exploitation of the capabilities of large generative language models. For example, De Cao et al. [7] proposed an autoregressive entity retrieval model and Tay et al. [28] introduced a differentiable search index (DSI) and represent documents as atomic ids, naive string, or semantic strings. Chen et al. [5] proposed a pre-trained generative retrieval model called Corpus-Brain to encode all information of the corpus within its parameters in a general way. However, the robustness of generative retrieval models has been overlooked by the community. Out-of-distribution in IR. Current studies on OOD robustness in IR have their own limitations. For example, Wu et al. [31] only explored the OOD generalization performance of neural ranking models. Some works have been devoted to alleviating the poor performance of dense retrieval in the scenarios of query variants [6,23,27,36] or zero/few-shot of corpus [17,29,34]. In this work, we focus on the OOD generalizability of generative retrieval models.",
            "score": 0.6934428722274391,
            "section_title": "RELATED WORK",
            "char_start_offset": 3158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "251718954"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1009,
                    "end": 1011,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "258418300"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1507,
                    "end": 1510,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "236976200"
                },
                {
                    "start": 2071,
                    "end": 2074,
                    "matchedPaperCorpusId": "250638661"
                },
                {
                    "start": 2074,
                    "end": 2077,
                    "matchedPaperCorpusId": "244709375"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "248524694"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "237347134"
                },
                {
                    "start": 2115,
                    "end": 2118,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "We now analyse the performance of the external expansion for dense retrieval models, where the pseudo-relevance feedback information is obtained using dense retrieval models then followed by the ColBERT-PRF contextualised expansion technique. Table 6 reports the results of the external expansion dense retrieval models as well as the sparse query expansion models, the dense retrieval model without any query reformulation techniques applied and the dense retrieval models with ColBERT-PRF applied. \n\nFrom Table 6, we observe that the dense external expansion models give the highest value for all the metrics on both Robust04 and WT10G title and description query sets. Indeed, ColBERT-PRF improves over ColBERT end-to-end, verifying the results of Wang et al. (2021) on these smaller document corpora. We also find that ColBERT end-to-end dense retrieval model exhibits lower performance than the two sparse query expansion models on both the Robust04 and WT10G query sets -this may indicate underfitting for the title-only (keyword) and description query formulations of Robust04 and WT10G, which differs from the ''question-style'' of the MSMARCO passage dataset used to train the ColBERT model. Next, we note that external expansion can significantly improve over all the dense baselines across all the metrics and significantly improve over sparse query expansion models, i.e. (a) and (b) baselines in Table 6, in terms of NDCG for title-only queries and MAP for the description queries. One interesting observation is that, although Recall obtained by applying external expansion using ColBERT-PRF outperforms that of both ColBERT end-to-end and ColBERT-PRF performed on the target corpus, it is still lower than the sparse expansion methods. Indeed, this explains the popular practice of applying a more expensive reranker on top of the sparse retrieval models rather than on top of the dense expansion models. Moreover, when we looking back to compare the external expansion dense model with the external expansion sparse model but using dense retrieved passage models in Table 5, we find that some latter model variants exhibit superior performance over the pure dense retrieval based external expansion model.",
            "score": 0.6924692480338057,
            "section_title": "RQ2(a): Dense expansion on multiple representation dense retrieval",
            "char_start_offset": 37173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 751,
                    "end": 769,
                    "matchedPaperCorpusId": "235490586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1319580078125
        },
        {
            "corpus_id": "248524694",
            "title": "Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings",
            "text": "With the advances in neural language modeling [3], learning dense representations for text has become a vital component in many information retrieval (IR) tasks. In passage ranking and open-domain question answering, dense retrieval has become a new paradigm to retrieve relevant passages [6,7,9]. In contrast to traditional termbased IR models (TF-IDF and BM25) that fail to capture beyond lexical matching, dense retrieval learns dense representations of questions and passages for semantic matching. \n\nA typical approach for dense retrieval involves learning a dualencoder for embedding the questions and passages [6]. A dualencoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the corresponding question and passage embeddings. Due to their high efficiency, dual-encoders are popular first-stage rankers in largescale settings (in contrast to cross-encoders where even though they can achieve higher performance, they are not indexable and therefore are used as re-rankers [4,15]). The whole corpus can be encoded and indexed offline, while at inference time, high-scoring passages with respect to a question can be found using efficient maximum inner product search [5]. \n\nSo far, dense retrieval models have been evaluated on clean and curated datasets. However, these models will encounter usergenerated noisy questions when deployed in real-life applications. Questions can include typos because of users mistyping words, such as keyboard typos (additional/missing character and character substitution), phonetic typing errors due to the close pronunciation, and misspellings. How these typos affect the encoding of questions and whether dense dual-encoder retrieval models are robust to them is not studied yet. \n\nWorks on text classification have shown that deep neural language models such as BERT are not robust against typos [13,16], even though they apply the WordPiece tokenization.",
            "score": 0.6911671248516946,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 294,
                    "end": 296,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1185,
                    "end": 1188,
                    "matchedPaperCorpusId": "231662379"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "237592852"
                },
                {
                    "start": 1379,
                    "end": 1382,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "253581551",
            "title": "Data-Efficient Auto-Regressive Document Retrieval for Fact Verification",
            "text": "In contrast to the previous approaches, where the content of a passage is scored for a query using it's content, autoregressive document retrieval (De Cao et al., 2020) uses a seq2seq model that is trained to predict a relevant document title, such as a Wikipedia page. Tokens are decoded incre-mentally left to right where the and scored with p(y|x) = i=1 p(y i |y <i , x) where the decoded document title \u0177 \u2208 E exists in a corpus E. To ensure this constraint is satisfied, constrained decoding sets p(y i |y <i,x , x) = 0 for token sequences (y 1 , . . . y i ) that do not occur in the index. In practice this works well in the Wikipedia domain where document titles are simple canonical descriptors of an entity or concept. An extension, mGENRE (Cao et al., 2021), has been trained for multi-lingual entity linking using Wikipedia hyperlinks and internationalized versions of the pages from the Wikidata graph as supervision targets in other languages. This has not been applied to an entity linking task, but not evaluated for document retrieval. \n\nSimilarly, GENRE did use hyperlink-based information by incorporating data from BLINK during training. However, its contribution to system performance appears low (De Cao et al., 2020, Table 8), warranting further investigation. While the use of pre-training with hyperlink information in retrieval has shown promise (Ma et al., 2021) in other formulations, the use of distant-supervision in autoregressive retrieval, using the article titles and hyperlinks in training is emerging and has been studied in contemporaneous work (Chen et al., 2022). Lee et al. (2022) train autoregressive models for multi-hop retrieval, with a data augmentation strategy. Alternative autoregressive retrieval formulations are designed to predict document sub-strings (Bevilacqua et al., 2022): this obviates the need to have unique document identifiers.",
            "score": 0.6897732987460297,
            "section_title": "Autoregressive Document Retrieval",
            "char_start_offset": 4111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 167,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "237266405"
                },
                {
                    "start": 1580,
                    "end": 1599,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1759033203125
        },
        {
            "corpus_id": "275931943",
            "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
            "text": "Karpukhin et al. [17] show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder. Later works in this direction mainly focused on optimizing the training process, e.g. through mining hard negatives [19], or on the domain shift issue [20,10]. \n\nIn the late-interaction based approach, ColBERT [21] is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs. \n\nThough the models based on pretrained language models show effectiveness in passage retrieval tasks, they face limitations with handling long documents.",
            "score": 0.6866116914550204,
            "section_title": "Representation-based Models for IR",
            "char_start_offset": 4206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "268384724"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31591796875
        },
        {
            "corpus_id": "248496840",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "text": "There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval. After BERT [6] was proposed, most of the recent dense retrieval models are designed based on it and achieve significant improvement on retrieval quality, e.g., Sentence-BERT [34] and MarkedBERT [2]. But as a heavy model, BERT has high inference latency, which limits its application on online retrieval that requires low-latency serving. To solve this problem, following works have proposed several variants to reduce its complexity, including DistilBERT [36], ColBERT [15], TCT-ColBERT [22], etc. \n\nTo better support the low-latency online retrieval, in addition to deploying more lightweight neural models, most state-of-the-art end-to-end dense retrieval systems also arrange a high-performance ANN search module following the neural model to fast look up the closest documents to the queries based on their embeddings. ANN indexes include four major categories, i.e., hashing, graph, quantization and tree based indexes. Among them the tree based indexes are more suitable to low-dimensional space, so dense retrieval systems normally choose from the other three types of indexes. For example, DPR [14] utilizes a graph based index, HNSW [26], ColBERT deploys IVFADC index which is based on product quantization, BPR [43] integrates learning-to-hash technique into DPR [14], etc. FAISS [13] is one of the most popular ANN index libraries in today's dense retrieval, as it implements high-performance indexes of all the three classes.",
            "score": 0.6846040145860862,
            "section_title": "RELATED WORK",
            "char_start_offset": 10570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "253080873",
            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
            "text": "Recently, with the success of large pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021) have surpassed the sparse vector space models, becoming the new de facto method. Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019;Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers. Recently, the retrieve-then-rank pipeline has also been applied to solve CommonGen (Wang et al., 2021;Li et al., 2021;Liu et al., 2022). Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus. To alleviate this, recent work, such as AR2 (Zhang et al., 2022), has focused on improving the retriever by distilling knowledge from the ranker. In this paper, we further extend this idea by distilling the order knowledge between the candidates and gold references to the ranker and retriever.",
            "score": 0.6837727187768505,
            "section_title": "Experiment on Keyword Generation",
            "char_start_offset": 25985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 90,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 146,
                    "end": 170,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 189,
                    "end": 205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 855,
                    "end": 874,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1039,
                    "end": 1058,
                    "matchedPaperCorpusId": "235166616"
                },
                {
                    "start": 1058,
                    "end": 1074,
                    "matchedPaperCorpusId": "244119606"
                },
                {
                    "start": 1074,
                    "end": 1091,
                    "matchedPaperCorpusId": "245144423"
                },
                {
                    "start": 1317,
                    "end": 1337,
                    "matchedPaperCorpusId": "238419331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3994140625
        },
        {
            "corpus_id": "267028109",
            "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
            "text": "The authors studied various types of architectures with multiple components of a broader category of RAG models: retrieval-augmented neural architectures. The components include retrievers, rankers, and encoder-decoders.When a user has a prompt or a query. The retriever component takes this prompt and searches for relevant documents from a large corpus. Once a set of potentially relevant documents is retrieved, the ranker sorts these documents based on their relevance to the user's prompt. This step ensures that the most pertinent information is prioritized. The encoder processes the user's prompt along with the top-ranked documents retrieved and produces an intermediate representation. This representation captures the essential information from both the user's input and the external knowledge sourced from the database. The decoder then uses this representation to generate a response. The encoder-decoder enables the system to provide responses that are grounded in factual information. \n\nThe authors discussed various retrieval augmentation mechanisms.Dense Passage Retrieval (DPR) [19] is a retrieval strategy that employs a dual-encoder architecture to score dialogue context-document pairs. Given a dialogue context, DPR retrieves relevant documents by computing a relevance score between the context and each document in a corpus. It encodes the queries and the documents separately. The term dense opposes sparse retrieval method, as it maps both the query and the documents into continuous vector spaces. Poly-encoders [16] produce a set of vectors (codes) to attentively process the context token outputs of a transformer encoder. These codes capture different distinct semantic aspects or features of the context. Each code interacts with a candidate's vector to compute a score. These scores from all codes are then combined for a final score for that candidate.The Fusion-in-Decoder (FiD) [17] mechanism fuses independent encoder outputs before decoding the final generation. While DPR and Poly-encoder are retrieval strategies used to find relevant documents or passages given the dialogue context, FiD is for integrating the information from these retrieved documents into the final response generated by the model. FiD then separately encodes the dialogue context and each retrieved document using a transformer-based encoder. The",
            "score": 0.6778709551357199,
            "section_title": "NEURAL PATH HUNTER: REDUCING HALLUCINATION IN DIALOGUE SYSTEMS VIA",
            "char_start_offset": 52956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
            "score": 0.6778518194850437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 579,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 679,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 883,
                    "end": 910,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370849609375
        },
        {
            "corpus_id": "268889861",
            "title": "NL2KQL: From Natural Language to Kusto Query",
            "text": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
            "score": 0.6763440808086955,
            "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
            "char_start_offset": 3014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "13403541"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 930,
                    "end": 932,
                    "matchedPaperCorpusId": "223637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "277502276",
            "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
            "text": "The fundamental principle of dense retrieval is to transform documents and queries into vector representations. The introduction of pre-trained language models, particularly BERT [8], has revolutionized information retrieval, paving the way for dense retrieval methods such as Dense Passage Retrieval (DPR) [7], ColBERT [9], and GTR [13]. Techniques like SimCSE [5] leverage contrastive learning with models such as BERT and Roberta to optimize embeddings. Additionally, dense retrieval methods often employ Approximate Nearest Neighbor (ANN) search [6,25], Maximum Inner Product Search (MIPS) algorithms [21], and SimLM [24] to ensure efficient retrieval in sub-linear time. \n\nUnlike traditional sparse retrieval, these methods utilize transformer encoders to create dense vector representations for queries and documents, enhancing semantic understanding and retrieval accuracy. This combination of semantic depth and computational efficiency positions dense retrieval as a leading approach in modern information retrieval.",
            "score": 0.6739910217917237,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "In this section, we will introduce some widely-adopted passage retrieval models (including both sparse and dense models) for experiments. BM25 BM25 is the most widely used term-based passage retrieval method. Practically, BM25 ranks a set of passages based on the  query terms appearing in each passage, regardless of their proximity within the passage. Doc2Query [39] Doc2Query is still a term-based passage retrieval method. Doc2Query alleviates the term mismatch problem in the BM25 via training a neural sequence-to-sequence model to generate potential queries from passages, and indexes the queries as passage expansion terms. Different from the BM25 method, the implementation of the doc2query method relies on labeled query-passage pairs. DPR [26] DPR is the most widely used dense passage retrieval method, which provides a strong baseline performance. It learns dense embeddings for the query and passage with a BERT-based encoder separately. The embeddings of query and passage are then fed into a \"similarity\" function to compute the final relevance score. The retrieval performance of the DPR model is mainly determined by two factors: the BERT backbone network and the labeled query-passage dataset adopted. Therefore, in order to gain a deep understanding of the DPR model in domain passage retrieval, we conduct various settings based on the DPR model architecture by replacing the original BERT model with a BERT model that has continuously trained on in-domain raw text (DPR-2) or leveraging different domain labeled datasets to carry out the training process (DPR-1).",
            "score": 0.6721332187997278,
            "section_title": "Methods",
            "char_start_offset": 19895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393310546875
        },
        {
            "corpus_id": "259370825",
            "title": "Effective Contrastive Weighting for Dense Query Expansion",
            "text": "Dense Retrieval Models: Different from the popular \"cross-encoder\" based BERT-rerankers (MacAvaney et al., 2019;Nogueira and Cho, 2019), dense retrieval models usually build upon a BERT-based \"bi-encoder\" structure. The query and document are encoded separately into dense representations. There are two families of dense retrieval models: single representation dense retrieval and multiple representation dense retrieval models (Macdonald et al., 2021). In particular, in the single representation dense retrieval paradigm, exemplified by DPR (Karpukhin et al., 2020) or ANCE (Xiong et al., 2021), each query or document is represented into a single dense representation. Thus,  with the pre-computed document representations, retrieval can be conducted using the Nearest Neighbour search. In contrast, a multiple representation dense retrieval model encodes each token of the query and document into a dense representation, for instance, ColBERT model introduced by Khattab and Zaharia (2020). During retrieval, ColBERT performs an approximate nearest neighbour search (using FAISS (Johnson et al., 2019)) for each query embedding, followed by an exact scoring. \n\nPseudo-Relevance Feedback: Traditional lexical pseudo-relevance feedback (PRF) approaches, such as RM3 (Abdul-Jaleel et al., 2004) and Bo1 (Amati and Van Rijsbergen, 2002), as well as some recent proposed neural PRF models (Naseri et al., 2021;Li et al., 2018;Zheng et al., 2020) are applied upon sparse retrieval. Some initial efforts of implementing PRF mechanism for dense retrieval have been proposed recently: for instance, ColBERT-PRF (Wang et al., 2021), which is the most similar work to ours, selects cluster centroids as expansion embeddings. Different from ColBERT-PRF, where the expansion embeddings are prioritised by the closest token's IDF, our work focuses on learning the contextualised weights of the PRF tokens and identifies the prominent ones as the expansion tokens that can better differentiate between the positive and negative documents.",
            "score": 0.6679526407056494,
            "section_title": "Related Work",
            "char_start_offset": 24747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 112,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 429,
                    "end": 453,
                    "matchedPaperCorpusId": "237048344"
                },
                {
                    "start": 544,
                    "end": 568,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 577,
                    "end": 597,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1084,
                    "end": 1106,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1305,
                    "end": 1337,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1389,
                    "end": 1410,
                    "matchedPaperCorpusId": "219162111"
                },
                {
                    "start": 1410,
                    "end": 1426,
                    "matchedPaperCorpusId": "53081945"
                },
                {
                    "start": 1426,
                    "end": 1445,
                    "matchedPaperCorpusId": "221703727"
                },
                {
                    "start": 1607,
                    "end": 1626,
                    "matchedPaperCorpusId": "235490586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244873046875
        },
        {
            "corpus_id": "264147039",
            "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
            "text": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
            "score": 0.6678212056059429,
            "section_title": "Retrieval models",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 183,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 467,
                    "end": 491,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 518,
                    "end": 546,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1523,
                    "end": 1544,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27587890625
        },
        {
            "corpus_id": "259949948",
            "title": "Offline Pseudo Relevance Feedback for Efficient and Effective Single-pass Dense Retrieval",
            "text": "Dense retrieval. The retrieval models based on PLMs [12] have been developed into a new trend of information retrieval. There are two typical architectures for dense retrieval, viz. cross-encoder and bi-encoder. The crossencoder architecture [13,14,15] takes the concatenation of the query and the document as input and utilizes PLMs to catch the semantic interaction. The bi-encoder [3,16,4] architecture adopts the two-tower architecture and transforms query and document into single-or multi-representation [3,2] allowing the pre-computation of document representation for efficiency. \n\nPseudo relevance feedback was first proposed to ease the term mismatch problem [17] and now increasingly raises attention due to its effectiveness in dense retrieval. Many [8,18,9] have put efforts to employ PRF techniques in dense retrieval and reach a satisfactory result. Though developed, current PRF approaches may be impeded by the increased computational requirements, which could restrict their widespread use for very large datasets. \n\nPLM-based reranker. Pre-trained language model [19] has been explored to construct re-ranker [20,21] to further re-order the candidate passages returned by the firstpass retriever. The cross-encoder architecture is typically employed for the re-ranker [13,22,23] with substantial improvement while burdening the online latency. The contextualized relevance offline weighting (CROW) [11] approach is proposed following the idea of splitting the pipeline into the online and the offline stages [24,25]. Despite the gain in efficiency, the heavy offline computational cost hinders further enhancing effectiveness with techniques like PRF, which is attempted to address in this paper through lightweight integration of PRF.",
            "score": 0.6675223918489483,
            "section_title": "Related Work",
            "char_start_offset": 2302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "5878197"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "3002280"
                },
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "53081945"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "213861855"
                },
                {
                    "start": 768,
                    "end": 770,
                    "matchedPaperCorpusId": "231648324"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1291,
                    "end": 1294,
                    "matchedPaperCorpusId": "211481850"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "235792532"
                },
                {
                    "start": 1527,
                    "end": 1531,
                    "matchedPaperCorpusId": "16346772"
                },
                {
                    "start": 1531,
                    "end": 1534,
                    "matchedPaperCorpusId": "53429039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23681640625
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "\u2022 DPR [12] is a representative dense retrieval model which uses dual-encoder architecture and is trained with in-batch negatives and a few hard negatives selected with BM25. \u2022 BART [16] is a Seq2Seq model applicable for sequence generation tasks. Following [5,7], we extract the query-title pairs from each dataset and fine-tune the BART for generative retrieval. \u2022 CorpusBrain [5] (C.B. for short) is a pre-trained generative retrieval model for knowledge-intensive language tasks. We finetune CorpusBrain on every specific downstream KILT task.",
            "score": 0.6632137864585095,
            "section_title": "Retrieval models",
            "char_start_offset": 11687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 546
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 10,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 260,
                    "end": 262,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "This model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
            "score": 0.6621921242580722,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "258426878",
            "title": "Synthetic Cross-language Information Retrieval Training Data",
            "text": "DPR computes the similarity of the query classification (CLS) token and the CLS token of each document. ColBERT computes similarities between each pair of query and document tokens and scores a document by the sum of the maximum similarity (MaxSim) of each query token [26]. Compared to other neural reranking models such as a cross-encoder [38], dense retrieval models limit ranking latency by separating query and document transformer networks to support offline indexing. \n\nDPR-X [54,55,59] and ColBERT-X [35] are the CLIR counterparts of DPR and ColBERT. Both use an mPLM as the underlying language model for crossing the language barrier. Exploiting both multilinguality and improved pre-training from XLM-R [9], DPR-X and ColBERT-X seek to generate similar contextual embeddings for terms with similar meanings, regardless of their language. These are the two retrieval models featured in our experiments.",
            "score": 0.6611376900554229,
            "section_title": "BACKGROUND 2.1 Cross-Language Information Retrieval",
            "char_start_offset": 6528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 911
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "248392078"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "246210468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "277857122",
            "title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models",
            "text": "Pre-trained contextualized language models based on the Transformer architecture, such as BERT, significantly improved retrieval effectiveness over the previous state-of-the-art methods in information retrieval (IR) like e.g. BM25 [42]. Among these models, dual encoders such as sparse [11,14] or dense [24] models, have the best efficiency while cross-encoders, e.g. MonoBERT [39], have the best effectiveness. \n\nAnother category of models, called late interaction models, like ColBERT [25,45], offer a good efficiency-effectiveness trade-off, as they are halfway between dual encoders and cross-encoders. Specifically, unlike dual encoders, late interaction models use embeddings at the token level rather than to represent the entire query or document. In ColBERT, the score of a document for one query token is defined as the maximum of the inner product over all the document tokens representations with the query token representation. This token-level granularity allows for finer semantic matching and improved retrieval accuracy. Moreover, compared to the cross-encoder mechanism like MonoBERT [39], this \"late interaction\" mechanism provides a way to build an index of a whole dataset by storing the document token representations, or document vectors in short. \n\nHowever, storing the vectors of the documents requires a large index space compared to dual encoding approaches. For example, for MS MARCO v1 [2], composed of 8.8M passages, the ColBERT index takes 142 GiB. PLAID [44], which is a highly optimized version of ColBERTv2, requires more than 20 GiB of space. This is in contrast to the 13 GiB needed to store the 768-dimensional vectors of a dense model (without any compression, using FP16). Furthermore, a large index leads to expensive computational costs; as in ColBERT's retrieval pipeline, the retrieval cost scales linearly with the query and document's length. \n\nTo reduce the index size, some works have proposed to prune tokens from the document. Some existing works [1,27,33] propose to use static methods based on heuristic or statistical approaches, but they degrade ColBERT performance with high pruning ratios, greater than 50%.",
            "score": 0.6590829043499987,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 487,
                    "end": 491,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "248887228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2432861328125
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "Dense Passage Retrieval Dense retrieval has garnered attention for its efficacy in semantic space exploration. A notable technique in this domain is DPR (Karpukhin et al., 2020), employing both query and passage encoders for efficient retrieval. \n\nVarious studies have delved into enhancing dense retrieval, including negative example mining techniques like RocketQA (Qu et al., 2021), and diverse data augmentation methods such as DRAGON (Lin et al., 2023a). ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) introduced the lateinteraction mechanism, offering an alternative architecture for dense retrieval. \n\nAnother line of research is pre-training strategies for dense retrieval. Approaches like Contriever (Izacard et al., 2021), coCondenser (Gao and Callan, 2022), and COCO-DR (Yu et al., 2022) have proposed contrastive pre-training techniques tailored for retrieval tasks. Concurrently, CoT-MAE (Wu et al., 2023) and RetroMAE (Xiao et al., 2022) have focused on masked auto-encoding for pre-training. \n\nAs large language models (LLMs) continue to evolve, their integration into dense retrieval has become increasingly widespread. GTR (Ni et al., 2022) utilized LLM encoders, showcasing performance gains with increased model size. Similarly, Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) employed LLMs to generate synthetic query-document pairs, effectively enhancing the training of dense retrievers. Building on a similar concept, (Huang et al., 2024) extended the approach to multilingual settings, enabling broader applicability. \n\nOur contribution is orthogonal to these studies as we concentrate on refining training signals for knowledge distillation. This suggests that our approach holds potential for integration with other methods to achieve further improvements. \n\nKnowledge Distillation for Dense Retrieval Enhancing the performance of dense retrievers often involves employing knowledge distillation techniques. Izacard and Grave (2020) pioneered the distillation of knowledge from the reader to the retriever, resulting in improved performance in open-domain question answering.",
            "score": 0.658437772150017,
            "section_title": "Related Work",
            "char_start_offset": 2741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 367,
                    "end": 384,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 468,
                    "end": 495,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 495,
                    "end": 519,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 758,
                    "end": 780,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 794,
                    "end": 811,
                    "matchedPaperCorpusId": "253157773"
                },
                {
                    "start": 914,
                    "end": 931,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1153,
                    "end": 1170,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1274,
                    "end": 1292,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 1304,
                    "end": 1328,
                    "matchedPaperCorpusId": "250340449"
                },
                {
                    "start": 1474,
                    "end": 1494,
                    "matchedPaperCorpusId": "268681628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343505859375
        },
        {
            "corpus_id": "248496840",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "text": "Passage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.",
            "score": 0.6582371104520821,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351806640625
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks. A notable example is the Dense Passage Retriever (DPR) [67], which employs a BERT-based backbone and is pre-trained specifically for the OpenQA task using question-answer pair data. DPR has demonstrated significant efficacy as a pre-trained retriever, contributing to the success of numerous RAG models across various downstream applications [68,22,65,69]. An alternative approach to dense retrieval that has gained significant traction in Retrieval-Augmented LLMs involves using a single encoder architecture [70,71].This encoder can be built upon Transformer models, BERT, or other readily available sequence modeling frameworks. \n\nTo improve the quality of retrieval, enhancement is necessary in pre-retrieval stage. These enhancements are mostly about optimizing indexing and optimizing query. Key areas for optimizing indexing quality include enhancing data granularity, refining index structures, incorporating metadata, improving alignment, and enabling mixed retrieval strategies [72,73,74,75]. To optimize query qualities, some research have applied query expansion [76], query rewriting [77,73] and query augmentation [78].",
            "score": 0.6576746721619149,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 35078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1290
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "8718737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20947265625
        },
        {
            "corpus_id": "266844358",
            "title": "Building Efficient and Effective OpenQA Systems for Low-Resource Languages",
            "text": "Table 15 shows the resource usage of the retriever models for each stage of the retrieval process. We observe that BM25 exhibits the lowest resource footprint across nearly all categories in each phase, offering the fastest indexing and retrieval speeds. In contrast, dense retrievers demonstrate varying resource consumption across different phases. \n\nIn training, DPR uses more memory than ColBERT-QA, mainly because DPR depends on in-batch negatives and its performance is tied to batch size. Additionally, DPR's final model size is twice as large as that of ColBERT-QA because DPR uses two separate BERT models for query and document encoding. In contrast, ColBERT-QA employs a single shared BERT model for both encoders. \n\nDuring indexing, BM25 demonstrates exceptional speed when indexing the knowledge source compared to dense retrievers. It also maintains a low memory footprint and minimal index size. DPR and ColBERT-QA have a similar memory footprint. DPR exhibits slower indexing performance relative to ColBERT-QA. (For all models, indexing speed can be boosted using multiple parallel processes.) Additionally, DPR produces a smaller index than ColBERT-QA, in line with the fact that DPR generates fixed-size vectors while ColBERT-QA produces a sequence of token embeddings. \n\nIn the retrieval and ranking step, BM25 maintains the minimum footprint across all categories while achieving the fastest speed. DPR and ColBERT-QA, on the other hand, both demand a considerable amount of memory because they load the entire index into memory during retrieval. The size of memory that DPR and ColBERT-QA consume is in line with the size of their index. Meanwhile, BM25 minimizes memory usage by accessing the index directly from disk.",
            "score": 0.6547883714618016,
            "section_title": "Resource Usage of Retriever Models",
            "char_start_offset": 84485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1741
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1268310546875
        },
        {
            "corpus_id": "267068650",
            "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
            "text": "Passage Retrieval. Passage retrieval aims to find relevant information from large passage collections based on a given search query. Traditional techniques for ad-hoc retrieval, such as TF-IDF and BM25 [26], perform relevant searches based on lexicon terms and inverted document frequencies. Such methods have efficient retrieval abilities, but they suffer from vocabulary mismatch problems [12]. Although there are continuous techniques like query expansions [10,21,30] and query rewrite [9] to alleviate such mismatch issues, additional preprocess of queries or passage collections make the retrieval pipelines much more complicated and hard to maintain. \n\nDense retrieval [12,22] has become popular in mainstream retrieval applications recently, because it maps the textual information into learnable representations and naturally alleviates the lexicon mismatch problems by learning the similarity traits of queries and passages through end-to-end contrastive learning. Dense retrieval encodes queries and passage collections as condensed lowdimension sentence representations with pre-trained language models (PLMs), and performs retrieval based on the similarity of representations with efficient search algorithms like maximum innerproduct search (MIPS) [19]. Notably, there are continuous efforts [3,5,27,37] to adapt the PLMs to sparse retrieval. There are also extended studies about hybrid retrievals [15,33], which combines both dense and sparse retrievals to bring superior retrieval performances. However, our work primarily focuses on dense retrieval pre-training and is orthogonal to these works.",
            "score": 0.6510462656830374,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1612
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "1448737"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "16464483"
                },
                {
                    "start": 1308,
                    "end": 1310,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1310,
                    "end": 1313,
                    "matchedPaperCorpusId": "251953412"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "266977305"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "253553743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357421875
        },
        {
            "corpus_id": "274982759",
            "title": "GraphAgent: Agentic Graph Language Assistant",
            "text": "References: ID: 1. Dense passage retrieval has gained significant attention in recent years, leveraging the power of neural networks to learn complex language patterns and query-document relations @CITE[2]@. This approach has many intriguing advantages over sparse retrieval, such as better capturing of language structures and improved performance on queries written in natural languages @CITE[2]@. However, the effectiveness of dense retrieval often requires combination with sparse retrieval, and the training mechanisms used in dense retrieval can be a bottleneck @CITE[3]@. To address this, various techniques have been proposed, including Approximate Nearest Neighbor Negative Contrastive Learning @CITE[3]@, unsupervised corpus-aware language model pre-training @CITE[4]@, and leveraging passage-centric similarity relations @CITE[5]@. These advancements have significantly improved the performance of dense passage retrieval models, making them a promising approach for various natural language processing tasks. \n\n[Llama3-8b Fine-tuned] Recent advancements in PLMs such as BERT (De-vlin et al., 2018), Roberta (@CITE[1]@, and GPT (@CITE[3]@ have enabled numerous unsupervised techniques to derive dense represen-tations of queries and passages for retrieval. These approaches primarily use a Bi-Encoder structure to embed text in a low-dimensional space and learn similarity relations via contrastive learning, con-trasting traditional sparse retrieval methods like BM25 or DeepCT (@CITE[2]@;. DPR ( pi-oneered an unsupervised dense passage retrieval framework, affirming the feasibility of using dense representations for retrieval independently. This efficient and operational approach was further refined by subsequent studies (@CITE[4]@; ; ; @CITE[5]@ that focused on high-quality negative sample min-  In our June 2019 report, we found that, while abuse deficiencies cited in nursing homes were relatively rare from 2013 through 2017, they became more frequent during that time, with the largest increase in severe cases. Specifically, abuse deficiencies comprised less than 1 percent of the total deficiencies in each of the years we examined, which is likely conservative.",
            "score": 0.6490261003467417,
            "section_title": "[GraphAgent]",
            "char_start_offset": 55957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. [40] recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap. We first study two alternatives for developing symmetric dense retrieval models: 4 (1) producing a textual representation of the image and using a symmetric uni-modal bi-encoder architecture for dense retrieval, and (2) converting passages to a multi-modal input format and using a symmetric multi-modal bi-encoder architecture. We observe that both alternatives suffer from information loss, but also that they produce complementary representations. This observation motivates us to not only combine these two encodings, but also transfer knowledge between them. In more detail, we propose an iterative knowledge distillation approach to transfer knowledge between these two alternative symmetric dense retrieval models. The proposed symmetric dual encoding approach leads to 11.6% and 30.9% MRR improvements compared to the state-of-the-art baseline on OK-VQA [38] and FVQA [56] test sets, respectively. \n\nFor the second stage of the pipeline, unlike much prior work on answer span detection for KI-VQA [12,14,37,50,57] (i.e., answer extraction from the retrieved passages), we focus on retrievalaugmented autoregressive answer generation. We propose MM-FiD, a simple yet effective extension of the Fusion-in-Decoder (FiD) [20] architecture to multi-modal input. FiD is a retrieval-augmented text generation model that has recently shown effective performance in question answering tasks [20].",
            "score": 0.6473003387080016,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "173991173"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "7483388"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "matchedPaperCorpusId": "226284018"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "250144531"
                },
                {
                    "start": 1708,
                    "end": 1711,
                    "matchedPaperCorpusId": "229339845"
                },
                {
                    "start": 1711,
                    "end": 1714,
                    "matchedPaperCorpusId": "231627467"
                },
                {
                    "start": 1714,
                    "end": 1717,
                    "matchedPaperCorpusId": "232320648"
                },
                {
                    "start": 1921,
                    "end": 1925,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3125
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "Dense retrieval (Lee et al., 2019;Karpukhin et al., 2020) is currently the de facto implementation of text retrieval. This method benefits from the powerful representation abilities of encoder-based language models and the MIPS algorithm (Shrivastava and Li, 2014), allowing for efficient passage retrieval from a large-scale corpus. Dense retrieval has been constantly developed through knowledge distillation, hard negative sampling, and better pretraining design (Chang et al., 2019;Wang et al., 2022a). Compared to dense retrieval, which relies on the dual-encoder architecture, the recently arsing generative retrieval shows promise in overcoming the missing fine-grained interaction problem through the encoder-decoder paradigm. Despite the huge potential, generative retrieval still lags behind the state-of-the-art dense retrieval method and leaves much room for investigation.",
            "score": 0.646437464005797,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 885
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 34,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 34,
                    "end": 57,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 238,
                    "end": 264,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "In this paper we address the challenges of using dense passage retrieval (DPR) in first stage retrieval for document-to-document tasks with limited labelled data. We propose the paragraph aggregation retrieval model (PARM), which liberates dense passage retrieval models from their limited input length and which takes the paragraph-level relevance for document retrieval into account. We demonstrate on two test collections higher first stage recall for dense document-to-document retrieval with PARM than with document-level retrieval. We also show that dense retrieval with PARM outperforms lexical retrieval with BM25 in terms of recall at higher cut-off values. As part of PARM we propose the novel vector-based aggregation with reciprocal rank fusion weighting (VRFF), which combines the advantages of rank-based aggregation with RRF [10] and topical aggregation with dense embeddings. We demonstrate the highest retrieval effectiveness for PARM with VRRF aggregation compared to rank and vector-based aggregation baselines. Furthermore we investigate how to train dense retrieval models for dense document-to-document retrieval with PARM. We find the interesting result that training DPR models on more, but noisy document-level data does not always lead to overall higher retrieval performance compared to training on less, but more accurate paragraph-level labelled data. Finally, we analyze how PARM retrieves relevant paragraphs and find that the dense retrieval model learns a structural paragraph relation which it exhibits with PARM and therefore benefits the retrieval effectiveness.",
            "score": 0.643553006559125,
            "section_title": "Conclusion",
            "char_start_offset": 27275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1598
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "12408211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298828125
        },
        {
            "corpus_id": "274233909",
            "title": "ScalingNote: Scaling up Retrievers with Large Language Models for Real-World Dense Retrieval",
            "text": "Information retrieval platforms, such as Google1 , Baidu 2 , and Xiaohongshu 3 , are important Internet applications that enable users to access relevant data from the vast amount of information available online [12,27,56]. Dense retrieval is a classic techniques in retrieval [17,37]. This method employs dual-tower architectures to encode both queries and documents into dense embeddings. The semantic relevance between queries and documents is measured using embedding similarity. With the rapid advancement of pretraining techniques, pre-trained models like BERT [18] and GPT [3] have greatly enhanced the semantic understanding capabilities of dense retrieval models, making dense retrieval a primary channel in industrial retrieval [8,12,28]. \n\nIn real-world retrieval systems, the massive number of real-time queries, combined with strict latency requirements, often limits the size of retrieval models to ensure quick responses [8,27,36]. To prevent burdening deployment, there is a greater focus on refining negative sampling techniques [11,49,51] to enhance model performance, while the potential advantages of scaling up model size are frequently underexplored. Recently, decoder-only Large Language Models (LLMs) have demonstrated huge potential in natural language understanding and generation [50,58]. This advancement has also impacted dense textual retrieval [2,23,25,34,59]. Concurrently, scaling laws based on bi-directional encoders for dense textual retrieval have been verified [6], indicating that more parameters and more data lead to more powerful retrieval models. This all suggests a promising direction for scaling up dense retrieval with larger-scale decoder-only LLMs in real industry scenarios. \n\nHowever, scaling up retrievers in industrial scenarios faces more challenges than existing research on scaling dense retrieval. In industrial applications, different towers of retrievers serve different purposes. Query towers process real-time queries from users, prioritizing low latency. They typically have fewer parameters [20,28,36] or use high-level quantization [12,27] to speed up online inference. On the other hand, document towers generate embeddings for existing documents to update search indexes.",
            "score": 0.6425471083647253,
            "section_title": "INTRODUCTION",
            "char_start_offset": 20,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "246822639"
                },
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "236980257"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "235358369"
                },
                {
                    "start": 942,
                    "end": 945,
                    "matchedPaperCorpusId": "251518382"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1381,
                    "end": 1384,
                    "matchedPaperCorpusId": "270710849"
                },
                {
                    "start": 1384,
                    "end": 1387,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "268723791"
                },
                {
                    "start": 2054,
                    "end": 2058,
                    "matchedPaperCorpusId": "272330578"
                },
                {
                    "start": 2058,
                    "end": 2061,
                    "matchedPaperCorpusId": "236980257"
                },
                {
                    "start": 2061,
                    "end": 2064,
                    "matchedPaperCorpusId": "251518382"
                },
                {
                    "start": 2100,
                    "end": 2103,
                    "matchedPaperCorpusId": "235358369"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2470703125
        },
        {
            "corpus_id": "249191630",
            "title": "Fast and Light-Weight Answer Text Retrieval in Dialogue Systems",
            "text": "In dense passage retrieval, q and p are separately encoded. All the passages can be encoded and indexed offline. During inference time, only the query needs to be encoded; ANN (approximate nearest neighbor) search libraries such as FAISS (Johnson et al., 2017) are used to efficiently search for the most relevant passage.\n\nIn single-vector retrieval models such as DPR  and BERT Siamese/Dual Encoder (Luan et al., 2021), the query and passages are separately encoded into single vectors, models are trained with the objective of mapping the relevant passage vector close to the query vector, and pushing the irrelevant passage vectors far away from the query vector. During inference time, ANN search is used to retrieve directly for the passage vectors closest to the query vector. Several other systems leverage multi-vector representations and attention-based reranking, including Poly-encoders (Humeau et al., 2020), PreTTR (MacAvaney et al., 2020), etc.. In late interaction models such as Col-BERT (Khattab and Zaharia, 2020;Santhanam et al., 2021), the query and passages are separately encoded to obtain query token vectors and passage token vectors. These models adopt token-decomposed scoring, e.g. the sum of maximum-similarity (SumMaxSim) scores to query vectors are used to model the relevance of passages. During training, models are trained with the objective of maximizing the SumMaxSim scores of relevant passage and minimizing those of irrelevant passages. During inference time, the passage tokens closest to query tokens are fetched, and then the relevant passages are re-ranked based on the SumMaxSim scores.\n\nWe experimented with two of the most popular dense retrieval models, DPR and ColBERT. As effective as they are, they still consume more computing resources and take longer response time than required in our real-world use case of hosting thousands of customized systems. Also, in our use case, dialogue system administrators want to reduce the time to fine-tune neural retrieval models on custom training data.",
            "score": 0.6424566997008208,
            "section_title": "Neural Dense Retrieval Preliminaries",
            "char_start_offset": 9093,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 899,
                    "end": 920,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1005,
                    "end": 1032,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297607421875
        },
        {
            "corpus_id": "248646720",
            "title": "Deep learning-based approach for Arabic open domain question answering",
            "text": "Dense Passage Retrieval (DPR) was introduced in 2020 by Karpukhin et al. (2020) for open-domain QA tasks as an alternative to the TF-IDF, and BM25 passage retrieval approaches. This retriever improves the reader by working as a lightweight filter reducing the number of documents that must be processed. Dense methods, such as a dual-encoder in DPR, have outperformed the sparse techniques in English open-domain QA. These methods use deep neural networks to embed both the document and the question into a shared embedding space. The dense model uses transformer-based encoders that are more sensitive to characteristics such as lexical variations or semantic relationships, whereas the sparse methods consider the text as a \"bag of words,\" without considering word order and grammar (Karpukhin et al., 2020). \n\nDPR uses two independent BERT encoders to train a retriever using pairwise questions and answers. To train DPR, we need a question, their answer, positive passages, and negative passages. The negative samples can be obtained using methods such as BM25 to return negative samples to a question from the corpus that does not contain the correct answer or the in-batch negatives method to return samples that are paired with other questions in the same batch. The representation-based method used in DPR can be very fast since passages can be calculated and indexed offline in advance. However, because the representations of the question and passage are generated independently, only shallow interactions between them are captured, which may reduce retrieval efficiency. The experiments on this method show that the inner product function is the best way to calculate a dual-encoder retriever's similarity score. \n\nThis allows the DPR model to capture the lexical or semantic similarities. Thus, phrases that contain different tokens (keywords), but the same meaning may still be mapped to vectors that are located relatively close to each other. For example, the DPR would be able to better match (\" \u202b\u0646\u202c \u202b\ufed8\u202c \u202b\u0648\u202c \u202b\u062f\u202c \" with \" \u202b\u0645\u202c \u202b\ufe8e\u202c \u202b\ufedf\u202c \" -money) and extract the correct context.",
            "score": 0.6418783838878361,
            "section_title": "Dense passage retriever",
            "char_start_offset": 11650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 79,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 785,
                    "end": 809,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2232666015625
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "Word embeddings are a common approach in natural language processing.Similar to sparse vector representations, they use deep learning techniques to project words into a higher-dimensional vector space.Several articles in the RALM architecture utilize this technique, and we have selected representative ones to describe.Karpukhin et al. (2020) proposed the DPR retrieval model, which indexes all passages in a lowdimensional and continuous space.This allows the reader to efficiently retrieve the first k passages associated with the input problem at runtime.A dense encoder is used to map any text passage to a d-dimensional real-valued vector, creating an index for all M passages used for retrieval.Due to its excellent performance as a retriever in RALM architectures, DPR has been widely adopted by researchers such as Lewis et al. (2020), Izacard and Grave (2020b), and Karpukhin et al. (2020).Izacard and Grave (2020a) takes a similar tactic, unlike DPR, in that he uses the same encoding function for questions and paragraphs through shared parameters.In order to further minimise the intervention and reduce the cost of manual annotation, Izacard et al. (2021) proposed another retriever called Contriever, which was trained using unsupervised data.It is based on successive dense embeddings and has a dual-encoder architecture.Average pooling was applied on the output of the previous layer to obtain one vector representation for each query or document.The similarity score between the query and each document was obtained by computing the dot product between their corresponding embeddings.Researchers (Siriwardhana et al., 2023), (Sachan et al., 2021), and (Guu et al., 2020) have used it as a retriever in the RALM architecture due to its ability to utilize unsupervised data.",
            "score": 0.6409629546547869,
            "section_title": "Word Embedding",
            "char_start_offset": 13287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 824,
                    "end": 843,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1614,
                    "end": 1641,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1670,
                    "end": 1688,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1834716796875
        },
        {
            "corpus_id": "257687445",
            "title": "Modular Retrieval for Generalization and Interpretation",
            "text": "As shown in Table 10, compared to the fine-tuned DPR model (DPR-vanilla), REMOP can achieve comparable zero-shot retrieval performance with fewer trainable parameters. The DPR-prompt model, which is only trained on a single prefix prompt (2.3M parameters), shows relatively poor performance on zero-shot retrieval tasks; and REMOP, using module composition to enhance model generalizability, outperforms the DPR-prompt on all the evaluation tasks. Compared to the other listed retrieval methods, REMOP shows its advantage in parameter-efficiency and generalizability that only training on a small number of parameters (2.3M\u00d77) can achieve comparable performance. It is worth noting that most of the dense passage retrieval methods and sparse passage retrieval methods underperform the basic lexical retrieval method BM25 on zero-shot retrieval tasks, and we give an analysis of this phenomenon in Section 5.3.",
            "score": 0.6394226642326648,
            "section_title": "Experimental Results",
            "char_start_offset": 28784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 909
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1419677734375
        },
        {
            "corpus_id": "269740786",
            "title": "ProCIS: A Benchmark for Proactive Retrieval in Conversations",
            "text": "To provide benchmark results, we evaluate a wide range of methods on our datasets.Four of them are well-known existing methods from different categories, such as term-matching, neural sparse retrieval, neural single-vector dense retrieval, and neural multi-vector dense retrieval.We also introduce a novel approach specifically designed for this task that deals with a long sequence as input, called LMGR.\n\nBM25 is a widely used ranking function that improves upon TF-IDF by incorporating term frequency saturation and document length normalization.It is the most common baseline in information retrieval tasks.For BM25 retrieval in our experiments, we used tantivy. 8 SPLADE is a neural retrieval model that uses sparse representations for documents and queries.It is based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations.This model offers a trade-off between effectiveness and efficiency, and is trained end-to-end in a single stage [11].For SPLADE, as well as ColBERT below, we relied on the implementations from the tevatron library [13] for our experiments.\n\nANCE is a dual-encoder neural retrieval model that uses dense representations to measure the similarity between queries and documents.It addresses the discrepancy between the data distribution used in training and testing by using an Approximate Nearest 8 https://github.com/quickwit-oss/tantivyNeighbor (ANN) index of the corpus to select more realistic negative training instances [43].We used the official implementation 9for training.\n\nColBERT is multi-vector dense retrieval model.It introduces a late interaction architecture that independently encodes the query and the document using BERT [10] and then employs a cheap yet powerful interaction step that models their similarity [19].\n\nLanguage Model Grounded Retrieval.The Language Model Grounded Retrieval (LMGR) framework, which we propose for this task, is inspired by the two-stage zero-shot entity linking approach of the BLINK model [42].For each entity mentioned in a text, BLINK first retrieves some candidates using a dense retrieval model and then re-ranks them with a cross-encoder to pick the final result.",
            "score": 0.6389323246444002,
            "section_title": "RETRIEVAL METHODS",
            "char_start_offset": 23797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 405
                },
                {
                    "start": 407,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 763
                },
                {
                    "start": 763,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1136
                },
                {
                    "start": 1138,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1576
                },
                {
                    "start": 1578,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1829
                },
                {
                    "start": 1831,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 2040
                },
                {
                    "start": 2040,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1735,
                    "end": 1739,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1824,
                    "end": 1828,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2035,
                    "end": 2039,
                    "matchedPaperCorpusId": "263877300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16015625
        },
        {
            "corpus_id": "248377381",
            "title": "Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval",
            "text": "Neural information retrieval (IR) performs query-passage matching at a semantic level, often using a dual-encoder architecture that encodes the queries and the passages separately. Examples of such models include the Dense Passage Retriever (DPR) [9] and ANCE [23], which fine-tune transformer-based [20] pre-trained language models [4] to compute contextualized representations of queries and passages. Table 1: Retrieval scores from DPR for the passage in Figure 1, against both a gold-standard question from NQ and three synthetic questions. The important terms in the question, that are also in the passage, are shown in italic. \n\nIn this paper, we first uncover a shortcoming in the passage encoder of such a dual-encoder IR model, namely DPR, which stems from its sparse attention pattern. To illustrate, in Figure 1 we show a heatmap of the attention weights of DPR's passage encoder over different tokens of an example passage (taken from the Natural Questions (NQ) dataset [10]). We can see that the attention given to many potentially important words and phrases, e.g, academy of management and twentieth century, are rather low. \n\nWhat is the effect of such attention, or lack thereof, on retrieval performance? Table 1 shows DPR's retrieval scores for a goldstandard question (from the NQ dataset) and three automatically generated synthetic questions (details in Section 2) when paired with the passage of Figure 1. The gold-standard question, which overlaps highly with the well-attended first sentence of the passage, receives a relatively high retrieval score. Among the synthetic questions, the one that refers to the highest-attended entity (principles of scientific management) gets the highest score, whereas the ones about To further quantify this, we randomly sampled 20k passages from Wikipedia and identified named entities that received the highest and lowest attentions from the DPR passage encoder (using the process described in Section 2.1). We then generated synthetic questions corresponding to those entities (using the process of Section 2.2).",
            "score": 0.637645467158618,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1695556640625
        },
        {
            "corpus_id": "271730744",
            "title": "Coherence-based Query Performance Measures for Dense Retrieval",
            "text": "More recently, pre-trained language models (PLMs) have introduced neural network architectures that encode the embeddings of queries and documents [15,27,28,56], and have led to increased retrieval effectiveness. Often, a BERT-based model is trained for use as a reranker of the result retrieved by (e.g.) BM25 [41] -such cross-encoders include BERT_CLS [36] and monoT5 [37]. On the other hand, dense retrieval approaches [26,56] are increasingly popular, whereby embedding-based representations of documents are indexed, and those with the similar embeddings to the query are identified through nearest-neighbour search (e.g. ANCE [56], TCT-ColBERT [28]). Compared to reranking setups, dense retrieval is attractive as recall is not limited by the initial BM25 retrieval approach, and improvements in the PLM can improve all aspects of the retrieval effectiveness. Therefore, dense retrieval models inspire us to develop predictors that are effective for predicting their rankings. \n\nIn parallel, neural architectures have also been adopted as methods for predicting query difficulty. These post-retrieval methods are supervised, and use refined neural architectures in order to produce a final performance estimate [2,14,23,58]. For instance, BERT-QPP [2] fine-tunes a BERT [15] model for QPP by estimating the relevance of the top-ranked document retrieved for each query. However, its performance is lower or outperformed by unsupervised predictors when using advanced retrieval methods and the TREC Deep Learning datasets [18]. In our view, the problem lies in the mismatch of representations between predictor and ranking, which is best described in Figure 1. On top, we see the pipeline resulting from a BM25 ranking, and, at the bottom, a ranking from a dense retrieval system [26,56]. While BERT-based QPP techniques can be used to predict the effectiveness of BM25 [2,14,23,58], single-representation dense retrieval models already contain representations that can accurately predict their corresponding ranking, thus eliminating the need to apply Step 3 (e.g.",
            "score": 0.6374422181025052,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1217,
                    "end": 1220,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "250340128"
                },
                {
                    "start": 1223,
                    "end": 1226,
                    "matchedPaperCorpusId": "197466307"
                },
                {
                    "start": 1226,
                    "end": 1229,
                    "matchedPaperCorpusId": "24786698"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1527,
                    "end": 1531,
                    "matchedPaperCorpusId": "257038618"
                },
                {
                    "start": 1785,
                    "end": 1789,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26611328125
        },
        {
            "corpus_id": "273654410",
            "title": "Deep Learning Based Dense Retrieval: A Comparative Study",
            "text": "Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but their robustness against tokenizer poisoning remains underexplored. In this work, we assess the vulnerability of dense retrieval systems to poisoned tokenizers by evaluating models such as BERT, Dense Passage Retrieval (DPR), Contriever, SimCSE, and ANCE. We find that supervised models like BERT and DPR experience significant performance degradation when tokenizers are compromised, while unsupervised models like ANCE show greater resilience. Our experiments reveal that even small perturbations can severely impact retrieval accuracy, highlighting the need for robust defenses in critical applications.",
            "score": 0.6362012977190131,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "267751308",
            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
            "text": "Dense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
            "score": 0.6356325439330948,
            "section_title": "Related Work",
            "char_start_offset": 6374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 65,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 65,
                    "end": 87,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 557,
                    "end": 589,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1146,
                    "end": 1162,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1203,
                    "end": 1226,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1389,
                    "end": 1406,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453369140625
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Monolingual Dense Passage Retrieval The past few years have witnessed growing interest in monolingual dense passage retrieval. DPR (Karpukhin et al., 2020) is built on a bi-encoder architecture, which is initialized with BERT (Devlin et al., 2019) and outperforms early dense retrieval methods. \n\nRocketQA (Qu et al., 2021) first mines hard negative samples with a trained retrieval model and then uses the mined negative samples to re-train the model. However, RocketQA requires a pretrained cross-encoder to filter out false negative samples, which is not efficient and must be trained in advance. TAS-B (Hofst\u00e4tter et al., 2021) is also a retrieval model using the bi-encoder architecture. \n\nIt utilizes topic-aware sampling to improve training with in-batch negative samples, and applies a dual-teacher supervision paradigm to achieve better knowledge distillation from both a crossencoder and a ColBERT (Khattab and Zaharia, 2020) teacher model simultaneously. Other studies further apply hard negative sample mining to train dense passage retrieval models. Gao et al. (2021b) and Karpukhin et al. (2020) use BM25 (Robertson et al., 2009) top passages as hard negative samples. ANCE (Xiong et al., 2021) enhances hard negative sampling by dynamically mining hard negative samples in the training phase. However, it requires periodically rebuilding the index and refreshing hard negative samples, which greatly increases computational cost. Zhan et al. (2021) combine static BM25 hard negative samples with dynamic hard negative samples retrieved from the entire corpus by the model at the current training step. \n\nMultilingual and Cross-lingual Dense Passage Retrieval Researchers have been utilizing crosslingual knowledge transfer to enhance monolingual retrieval for low-resource languages since the advent of multilingual pretrained language models. Both MacAvaney et al. (2020b) and Shi et al. (2020) investigate zero-shot transfer using a cross-encoder architecture, in which they first fine-tune mBERT on the source language, then apply the model to the target language directly. However, the crossencoder architecture they use is slow in practice.",
            "score": 0.6336722154910135,
            "section_title": "Related Work",
            "char_start_offset": 6663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 155,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 606,
                    "end": 631,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 908,
                    "end": 934,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1063,
                    "end": 1081,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 1086,
                    "end": 1109,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1119,
                    "end": 1143,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1445,
                    "end": 1463,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1864,
                    "end": 1888,
                    "matchedPaperCorpusId": "209515542"
                },
                {
                    "start": 1893,
                    "end": 1910,
                    "matchedPaperCorpusId": "226284008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336181640625
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "Information retrieval (IR) is the task of finding relevant knowledge or passages corresponding to a given query. Traditional exact lexical-matching approaches, such as TF-IDF (term frequency and inverse document frequency) or BM25 (Robertson et al. 1995;Robertson and Zaragoza 2009), have performed well on some IR tasks. Recently, neural retrieval with contextualized dense sentence embedding has shown to be effective for tasks such as open-domain question answering (e.g., Karpukhin et al. 2020;Xiong et al. 2021). \n\nDense retriever, or dense passage retrieval (DPR), uses a pair of neural language models as a bi-encoder to obtain latent representations of questions and passages (Lee, Chang, and Toutanova 2019). Bi-encoders require to be finetuned with contrastive learning to embed a question and the relevant passages on semantically similar sentence vectors. Recent works on DPR reported in-domain (tested on Accepted to Workshop on Knowledge Augmented Methods for Natural Language Processing, in conjunction with AAAI 2023. the same dataset which was used to fine-tune) retrieval performances exceeding those from sparse retrievers such as BM25 (Karpukhin et al. 2020). \n\nHowever, it has been shown that conventional DPRs struggle with retrieval on BEIR benchmark (Thakur et al. 2021) including various domains and even on EntityQuestions (EQ) (Sciavolino et al. 2021) built within a confined domain (Wikipedia), whereas BM25 shows better performance on both of them. This indicates that the generalizability of DPR is limited not only out of domain but even in domain when a named entity in a question is a dominant clue for retrieval. In order to address this, Sciavolino et al. (2021) showed that DPR trained with PAQ dataset (Lewis et al. 2021) which consists of 65 million of question-answer pairs containing many of named entities in Wikipedia still performs far worse than BM25 in EQ dataset. The results showed that supervised learning with a huge dataset that covers the domain extensively could not lead to good generalization within the domain.",
            "score": 0.6329165023477898,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 254,
                    "end": 282,
                    "matchedPaperCorpusId": "3641284"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 684,
                    "end": 716,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1155,
                    "end": 1177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1673,
                    "end": 1697,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1739,
                    "end": 1757,
                    "matchedPaperCorpusId": "231924957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29345703125
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "Our research questions and experiments have focused exclusively on interpolation-based re-ranking using dual-encoders and Fast-Forward indexes. However, the most common application of dualencoders in the field of IR is the use as dense retrieval models; a natural question that occurs is, whether the encoders proposed in Section 5 can be used for more efficient dense retrieval. \n\nIn Table 6, we present passage and document retrieval results on the MS MARCO corpus. Dense retrievers use a FAISS [32] vector index; no interpolation or re-ranking is performed. It is immediately obvious that our models do not achieve competitive results; on the contrary, the embedding-based encoder yields far worse performance than dense retrievers and even BM25, and even the attention-based encoder fails to improve over sparse retrieval. \n\nFrom these results, we infer that the models we trained are not suitable for dense retrieval. However, we assume that the main reason for this is not the architecture of the query encoder, but instead the following: \n\n\u2022 We use a simple in-batch negative sampling strategy [34], which has been shown to be less effective than more involved strategies [51,53,86,88]. \u2022 The hardware we use for training the models is limiting w.r.t. the batch size and thus the number of negative samples, i.e., we cannot use a batch size greater than 4. \u2022 We perform validation and early stopping based on re-ranking. \n\nConsidering the points above, we expect that our dual-encoder models, including ones with lightweight encoders, could also be used in retrieval settings if the shortcomings of the training setup are addressed, for example, by using more powerful hardware and state-of-the-art training approaches. On the other hand, we argue that the fact that our models perform well in the reranking setting (see Section 7) shows that it is both easier and more efficient (in terms of time and resources) to train models to be used with Fast-Forward indexes instead of for dense retrieval.",
            "score": 0.6320210095424079,
            "section_title": "Efficient Encoders for Dense Retrieval",
            "char_start_offset": 63161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1373291015625
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Dense Passage Retrieval (DPR), introduced by Karpukhin et al. ( 2020), represents a more modern approach to information retrieval. It uses a dense vector space in which both the query and the documents are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, where the query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xiong et. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query and documents, making it highly effective for open-domain question-answering tasks. The strength of DPR lies in its ability to retrieve relevant information based on semantic meaning rather than keyword matching. By training the retriever on a large corpus of question-answer pairs, DPR can find documents that are contextually related to the query, even when the query and the document do not share exact terms. Recent research has further improved DPR by integrating it with pre-trained language models and an example is LLM adapted for the dense RetrievAl approach (Li et. al. 2023)",
            "score": 0.6290182033647154,
            "section_title": "Dense Passage Retrieval (DPR)",
            "char_start_offset": 14303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397216796875
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "Our contributions are: \n\n-We propose a paragraph aggregation retrieval model (PARM) for dense documentto-document retrieval and demonstrate higher retrieval effectiveness for dense retrieval with PARM compared to retrieval without PARM and to lexical retrieval with PARM. -We propose vector-based aggregation with reciprocal rank fusion weighting (VRRF) for dense retrieval with PARM and find that VRRF leads to the highest recall for PARM compared to other aggregation strategies. -We investigate training DPR for PARM and compare the impact of fewer, more accurate paragraph-level labels to more, potentially noisy document-level labels. -We publish the code at https://github.com/sophiaalthammer/parm 2 Related work Dense passage retrieval. Improving the first stage retrieval with DPR models is a rapidly growing area in neural IR, mostly focusing on the web domain. Karpukhin et al. [19] propose dense passage retrieval for open-domain QA using BERT models as bi-encoder for the query and the passage. With ANCE, Xiong et al. [39] train a DPR model for open-domain QA with sampling negatives from the continuously updated index. Efficiently training DPR models with distillation [17] and balanced topic aware sampling [18] has demonstrated to improve the retrieval effectiveness. As opposed to this prior work, we move from dense passage to dense document-to-document retrieval and propose PARM to use dense retrieval for document-to-document tasks. \n\nDocument retrieval. The passage level influence for retrieval of documents has been analyzed in multiple works [7,22,37,38] and shown to be beneficial, but in these works the focus lies on passage-to-document retrieval. Cohan et al. [9] present documentlevel representation learning strategies for ranking, however the input length remains bounded by 512 tokens and only title and abstract of the document are considered. Abolghasemi et al. [1] present multi-task learning for document-to-document retrieval. Liu et al. [40] propose similar document matching for documents up to a length of 2048 however here the input length is still bounded and the computational cost of training and using the model is increased.",
            "score": 0.6279893802816432,
            "section_title": "Introduction",
            "char_start_offset": 4493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "10714285"
                },
                {
                    "start": 1574,
                    "end": 1577,
                    "matchedPaperCorpusId": "215875147"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "197928351"
                },
                {
                    "start": 1690,
                    "end": 1693,
                    "matchedPaperCorpusId": "215768677"
                },
                {
                    "start": 1898,
                    "end": 1901,
                    "matchedPaperCorpusId": "246442016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "277043051",
            "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
            "text": "RAG has become an influential approach for tasks requiring knowledge-intensive natural language processing, particularly in open-domain question answering [4]. The foundational work by Lewis et al. [4] demonstrated that combining dense retrieval systems with generative models significantly improved accuracy in generating factually correct responses. In such setups, a dense embedding index of textual content, created using transformer-based encoders, supports efficient retrieval by measuring the semantic similarity between the query and stored documents. \n\nAt the core of RAG systems lies the embedding model, usually based on transformer architectures like BERT, which encode queries and documents into dense vector representations. Dense Passage Retrieval (DPR), introduced by Karpukhin et al. [5], exemplifies this approach by training dual-encoder models to produce embeddings that closely align queries with relevant document passages. The key idea behind DPR is to use contrastive learning methods, ensuring queries are embedded in close proximity to their corresponding answer-containing passages while distancing unrelated texts. However, despite extensive training, discrepancies between the query and document embedding spaces can still occur, particularly if the query formulation significantly differs from the format of the indexed documents. These differences often result in reduced retrieval effectiveness, especially when generic embeddings or pre-trained models without fine-tuning are employed. \n\nRecent advancements have aimed at reducing these embedding mismatches through innovative strategies. One such method is the Hypothetical Document Embedding (HyDE), proposed by Gao et al. [6]. In HyDE, an instruction-driven language model like GPT-3 generates a plausible but hypothetical document in response to a query, transforming the original query into a form more akin to a assertive statement. Embedding this hypothetical text, rather than the original question, results in vectors more closely aligned with document embeddings, thereby improving retrieval accuracy. This generative step effectively addresses the inherent mismatch between interrogative queries and assertive knowledge statements, resulting in enhanced zeroshot retrieval capabilities. By reframing the query as a factual-like representation, HyDE achieves substantial performance gains without requiring additional annotated data for training.",
            "score": 0.6277616544924629,
            "section_title": "Retrieval-Augmented Generation (RAG) and Vector Embeddings",
            "char_start_offset": 8347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2439
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369384765625
        },
        {
            "corpus_id": "247450700",
            "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
            "text": "the large language model. However, since our interpolation and perturbation methods only manipulate the already   obtained representations of the documents from the encoder E D , we don't have to newly generate document texts and also to forward generated documents into the model, which greatly saves time and memory (see Table 3). We provide a detailed analysis and discussion of efficiency in Appendix B.  (Kwiatkowski et al., 2019); 2) TriviaQA (TQA) is a QA collection scraped from the Web (Joshi et al., 2017).\n\nRetrieval Models 1) BM25 is a sparse termbased retrieval model based on TF-IDF (Robertson et al., 1994). 2) Dense Passage Retriever (DPR) is a dense retrieval model with a dual-encoder of query-document pairs (Karpukhin et al., 2020). 3) DPR with Query Augmentation (DPR w/ QA) augments pairs with query generation for the document, adopting (Liang et al., 2020;Mao et al., 2021a). 4) DPR with Document Augmentation (DPR w/ DA) augments pairs by replacing words in the document (Ma, 2019). 5) DPR with Axiomatic Regularization (DPR w/ AR) regularizes the retrieval model to satisfy certain axioms (Rosset et al., 2019). 6) DAR is ours with interpolation and perturbation of document representations. Implementation Details For the dense retrieval model based on the DPR framework, we refer to the publicly available code from DPR (Karpukhin et al., 2020). We set the training epoch as 25 and batch size as 32 under academic budgets with a single GeForce RTX 3090 GPU having 24GB memory. We use in-batch negative sampling as our negative sampling strategy without hard negative samples. Also, we retrieve 100 passages per question. We use both interpolation and perturbation schemes for our augmentation methods. Specifically, for the interpolation method, we set \u03bb \u2208 [0, 1] in equation 3 to be sampled from the uniform distribution. Also, for the perturbation method, we set the dropping rate as 0.1, and the",
            "score": 0.6259091833819814,
            "section_title": "Method",
            "char_start_offset": 10233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 515,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 597,
                    "end": 621,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 727,
                    "end": 751,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 880,
                    "end": 898,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1348,
                    "end": 1372,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1573486328125
        },
        {
            "corpus_id": "258685571",
            "title": "Efficient and Effective Tree-based and Neural Learning to Rank",
            "text": "One of the major innovations in neural LtR research was the evolution of \"cross-encoders\" such as monoBERT and duoBERT to two-tower or \"bi-encoder\" models. Instead of learning a parameterized function (like BERT) that takes a query-document pair (or multiple documents) as input and simultaneously learns representations (i.e., features) and predicts their relevance, bi-encoders disentangle the relevance prediction function from representation learning, and simplify the former as much as possible. The idea is to push much of the complex, time-consuming inference operations offline and thereby speed up query processing.\n\nIn its most basic form, this design resembles the representation-based neural ranker architecture of Figure 7.1(a), but where \u03c6 d and \u03c6 q are pre-trained large language models that may be further fine-tuned. Because the representation of documents is independent of queries, we may store the document vectors in an offline index and, during retrieval, compute the representation of the query and find its closest document vectors. This paradigm is often referred to as \"dense retrieval\" or \"semantic search,\" which is typically solved using a k nearest neighbor (NN) search or k approximate nearest neighbor (ANN) search algorithm. We illustrate this procedure in Figure 8.3.\n\nThe feasibility of this basic idea was demonstrated by several works (Zhan et al., 2020;Karpukhin et al., 2020;Ma et al., 2021a;Qu et al., 2021). Karpukhin et al. (2020) present what they call Dense Passage Retrieval (DPR) and show that dense vector representations can indeed be used to perform the retrieval task following the recipe above. Interestingly, effective representations can be learnt from a small number of questions and passages (in a question-answering task) by a simple bi-encoder framework. They evaluate DPR on a wide range of open-domain QA datasets, with the results showing that DPR outperforms a strong BM25 system by 9%-19% absolute points in terms of top-20 passage retrieval accuracy.\n\nIn the general dense retrieval framework of Figure 8.3, there are two key factors that contribute to the overall efficiency and effectiveness: the ANN algorithm itself",
            "score": 0.6247701746145141,
            "section_title": "Dense retrieval",
            "char_start_offset": 149848,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1414,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1449,
                    "end": 1472,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "Besides the dense pseudo-relevance feedback retrieval based on the dense external retrieval as the first stage, we further investigate the performance of the external expansion on the sparse retrieval scenarios. More specifically, we study two external sparse retrieval using BM25 and DPH as the initial stage for both single and multiple representation dense-PRF paradigms. \n\nTable 6 presents the results of the sparse external dense retrieval followed by the ColBERT-PRF query expansion and ColBERT retrieval for both Robust04 and WT10G. Firstly, compared with the baselines runs, we see that using either BM25 or DPH as initial stage retrieval models lead significantly improvement over ColBERT E2E and ColBERT-PRF but underperform the ColBERT-PRF model on the target on other metrics as well as other baseline runs. This demonstrates that sparse external expansion is benefit for retrieving more relevant documents. Secondly, compared with the dense external dense retrieval models, sparse external dense retrieval model exhibit lower performance. These observations are consistent for both Robust04 and WT10G experiments. This indicates that in an end-to-end ColBERT-PRF retrieval paradigm, the dense retrieval is more useful than the sparse retrieval as the first stage to produce high quality pseudo-relevance feedback documents. Moreover, in an single-representation based ANCE-PRF scenario, Table 9 shows the performance of sparse external dense retrieval models for both Robust04 and WT10G datasets. Firstly, we analyse the top-half table for Robust04. We make the following observations for both the sparse external ANCE-PRF dense retrieval models: (1) they exhibit higher performance than both ANCE (row (e)) and ANCE-PRF (row (f)) performed only on target collection; (2) they show slightly lower performance compared with ANCE reranking models in row (e) and row (f); (3) they show similar performance with the dense external dense retrieval models.",
            "score": 0.6245184412237477,
            "section_title": "RQ3: Sparse-obtained external feedback for dense retrieval",
            "char_start_offset": 46447,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1963
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Recent works have investigated the use of autoregressive language models to generate identifier strings for documents as an intermediate target for retrieval (Yu et al., 2022), such as Wikipedia page titles (De Cao et al., 2020), root-to-leaf paths in a hierarchical cluster tree (Tay et al., 2022), or distinctive n-grams that can be mapped to full passages (Bevilacqua et al., 2022)  index (DSI), and neural document indexers (NDI). \n\nCompared to traditional dense document retrievers, these methods leverage a generation model to produce the document indexes. By forcing the generation model to explain every token in the question and document using cross-attention, the generation abilities of the model significantly improve. \n\nOur work is closely related to these works, showing experimentally that properly prompting pretrained large language models can achieve better performance than traditional dense retrieval models (Ouyang et al., 2022;Yu et al., 2023) .",
            "score": 0.6244492540586868,
            "section_title": "Autoregressive Search Engines",
            "char_start_offset": 5642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 967
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 228,
                    "matchedPaperCorpusId": "222125277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "258762633",
            "title": "TOME: A Two-stage Approach for Model-based Retrieval",
            "text": "Text Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009). In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019;Raffel et al., 2020), dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020;Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021). The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020;Ren et al., 2021a,b;Asai et al., 2021;Zhou et al., 2022a). Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\n\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021;Tay et al., 2022). These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021), which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c;Bevilacqua et al., 2022;Zhuang et al., 2022;Wang et al., 2022;Lee et al., 2022;Chen et al., 2022;Zhou et al., 2022b). As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to",
            "score": 0.6240009852702478,
            "section_title": "Related Works",
            "char_start_offset": 4771,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 387,
                    "end": 408,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 528,
                    "end": 555,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 555,
                    "end": 578,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 578,
                    "end": 597,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 597,
                    "end": 613,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 798,
                    "end": 817,
                    "matchedPaperCorpusId": "253080762"
                },
                {
                    "start": 1220,
                    "end": 1237,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1416,
                    "end": 1434,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1621,
                    "end": 1641,
                    "matchedPaperCorpusId": "247187834"
                },
                {
                    "start": 1641,
                    "end": 1665,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2410888671875
        },
        {
            "corpus_id": "277501850",
            "title": "LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback",
            "text": "The rapid evolution of information retrieval systems has been largely influenced by the emergence of neural language models, particularly in the domain of dense retrieval [40,46,20,21,12]. While traditional lexical retrieval methods rely on exact term matching, dense retrievers leverage semantic understanding through learned representations, enabling more effective passage retrieval. The introduction of BERT-based dense retrievers is a great advancement in this field, demonstrating superior performance in capturing semantic relationships between queries and passages compared with lexical models at that time. \n\nRecent developments in Large Language Models (LLMs) have pushed the boundaries of natural language understanding and generation even further, presenting new opportunities for enhancing information retrieval systems [30,35,36]. These models, with their improved semantic comprehension and contextual understanding, have shown remarkable capabilities across various natural language processing tasks. Methods that adapt these LLMs to the dense retrieval task have also recently emerged [2,25,16,36], leading to improved retrieval effectiveness, even without requiring the contrastive training of the dense representations typical of other dense retrieval backbones [48]. However, their potential in combining with PRF, particularly through techniques like Vector-based PRF, remains relatively unexplored. \n\nVector Pseudo Relevance Feedback (VPRF), initially proposed for BERTbased dense retrievers, has demonstrated promising results in improving retrieval performance through iterative refinement of query representations [17,18]. This technique leverages the initial retrieval results to modify the query embeddings in vector space, effectively capturing relevant information within the embeddings that might be missing from the original query. The success of VPRF with BERTbased models raises an important question: Can this technique be effectively generalized to current more powerful LLM-based dense retrievers? \n\nThis paper presents LLM-VPRF, an extension of the VPRF technique [17] adapted for Large Language Model-based dense retrievers. Our work investigates the generalizability of VPRF across different models, focusing particularly on its application to state-of-the-art LLM-based retrievers [2,25,48]. We examine whether the improvements in retrieval performance observed with BERT-based models(i.e.",
            "score": 0.6220378531822144,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2331
                },
                {
                    "start": 2332,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "269009682"
                },
                {
                    "start": 1108,
                    "end": 1111,
                    "matchedPaperCorpusId": "267523323"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "269449435"
                },
                {
                    "start": 1639,
                    "end": 1643,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 1643,
                    "end": 1646,
                    "matchedPaperCorpusId": "254594093"
                },
                {
                    "start": 2101,
                    "end": 2105,
                    "matchedPaperCorpusId": "237290157"
                },
                {
                    "start": 2321,
                    "end": 2324,
                    "matchedPaperCorpusId": "269009682"
                },
                {
                    "start": 2327,
                    "end": 2330,
                    "matchedPaperCorpusId": "269449435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47802734375
        },
        {
            "corpus_id": "259203489",
            "title": "GLIMMER: generalized late-interaction memory reranker",
            "text": "In particular, our work is very closely based on LUMEN (de Jong et al., 2023). \n\nReranking Like the language model itself, retrieval procedures face a trade-off between expensive ranking with full interaction (Chen et al., 2020) and the more common dual encoder approaches such as DPR (Karpukhin et al., 2020) and GTR (Ni et al., 2021) that scores based on inner product similarity with a corpus of pre-computed passage representations. Often different models for retrieval are applied in a pipeline approach, with an initial cheap scoring model followed by a more powerful and expensive reranker (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022). Many rerankers also make use of late interaction to obtain a good trade-off between ranking quality and speed, such as COLBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022), PreTTR (MacAvaney et al., 2020), SDR (Cohen et al., 2022), and Poly-encoders (Humeau et al., 2020). GLIMMER combines late-interaction memory and reranking into a single model, sharing the pre-computed representations for both use cases. \n\nLearning to retrieve Retrieval models are often trained with supervised data (Karpukhin et al., 2020;Ni et al., 2021), using gold retrievals from datasets such as MS-MARCO (Nguyen et al., 2016) or TREC CAR (Dietz et al., 2018). When selecting passage to use for retrieval-augmented generation, we have an additional signal, namely which passages are most helpful for the reader model. A number of existing works use this signal to improve retrieval (Guu et al., 2020;Sachan et al., 2021;Jiang et al., 2022;Sachan et al., 2021;Izacard et al., 2022). We follow ATLAS (Izacard et al., 2022) and employ perplexity distillation to train our reranker to select passages that help lower reader model perplexity.",
            "score": 0.6206592121827396,
            "section_title": "Related Work",
            "char_start_offset": 19685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "228093856"
                },
                {
                    "start": 285,
                    "end": 309,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 633,
                    "end": 649,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 778,
                    "end": 805,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 805,
                    "end": 827,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 867,
                    "end": 887,
                    "matchedPaperCorpusId": "238353991"
                },
                {
                    "start": 907,
                    "end": 928,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1146,
                    "end": 1170,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1241,
                    "end": 1262,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1275,
                    "end": 1295,
                    "matchedPaperCorpusId": "4987800"
                },
                {
                    "start": 1536,
                    "end": 1556,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1556,
                    "end": 1575,
                    "matchedPaperCorpusId": "254246471"
                },
                {
                    "start": 1575,
                    "end": 1595,
                    "matchedPaperCorpusId": "235390519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1385498046875
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "\u2022 The pseudo-relevance feedback information from the top-returned documents in multiple representation dense retrieval is beneficial for improving the retrieval effectiveness on passage retrieval (Section 5) and document retrieval (Section 6). Indeed, our proposed pseudorelevance feedback mechanism can significantly improve the retrieval effectiveness over than ColBERT end-to-end model, the single representation dense retrieval models, as well as most of the baselines for both passage ranking and document ranking tasks; \u2022 Techniques based on statistical information, namely IDF and ICTF, and on embedding coherency, namely Mean Cosine Similarity, can be used to measure the informativeness of expansion embeddings of ColBERT-PRF (Section 7); \u2022 The trade-off of the retrieval effectiveness and efficiency of ColBERT-PRF can be attained using different clustering techniques and/or candidate selection techniques based on approximate scoring (Section 8). \n\nOverall, our work makes it feasible to implement the pseudo-relevance feedback technique in a multiple-representation dense retrieval setting. In particular, the provided extensive experimental results demonstrate the effectiveness of our proposed ColBERT-PRF model. However, how this proposed dense PRF technique can be applied to the single-representation dense retrieval models remains an open problem. In addition, while the performance of most of the queries can benefit from the expansion embeddings, the performance of some of the queries is still degraded. Thus, a more cautious design that applies selective query embedding expansion will likely alleviate this issue. We leave this as one of our future works. instance, for the ColBERT-PRF (Bo1) implementation, the expansion embeddings are weighted according to the following equation: \n\nwhere  =    /  ,    denotes the frequency of (BERT WordPiece) token  in the pseudorelevant feedback documents and   denotes the number of feedback documents.    denotes the number of unique tokens in the pseudo-relevant document set. \n\nSimilarly, for the ColBERT-PRF (RM3) variant, the expansion embeddings are selected using: \n\nwhere (, ) =",
            "score": 0.6200609163648128,
            "section_title": "RQ8 -Clustering",
            "char_start_offset": 80424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2135
                },
                {
                    "start": 2138,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1795654296875
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
            "score": 0.6192313579249983,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "270688320",
            "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models",
            "text": "Passage ranking, which aims to rank each passage in a large corpus according to its relevance to the user's information need expressed in a short query, is an important task in information retrieval and natural language processing and plays a crucial role in many applications such as web search and retrieval-augmented generation. To achieve both effectiveness and efficiency, current mainstream approaches usually follow a two-stage paradigm known as \"retrievalthen-rerank\", which involves efficiently retrieving a set of candidates first, and further reranking them with a reranker to boost the effectiveness [18,25]. \n\nIn the first retrieval stage, dense retrieval models based on a bi-encoder architecture are widely used [12]. Trained on large-scale datasets of text pairs through contrastive learning, these models can encode text into a low-dimensional dense embedding and capture the relevance between query and passage using vector similarity. \n\nIn the second reranking stage, we can employ more sophisticated models for better ranking performance. A common reranking model is a supervised model based on the cross-encoder design [25]. With the emergence of large language models (LLMs), such as , a series of studies have tried to leverage LLMs' text comprehension and reasoning abilities for zero-shot reranking. Typically, there are three main prompting approaches: pointwise [13,32], pairwise [30], and listwise [28,33]. Among these methods, listwise approaches like RankGPT [33] have achieved state-of-the-art performance by directly producing a final ranking list for multiple passages, rather than merely assessing the relevance of a single passage or the relative position between two passages. \n\nWhile the listwise approaches demonstrate good performance in the reranking task, they are limited by two challenges. Firstly, some LLMs are limited by context length and cannot rank multiple passages simultaneously, necessitating techniques such as a sliding window strategy to complete the ranking process [33]. Secondly, incorporating entire passages into prompts significantly increases inference costs, resulting in high latency in practice [2], which is untenable in the ranking scenario.",
            "score": 0.6190855744817575,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "9346130"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 2023,
                    "end": 2027,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "254877418",
            "title": "Adam: Dense Retrieval Distillation with Adaptive Dark Examples",
            "text": "Information retrieval (IR) that aims to identify relevant passages for a given query is an important topic for both academic and industrial areas, and has powered many downstream tasks such as opendomain QA (Chen et al., 2017) and knowledgegrounded conversation (Dinan et al., 2018). Typically, IR systems usually follow the retrieve-andre-rank paradigm (Hofst\u00e4tter et al., 2020;Huang et al., 2020;Zou et al., 2021) where a fast retriever -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.  (Zhou et al., 2023) over MS-MARCO. POS and NEG mean the distribution of positive and hard negatives respectively. The hard negatives are provided by RocketQAv2 (Ren et al., 2021c). \n\nfirst retrieved a bundle of relevant passages from a large-scale corpus through pre-built indices and then a more sophisticated ranker comes to re-rank these candidate passages to further obtain more accurate retrieval results. Under this paradigm, recent years have witnessed a growing number of works that utilize pre-trained language models (PLMs) (Qu et al., 2021;Gao and Callan, 2021b) as retrievers and rankers to build IR systems. Among these efforts, there are two commonly adopted architectures: cross-encoder (Devlin et al., 2019a) that measure the relevance of a query-passage pair through jointly modeling their deep interactions; dual-encoder (Karpukhin et al., 2020;Qu et al., 2021) that encodes queries and passages separately into dense representations and calculate the similarity. Although dual-encoders are efficient for billions of indices, they suffer from inferior performance compared with cross-encoders since they can't capture the fine-grained semantic relevance between the query and the passage due to the absence of their deep interactions (Luan et al., 2021a).",
            "score": 0.6187208596449602,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "249712198"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1006,
                    "end": 1023,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1174,
                    "end": 1195,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1311,
                    "end": 1335,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1335,
                    "end": 1351,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12408447265625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "Figure 2 depicts a pipeline for knowledge-intensive visual question answering tasks. As shown in the pipeline, the input to the dense retrieval model is asymmetric -query encoder takes multi-modal input (i.e., a question and an image), while the passage encoder takes a uni-modal text input (i.e., a passage from C). This asymmetric property in the input modalities makes it challenging to design an effective symmetric dense retrieval model. This is why the current state-of-the-art dense retrieval model proposed by Qu et al. [40] uses an asymmetric architecture, where a pre-trained multi-modal language model (i.e., LXMERT [53]) is used for query encoding and a pre-trained uni-modal language model (i.e., BERT [8]) is used for document encoding. Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B [17], ColBERT [24,49], RocketQA [41,44], and CLDRD [62], use symmetric architectures. Motivated by this observation, our goal is to learn a symmetric dense retrieval model for KI-VQA tasks. \n\nTo this aim, we study two alternative solutions. First, we convert all model inputs to a uni-modal textual form and then use uni-modal language models for both query and document encoding (Section 4.1). Second, we convert all inputs to the same multi-modal (text and image) form and then use multi-modal language models for both encoders (Section 4.2). We hypothesize that these two models learn complementary representations for the following reasons: (1) they take different input formats, and (2) the pre-training process and data in uni-modal and multi-modal language models are different.",
            "score": 0.616677888070836,
            "section_title": "DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK",
            "char_start_offset": 11555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1187,
                    "end": 1190,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380615234375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.",
            "score": 0.6166722016425659,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365966796875
        },
        {
            "corpus_id": "264110387",
            "title": "Hybrid Retrievers with Generative Re-Rankers",
            "text": "Recently, dense retrieval approaches have also been proposed. They are capable of capturing semantic matches, and attempt to overcome the (potential) lexical gap. Dense retrievers map queries and documents in a single, common dense vector space. A bi-encoder architecture based on BERT-type models demonstrated strong performance for various opendomain question answering tasks. \n\nAn important recent innovation for passage retrieval is the introduction of dense retrieval models that take advantage of a bi-encoder design. Bi-encoders produce two corresponding embeddings for a given two-sentence pair (e.g. a query and a passage), which can then be compared efficiently using cosine similarity. \n\nBi-encoders are used whenever a sentence embedding is needed in a vector space for efficient comparison in applications such as information retrieval, semantic search, or clustering. Cross-encoders would be the wrong choice for these applications, because a cross-encoder does not produce a sentence embedding; it processes both sentences simultaneously through the Transformer network, which is very computationally expensive with such a large scale of data. \n\nWith sufficient labeled data, we can learn encoders (typically, Transformer-based models) that project queries and documents into a dense (semantic) representation space (e.g. 768 dimensions) where the relevance ranking can be recast as a nearest neighbor search over representation vectors [4]. \n\nBi-encoders can be used also as re-rankers, working not on all documents in corpora, but only on subsets of them. There are settings in which the first-stage retriever returns a limited number of documents and passes them to the re-ranker. Reranking can be also performed as a second stage retrieve on a limited collection of documents from stage 1 without any top-k constraints. \n\nThe two most popular bi-encoders are DPR and ANCE. DPR [5] is a two-tower bi-encoder trained with the hard negatives and single-batch negatives of a single BM25. The Multi-DPR model is a BERT-base-uncased model trained on four QA datasets: NQ, TriviaQA, WebQuestions, and Curat-edTREC.",
            "score": 0.6165874766112543,
            "section_title": "B. Dense retrieval",
            "char_start_offset": 5433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "We use representative samples of models from different families: \n\n-BM25 [42] is a representative sparse retrieval model that estimates the relevance based on term frequency, document length, and document frequency. -DPR [19] is a representative dense retrieval model that uses dual-encoder architecture and is trained with in-batch negatives and a few hard negatives selected with BM25. -BART [23] is a Seq2Seq model applicable for sequence generation tasks. Following [3,7], we extract the query-title pairs from each dataset and fine-tune the BART for generative retrieval. -CorpusBrain [3] is a pre-trained generative IR model for knowledge-intensive language tasks. We fine-tune CorpusBrain on every specific downstream KILT task.",
            "score": 0.6162727821741792,
            "section_title": "Retrieval models",
            "char_start_offset": 14328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 67,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 735
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 473,
                    "end": 475,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1055908203125
        },
        {
            "corpus_id": "261076036",
            "title": "Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track",
            "text": "The text retrieval task is usually divided into two sub tasks according to the length of the text: passage retrieval and document retrieval. Whether it is a Document ranking or Passage ranking task, under the setting of full ranking, it generally needs to be processed in two stages: retrieval and ranking. Among them, the retrieval stage needs to quickly find the most relevant top-k candidates in the entire corpus set given the input query, and then a more complex and accurate ranking stage will be performed over the top-k candidates thus producing the final ranked result. \n\nFor the retrieval stage, BM25 [RZ + 09] is a bag-of-words retrieval method that ranks a set of documents based on the query terms appearing in each document and is one of the best retrieval algorithms. Recently, various improved methods [NLE19, FLPC21] based on BM25 have been proposed in the sparse retrieval research field. With the development of deep neural network models, the performance of dense retrieval models such as DPR [KOM + 20], and coCondenser [GC21,GC22] have surpassed traditional methods with a large margin. For the ranking stage, since the amount of processed data is greatly reduced, a more complex model structure can be adopted. Benefit from the excellent performance of large-scale pre-trained language model (e,g. BERT [DCLT19]) on various natural language processing tasks, the ranking model is also gradually turning to the BERT model as the relevance scoring model. Specifically, the query and doc will be concatenated as the input of BERT or other pre-trained language models like RoBerta [LOG + 19], ERNIE [LLL + 22], ELEC-TRA [CLLM20], and Deberta [HLGC20]. \n\nBased on this multi-stage processing method, our main optimization methods in this evaluation are: 1. In the retrieval stage, a hybrid retrieval method that combines traditional sparse retrieval and dense retrieval model is adopted, and the ROM [LZXX22] model we proposed for retrieval tasks is used on the dense retrieval pre-training model.",
            "score": 0.6162170146003391,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1047,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1047,
                    "end": 1052,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1326,
                    "end": 1334,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1661,
                    "end": 1669,
                    "matchedPaperCorpusId": "219531210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1756591796875
        },
        {
            "corpus_id": "258833383",
            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
            "text": "Passage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
            "score": 0.6159088169370031,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 136,
                    "end": 153,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 263,
                    "end": 282,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 508,
                    "end": 532,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1314,
                    "end": 1330,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1708,
                    "end": 1726,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1747,
                    "end": 1765,
                    "matchedPaperCorpusId": "252917569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40185546875
        },
        {
            "corpus_id": "258762633",
            "title": "TOME: A Two-stage Approach for Model-based Retrieval",
            "text": "For comparison, we chose the following baselines including sparse retrieval, dense retrieval, and model-based retrieval.\n\nBM25 (Robertson et al., 2009) is a classical sparse retriever that uses the inverted index to find relevant passages by term overlap. DPR (Karpukhin et al., 2020) and ANCE (Xiong et al., 2021) are two representative dense retrievers that adopts dual-encoder architecture. For modelbased retrievers, DSI (Tay et al., 2022) is a pioneer work for model-based retrieval that uses a sequence-to-sequence model to map the input query to the relevant docid. We use the open-source code released by DSI-QG for reproducing DSI baseline on MS MARCO. SEAL (Bevilacqua et al., 2022) is proposed to generate multiple ngrams for a query with an auxiliary Ferragina Manzini index. DSI-QG (Zhuang et al., 2022) proposes to improve DSI with augmented data constructed by query generation. NCI (Wang et al., 2022) also utilizes pseudo queries for improving model-based retrieval with tailored architecture. Due to the different experimental settings of different methods, we copy the performance values for some baselines on NQ in NCI and reproduce all of the baselines on MS MARCO under the same evaluation strategy. All the model-based retrieval baselines adopt the \"large\" version of PLMs.",
            "score": 0.6156304008960285,
            "section_title": "Baselines",
            "char_start_offset": 18341,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 150,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 260,
                    "end": 284,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 294,
                    "end": 314,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 425,
                    "end": 442,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 667,
                    "end": 692,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "268681628",
            "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
            "text": "Information Retrieval In recent years, deep learning-based retrieval models have achieved remarkable performance across various information retrieval tasks.The dense passage retriever (DPR) framework, which encodes documents and queries into dense representations, has emerged as a popular approach for dense retrieval (Karpukhin et al., 2020).With the advent of large language models (LLMs), numerous methods have leveraged these models for dense retrieval.GTR (Ni et al., 2022) utilizes LLM encoders for dense retrieval and demonstrates performance improvements with increased model size.Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) propose the use of LLMs to generate synthetic query-document pairs, which are then employed for training dense retrievers.Our work is orthogonal to these methods, as we focus on utilizing LLMs for second-stage passage reranking.\n\nPassage Reranking Passage reranking typically serves as a second-stage component following large-scale retrieval.Several studies have proposed deep reranking models that encode query-document pairs to predict relevance scores (Nogueira and Cho, 2019).Nogueira et al.\n\n(2020) introduced a generation-based method for passage reranking by fine-tuning LLMs on MS-MARCO (Bajaj et al., 2016), a large-scale retrieval dataset with relevance annotations.Their model, MonoT5, generates the word true for relevant pairs and false for irrelevant pairs.Similarly, our method also adopts a generation-based approach.\n\nThe main difference is that our method does not require relevance annotations nor fine-tuning; instead, we leverage the instruction-following capabilities of LLMs to enable unsupervised estimation.TART (Asai et al., 2022) fine-tunes LLMs on extensive retrieval supervision from various tasks with instructions.Our method differs from TART in that we do not require any retrieval supervision and employ a generation-based approach in an unsupervised fashion.\n\nAnother research line is unsupervised passage reranking with LLMs, which eliminates the need for retrieval supervision.",
            "score": 0.6139911950629054,
            "section_title": "Related Work",
            "char_start_offset": 2432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1492
                },
                {
                    "start": 1494,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1951
                },
                {
                    "start": 1953,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 343,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 462,
                    "end": 479,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 633,
                    "end": 657,
                    "matchedPaperCorpusId": "250340449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38916015625
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "Dense passage retrieval (DPR) models brought substantial effectiveness gains to information retrieval (IR) tasks in the web domain [14,19,39]. The promise of DPR models is to boost the recall of first stage retrieval by leveraging the semantic information for retrieval as opposed to traditional retrieval models [31], which rely on lexical matching. The web domain is a setting with query-to-passage or query-to-document retrieval tasks and a large amount of training data, while training data is much more limited in other domains. Furthermore we see recent advances in neural retrieval remain neglected for document-to-document retrieval despite the task's importance in several, mainly professional, domains [24,28,29,30]. \n\nIn this paper we investigate the effectiveness of dense retrieval models for documentto-document tasks, in particular legal case retrieval. We focus on first stage retrieval with dense models and therefore aim for a high recall. The first challenge for DPR models in document-to-document retrieval tasks is the input length of the query documents and of the documents in the corpus. In legal case retrieval the cases tend to be long documents [35] with an average length of 1269 words in the COLIEE case law corpus [29]. However the input length of DPR models is limited to 512 tokens [19] and theoretically bound of how much information of a long text can be compressed into a single vector [25]. Furthermore we reason in accordance with the literature [7,33,37,38] that relevance between two documents is not only determined by the complete text of the documents, but that a candidate document can be relevant to a query document based on one paragraph that is relevant to one paragraph of the query document. In the web domain DPR models are trained on up to 500k training samples [6], whereas in most domain-specific collections only a limited amount of hundreds of labelled samples is available [13,15,29]. \n\nIn this paper we address these challenges by proposing a paragraph aggregation retrieval model (PARM) for dense document-to-document retrieval. PARM liberates dense passage retrieval models from their limited input length without increasing the computational cost. Furthermore PARM gives insight on which paragraphs the document-level relevance is based, which is beneficial for understanding and explaining the retrieved results.",
            "score": 0.6139160387348764,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "216642169"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "37467052"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "7185758"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "221885005"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "3251655"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "221885005"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1486,
                    "end": 1489,
                    "matchedPaperCorpusId": "267909336"
                },
                {
                    "start": 1489,
                    "end": 1492,
                    "matchedPaperCorpusId": "215875147"
                },
                {
                    "start": 1492,
                    "end": 1495,
                    "matchedPaperCorpusId": "197928351"
                },
                {
                    "start": 1813,
                    "end": 1816,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1933,
                    "end": 1936,
                    "matchedPaperCorpusId": "5993893"
                },
                {
                    "start": 1936,
                    "end": 1939,
                    "matchedPaperCorpusId": "221885005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "269430346",
            "title": "ExcluIR: Exclusionary Neural Information Retrieval",
            "text": "To evaluate the performance of various retrieval models on exclusionary retrieval, we select three types of retrieval models with different architectures: sparse retrieval, dense retrieval, and generative retrieval.Sparse retrieval methods calculate the relevance score of documents using term matching metrics such as TF-IDF (Robertson and Walker, 1997).\n\n\u2022 BM25 (Robertson et al., 2009) is a classical probabilistic retrieval method based on the normalization of the frequency of the term and the length of the document.\n\n\u2022 DocT5Query (Nogueira et al., 2019) expands documents by generating pseudo queries using a fine-tuned T5 model before building the BM25 index (Raffel et al., 2020).\n\nDense retrieval utilizes pre-trained language models (PLMs) as the backbones to represent queries and documents as dense vectors for computing relevance scores.\n\n\u2022 DPR (Karpukhin et al., 2020) is a dense retrieval model based on dual-encoder architecture, which uses the representation of the [CLS] token of BERT (Kenton and Toutanova, 2019).\n\n\u2022 Sentence-T5 (Ni et al., 2022a) uses a fine-tuned T5 encoder model to encode queries and documents into dense vectors.\n\n\u2022 GTR (Ni et al., 2022b) has the same architecture as Sentence-T5 and has been pretrained on two billion question-answer pairs collected from the Web.\n\n\u2022 ColBERT (Khattab and Zaharia, 2020) is a late interaction model that learns embeddings for each token in queries and documents, and then uses a MaxSim operator to calculate the relevance score.\n\nGenerative retrieval is an end-to-end retrieval paradigm.\n\n\u2022 GENRE (De Cao et al., 2020) retrieves entities by generating their names through a seq-to-seq model, it can be applied to document retrieval by directly generating document titles.The original GENRE is trained based on BART as the backbone, and we reproduce it using T5.\n\n\u2022 SEAL (Bevilacqua et al., 2022) retrieves documents by generating n-grams within them.",
            "score": 0.6136832467295397,
            "section_title": "Methods for comparison",
            "char_start_offset": 10291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1306
                },
                {
                    "start": 1308,
                    "end": 1503
                },
                {
                    "start": 1505,
                    "end": 1562
                },
                {
                    "start": 1564,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1836
                },
                {
                    "start": 1838,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 354,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 364,
                    "end": 387,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 537,
                    "end": 560,
                    "matchedPaperCorpusId": "208612557"
                },
                {
                    "start": 667,
                    "end": 688,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 859,
                    "end": 883,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1066,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1572,
                    "end": 1593,
                    "matchedPaperCorpusId": "222125277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "258865354",
            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
            "text": "Naive approaches are either computationally infeasible (scoring all possible queries), or rely on sampling a small, incomplete set of samples (such as in Lewis et al. [22]). \n\nTo overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14,26]. We call the retrieval system based on this proposed model nail (Non-Autoregressive Indexing with Language models). \n\nWe summarize our contributions as follows: \n\n(1) We advance prior work on learned sparse retrieval by leveraging pretrained encoder-decoder LMs with a novel nonautoregressive decoder. (2) We describe a range of experiments using the BEIR benchmark [39] that explore the performance and efficiency of our model as a reranker and as a retriever compared with a variety of existing systems. As a reranker, nail can recover 86% of the performance of a large cross-attention reranker [27], while requiring 10 \u22126 % of the inference-time FLOPS per query. As a retriever, nail has an extremely high upper bound for recall-exceeding the performance of all other retrievers in the zero-shot setting. Finally, by using BM25 as a retriever and nail as a reranker, we can match state-of-the-art dual-encoders [14,26] with 10 \u22124 % of the inference-time FLOPS. (3) We propose our model as a preferred solution when significant compute is available at indexing time, but not ondemand at serving time, and we provide a cost analysis that illustrates when our approach could be preferred to previous work that harnesses LLMs.",
            "score": 0.6133860403659526,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1599,
                    "end": 1602,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2149658203125
        },
        {
            "corpus_id": "270214302",
            "title": "A Survey of Generative Information Retrieval",
            "text": "This marks a significant shift from matching pre-existing vector representations to dynamically generating textual outputs that directly address user needs.In the pre-retrieval stage, gen- Figure 1: Progression of information retrieval from sparse vector similarity techniques, such as the bag-of-words and Vector Space Model, to dense retrieval with innovations like Word2Vec and BERT, culminating in sophisticated systems like DPR.Advances in generative retrieval now integrate language models for direct response generation.\n\nerative models are utilized to enhance dense retrieval efficiency through innovative approaches like a retrieval-oriented pre-training paradigm using a Masked Auto-Encoder (MAE), as demonstrated by Xiao et al. (2022).This model trains to reconstruct sentences from their embeddings and masked inputs, delivering superior performance across various benchmarks.During the retrieval phase, the adoption of generative models is exemplified by Lewis et al. (2020)'s retrieval-augmented generation model, which selects documents via a dense passage retriever and generates answers for complex NLP tasks, achieving top-tier performance.Moreover, Tay et al. (2022)'s Differentiable Search Index (DSI) highlights this stage by mapping queries directly to relevant documents, significantly surpassing traditional methods and demonstrating robust generalization in zero-shot setups.In the post-retrieval phase, deep learning techniques are applied to rerank retrieved documents, where efforts like those by Guo et al. (2016) focus on refining document rankings by analyzing complex matching patterns between queries and documents.Similarly, Mitra et al. (2017) enhances web search reranking by merging local and distributed text representations, leveraging both local and global contexts to improve search result quality.Through these innovations, including the Two-Tower model architecture and the Differentiable Search Index (DSI) (Tay et al., 2022), generative retrieval not only effectively responds to queries but also identifies pertinent information within a corpus, utilizing end-to-end training architectures that integrate deep learning processes to streamline the retrieval experience.",
            "score": 0.6127511643140631,
            "section_title": "Introduction",
            "char_start_offset": 2273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 527
                },
                {
                    "start": 529,
                    "end": 746
                },
                {
                    "start": 746,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 968,
                    "end": 987,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1168,
                    "end": 1185,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1525,
                    "end": 1542,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 1951,
                    "end": 1969,
                    "matchedPaperCorpusId": "246863488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "258461097",
            "title": "Understanding Differential Search Index for Text Retrieval",
            "text": "The details of our baselines are as follows: \n\n\u2022 BM25 (Robertson and Zaragoza, 2009) is a classical sparse retrieval model that utilizes lexical weights. In this study, we employ a pyserini-based implementation (Lin et al., 2021b), which leverages the Lucene (Bialecki et al., 2012) as the underlying infrastructure. \n\n\u2022 TCT-ColBERT (V2) (Lin et al., 2021c) is a state-of-the-art single-vector dense retrieval model that utilizes knowledge distillation and hard negative example sampling techniques. \n\nThe model combines the performance of Col-BERT (Khattab and Zaharia, 2020) with the computational efficiency of a bi-encoder. The implementation of TCT-ColBERT is based on the Faiss vector index and is implemented through the pyserini library. This model is utilized to demonstrate the effectiveness of the current state-of-the-art model for retrieval and serves as a guide for training a differentiable search index. Note that the TCT-ColBERT model is exclusively trained utilizing the MS MARCO Passage dataset. Our analysis of the retrieval performance of various dense retrieval models on NQ data revealed that TCT-ColBERT displayed remarkable results despite not having been specifically trained on that dataset. \n\n\u2022 DSI (Tay et al., 2022) is a T5-based approach for learning text-to-identifier mappings. Specifically, DSI defines the process of mapping original text to identifiers as an indexing task and the mapping of query text to identifiers as a retrieval task. This study reproduces the DSI model using the Naive String Docid and Semantic String Docid techniques, based on T5-base and T5-large architectures, utilizing open-source implementations2 . As access to the original training data of DSI is not available, we followed the settings of the open-source implementation to set the indexing and retrieval ratio to 1:1 for MS MARCO and 3:1 for NQ. \n\n\u2022 DSI-QG (Zhuang et al., 2022) improves upon DSI by incorporating pseudo-queries that are generated utilizing DocT5Query during the training process.",
            "score": 0.6125769430421039,
            "section_title": "A.3 Baseline Details",
            "char_start_offset": 35370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 47,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1863
                },
                {
                    "start": 1866,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 83,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 211,
                    "end": 230,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 259,
                    "end": 282,
                    "matchedPaperCorpusId": "17420900"
                },
                {
                    "start": 338,
                    "end": 356,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1771240234375
        },
        {
            "corpus_id": "251718954",
            "title": "A Contrastive Pre-training Approach to Discriminative Autoencoder for Dense Retrieval",
            "text": "Recently, dense retrieval (DR) has achieved great success on many information retrieval (IR) related tasks, such as web search [22,23], open-domain Question Answering (QA) [10,17] and fact verification [4]. DR models generally employ pre-trained language models as text encoder to obtain dense representations for queries and documents. Then, retrieval with simple similarity metrics can be conducted effectively in the representation space. Effective search is based on high-quality text representation learning [13]. \n\nDespite the effectiveness of BERT-like language models [6] on learning word representations, they are not good at producing text sequence representations [11,18,20]. Recent studies have demonstrated that autoencoder-based language models can significantly advance the effectiveness and few-shot ability of DR models [13]. The basic idea is to train a weak autoregressive decoder that reconstructs the input text only from the encoder's encodings. In this way, the encoder creates a bottleneck to provide high-quality text sequence representations. However, even a weak autoregressive decoder has the bypass effect in which the decoder may ignore the representation and predict the next token only based on previous tokens. More importantly, the decoder treats each token equally important but common words like in, the, and of, are the majority part of the text. Therefore, the discriminative ability of dense representations may be limited since the representation will focus more on the common words and thus is not differential with other representations. \n\nTo address the above problems, in this paper, we propose a contrastive pre-training approach to learn a discriminative autoencoder with a lightweight multi-layer perception (MLP) decoder. Specifically, rather than reconstructing texts in an autoregressive fashion, the MLP decoder generates word distributions of input texts in a non-autoregressive fashion to avoid the bypass effect. We then introduce a novel contrastive learning method to pull the word distributions of two masked versions of one text close while pushing away from others. We theoretically show that our contrastive strategy can suppress the common words and highlight the representative words when decoding, leading to discriminative representations.",
            "score": 0.6125642665005876,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 202,
                    "end": 205,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.379638671875
        },
        {
            "corpus_id": "268066341",
            "title": "Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling",
            "text": "Dense Retrieval Dense retrieval has garnered significant attention for its potential to enable retrieval in the semantic space. A prominent method in this area is the dense passage retriever (DPR) (Karpukhin et al., 2020), which comprises a query encoder and a passage encoder. Several studies have also explored efficient training approaches, such as RocketQA (Qu et al., 2021) and alternative architectures for dense retrieval, e.g., ColBERT (Khattab and Zaharia, 2020). A common technique for training performant dense retrievers is knowledge distillation from cross encoders. BERT-CAT (Hofst\u00e4tter et al., 2020) proposed cross-architecture knowledge distillation to improve dense retrievers and rankers. Izacard and Grave distilled knowledge from the reader model to the retriever model, thus improving its performance on open-domain question answering. However, the majority of previous work has primarily focused on English retrieval, limiting its applicability to other languages. \n\nMultilingual Dense Retrieval Multilingual information retrieval has been an active research area for several decades. Early work in this field primarily focused on cross-lingual information retrieval (CLIR), aiming to retrieve relevant documents in a different language from the query language (Nasharuddin and Abdullah, 2010). Traditional CLIR systems relied on aligning bilingual dictionaries or parallel corpora to translate queries or documents into a common language for retrieval. However, these systems often faced limitations in translation quality, vocabulary coverage, and handling domain-specific expressions (Ballesteros and Croft, 1996;Vuli\u0107 and Moens, 2015;Sharma and Mittal, 2016). \n\nIn recent years, dense retrieval has emerged as a promising approach for multilingual information retrieval. Various studies have demonstrated the effectiveness of dense retrieval methods in crosslingual and multilingual scenarios. Models such as XLM-R (Conneau et al., 2020) and mBERT (Devlin et al., 2019) have achieved remarkable performance on diverse natural language processing tasks, including similarity-based retrieval tasks. The success of these models has spurred researchers to explore their application in multilingual information retrieval (Jiang et al., 2020).",
            "score": 0.6123976548384866,
            "section_title": "Related Work",
            "char_start_offset": 3221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 221,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 361,
                    "end": 378,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 444,
                    "end": 471,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1609,
                    "end": 1638,
                    "matchedPaperCorpusId": "7109112"
                },
                {
                    "start": 1638,
                    "end": 1660,
                    "matchedPaperCorpusId": "2583305"
                },
                {
                    "start": 1660,
                    "end": 1684,
                    "matchedPaperCorpusId": "63311109"
                },
                {
                    "start": 1941,
                    "end": 1963,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1974,
                    "end": 1995,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3076171875
        },
        {
            "corpus_id": "257622671",
            "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
            "text": "In the context of retrieval-augmented models, numerous works rerank retrieved passages by their relevance to the query and process only the highest scoring passages (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022) and vary the number of processed passages depending on model confidence (Kratzwald and Feuerriegel, 2018;Varshney et al., 2022).Concurrent work CoDA (Lei et al., 2023) employs a related conditional computation mechanism, designed for efficient adaptation rather than modeling long documents.\n\nDevice utilization FLOPs do not tell the whole story, as modeling choices can influence the effective speed of operations achieved by accelerators.\n\nFor long text inputs, autoregressive decoder inference is very slow due to memory bandwidth constraints from repeatedly loading the long sequence of keys and values (Shazeer, 2019;de Jong et al., 2022).Shazeer (2019) introduces multi-query attention (MQA), sharing heads for keys and values to reduce memory bandwidth overhead.Pope et al. (2022) studies how to shard large models, especially in the context of MQA, to obtain optimal device utilization and therefore speed.\n\nTraining objectives T5 introduced the span corruption objective (Raffel et al., 2020), a modification of masked language modeling (Devlin et al., 2019).LONGT5 made use of the PEGA-SUS (Zhang et al., 2020) sentence reconstruc-tion objective for improved summarization performance.Tay et al. (2022) proposes UL2, a mixture of span corruption, prefix, and causal language modeling, and shows that it leads to strong performance on both short-output and generative tasks.",
            "score": 0.6111738716979872,
            "section_title": "Sparse attention",
            "char_start_offset": 5404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 509
                },
                {
                    "start": 511,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1132
                },
                {
                    "start": 1134,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 201,
                    "end": 217,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 290,
                    "end": 323,
                    "matchedPaperCorpusId": "52054464"
                },
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "258060039"
                },
                {
                    "start": 1198,
                    "end": 1219,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1318,
                    "end": 1338,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09967041015625
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.",
            "score": 0.6111105202769374,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
            "score": 0.6109473589451277,
            "section_title": "Traditional Document Retrievers",
            "char_start_offset": 4664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 178,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 220,
                    "end": 244,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 558,
                    "end": 578,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 594,
                    "end": 614,
                    "matchedPaperCorpusId": "258865472"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 824,
                    "end": 848,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 921,
                    "end": 943,
                    "matchedPaperCorpusId": "220302658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "Text retrieval is a crucial task in information retrieval and has a significant impact on various language systems, including search ranking (Nogueira and Cho 2019) and open-domain question answering (Chen et al. 2017). At its core, text retrieval involves learning a ranking model that assigns scores to documents based on a given query, a process known as learning to rank. This approach has been enduringly popular for decades and has evolved into point-wise, pair-wise, and list-wise methods. Currently, the dominant implementation is the dual-encoder approach (Lee, Chang, and Toutanova 2019; Karpukhin et al. 2020), which encodes queries and passages into vectors in a semantic space and employs a list-wise loss to learn the similarities. \n\nAn emerging alternative to the dual-encoder approach in text retrieval is generative retrieval (Tay et al. 2022;Bevilacqua et al. 2022). Generative retrieval employs autoregressive language models to generate identifier strings of passages as an intermediate target for retrieval. An identifier is a distinctive string to represent a passage, such as Wikipedia titles to Wikipedia passages. The predicted identifiers are then mapped to ranked passages as the retrieval results. In this manner, generative retrieval treats passage retrieval as a standard sequence-to-sequence task, maximizing the likelihood of the passage identifiers given the input query, distinct from previous learning-to-rank approaches. \n\nThere are two main approaches to generative retrieval regarding the identifier types. One approach, exemplified by the DSI system and its variants (Tay et al. 2022), assigns a unique numeric ID to each passage, allowing predicted numeric IDs to directly correspond to passages on a one-to-one basis. However, this approach requires memorizing the mappings from passages to their numeric IDs, making it ineffective for large corpus sets. The other approach (Bevilacqua et al. 2022) takes text spans from the passages as identifiers. While the text span-based identifiers are effective in the large-scale corpus, they no longer uniquely correspond to the passages. In their work, a heuristic-based function is employed to rank all the passages associated with the predicted identifiers.",
            "score": 0.6100432594335341,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 218,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 598,
                    "end": 620,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 843,
                    "end": 860,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1606,
                    "end": 1623,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372314453125
        },
        {
            "corpus_id": "276482242",
            "title": "A Survey of Model Architectures in Information Retrieval",
            "text": "PARADE (Li et al., 2020) uses an additional neural network to aggregate contextualized representations from CLS tokens of passages to get the final document relevance score. \n\nIn the second direction, MacAvaney et al. ( 2019) discovered via CEDR that the effectiveness of reranker could be enhanced when aggregating the contextualized representations with neural ranking models such as K-NRM and DRMM. Zhang et al. (2024a) later observed that integrating token representations with late interactions could also effectively improve the reranking robustness on outof-domain scenarios, especially for long queries. \n\nLearned Dense Retrieval Here we use the term \"bi-encoder\" (Humeau et al., 2020) to refer to the model architecture commonly used for dense retrieval.2 Bi-encoder uses the backbone to encoder query Q and document D separately, then uses the encoded dense vector representations to compute the relevance score with similarity functions such as dot product or cosine similarity (Xiong et al., 2020;Karpukhin et al., 2020). After training, the model encodes the collection into a dense vector index, and retrieval is performed with fast nearest neighbor search techniques (Johnson et al., 2019;Malkov and Yashunin, 2016). Different from representation-based neural ranking models where distinct architectures are proposed ( \u00a7 5), existing dense retrieval methods are mostly based on pretrained transformer language models but vary in training strategy (detailed in Appx. D). \n\nLearned Sparse Retrieval Similar to learned dense retrieval, learned sparse retrieval (LSR) also uses bi-encoder architecture with language models as the backbone, to transform documents into a static index for later retrieval (Zamani et al., 2018). In order to utilize traditional inverted index method for faster retrieval (Bruch et al., 2024), query Q and document D are represented as sparse vectors, whose dimensionality typically matches the vocabulary size of the backbone pre-trained transformer model (Yu et al., 2024). Here sparsity is enforced through regularization (Formal et al., 2021b;Paria et al., 2020) and usually serves as a trade-off between effectiveness and efficiency.",
            "score": 0.6086295727067187,
            "section_title": "IR with Pre-trained Transformers",
            "char_start_offset": 15218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 7,
                    "end": 24,
                    "matchedPaperCorpusId": "221186870"
                },
                {
                    "start": 402,
                    "end": 422,
                    "matchedPaperCorpusId": "270514468"
                },
                {
                    "start": 1182,
                    "end": 1204,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1714,
                    "end": 1735,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1997,
                    "end": 2014,
                    "matchedPaperCorpusId": "266977305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229248046875
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "The goal of dense passage retrieval is to retrieve a subset of relevant passages, denoted as D + , from a large collection of passages \n\nIn order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval. \n\nThe relevance of a query q to a passage d i is measured using a similarity function: \n\nwhere a higher similarity score indicates a greater relevance of the passage to the query. Common choices of the similarity function are dot product, cosine similarity, or the Max-Sum operator introduced in ColBERT (Khattab and Zaharia, 2020). \n\nGiven a labeled dataset of relevant passage-query pairs (q, d + ), dense retrievers are typically trained with a contrastive learning objective such as the InfoNCE loss (Oord et al., 2018): \n\n, \n\nwhere D \u2032 denotes the union of the positive and negative examples. Optimizing this objective promotes the similarity of the positive pair s(q, d + ) in contrast to the negative examples.",
            "score": 0.6086185141060255,
            "section_title": "Dense Retrieval",
            "char_start_offset": 7520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1354
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 968,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437255859375
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "We make the following observations for both the sparse external ANCE-PRF dense retrieval models: (1) they exhibit higher performance than both ANCE (row (e)) and ANCE-PRF (row (f)) performed only on target collection; (2) they show slightly lower performance compared with ANCE reranking models in row (e) and row (f); (3) they show similar performance with the dense external dense retrieval models. On the half-bottom table, we make the following observations for the sparse external dense retrieval models as follows: (1) similar to Robust04, both models outperform the ANCE and ANCE-PRF on WT10G target collection; (2) however, they show a large drop compared with the ANCE reranking models in row (e) and (f); (3) different to the observation for Robust04, sparse external dense retrieval exhibits higher performance than dense external dense retrieval models. Based on this, we find that for the singlerepresentation ANCE-PRF dense retrieval model, sparse external retrieval as the first stage could also produce high quality feedback documents to refine the query representation using ANCE-PRF model. \n\nThus, in response to RQ3, we find that sparse external retrieval as the initial ranking stage is not sufficient to improve the performance of a multiple representation-based ColBERT-PRF dense retrieval model. However, for the single-representation ANCE-PRF dense retrieval model, sparse external first stage retrieval can improve the retrieval performance over the ANCE baseline, although the ANCE baseline is comparatively weak (emphasising the difficulty of zero-shot single representation dense retrieval).",
            "score": 0.6081613617216755,
            "section_title": "RQ3: Sparse-obtained external feedback for dense retrieval",
            "char_start_offset": 48010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "Dense retrieval aims to learn effective representations of queries and documents by making representations of relevant query-document pairs to be similar (Chopra et al., 2005;Van den Oord et al., 2018). With the success of dense passage retrieval for open-domain question answering (QA) Karpukhin et al., 2020), recent studies build an index for a finer granularity such as dense phrase retrieval (Lee et al., 2021a), which largely improves the computational efficiency of open-domain QA * JL currently works at Google Research. \u2020 YZ currently works at Apple. by replacing the retriever-reader model (Chen et al., 2017) with a retriever-only model (Seo et al., 2019;. Also, phrase retrieval provides a unifying solution for multi-granularity retrieval ranging from open-domain QA (formulated as retrieving phrases) to document retrieval (Lee et al., 2021b), which makes it particularly attractive. Building a dense retrieval system involves multiple steps (Figure 1) including training a dual encoder ( \u00a74), selecting the best model with validation ( \u00a73), and constructing an index (often with filtering) for an efficient search ( \u00a75). However, these components are somewhat loosely connected to each other. For example, model training is not directly optimizing the retrieval performance using the full corpus on which models should be evaluated. In this paper, we aim to minimize the gap between training and inference of dense retrievers to achieve better retrieval performance.\n\nHowever, developing a better dense retriever requires validation, which requires building large indexes from a full corpus (e.g., the entire Wikipedia for open-domain QA) for inference with a huge amount of computational resources and time. To tackle this problem, we first propose an efficient way of validating dense retrievers without building large-scale indexes. Analysis of using a smaller random corpus with different sizes for the validation reveals that the accuracy from small indexes does not necessarily correlate well with the retrieval accuracy on the full index. As an alternative, we construct a compact corpus using a pre-trained dense retriever so that validation on this corpus better correlates well with the retrieval on the full scale while keeping the size of the corpus as small as",
            "score": 0.607926113359023,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "5555257"
                },
                {
                    "start": 287,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 397,
                    "end": 416,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 600,
                    "end": 619,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 837,
                    "end": 856,
                    "matchedPaperCorpusId": "237532725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2939453125
        },
        {
            "corpus_id": "276724989",
            "title": "TempRetriever: Fusion-based Temporal Dense Passage Retrieval for Time-Sensitive Questions",
            "text": "The research on Temporal Information Retrieval (TIR) [3,4,16,20] emerged in the past as a specialized type of information retrieval, focusing on searching documents with respect to time dimension and temporal signals. While the previous research primarily addressed temporal representation and understanding of queries and documents, there has been limited exploration of integrating temporal information with modern neural network-based retrieval technologies and its application in the context of question answering. Our work seeks to fill that gap by incorporating temporal embeddings and alignment into dense retrieval architectures. Passage retrieval is a critical component in open-domain question answering, as the efficiency and precision of retrieving relevant passages directly impact the overall performance of the system [5,6,12,17]. Both sparse and dense vector models have been developed to enhance passage retrieval. Sparse retrieval methods represent queries and documents as sparse vectors, where each element corresponds to a term in the vocabulary. Dense retrieval methods, on the other hand, encode queries and documents into a continuous vector space, often leveraging pre-trained language models to capture richer semantic representations. \n\nNotable contributions to dense passage retrieval (DPR) include various architectures designed to improve the representation and retrieval of passages in QA tasks [18,21,24,32,45,48]. These models utilize deep learning techniques to encode queries and documents as dense vectors, enabling more effective semantic matching compared to traditional lexical methods. Despite these advancements, dense retrieval models often overlook temporal signals, a key aspect of many time-sensitive information retrieval tasks. \n\nIn the recent years, several studies [8,33,34,44,47,50] have explored the integration of temporal reasoning into language models to address the challenges posed by dynamic information needs. Cole et al. [9] proposed a novel pre-training technique called Temporal Span Masking (TSM), which enhances temporal reasoning by masking and predicting temporal spans such as dates and periods during training. This approach improves the model's ability to understand and reason about temporal relationships. Other works have focused on adapting language models to handle time-sensitive information. For example, Giulianelli et al. [13] analyzed contextualized word embeddings of BERT to track lexical semantic change over time, demonstrating how word meanings shift across different time periods.",
            "score": 0.6079091015181428,
            "section_title": "Related Work",
            "char_start_offset": 4170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2275
                },
                {
                    "start": 2276,
                    "end": 2366
                },
                {
                    "start": 2367,
                    "end": 2564
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 58,
                    "matchedPaperCorpusId": "14176218"
                },
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "100776"
                },
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "31909930"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 836,
                    "end": 838,
                    "matchedPaperCorpusId": "61248440"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "235669861"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "13865788"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "53035265"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "202558815"
                },
                {
                    "start": 1439,
                    "end": 1442,
                    "matchedPaperCorpusId": "211003735"
                },
                {
                    "start": 1442,
                    "end": 1445,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 1980,
                    "end": 1983,
                    "matchedPaperCorpusId": "257687447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "259991673",
            "title": "Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval",
            "text": "Dense retrieval has become a core component within several downstream NLP and web-search tasks such as question-answering [1,2], semantic similarity [3], conversational search [4], entity retrieval [5], fact-checking [6] and passage retrieval [7,8,9,10,11]. In dense retrieval, dual-encoder models encode semantically correlated queries and documents and represent them as spatially close embeddings, i.e., dense vector representations. Dense retrieval can be efficiently conducted via approximate nearest neighbor (ANN) search at inference [12]. \n\nDual-encoder models encode and store document embeddings within an index, which starts to become bulky, once the documents start to increase within the corpus. It becomes expensive to serve dense retrievers practically. For example, storing 21M (million) passages within a dense index requires about 65 GB of memory [13]. As nearest-neighbor (exact) search in large vector spaces is rather slow, ANN techniques such as HNSW [14] are popularly used, which results in even higher memory requirements such as 150 GB [13]. As a result, for searching on a very large corpus, we require a high-memory and expensive machine to host the bulky index, which hinders the practical application of dense retrieval. \n\nAware of the issue, there have been recent efforts to improve the efficiency of dense retrieval models by compressing the vector representation to lower memory, i.e., space requirements. Traditionally, out-of-the-box unsupervised strategies such as dimension reduction with principal component analysis (PCA) or product quantization (PQ) [15] have been popular for retrieval [16,17]. More recently, supervised learning-to-hash (LTH) techniques have been introduced such as BPR [13] and JPQ [18]. LTH techniques map the original dense embeddings into a Hamming space (binary space) and obtain the low-dimensional binary codes. Unfortunately, all existing vector compression methods are proposed under the single-domain retrieval assumption. However, in practice, many retrieval applications span across various domains with diverse distributions; often with scarce training data [19].",
            "score": 0.6062881609720363,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 125,
                    "end": 127,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "231779510"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "9896397"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1632,
                    "end": 1635,
                    "matchedPaperCorpusId": "239993003"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "236772078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323974609375
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "ACM Trans. Web, Vol. In this section, we further investigate the effectiveness of our proposed ColBERT-PRF for document ranking task. Table 5 and Table 6 present the performance of ColBERT-PRF models as well as the baselines on the MSMARCO document dataset and the Robust04 dataset, respectively. 6.3.1 Results for RQ5. Similar to the passage retrieval task, in this section we validate the effectiveness of the pseudo-relevance feedback technique for the ColBERT dense retrieval model on the document retrieval task. On analysing Table 5, we found that both ColBERT-PRF Ranker and ReRanker models significantly outperform both the single representation dense retrieval, namely ANCE, and the multiple representation dense retrieval model, namely ColBERT E2E, in terms of MAP and Recall on both TREC 2019 and TREC 2020 query sets. In particular, the application of ColBERT-PRF leads to upto 21% and 14% improvements over ColBERT E2E in terms of MAP for TREC 2019 and TREC 2020 query sets, respectively. Indeed, ColBERT-PRF outperforms all document retrieval runs to the TREC 2019 Deep Learning track, exceeding the highest observed MAP by 23% in terms of MAP. Similarly, on the TREC 2020 query set, the MAP observed is markedly above that attained by the second-ranked group on the leaderboard [7]. 16 In terms of NDCG@10, ColBERT-PRF outperforms over both the ANCE and ColBERT E2E models on both MSMARCO query sets. Moreover, both the ColBERT-PRF Ranker and ReRanker models significantly outperform the ColBERT and ANCE models w.r.t. Recall@100, indicating the effectiveness of the ColBERT-PRF refined query representations. \n\nSimilarly, when comparing the performances of ColBERT-PRF with the dense retrieval models without pseudo-relevance feedback on Robust04 in Table 6, we note that both ColBERT-PRF Ranker and ReRanker models are markedly improved over the ANCE and ColBERT E2E models on MAP, NDCG@10 and Recall on both title-only and description-only type of queries.",
            "score": 0.6054115524602667,
            "section_title": "Document Ranking Results",
            "char_start_offset": 54873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 1293,
                    "end": 1296,
                    "matchedPaperCorpusId": "212737158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25 (Robertson and Zaragoza, 2009) serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021), which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks (Karpukhin et al., 2020a), which can lead to retrieving irrelevant content. Second, dualencoder architectures encode queries and documents separately, limiting direct interaction between them (Khattab et al., 2021). Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs) (Levine et al., 2022). \n\nTo address these limitations, generative models such as GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead (Yu et al., 2022) first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information (Huang et al., 2023), making them less reliable for knowledge-intensive tasks. Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy.",
            "score": 0.6049111531835994,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 371,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 578,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 904,
                    "end": 929,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1427,
                    "end": 1447,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.335693359375
        },
        {
            "corpus_id": "250340145",
            "title": "SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts",
            "text": "To retrieve the list of passages containing the answer to a question we consider several models. We start with sparse vectors using TF-IDF, and cosine similarity and BM25 [28] as ranking functions. We use Elasticsearch to index the passages extracted from the CDF reports, and set the search engine to use either cosine similarity or BM25 as ranking function. In addition, we use dense representations generated with transformer bi-encoders [26] to encode questions and passages and calculate their similarity using vector dot product. Dense representations are able to capture semantic relations between words that are not possible by means of bag-of-words models used in the sparse representations. We evaluate the following alternative approaches to dense passage retrieval. \n\nDense Passage Retrieval (DPR) [16] uses two BERT encoders and its loss function minimizes the distance between the CLS token representation of question and relevant passages. DPR was trained on the Natural Questions corpus NQ [18]. The NQ corpus consists of 307,373 queries issued to Google search engine and the answers are spans in Wikipedia articles. \n\nColBERT [17] adapts BERT for efficient retrieval by adopting a late interaction architecture to compare bag of embeddings representing the question and the passage. The implementation5 that we use is fine tuned on the MS MARCO passage ranking dataset [1]. This dataset contains 1 million questions extracted from Bing query logs and 8.8 million passages, extracted from 3.5 million Web pages, which provide the necessary information to answer the questions. \n\nFinally, CoCondenser [12], a bi-encoder pre-training architecture. This architecture adds a condenser head on top of the transformer that receives input from the output of an early layer for the tokens, while the input of the CLS is the output of the previous layer. Authors claim that this architecture enable the CLS representation to focus on the global meaning of the input text. We test a CoCondenser6 trained on the MS Marco passage retrieval task. \n\nThe three bi-encoders DPR, ColBERT and CoCondenser have reported state-of-the-art results in passage retrieval datasets.",
            "score": 0.6048384657004927,
            "section_title": "Passage Retrieval",
            "char_start_offset": 6040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1006,
                    "end": 1010,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "237581068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "278129839",
            "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
            "text": "RAG frameworks [5] augment LLM generation by first retrieving relevant documents or passages from an external corpus. Initial approaches often employed Dense Passage Retrieval (DPR) [6], using embedding similarity to find relevant context. Significant progress has been made in developing more powerful embedding models [15,16,17,18,7], leading to improved performance on tasks requiring retrieval of single relevant passages. However, the standard RAG paradigm, which typically retrieves and scores passages independently based on query similarity, struggles with questions that require synthesizing information scattered across multiple documents [19].",
            "score": 0.6047452938792743,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 1899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 654
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 18,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 649,
                    "end": 653,
                    "matchedPaperCorpusId": "208267807"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2420654296875
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "In this study, we employ StableLM-3B [44] and Llama2-7B [43] as passage retrieval backbones. For document retrieval, we employ StableLM-1.6B [4] for NQ320K and StableLM-3B for MS MARCO. We train the models using ZeRO stage-2 optimization on 8 NVIDIA A100 (80 GB) GPUs with the AdamW optimizer, a batch size of 16 per GPU, and BFloat16 precision. \n\nThe models are trained for 3 epochs with a learning rate of 2e-5. During inference, we use beam search to generate 5 titles and 10 passages for each title, with hyperparameters \u03c4 and \u03b4 set to 0.4 across all models and datasets. \n\nBaselines We evaluate Self-Retrieval models for both passage retrieval and document retrieval, comparing them with sparse, dense, and generative retrieval baselines. The sparse retrieval baselines are:BM25 [37] and DocT5Query [28]. The dense retrieval baselines include: DPR [19], Sentence-T5 [31], GTR [32], BGE [50], text-embedding-ada-002 [30], GritLM [29], and their fine-tuned variants, DPR-FT and BGE-FT. The generative retrieval baselines comprise: DSI [42], DSI-QG [59], NCI [49], Ultron [55], DynamicRetriever [54], GenRet [39], and SEAL [5]. Additionally, to ensure a comprehensive comparison, we also evaluate combinations of strong retrieval baselines with various rerankers, including BGE-Reranker, BGE-Reranker-FT, and RankGPT [41]. In the passage retrieval task, we use the official pre-trained models for all non-fine-tuned dense retrieval baselines. For fine-tuned dense models and generative models, we use their official implementations to replicate the experiments on our dataset. In the document retrieval task, we report the baseline performances from their original paper. For comprehensive details about these baselines, please refer to Appendix B.",
            "score": 0.6038478586598561,
            "section_title": "Implementation details",
            "char_start_offset": 18194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "258048596"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1319,
                    "end": 1323,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1539306640625
        },
        {
            "corpus_id": "272881140",
            "title": "Unsupervised Text Representation Learning via Instruction-Tuning for Zero-Shot Dense Retrieval",
            "text": "Dense retrieval systems commonly employ dual-encoder retrieval models which use two separate encoders, either symmetric or asymmetric, to represent the query and corpus [8,9,13,33]. The corpora are indexed with representation and will be retrieved in response to each query based on the relevance scores. The scores are usually calculated based on embedding similarity, such as dot product or cosine similarity. Although dense retrieval systems have developed rapidly, the model performance largely depends supervised text representation learning and relevancy capturing between the query and corpus [36]. Yet, it remains to be a major challenge to properly retrieve when lacking labeled modeling data. Existing work [21,22] leveraged pre-trained large encoders (specifically T5 models, Raffel et al. [26]) to alleviate the data thirst. However, their proposals still required annotated datasets either by web mining or manual annotation for fine-tuning in order to improve the generalization ability of dual-encoder retrieval models, for example, dealing with out-ofdomain data. An alternative solution is to train a dense retrieval on synthetic query-corpus relevance pairs. [18] trains a question generation system on general domain data and applies it to the targeted domain to construct synthetic question-passage data. To save the effort of training a task-specific generation model on general data, like Natural Questions [15] or MSMARCO [20], Promptagator [5] proposes to use pre-trained LLMs, like FLAN [32], as a few-shot query generator to build the data for training the dual-encoder. However, the synthetic queries are not directly leveraged at inference, potentially causing gaps between training and inference of dense retrievers [2]. Earlier work, e.g., doc2query [24], concatenates the generated queries with the corresponding corpus, aiming to enrich the corpus representation with questions that the corpus can potentially answer. An improved version, docTTTTTquery [23] leverages pre-trained T5 models as the expansion model, leading to more relevant synthetic queries and better retrieval performance.",
            "score": 0.603688517106856,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "248177840"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1464,
                    "end": 1467,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1745,
                    "end": 1748,
                    "matchedPaperCorpusId": "248780378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230712890625
        },
        {
            "corpus_id": "258947132",
            "title": "Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering",
            "text": "We compare EAR with 1) DPR (Karpukhin et al., 2020): a standard BERT-based dense retriever; 2) BM25 (Robertson et al., 2009): a standard sparse retriever based on term matching; 3) GAR (Mao et al., 2021a): generation-augmented retrieval with BM25; 4) Liu et al. (2022): a concurrent work that uses a GAR-like generative model to perform beam search decoding, followed by filtering to obtain multiple expanded queries for performing multiple retrievals with BM25, and then fusion of the results; and 5) SEAL (Bevilacqua et al., 2022): an autoregressive search engine, proposing constrained decoding with the FM-index data structure that enables autoregressive models to retrieve passages.",
            "score": 0.603112240380272,
            "section_title": "Baselines",
            "char_start_offset": 13984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 51,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 100,
                    "end": 124,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 185,
                    "end": 203,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 507,
                    "end": 532,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3701171875
        },
        {
            "corpus_id": "252872857",
            "title": "Query Expansion Using Contextual Clue Sampling with Language Models",
            "text": "In Table 2, we show both the retrieval accuracy and index size. Note that the index size should be considered with salt since it largely depends on the system implementation. The baseline models are reported in their open-sourced versions. We additionally compare with other memory-efficient neural retrieval models in the appendix A.4 and report retrieval time latency in the appendix A.5. Compared with other lexical retrieval models, our method significantly outperforms both GAR and SEAL, showing the effectiveness of extensively sampled contextual clues. We also find that Oursmulti consistently improves over Ours-single. We surmise that ground-truth answers serve as useful signals during retrieval and they are more likely to be covered when directly sampling answers. Most of the traditional lexical retrieval methods always trail behind dense retrieval by a large margin, as illustrated in Table 2. Surprisingly, our method even outperforms the DPR model by 0.6 and 1.0 points in terms of top-100 accuracy on two datasets, while requiring 96% less index storage space. For the purpose of pushing the limit of retrieval performance, we also show the accuracy of different lexical-based methods fused with DPR. Overall, our method fused with DPR achieves the highest accuracy across all baseline methods on both datasets. ference between FiD and Ours is that FiD retrieves from DPR. Although ours-single is 0.2 points lower on Top-100 accuracy than that of DPR, the EM score of Ours-single is \u223c 2 points higher than FiD. It demonstrates even with relatively same top-k retrieval accuracy, that our approach could retrieve qualitatively better passages that are easier for the reader model to answer.",
            "score": 0.6016843034248949,
            "section_title": "Main Retrieval Results",
            "char_start_offset": 10732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1707
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138916015625
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "for RQ1 -Overall Effectiveness of ColBERT-PRF. In this section, we examine the effectiveness of a pseudo-relevance feedback technique for the ColBERT dense retrieval model on passage ranking task. On analysing Table 2, we first note that the ColBERT dense retrieval approach outperforms the single representation based dense retrieval models, i.e., ANCE and its PRF variant ANCE-PRF for all metrics on both test query sets, probably because the single representation used in ANCE provides limited information for matching queries and documents [23]. In particular, compared with ANCE-PRF, ColBERT-PRF shows markedly improvement on all metrics for both query sets and shows significant improvement in terms of MAP on TREC 2019 and NDCG@10 on TREC 2020. This indicates that the PRF mechanism that explicitly expands query with expansion embeddings to refine the query representation is superior to implicitly learning from PRF information to form a better query representation. \n\nBased on this, we then compare the performances of our proposed ColBERT-PRF models, instantiated as ColBERT-PRF Ranker & ColBERT-PRF ReRanker, with the more effective ColBERT E2E model. We find that both the Ranker and ReRanker models outperform ColBERT E2E on all the metrics for both used query sets. Typically, on the TREC 2019 test queries, both the Ranker and ReRanker models exhibit significant improvements in terms of MAP over the ColBERT E2E model. In particular, we observe a 26% increase in MAP on TREC 2019 8 and 10% for TREC 2020 over ColBERT E2E for the ColBERT-PRF Ranker. In addition, both ColBERT-PRF Ranker and ReRanker exhibit significant improvements over ColBERT E2E in terms of NDCG@10 on TREC 2019 queries. \n\nThe high effectiveness of ColBERT-PRF anker (which is indeed higher than ColBERT-PRF ReRanker) can be explained in that the expanded query obtained using the PRF process introduces more relevant passages, thus it increases recall after re-executing the query on the dense index. As can be seen from Table 2, ColBERT-PRF Ranker exhibits significant improvements over both ANCE and ColBERT E2E models on Recall.",
            "score": 0.6011931267383075,
            "section_title": "Results",
            "char_start_offset": 31455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2193603515625
        },
        {
            "corpus_id": "271534382",
            "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
            "text": "Dense retrieval has become an important research area following the development of pre-trained Transformer language models (PLMs) [2], [19]- [22]. To enhance text retrieval performance, dense retrieval leverages PLM-based text embeddings to encode queries and documents into a shared semantic vector space, focusing on matching semantic contents beyond mere keywords. This text embedding application in retrieval is fundamental to Retrieval-Augmented Generation (RAG) [23], which reduces the hallucinations in LLMs. Recent advancements in dense retrieval include architectural innovations, optimized training methodologies, and efficient indexing techniques, all of which contribute to improved retrieval accuracy and efficiency [4]- [8], [24]. Since the introduction of Sentence-BERT [25] and Dense Passage Retrieval (DPR) [2], numerous sentence embedding models have been proposed to enhance dense passage retrieval. Advanced sentence embedding models, which have been highlighted in the retrieval task of massive text embedding benchmark (MTEB) [26], include Contriever [27], M3E 2 , BGE [9], etc. Our text augmentation method serves as a preprocessing module for dense retrieval and is compatible with various embedding models mentioned above.",
            "score": 0.601077064328086,
            "section_title": "A. Dense Retrieval",
            "char_start_offset": 10070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "236772078"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "149698624"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "252968355",
            "title": "Making a MIRACL: Multilingual Information Retrieval Across a Continuum of Languages",
            "text": "As the relevance judgements were provided on query-passage pairs, the total number of articles is provided primarily as a reference. The final column indicates whether the language is part of Mr. TyDi and TyDi QA, or is one of the new languages. It is our expectation that there are sufficient examples in the training and development sets of MIRACL to train (specifically, finetune) transformer-based retrieval models. We believe that MIRACL will provide a valuable dataset for building and evaluating dense retrieval models such as DPR [10], late-interaction models such as ColBERT [11], as well as reranking models such as monoBERT [19] and monoT5 [20]. For these classes of retrieval models, there is an emerging thread of research [17,18,23,29] we hope that MIRACL will further catalyze.",
            "score": 0.6009267500921598,
            "section_title": "DATASET OVERVIEW",
            "char_start_offset": 10054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 792
                }
            ],
            "ref_mentions": [
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "209515542"
                },
                {
                    "start": 740,
                    "end": 743,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 743,
                    "end": 746,
                    "matchedPaperCorpusId": "226284008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0855712890625
        },
        {
            "corpus_id": "270710900",
            "title": "DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs",
            "text": "To Answer RQ1, we evaluate lexical, late-interaction, and diverse dense-retrieval models on the curated datasets in an open-domain setup.From Table 2, we observe that the lexical model BM25 is a strong baseline across all complex QA tasks, including reasoning over hybrid sources.We observe that DPR has the lowest performance, indicating that while this model is suited for simpler open-domain QA tasks, it falls short on complex QA tasks.We observe that though DPR was trained on Natural Questions it falls short on AmbigQA which is a derivative of natural questions.We posit that this is because the bi-encoder model was not trained to handle ambiguity, indicating that complex QA requires further care compared to traditional questions.They fall short on tasks that require multistep reasoning during retrieval or when retrieving hybrid evidence like tables and text from multiple sources.Surprisingly, we also observe that models like contriever that are pre-trained for retrieval underperform compared to lexical models like BM25 and other dense retrieval models like tas-b and MPNet.We also observe that neural sparse models like SPLADE are competitive or even outperform dense retrieval models and prove to be strong baselines following BM25.\n\nLate-Interaction model ColBERT (v2) [43] outperforms other methods on 5/7 datasets.We posit that cross-attention-based operation on tokenwise representations helps capture intricate relationships between query and context and also between different modalities of evidence such as table and text.This aids in higher retrieval performance on datasets like StrategyQA that require implicit reasoning and also on all table and text-based QA datasets compared to other dense retrieval models.We also find similar observations in individual works like [65] which employs cross-attention fusion between table and text evidence and [59] which employs cross-attention for multi-hop reasoning.The model used is gpt-3.5-turbo,unless otherwise specified.Insight 1: Pre-trained dense retrieval models fall short on retrieval performance for compositional questions and when retrieving from hybrid knowledge sources.In contrast, lexical models serve as strong baselines and late-interaction models demonstrate significant potential.",
            "score": 0.6004607259841588,
            "section_title": "Comparison of Retrieval Models on Diverse Complex QA Tasks",
            "char_start_offset": 19839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1250
                },
                {
                    "start": 1252,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1739
                },
                {
                    "start": 1739,
                    "end": 1935
                },
                {
                    "start": 1935,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2154
                },
                {
                    "start": 2154,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 1288,
                    "end": 1292,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "268031986",
            "title": "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision",
            "text": "We compare our proposed model with a range of strong baselines: \n\n\u2022 mDPR. This is the multilingual version of Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) encoder, which undergoes initial training on English NQ queries followed by fine-tuning on XOR-Retrieve. \u2022 DPR+MT (Asai et al., 2021a). This is a translate-test baseline that involves the translation of queries into English during test time, followed by monolingual passage retrieval using the English DPR encoder. \u2022 CORA (Asai et al., 2021b). This method trains a multilingual DPR encoder iteratively, with positive and negative passages identified by a multilingual QA model. \u2022 Sentri (Sorokin et al., 2022). An iterative self-training method that uses the latest retriever to identify positive and negative passages through answer string matching for updating the training dataset. Machine translation is used for data augmentation. \u2022 QuiCK (Ren et al., 2022). A knowledge distillation method that trains a multilingual biencoder retriever, learning from a query generator as the teacher. The query generator is also used for generating synthetic multilingual queries to enhance knowledge distillation. \u2022 DrDecr (Li et al., 2022). A multilingual ColBERT model that learns from an English ColBERT on parallel queries, sourced from both parallel corpora and human-translated gold queries released by XOR-Retrieve. \u2022 LAPCA (Abulkhanov et al., 2023). A pretraining method that takes the first paragraphs of parallel Wikipedia pages as the parallel corpus for cross-lingual pre-training, with augmented data through machine translation. \u2022 SWIM-X (Thakur et al., 2023). A method that uses large language models to generate synthetic queries from unlabelled corpus with textual summary generation as an intermediate step. A multilingual dense retrieval model is fine-tuned exclusively on synthetic data.",
            "score": 0.6004196452544079,
            "section_title": "A.3.1 Cross-lingual Passage Retrieval",
            "char_start_offset": 32959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 66,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 164,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 279,
                    "end": 299,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 487,
                    "end": 507,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 652,
                    "end": 673,
                    "matchedPaperCorpusId": "250390748"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "256461230"
                },
                {
                    "start": 1180,
                    "end": 1197,
                    "matchedPaperCorpusId": "245144717"
                },
                {
                    "start": 1388,
                    "end": 1413,
                    "matchedPaperCorpusId": "259950018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.163818359375
        },
        {
            "corpus_id": "247594093",
            "title": "Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval",
            "text": "On the other hand, recent bi-encoder dense retrieval models (Karpukhin et al., 2020;Santhanam et al., 2021;Hofst\u00e4tter et al., 2021;Formal et al., 2021;Gao and Callan, 2021) have been shown to be highly effective for in-domain retrieval while remaining efficient for large corpora in practice. Compared to traditional retrieval methods, these models use dual deep encoders, usually built on top of a Transformer encoder architecture (Vaswani et al., 2017;Devlin et al., 2019), to encode query and document passages separately and eventually output contextual embeddings. Similarity scores can be efficiently computed given these embeddings, which limits costly neural inference to indexing time. The effectiveness of these models can be attributed to the encoder's ability to capture contextual connections or even high-level semantics without the necessity for exact lexical matching. This very complementary benefit compared to more rigorous structure search methods motivates us to investigate whether dense retrieval models can improve MIR results when combined with existing structure search methods. We summarize the contributions of this work as follows: \n\n\u2022 We have performed a fair effectiveness comparison of a token-level and a passage-level dense retrieval baseline in the MIR domain. To our knowledge, this is the first time that a DPR model has been evaluated in this domain. \n\n\u2022 We have successfully combined dense retrievers with a structure search system and have been able to achieve new state-of-the-art effectiveness in recent MIR datasets. \n\n\u2022 A comprehensive list of dense retrievers and strong baselines for major MIR datasets are covered and compared. We believe our well-trained models and data pipeline 1 can serve as a stepping stone for future research in this domain, which suffers from a scarcity of resources. \n\n2 Background and Related Work",
            "score": 0.600134586841256,
            "section_title": "Introduction",
            "char_start_offset": 2041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 131,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "259275169",
            "title": "Confidence-Calibrated Ensemble Dense Phrase Retrieval",
            "text": "The passage retrieval problem, which is of central importance in search engine optimization and text analytics, entails the following: given a set of documents and a query, determine which document best answers the question. This document retrieval step is an important intermediary in the more involved problem of open-domain (extractive) question answering (OPQA), in which one is tasked with finding the specific substring in the relevant document that contains the answer to the question. In this paper, we focus on one of the state-of-the-art models for automated document retrieval: the transformerbased dense passage retrieval algorithm, developed by (Karpukhin et al., 2020). \n\nOur analysis is primarily motivated by the observation in (Lee et al., 2021) that DPR can yield improved performance when applied to smaller chunks of text (e.g. sentences rather than paragraphs). This slight adaptation of DPR is known as dense phrase retrieval. In our work, we view phrase length as a parameter which can be altered to create slightly different DPR models. One can view the embeddings from each phrase length as the \"expert\" of a certain granularity, and our prediction proceeds in an ensemble fashion, where we choose the expert with the highest confidence. This motivates the use of confidence calibration, which we accomplish using temperature scaling of the softmax output (Guo et al., 2017). \n\nWe apply this method to a range of datasets, including both large canonical datasets (Google NQ, SQuAD) and smaller domain-specialized datasets relating to information retrieval in law, medicine, and scientific research, respectively.",
            "score": 0.6000510220153783,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1637
                }
            ],
            "ref_mentions": [
                {
                    "start": 1381,
                    "end": 1399,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Large-scale passage retrieval [2] aims to fetch relevant passages from a million-or billion-scale collection for a given query to meet users' information needs, serving as an important role in many downstream applications including open domain question answering [20], search engine [56], and recommendation system [54], etc. Recent years have witnessed an upsurge of interest and remarkable performance of dense passage retrievers on first-stage retrieval. Built upon powerful pre-trained language models (PLM) [9,30,36], dense retrievers [20,35,49] encode queries and passages into a joint low-dimensional semantic space in a Siamese manner (i.e. dual-encoder), so that the passages could be offline pre-indexed and query could be encoded online and searched via approximate nearest neighbor [19], reaching an efficiency-effectiveness trade-off. \n\nAlthough dense retrieval becomes indispensable in modern systems, a long-term challenge is that the dense representations in a latent semantic space are abstractive and condensed, exposing the systems to a risk that pivot phrases and mentions may be overlooked and thus leading to sub-optimal efficacy. For example, DPR [20] didn't regard \"Thoros of Myr\" as an entity mention in the query \"Who plays Thoros of Myr in Game of Thrones?\". Analogously, given the query \"What is an active margin\", ANCE [49] overlooked the \"active margin\" as an entire local salient phrase and hence retrieved passages related to the financial term \"margin\". As a remedy, prior works resort to either coupling a dense retriever with the term matching scores (e.g., TF-IDF, BM25) [7,14,26,33] or learning BM25 ranking into a dense model as additional features to complement the original one [4]. But, these approaches are limited by superficial combinations and almost unlearnable BM25 scoring.",
            "score": 0.5995840274903138,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 544,
                    "end": 547,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1610,
                    "end": 1613,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2802734375
        },
        {
            "corpus_id": "265213270",
            "title": "Scalable and Effective Generative Information Retrieval",
            "text": "Fine-tune the Pre-trained language models (LMs) [11,24,34,39] on information retrieval (IR) tasks have proven to be more effective compared to traditional models [19,30,46], such as BM 25 in various scenarios. This might because LMs, pre-traiend on vast amounts of text data, can have more deep understanding of language semantics. The contextualized representations by LMs also provide flexibility to make it adapt to different IR model designs. The integration of these LMs with neural IR models can be broadly categorized into four main streams: (1) neural sparse retrieval models, (2) neural re-ranking models, (3) dense retrieval models, and (4) generative retrieval models. \n\nNeural sparse retrieval models, inspired by conventional bag-ofwords approaches like TF-IDF [36] and BM25 [37], adapt BERT to reweight subwords, thereby enhancing IR performance. To maintain the sparsity of high-dimensional vectors, they utilize L1 [48] or Flop [13] regularizers. This characteristic sparsity allows them to be incorporated into fast search frameworks based on the inverted index [38]. \n\nRe-ranking with LMs is another approach where LMs serve as re-rankers [30,52]. By feeding a concatenated query and document, these models produce a relevance score. Despite their often superior performance, they are only suited for document re-ranking due to efficiency constraints. \n\nDense retrieval models are based on bi-encoder architectures [16,17,19,20,33,46,49,50]. These models, typically leveraging BERT, encode each document and query into dense representations. For efficient retrieval, they employ approximated nearest neighbor (ANN) search [26,46]. Lastly, the generative retrieval paradigm [3,41] is an innovative approach drawing inspiration from successful generative LMs [8,31,34]. In this paradigm, models like T5 are treated as retrievers. Each document is mapped to a distinct sequence, often denoted as a DocID. At inference, given a specific query, a constrained beam search [41,51] retrieves a list of the most probable DocIDs.",
            "score": 0.5988483429566986,
            "section_title": "RELATED WORK",
            "char_start_offset": 32782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 1440,
                    "end": 1443,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1859130859375
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "PLM-based retriever. PLM-based retrievers usually compute low dimensional representations for the query and passage using the encoder of pre-trained transformer [55], such as BERT [9] and RoBERTa [31]. DPR [24] is the first to leverage PLM for the task of semantic retrieval, while extensive methods are subsequently proposed to improve the effectiveness. In particular, most of the existing PLM-based retrievers improve the model performance from the following aspects. (1) By introducing late-interactions after encoding: ColBERT [25], COIL [16] and ME-BERT [35] are three representative studies that explicitly model the interactions after query/passage encodings. The performance is largely boosted by the interactions compared with DPR (i.e., vanilla dual-encoder), while the late interaction also brings significant computational overhead. \n\n(2) By designing effective fine-tuning processes: For example, ANCE [63] proposes to a hard negative sampling technique that greatly improve the effectiveness. Moreover, RocketQAv1 [46] and RocketQAv2 [49] boost the performance of dense retrieval models by leveraging the power of cross-encoder. The relevance features captured through sufficient interaction by the cross-encoder could be properly transferred to retrievers in a cascade or joint training manner. ERNIE-Search [34] narrows the divide between crossencoder and dual-encoder models through on-the-fly distillation in the process of fine-tuning. ColBERTv2 [52] further improves ColBERT by employing fine-tuning with distillation. (3) By designing pre-training tasks tailored for retrieval: A handful of studies focused on constructing pseudo-training data for retrievaloriented pre-training, such as ICT [2], COSTA [37], DCE [29], etc. Besides, several studies [27, 32, 33, 58-60, 60, 67] employ weak generative modules (i.e. decoder) to enhance the query/passage encoding through pre-training. Notably, after pre-training, the weak decoder is discarded and only the enhanced encoder is employed as the backbone of retriever.",
            "score": 0.5986826893630423,
            "section_title": "PLM-based IR models",
            "char_start_offset": 6947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 165,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1466,
                    "end": 1470,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311767578125
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "Dense retrieval, a novel paradigm in Information Retrieval (IR), has emerged with the advancement of deep neural networks. Unlike traditional IR methods, dense retrieval encodes both queries and documents as embeddings within a shared latent space, capturing their semantic relationships through embedding similarities. Dense retrieval models have become the predominant choice in recent neural retrieval approaches and are widely applied in various downstream tasks such as web search, question answering, and sentence similarity (Karpukhin et al., 2020;Xiong et al., 2020;Muennighoff et al., 2022). \n\nIn the past few years, dense retrieval models intensively adopted pre-trained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020), as their backbone encoders. These models excel in identifying semantic similarities between queries and documents. However, they still face significant challenges in becoming versatile enough to handle a wide range of retrieval tasks (Muennighoff et al., 2022). Their in-domain retrieval accuracy is often constrained by the capacity of their backbone encoders, such as the number of parameters (Ni et al., 2021). Additionally, dense retrieval models typically struggle to generalize to unseen data, necessitating fine-tuning with a large amount of labeled data to perform well in the target domain. Finally, achieving versatility in dense retrieval models requires training on multiple retrieval tasks simultaneously, which demands sufficient capacity from the backbone encoder (Zhang et al., 2023;Xiao et al., 2023). \n\nRecently Large Language Models (LLMs) have been prompted or fine-tuned as dense retrieval models and achieved improved performance across a wide range of retrieval tasks, thanks to their superior capability for semantic understanding and rich world knowledge (Li et al., 2023;Wang et al., 2023;Zhuang et al., 2024;Muennighoff et al., 2024). These models vary in parameters from 2 billion to 56 billion, with pre-training sufficiency ranging from hundreds of billions to tens of trillions of tokens, and include both base models and human preference aligned chat models.",
            "score": 0.5971652550243189,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30615234375
        },
        {
            "corpus_id": "272654178",
            "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
            "text": "Formalization and Challenge Given a query utterance u containing a rare word w, we aim to retrieve a relevant example (u e , y e ) from an example pool D = {(u 1 , y 1 ), . . . , (u m , y m )} with a retrieval model r, such that the rare word w is spoken in utterance u e . Here u i indicates the i-th utterance and y i its translation. As the query u is only in speech, we face additional complexities compared to text-based retrieval. First, speech is versatile, unlike text, which often has a standard writing system. The speaking condition for the same word varies in every recording, requiring a robust retriever that accounts for pronunciation variations. Second, speech sequences are magnitudes longer than text. The retriever must find fine-grained local features corresponding to the keywords in long sequences. Third, transcribing the query utterance first and then using text-based retrieval is suboptimal due to ASR errors, especially on rare words. \n\nArchitecture As the nature of our example retrieval task resembles information retrieval (IR) where relevant answers are retrieved given a question, we take inspiration from IR approaches for our retriever. In text-to-text IR, a prominent architecture is the Dense Passage Retriever (DPR) (Karpukhin et al., 2020). It has a dual-encoder architecture, where one encoder encodes the questions, and the other encodes the passages potentially containing answers to the questions. The retrieval model is trained with a contrastive objective, mapping question-passage (positive) pairs closer to each other in the latent space while pushing irrelevant (negative) pairs further apart. During inference, passages closer to the encoded question by the dot-product similarity are returned as answers. In our case, the utterances containing the same rare words are considered positive pairs, while those not sharing the same rare words are negative pairs. Speech-to-Speech/Text Retrieval We propose to extend the DPR model to support querying from speech.",
            "score": 0.5963502531752048,
            "section_title": "Example Retrieval",
            "char_start_offset": 6614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1253,
                    "end": 1277,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1864013671875
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Passage retrieval, which matches relevant passages to queries, is an important task in Information Retrieval (IR). It can be also integrated as a core component to solve many Natural Language Processing (NLP) problems, e.g., open domain question answering (Chen et al., 2017), fact checking (Thorne et al., 2018), etc. Powered by large scale pretrained language models (e.g. BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020)), dense passage retrieval, which explores dense vector representations to match relevant passages, has attracted growing interest (Gao et al., 2020;Khattab and Zaharia, 2020;Karpukhin et al., 2020;MacAvaney et al., 2020a;Qu et al., 2021;Zhan et al., 2021;Xu et al., 2022;Gao et al., 2021a). Dense passage retrieval models usually adopt a bi-encoder (also known as dual-encoder) architecture, one encoder for encoding queries, the other for learning passage representations, which can be computed offline. With dense representations of a query and document, passage ranking is recast as a nearest neighbor search problem that can be efficiently solved by similarity search toolkits tailored for dense vectors, such as Faiss (Johnson et al., 2019). \n\nDense passage retrieval models are usually trained via Contrastive Learning (CL), which encourages query representations to be close to positive (i.e., relevant) passages and away from negative (i.e., irrelevant) passages in the learned semantic space. Many previous studies demonstrate the effectiveness of CL in dense passage retrieval (Karpukhin et al., 2020;Qu et al., 2021;Xu et al., 2022). Under this CL-based dense passage retrieval setting, using more negative samples has proved beneficial to models (Wu et al., 2020;Chen et al., 2020;He et al., 2020;Giorgi et al., 2021;Gao et al., 2021c).",
            "score": 0.5960349854998596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 438,
                    "end": 459,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "222205492"
                },
                {
                    "start": 608,
                    "end": 634,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 634,
                    "end": 657,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 681,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 681,
                    "end": 697,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 715,
                    "end": 731,
                    "matchedPaperCorpusId": "247411106"
                },
                {
                    "start": 731,
                    "end": 749,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1183,
                    "end": 1205,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1547,
                    "end": 1571,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1571,
                    "end": 1587,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1587,
                    "end": 1603,
                    "matchedPaperCorpusId": "247411106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35107421875
        },
        {
            "corpus_id": "266902943",
            "title": "Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation",
            "text": "The classification head is attached to the last hidden state of the starter <s> token. We concatenate the training query and passage as the input text sequence and use crossentropy loss to train the model. For the English-Trained (ET) cross-encoder, we provide the native MS MARCO training queries and passages in English; for Translate-Train (TT), we translate the passages into the document language of the final CLIR task (Chinese, Persian, or Russian) and use those with English queries, resulting in three TT cross-encoder models. \n\nCLIR Student Dual-encoder. We use ColBERT-X, a CLIR variant of the ColBERT [15] retrieval architecture, as the dual-encoder model. Prior work has shown that singlevector dual-encoder models, such as DPR-X [2,42], and learned-sparse models, such as BLADE [26], are substantially less effective than multi-vector dense dual-encoders [42]. Therefore, we evaluate Translate-Distill with ColBERT-X as the student model. Trained models are available on Huggingface Models. \n\nWe use the PLAID [36] implementation for ColBERT-X retrieval. Both evaluation queries and passages are processed in their native languages without any translation. The number of residual bits is set to one for each dimension of the passage representation. Based on our preliminary studies, the number of residual bits has a substantial impact on index size and query latency but not on retrieval effectiveness. In some cases, using only one bit may result in numerically better retrieval results than using two or four bits. \n\nThe student CLIR dual-encoder models are trained with KL-divergence loss on the predicted query-passage scores, on a batch of 64 queries distributed across eight NVidia V100 GPUs (with the DGX platform). Each query is associated with six passages randomly sampled from the candidate set at every epoch. Such random selections enable larger batches while stochastically allowing the models to see all candidate passages. We use the AdamW optimizer with a learning rate of 5 \u00d7 10 \u22126 with 16-bit floating point.",
            "score": 0.595699694112765,
            "section_title": "Teacher and Student Models",
            "char_start_offset": 20109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "248392078"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "259371268"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "248392078"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "248887228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13671875
        },
        {
            "corpus_id": "270380194",
            "title": "Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval",
            "text": "Generative Retrieval (GR) has emerged as a promising approach within Information Retrieval, particularly for text retrieval tasks [3,40,42,21].This approach involves creating a set of document identifiers that represent documents from the original corpus.A generative model is then trained to generate document identifiers for an input query.The generated identifiers are subsequently mapped back to the corresponding documents in the corpus.GR methods typically utilize an autoregressive (AR) language model to generate the document identifier as a sequence of words or sub-words tokens from a predefined target vocabulary.By leveraging high-quality document identifiers and capturing complex dependencies between tokens through the autoregressive generation process, GR has achieved substantial improvements in retrieval performance in recent years [3,21,20].Despite these advancements, deploying GR models in low-latency applications, such as sponsored search, remains a significant challenge due to the high inference complexity of AR models [19,28].This stems from their sequential token-by-token generation mechanism [13].To address this challenge, our paper explores the use of non-autoregressive (NAR) language models for GR.These models significantly reduce inference costs by generating all tokens of the document identifier simultaneously.However, this parallel generation limits the model's ability to capture dependencies among tokens (words, sub-words) in the output identifier, leading to inferior retrieval performance compared to AR-based GR models.To enable NAR-based GR to leverage word and sub-word interactions during generation, we propose expanding the model's target vocabulary by incorporating phrases within the document identifiers as tokens.Intuitively, predicting high-probability phrases at each position in the output sequence allows the NAR model to better understand the intricate relationships between words and sub-words within each predicted phrase, potentially enhancing retrieval performance.This forms the basis of our first research question:\n\n(RQ1)-How does the retrieval accuracy of a NAR-based GR model (with a target vocabulary containing word/sub-word level tokens) change when the target vocabulary is expanded to include phrases from document identifiers as additional tokens?",
            "score": 0.594780001693362,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2082
                },
                {
                    "start": 2084,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "259262395"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "259075708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267822265625
        },
        {
            "corpus_id": "270370926",
            "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
            "text": "We now investigate information retrieval capabilities across learning objectives through the lens of different factorizations of the joint sequence probability.Specifically, we compare \u2022 AR: The standard autoregressive causal next-token prediction.Though all models generate tokens autoregressively, we use AR as a shorthand for left-to-right models, in line with Equation (1).\n\n\u2022 AR w/reverse: AR prediction on sequences and their token-level reverse.2\n\n\u2022 MLM r: BERT-like masked language modeling with fixed random masking rate, r.\n\n\u2022 MLM-U: MLM with r \u223c U(0, 1).PLM results are similar, and are reported in the Appendix.\n\nTo ensure a fair comparison and allow each objective to perform optimally, we employ model architectures specifically designed for each objective.For autoregressive (AR) training, we use GPT-2 (Radford et al., 2019) and Mistral (Jiang et al., 2023).For masked language modeling (MLM), we use BERT (Devlin et al., 2019).\n\nFinally, for MLM-U, we employ an encoder-decoder model3 based on the GPT architecture (see Appendix G for details).\n\nWe study these models across increasing levels of complexity and realism, beginning with a controlled retrieval task using synthetic tokens to a retrieval task using natural text from Wikipedia articles.In our evaluation, we find that the degree to which the learning objective factorizes the input reliably explains performance across this wide range of information retrieval tasks.Factorization-agnostic methods show improved knowledge retrieval capabilities.",
            "score": 0.594130422305482,
            "section_title": "Experiments",
            "char_start_offset": 11176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 623
                },
                {
                    "start": 625,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 944
                },
                {
                    "start": 946,
                    "end": 1061
                },
                {
                    "start": 1063,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1524
                }
            ],
            "ref_mentions": [
                {
                    "start": 922,
                    "end": 943,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1884765625
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "This is particularly noteworthy as it marks the first time that generative retrieval has surpassed DPR in all metrics under the full corpus set setting. Turning to TriviaQA, our results show that LTRGR outperforms DPR in hits@100, but falls behind in hits@5 and hits@20. The reason for this is that MINDER, upon which LTRGR is based, performs significantly worse than DPR on TriviaQA. It's worth noting that generative retrieval methods rely on identifiers and cannot \"see\" the content of the passage, which may explain the performance gap between MINDER and DPR on TriviaQA. Additionally, generative retrieval methods have an error accumulation problem in an autoregressive generative way.",
            "score": 0.5939041490399966,
            "section_title": "Implementation Details",
            "char_start_offset": 17044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06744384765625
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Dense Retrieval The field of information retrieval (IR) (Manning et al., 2005) aims to find the relevant information given an ad-hoc query and has played a key role in the success of modern search engines. In recent years, IR has witnessed a paradigm shift from traditional BM25-based inverted index retrieval to neural dense retrieval (Yates et al., 2021;Karpukhin et al., 2020). BM25-based retrieval, though efficient and interpretable, suffers from the issue of lexical mismatch between the query and passages. Methods like document expansion (Nogueira et al., 2019) or query expansion (Azad and Deepak, 2019;Wang et al., 2023) are proposed to help mitigate this issue. In contrast, neural dense retrievers first map the query and passages to a low-dimensional vector space, and then perform semantic matching. Popular methods include DSSM (Huang et al., 2013), C-DSSM (Shen et al., 2014), and DPR (Karpukhin et al., 2020) etc.\n\n1 Also called dual-encoder / two-tower encoder.\n\nInference can be done efficiently with approximate nearest neighbor (ANN) search algorithms such as HNSW (Malkov and Yashunin, 2020). Some recent works Reimers and Gurevych, 2021;Sciavolino et al., 2021) show that neural dense retrievers may fail to capture some exact lexical match information. To mitigate this issue,  proposes to use BM25 as a complementary teacher model, ColBERT (Khattab and Zaharia, 2020) instead replaces simple dot-product matching with a more complex token-level MaxSim interaction, while COIL (Gao et al., 2021) incorporates lexical match information into the scoring component of neural retrievers. Our proposed pre-training method aims to adapt the underlying text encoders for retrieval tasks, and can be easily integrated with existing approaches.\n\nPre-training for Dense Retrieval With the development of large-scale language model pre-training (Dong et al., 2019;Clark et al., 2020), Transformerbased models such as BERT (Devlin et al., 2019) have become the de facto backbone architecture for learning text representations",
            "score": 0.5937118300782432,
            "section_title": "Related Work",
            "char_start_offset": 3616,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 356,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 356,
                    "end": 379,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 872,
                    "end": 891,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 1086,
                    "end": 1113,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1133,
                    "end": 1160,
                    "matchedPaperCorpusId": "229680000"
                },
                {
                    "start": 1160,
                    "end": 1184,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1501,
                    "end": 1519,
                    "matchedPaperCorpusId": "237581068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "267320756",
            "title": "Re3val: Reinforced and Reranked Generative Retrieval",
            "text": "\u2022 We incorporate REINFORCE (Williams, 1992) to integrate information during the decoding process of generative retrieval. Combined with question generation, REINFORCE enables Re3val to outperform CorpusBrain zero-shot retrieval with an average improvement of 8% in R-Precision across five tasks. \n\n\u2022 We suggest a new generative \"retrieve and read\" pipeline that extracts the contexts for the reranked page titles, applies our context reranker, and grounds answers with the reranked contexts. As a result, Re3val distinguishes itself by achieving the highest KILT scores among other generative retrieval models, with an average increase of 2.1%. \n\nIn summary, Re3val uses DPR contexts for reranking page titles, leading to improved R-Precision. Re3val enhances performance by integrating generated questions in pre-training and utilizing REINFORCE during distant supervision. Moreover, Re3val achieves more accurate answers by reading reranked contexts retrieved with the reranked page titles. These advancements enable Re3val to achieve state-of-the-art performance while also offering cost savings by reducing training time and minimizing the need for extensive data labeling. \n\n2 Related Work 2.1 Document Retrieval TF-IDF (Johns, 1972) and BM25 (Robertson et al., 2009) assign weight to terms in a document based on their term frequency and inverse document frequency. These methods cannot inherently consider semantic shift or distribution similarity while computing similarity metrics. In light of this limitation, Karpukhin et al. (2020) introduce the Dense Passage Retrieval (DPR), establishing a bi-encoder that creates dense embeddings of questions and related passages within a corpus. These embeddings are subsequently compared using a dot product operation. During inference, DPR retrieves the top-k relevant contexts employing either Nearest Neighbor Search or Maximum Inner Product Search on the FAISS index. Guu et al. (2020) and Lewis et al. (2020) retrieve knowledge from a corpus using DPR and generate an answer using a variant of the Transformer models. FiD (Fusion in Decoder) (Izacard and Grave, 2021) extends T5 (Wolf et al., 2020) by combining independently encoded queries and retrieved passages to decode an answer.",
            "score": 0.593394904812674,
            "section_title": "Introduction",
            "char_start_offset": 2164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 43,
                    "matchedPaperCorpusId": "2332513"
                },
                {
                    "start": 1248,
                    "end": 1272,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1520,
                    "end": 1543,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1923,
                    "end": 1940,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1945,
                    "end": 1964,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "267320756",
            "title": "Re3val: Reinforced and Reranked Generative Retrieval",
            "text": "The primary objective of retrieval models is to enhance the accuracy of answers by selecting the most relevant documents retrieved for a given query, ensuring models have sufficient information to help the downstream reasoning process. For instance, DRQA (Chen et al., 2017) introduces a \"retrieve and read\" pipeline using TF-IDF to return documents for a question answering model to achieve this goal. More recently, NLP researchers have studied neural retrieval models like Dense Passage Retrieval (DPR) (Karpukhin et al., 2020)    \u2020 Work performed while at KAIST AI. \n\nFigure 1: Re3val's Page Title Reranker (g \u03d5 ) enhances generated page titles (X) with DPR contextual information (Y ), producing reranked titles (Z). This is crucial when documents in X lack a suitable answer to a query (q), as depicted in the figure . with a seq2seq model to build retrieval augmented language models. \n\nRather than using inner-product-based retrieval, generative retrieval models such as GENRE (Cao et al., 2021) and CorpusBrain (Chen et al., 2022) generate page titles through constrained decoding, attaining higher R-Precision and Recall compared to DPR. In our work, we further evaluate how additional contextual information can benefit the generative retrieval models through reranking and how reinforcement learning can enhance relevance through reward signals. \n\nWe introduce Re3val: Reinforced and Reranked Generative Retrieval, a novel framework specifically designed to address the challenges in neural information retrieval. Our approach utilizes 500k pre-training data and 48k task-specific data for training. Despite the reduced data used in distant supervision, Re3val achieves exceptional performance. Our contributions are described as below: \n\n\u2022 We minimize the entropy of the initially retrieved page titles with contexts obtained from DPR, facilitating the novel generative reranking process. Through this reranking procedure, Re3val outperforms other generative retrieval models, including GENRE, Corpus-Brain, and SEAL (Bevilacqua et al., 2022) in terms of average R-Precision across five tasks, showcasing an average increase of 1.9%. \n\n\u2022 We incorporate REINFORCE (Williams, 1992) to integrate information during the decoding process of generative retrieval.",
            "score": 0.5933938940404232,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2146
                },
                {
                    "start": 2149,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 274,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1020,
                    "end": 1039,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 2030,
                    "end": 2055,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2166748046875
        },
        {
            "corpus_id": "271270159",
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "text": "Queries and documents can be represented using low-dimensional (compared to the vocabulary size) dense vectors. Such dense vectors are often obtained using pre-trained language models, such as BERT [37], that are fine-tuned for retrieval tasks [96]. Dense retrieval models are commonly based on bi-encoder architectures -one encoder for representing the query and another for the document. These encoders could share parameters. Some dense retrieval methods, such as DPR [96], represent each query or document by a single dense vector. While others, such as ColBERT [103], use one vector per token, resulting in multiple vectors for each query and document. Approximate nearest neighbor (ANN) algorithms, such as HNSW [140], are used for efficient retrieval when dealing with dense representations. Dense retrieval approaches are also commonly used when dealing with multi-media and multi-modal data [172,184].",
            "score": 0.5933284264242702,
            "section_title": "Retrieval Models with Dense Representations",
            "char_start_offset": 18515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 910
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 566,
                    "end": 571,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 900,
                    "end": 905,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "258331649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], \n\nto probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance. \n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [51,54], late interaction [20]. Recently, researchers have also investigated pre-training tasks for dense retrieval [12,33]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually come with considerable memory demands and computational overheads.",
            "score": 0.5931601742280778,
            "section_title": "Sparse and dense retrieval models",
            "char_start_offset": 3413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "12255087"
                },
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1417,
                    "end": 1420,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2322998046875
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.",
            "score": 0.5931117589035786,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 188,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 246,
                    "end": 263,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1025,
                    "end": 1049,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1068,
                    "matchedPaperCorpusId": "247476289"
                },
                {
                    "start": 1488,
                    "end": 1510,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1510,
                    "end": 1526,
                    "matchedPaperCorpusId": "233307194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.240234375
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages. For a comprehensive comparison, we report Recall@50 and Recall@1K on the test queries. Following the evaluation settings of MS-MARCO and DuReader retrieval , MRR is defined as the average of the reciprocal ranks of the first relevant passage for a set of queries. The MRR is a value between 0 and 1, with a higher value indicating that the system is better at ranking the most relevant passage higher in the list. Meanwhile, Recall is defined as the fraction of relevant passages that are retrieved among all relevant passages, also with a value between 0 and 1, where a higher value indicates that the system is better at retrieving all relevant passages. MRR and Recall measure different aspects of retrieval performance. MRR@ and Recall@ can be depicted as: \n\n@ = \n\nwhere I(\u2022) is a indicator function. The  in Eq. 5 denotes the position of the first relevant passage in the retrieved candidates of query . The   and  K   represent the relevant passages of query  and the position of passage  in the candidate list K  . Retrieval performance. We report the retrieval performance of baselines in Table 5. Compared to the traditional sparse retrieval method BM25, dual-encoder models significantly boost the retrieval performance on our dataset. The improvement can be attributed to the integration of two distinct sources of knowledge, i.e., latent knowledge obtained through unsupervised pre-training of language models on a massive corpus and relevance knowledge acquired through supervised training on our large-scale annotated dataset. Equipped with the strategy of negative mining proposed in recent studies [28], the retrieval performance of dual-encoder models could be further improved on T 2 Ranking. It is worth noting that the Recall@ metrics observed in T 2 Ranking are lower than those reported in other benchmarks with coarse-grained annotations. For instance, the Recall@50 of BM25 is .601 and .700 on MS-MARCO-DEV Passage and DuReader retrieval , respectively, and 0.4918 on our dataset.",
            "score": 0.5928623549002844,
            "section_title": "Retrieval Performance",
            "char_start_offset": 21771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1531982421875
        },
        {
            "corpus_id": "267938789",
            "title": "OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining",
            "text": "For academic knowledge acquisition, this subsection introduces the results of academic question answering. \n\nBaselines. We adopt sparse and dense retrieval methods: (1) Sparse retrieval methods: BM25, (2) Dense retrieval methods: DPR-FT (full fine-tuning of Dense Passage Retriever (DPR) [27]), DPR-PT2 (parameter-efficient fine-tuning of DPR with P-Tuning v2 [35]), ColBERT-FT (full fine-tuning of ColBERT [29]), ColBERT-PT2 (parameter-efficient fine-tuning of ColBERT with P-Tuning v2), and LLM-Embedder [71] (a fine-tuned LLM based on various retrievalrelated tasks). \n\nEvaluation Metrics. Hit@K is used to measure retrieval accuracy, reporting if the top  retrieved papers contain the correct answer. The average Hit@K across all questions is reported. \n\nExperimental Results. Figure 4 presents the results of the OAG-QA dataset. Generally speaking, dense retrieval methods outperform sparse retrieval methods. ColBERT-based methods are significantly better than DPR-based ones. This shows that by employing late interaction patterns and multi-vector representations, ColBERT models the correlation between questions and papers better. Interestingly, efficient parameter fine-tuning methods excel over full fine-tuning, possibly due to better knowledge retention and generalization from the pre-trained model. The effect of LLM-Embedder suggests there still exists noticeable gap between LLM and academic retrieval. Overall, these methods' retrieval effects are suboptimal, suggesting room for improvement.",
            "score": 0.5924079717855126,
            "section_title": "Academic Question Answering",
            "char_start_offset": 27019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1763916015625
        },
        {
            "corpus_id": "277272101",
            "title": "Dense Passage Retrieval in Conversational Search",
            "text": "Dense Passage Retrieval (DPR) is a first stage retriever having an architecture of dual-encoders where each encoder is an instantiation of BERT [7]. These encoders encode the query and passage into a dense vector space where relevant pairs are placed closer together than irrelevant ones. DPR uses the following similarity function to measure the relevance of a query q and passage p: \n\nThe model is then trained using the negative log likelihood as seen in the following equation: L(\u20d7 e q,i , \u20d7 e + p,i , \u20d7 e \u2212 p,i,1 , ..., \u20d7 e \u2212 p,i,n ) = \u2212log e sim(\u20d7 eq,i,\u20d7 e + p,i ) \n\ne sim(\u20d7 eq,i,\u20d7 e + p,i ) + n j=1 e sim(\u20d7 eq,i,\u20d7 e \u2212 q,i,j ) \n\nAs seen the conditional probability P r(\u20d7 e + p,i |\u20d7 e q,i ) is estimated using a sampled SoftMax for practicality. Additionally, DPR uses in-batch negatives for memory efficiency and one additional gold or hard negative that comes from a negative within the top-1000 retrieved passages from a BM25. Additionally, DPR can be indexes efficiently since the dense embeddings are in a lower dimensional continuous space. As previously mentioned in the introduction section we were unable to use hard-negatives due to lack of annotation, in the case of MSMARCO 70% [8] of the negative candidates in the top-1000 BM25 pool are false negatives. Therefore we used in-batch negatives for fine-tuning DPR and used DPR's [11] model weights to instantiate our base DPR model. To resolve this issue Approximate nearest neighbor Constrastive Estimation (ANCE) [17] is a computationally intensive technique for training a dense retriever. This technique asynchronously creates embeddings for the entire corpus from a checkpoint during training. Next the ANN index is updated so that negatives which are closest to the queries can be utilized for negative samples. As these samples are more informative during training and improve training convergence by creating large gradient norms.",
            "score": 0.5923722105253864,
            "section_title": "B. Dense Retrieval",
            "char_start_offset": 9560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2100830078125
        },
        {
            "corpus_id": "257495804",
            "title": "NeuroQL: A Neuro-Symbolic Language and Dataset for Inter-Subjective Reasoning",
            "text": "The two methods (BM25 and DPR) provide a trade-off for our users between speed and accuracy, with BM25 being the fastest and DPR being the most accurate. \n\nDense Passage Retrieval: DPR (Dense Passage Retrieval) is a method that uses dense text embeddings for both query and documents that need to be matched. It is based on a BERT bi-encoder architecture that computes a dot product similarity between a document and a query. Our DPR is based on [21], using question & context encoders trained with the Natural Questions dataset [25,23]. This DPR is then used as a backend for of our neural_match primitive. The neural_match primitive receives a sub-query whose results are used to create a set of (id, document) pairs. It then tries to match the documents against a target query text, using our DPR to return the top_k results that matched (see full example of neural_match in Section 2.1.1). \n\nReader Model: A Reader or reading comprehension model is a neural network that can perform extractive Q&A by returning relevant text intervals of documents. In our work we use the MiniLM [49] model initially trained on the SQuAD 2.0 dataset [37] and further fine-tuned on the NeuroQL training set to improve its performance (as we describe in Section 2.4). This Reader is then used as a backend for of our neural_extract primitive. The neural_extract primitive receives the name of a new variable to bind for extracted answers, a query to create (id, document) pairs and finally a target query text. It then tries to extract relevant text intervals from our documents using the Reader to return the top_k results found. We fine-tuned our reader model for 3 epochs, using a 384-token sequence length with a 128-token document stride. We used a batch size of 16, learning rate of 1e-5 with a 0.2 warmup and included the ability to return no answers when predicting results. \n\nTranslation Model: A translation model is a sequence-to-sequence neural network trained over pairs of input and target sequences.",
            "score": 0.5923462546195386,
            "section_title": "Methods",
            "char_start_offset": 27153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "266163052",
            "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
            "text": "Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.",
            "score": 0.5922916799181308,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36328125
        },
        {
            "corpus_id": "267334785",
            "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
            "text": "Research on the reader component includes Fusion-in-Decoder (FiD) (Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages independently in the encoder and RETRO (Borgeaud et al., 2022;Wang et al., 2023), which utilizes cross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context. End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoderdecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Generation) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural retriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a treedecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hierarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements in retrieval accuracy by combining document and passage level retrievals and integrating sparse and dense retrieval methods, respectively (Liu et al., 2021;Arivazhagan et al., 2023). Despite a diversity in methods, the retrieving components of models predominantly rely on standard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous segmentation might not capture the complete semantic depth of the text. Reading extracted snippets from technical or scientific documents may lack important context making them difficult to read or even misleading. (Cohan & Goharian, 2017;Newman et al., 2023;Zhang et al., 2023). \n\nRecursive summarization as Context Summarization techniques provide a condensed view of documents, enabling more focused engagement with the content (Angelidis & Lapata, 2018).",
            "score": 0.59204164223132,
            "section_title": "Why Retrieval?",
            "char_start_offset": 4782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 222,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 506,
                    "end": 524,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 641,
                    "end": 661,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 1151,
                    "end": 1169,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 1169,
                    "end": 1194,
                    "matchedPaperCorpusId": "259858975"
                },
                {
                    "start": 1413,
                    "end": 1431,
                    "matchedPaperCorpusId": "258378282"
                },
                {
                    "start": 1694,
                    "end": 1718,
                    "matchedPaperCorpusId": "12665229"
                },
                {
                    "start": 1738,
                    "end": 1757,
                    "matchedPaperCorpusId": "252118883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2276611328125
        },
        {
            "corpus_id": "266176301",
            "title": "Exploring In-Context Learning for Knowledge Grounded Dialog Generation",
            "text": "In dense retrieval, queries and documents are often directly encoded by large neural models into single or multiple vectors, which allow for efficient dense retrieval based on simple similarity function such as dot-product or ANN. These vectors compress high dimensional semantic information. Four types of dense retrieval are implemented in our experiments, the first three are bi-directional contextual language models (BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019)) using [CLS] embedding as the final input representation. The same encoder is used for both query and document. We also implement TCT-ColBERT (Wang et al., 2022) using index service provided by Pyserini3 and FAISS4 libraries.",
            "score": 0.5918185677742893,
            "section_title": "Dense Retrieval",
            "char_start_offset": 14733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 729
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 665,
                    "matchedPaperCorpusId": "250340125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1339111328125
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "Text retrieval aims to find the relevant documents for a given query from a large collection of documents, playing an indispensable role in opendomain question answering (Chen et al., 2017), fact verification (Thorne et al., 2018) and retrievalaugmented generation (Lewis et al., 2020;He et al., 2022). At the early stage, sparse retrieval methods such as TF-IDF or BM25 dominated passage retrieval by relying mainly on lexical term matching to compute relevance between the query and document. Recently, there has been a surge of research interest in neural network-based dense retrieval (Karpukhin et al., 2020;Xiong et al., 2021). Different from sparse retrieval, dense retrieval resorts to neural encoders to compute the dense representations of the query and document. This enables dense retrieval to infer the relevance between them at the semantic level rather than the surface level, thus circumventing the term mismatch problem suffered by the sparse retrieval models. \n\nIn recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder. \n\nTo enhance the retrieval models' ability to capture interactions between queries and documents while maintaining retrieval efficiency, previous work mainly focuses on generating query-informed document representations. One approach, known as late interaction (Khattab and Zaharia, 2020), involves encoding the query and document independently in the early layers, while the later layers model their interactions. Late interaction combines dual-encoder and cross-encoder, making a trade-off between retrieval efficiency and performance. On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder.",
            "score": 0.5917529466549674,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 589,
                    "end": 613,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 613,
                    "end": 632,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1936,
                    "end": 1963,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37841796875
        },
        {
            "corpus_id": "266362497",
            "title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP",
            "text": "Dense retrieval is a method used in information retrieval that uses vector representations of passages and queries to match their relevance. Compared with sparse retrieval, it has the advantage of being able to dig into the hidden semantics relations between queries and passages. There are two mainstream directions for training dense retrievers, i.e., distillation (Izacard and Grave, 2020;Ren et al., 2023;Sun et al., 2023a;Zeng et al., 2022;Lin et al., 2023b;Bai et al., 2023) and better encoding (Gao and Callan, 2021;Xiao et al., 2022;Wu et al., 2022;Zhang et al., 2023a;Zhuang et al., 2022). For distillation techniques, RocketQAv2 (Ren et al., 2023) presents a joint training approach for DPR and passage reranking powered by dynamic listwise distillation and hybrid data augmentation. Besides, PROD (Lin et al., 2023b) gradually improves the student through a teacher progressive and data progressive distillation. In terms of improving DPR with better encoding methods, Condenser (Gao and Callan, 2021) proposes a new transformer architecture to establish better encoding. Meanwhile, CoT-MAE (Wu et al., 2022) employs an asymmetric encoder-decoder architecture to encode sentence semantics.",
            "score": 0.5915250703188035,
            "section_title": "Training Strategies for Dense Retrieval",
            "char_start_offset": 24491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "250390910",
            "title": "An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering",
            "text": "In the previous sections, we manage to identify the importance of the passage encoder and its affecting factors such as positive passage overlap and corpus coverage of the training data. We find that our discoveries are consistent with some previous work's conclusions. For example, Zhan et al. (2021Zhan et al. ( , 2020a)); Sciavolino et al. (2021) all find that it is sufficient to achieve reasonable retrieval accuracy by just fine-tuning the question encoder with a fixed passage encoder, which demonstrates the importance of a robust passage encoder in domain adaptation and hard-negative mining. \n\nHowever, how to learn such a robust passage encoder is challenging as pre-training DPR on a single QA dataset will introduce biases. Multi-task dense retrieval (Maillard et al., 2021;Li et al., 2021;Metzler et al., 2021) uses multiple experts learned in different domains to solve this problem. These solutions are effective but not efficient as they build multiple indexes and perform searches for each expert, requiring a lot of resources and storage space. \n\nAnother solution is to build a question-agnostic passage encoder so that the model is not biased towards particular QA tasks. DensePhrases (Lee et al., 2021a,b) pioneers this direction by building indexes using phrases instead of chunks of passages for multi-granularity retrieval. By breaking passages into finer-grained units, DensePhrases indeed improve the generalization of dense retrieval in different domains with query-side fine-tuning. However, similar to multi-task learning, it is not efficient as the phrase index can be enormous for a corpus like Wikipedia. Although techniques such as product quantization (Gray and Neuhoff, 1998) can be applied to improve efficiency, it comes at the cost of effectiveness. \n\nOverall, it is desirable to have a robust passage encoder for efficient dense retrieval according to previous work and our analysis, but challenges still remain in the effectiveness-efficiency trade-off.",
            "score": 0.5914899576705839,
            "section_title": "Impact of Passage Encoders",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 300,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 764,
                    "end": 787,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 787,
                    "end": 803,
                    "matchedPaperCorpusId": "240286557"
                },
                {
                    "start": 1686,
                    "end": 1710,
                    "matchedPaperCorpusId": "119156912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "247594093",
            "title": "Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval",
            "text": "Rapid progress in dense retrieval models using deep neural networks has greatly influenced many IR tasks. In this paper, we provide a thorough evaluation of both token-level and passage-level biencoder models in the math information retrieval domain. Our DPR and ColBERT models adapted to this domain in both pretraining and fine-tuning are made publicly accessible to provide stepping stones for future research. Our study also highlights the importance of combining structure search with dense retrieval models for better math-aware search. We show that bi-encoder dense retrieval models alone can be less effective than cross encoders, but when combined with strong structure search methods, they can further improve stateof-the-art effectiveness. With the huge modeling capacity of dense retrieval models, we believe it is worth exploring other directions for improvements so that we can unleash the potential of deep models in this domain, for example, to better identify similarities in mathematically transformed expressions with different structures.",
            "score": 0.5903994065146677,
            "section_title": "Conclusions",
            "char_start_offset": 29279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202880859375
        },
        {
            "corpus_id": "258947148",
            "title": "Multiview Identifiers Enhanced Generative Retrieval",
            "text": "The retrieval performance on NQ and TriviaQA is summarized in Table 1. By jointly analyzing the results, we gained the following findings.\n\n(1) Among the generative retrieval methods, MINDER achieves the best performance. We found that SEAL which takes natural identifiers surpasses DSI based on numeric identifiers. This is because numeric identifiers lack semantic information and DSI requires the model to memorize the mapping from passages to their numeric IDs. As such, it becomes more challenging for DSI on the NQ and TriviaQA datasets with more than 20 million passages. Despite the superiority of SEAL, MINDER still outperforms it. Specifically, the improvements in terms of hits@5 are 4.5% and 1.6% on NQ and TriviaQA, respectively. This verifies the effectiveness of our proposed multiview identifiers, which could rank passages from different perspectives.\n\n(2) On NQ, MINDER achieves the best performance in terms of hits@100 and the second-best results in terms of hits@5, 20. However, generative retrieval methods, including MINDER, perform worse than dual-encoder approaches on TriviaQA. Generative retrieval methods rely on the identifiers to represent passages, and cannot \"see\" the content of the passage. Although the QG module in Methods Natural Questions TriviaQA @5 @20 @100 @5 @20 @100  Table 1: Retrieval performance on NQ and TriviaQA. We use hits@5, @20, and @100, to evaluate the retrieval performance. Inapplicable results are marked by \"-\". The best results in each group are marked in Bold, while the second-best ones are underlined. \u2020 denotes the best result in generative retrieval.  our work generates pseudo-queries based on a passage's content, the autoregressive language model AM still cannot directly \"see\" the original content of the passage. Besides, autoregressive generation has the error accumulation problem. These are the disadvantages of generative retrieval and why it may not perform as well as dense retrievers in some scenarios.",
            "score": 0.5902751966723517,
            "section_title": "Retrieval Results on QA",
            "char_start_offset": 18728,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1546630859375
        },
        {
            "corpus_id": "269449724",
            "title": "Efficiency-Effectiveness Tradeoff of Probabilistic Structured Queries for Cross-Language Information Retrieval",
            "text": "While recent developments in IR have focused on neural approaches with dense vectors produced by pretrained language models, such as DPR [21] and ColBERT [22], studies have demonstrated that incorporating lexical retrieval models into the neural pipeline is \u2f24\u8c61\u662f\u56db\u96bb\u8173 \u7684\u54fa\u4e73\u52d5\u7269....",
            "score": 0.5898881368721741,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0618896484375
        },
        {
            "corpus_id": "269982279",
            "title": "Control Token with Dense Passage Retrieval",
            "text": "This study addresses the hallucination problem in large language models (LLMs). We adopted Retrieval-Augmented Generation(RAG) (Lewis et al., 2020), a technique that involves embedding relevant information in the prompt to obtain accurate answers. However, RAG also faced inherent issues in retrieving correct information. To address this, we employed the Dense Passage Retrieval(DPR) (Karpukhin et al., 2020) model for fetching domain-specific documents related to user queries. Despite this, the DPR model still lacked accuracy in document retrieval. We enhanced the DPR model by incorporating control tokens, achieving significantly superior performance over the standard DPR model, with a 13% improvement in Top-1 accuracy and a 4% improvement in Top-20 accuracy.",
            "score": 0.589840901147682,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184814453125
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation. For example, [15,16] proposed the Condenser family models. 2) Applying pre-training methods for dense retrieval is to use pre-trained models as encoders, and then fine-tuned with labeled dataset. In the fine-tuning stage, existing research work attempted to select more reasonable hard negative samples [49,52]. 3) Existing stateof-the-art retrieval systems usually leverage a two-stage (retrievalthen-reranking) framework. Different from the previous traditional pipeline model, [45] and [53] proposed to better leverage the feedback from reranker to promote the performance of retrieval stage via joint learning and adversarial learning respectively. Related Datasets As mentioned above, the emergence of largescale high-quality labeled data has greatly promoted the optimization process of passage retrieval models. Among all these datasets, MS MARCO [5] is the most representative dataset in the English field. MS MARCO is a passage and document ranking dataset introduced by Microsoft. The passage ranking task focuses on ranking passages from a collection of about 8.8 million, which are gathered from Bing's results to real-world queries. About 808 thousand queries paired with relevant passages are provided for supervised training. Each query is associated with sparse relevance judgments of one (or very few) passages marked as relevant and no passages explicitly indicated as irrelevant. \n\nIn the Chinese field, there are some datasets built based on web page retrieval systems, for example, Sougou-QCL [55]. The Sogou-QCL dataset was created to support research on information retrieval and related human language technologies. The dataset consists of 537,366 queries, more than 9 million Chinese web pages, and five kinds of relevance labels assessed by click models. However, this dataset is concentrated in the general domain, and the labels are obtained based on click behavior, rather than human annotation. Dureader is a recently released large-scale MRC dataset in Chinese [22]. The data distribution is mainly concentrated in the general domain. It can be converted into an information retrieval dataset.",
            "score": 0.5893964879504043,
            "section_title": "RELATED WORK",
            "char_start_offset": 7144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 465,
                    "end": 468,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "49646876"
                },
                {
                    "start": 2150,
                    "end": 2154,
                    "matchedPaperCorpusId": "3662564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1934814453125
        },
        {
            "corpus_id": "251320151",
            "title": "Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models",
            "text": "We recap related work in neural retrieval and studies that investigate its generalization ability. \n\nRecent efforts in improving the effectiveness of ranking models can be roughly classified into two directions, i.e., model architectures and training techniques. ColBERT [24,43], dense retrieval [18,29,51,55] and neural sparse retrieval [15,16,27] are popular model architectures. ColBERT utilizes late-interaction to model the term-term interactions and thus is interaction-based. Neural sparse retrieval and dense retrieval encode queries and documents separately to vectors and utilize vector similarity for search. Unlike ColBERT, they are representation-based. Neural sparse retrieval predicts term weights and outputs a sparse vector of vocabulary size, while dense retrieval embeds text to dense vectors of much lower dimensions. Besides innovating model architectures, some researchers propose effective training techniques. Xiong et al. [51] and Zhan et al. [55] highlight the importance of using hard negatives and investigate how to mine them during training. Hofst\u00e4tter et al. [21] and Lin et al. [29] explore using knowledge distillation methods to train dense retrieval models with expressive yet slow interactionbased models like BERT rerankers [37] and ColBERT [24]. Gao and Callan [17] and Lu et al. [32] study pretraining techniques tailored for dense retrieval. The three training techniques complement each other and can be combined in practice. \n\nRecently, several researchers challenge the generalization ability of neural retrieval models by constructing a new test set that follows a different distribution from the training set. Sciavolino et al. [44] propose a new entity-rich dataset and find dense retrieval performs poorly. Thakur et al. [46] introduce BEIR dataset which contains multiple test sets from different domains. Although proposing a new out-of-distribution test set enables evaluating generalization performance, annotating such a test set is costly. Usually, the out-of-distribution test set should be in a low-resource domain such as medicine where annotation requires expertise. Furthermore, evaluating neural retrieval models may suffer from 'false negative' issue and thus a large annotation pool is required.",
            "score": 0.588749464820245,
            "section_title": "RELATED WORK",
            "char_start_offset": 36113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "matchedPaperCorpusId": "237562875"
                },
                {
                    "start": 1768,
                    "end": 1772,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274169921875
        },
        {
            "corpus_id": "248366293",
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "text": "Surfacing knowledge from large corpora is a crucial step when dealing with knowledge intensive language tasks (Levy et al., 2017;Dinan et al., 2019;Elsahar et al., 2018;Petroni et al., 2021), such as open-domain question answering (Voorhees et al., 1999;Joshi et al., 2017;Yang et al., 2018;Kwiatkowski et al., 2019) and fact checking (Thorne et al., 2018). A popular paradigm to approach such tasks is to combine a search engine with a machine reader component. The former retrieves relevant context, usually in the form of short passages, which the latter then examines to produce answers (Chen et al., 2017;Lewis et al., 2020;Izacard and Grave, 2021). \n\nIn recent years we have witnessed a surge of research and development in autoregressive language models (Radford et al., 2019;Lewis et al., 2019;Raffel et al., 2019;Brown et al., 2020;Rae et al., 2021;Artetxe et al., 2021;Smith et al., 2022), with ever increasing size and natural language understanding (NLU) capabilities. Such models are currently the de-facto implementation of the machine reader component in retrieval-reader architectures, and have contributed to rapid progress on a wide range of benchmarks (Joshi et al., 2017;Kwiatkowski et al., 2019;Petroni et al., 2021). However, these tremendous advances in aggressive modelling has yet to bring similar transformational changes in how retrieval is approached. \n\nTransferring the NLU capabilities of modern autoregressive models to retrieval is non-trivial. Some works have demonstrated that knowledge stored in the parameters of these models can be retrieved to some extend by directly generating evidence given a query (Petroni et al., 2019(Petroni et al., , 2020;;Roberts et al., 2020). However, such approaches have been shown to be unreliable because of their tendency to hallucinate non-factual content (Massarelli et al., 2019;Metzler et al., 2021;Ji et al., 2022).",
            "score": 0.5884699473813355,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 169,
                    "end": 190,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 254,
                    "end": 273,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 335,
                    "end": 356,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 629,
                    "end": 653,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 761,
                    "end": 783,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1171,
                    "end": 1191,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1216,
                    "end": 1237,
                    "matchedPaperCorpusId": "221507798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279541015625
        },
        {
            "corpus_id": "264172563",
            "title": "On Coherence-based Predictors for Dense Query Performance Prediction",
            "text": "Often, a BERT-based model is trained for use as a reranker of the result retrieved by (e.g.) BM25 [38] -such cross-encoders include BERT_CLS [34] and monoT5 [35]. On the other hand, dense retrieval approaches [25,51] are increasingly popular, whereby an embedding-based representation of the documents are indexed, and those with the similar embeddings to the query are identified through nearest-neighbour search (e.g. ANCE [51], TCT-ColBERT [27]). Compared to reranking setups, dense retrieval is attractive as recall is not limited by the initial BM25 retrieval approach, and improvements in the PLM can improve the retrieval effectiveness. Therefore, dense retrieval models inspire us to develop predictors that are effective for predicting their rankings. \n\nIn parallel, neural architectures have also seen some adoption as methods for predicting query difficulty. These post-retrieval methods are supervised, and use refined neural architectures in order to produce a final performance estimate [2,13,22,53]. For instance, BERT-QPP [2] fine-tunes the BERT language model [14] embeddings to the QPP task by estimating the relevance of the top-ranked document retrieved for each query. Still, BERT-based supervised QPP approaches were initially evaluated of in terms of their correlation with the effectiveness of a BM25 ranking. However, it was recently shown that the performance of BERT-QPP is significantly lower or is outperformed by unsupervised predictors when using more advanced retrieval methods and the TREC Deep Learning datatsets [17]. Instead, we believe that less expensive unsupervised predictors that take advantage of the nature of advanced retrieval methods could be more indicative of query performance. \n\nTherefore, in this work, we aim to accurately predict the effectiveness of recent dense retrieval methods. In this regard, we extend current predictors and propose ways to improve their performance under the new models. We reason that the confidence of well-trained and effective ranking methods should be indicative of the likely effectiveness of the system for a given query.",
            "score": 0.5879175809678285,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1004,
                    "end": 1007,
                    "matchedPaperCorpusId": "250340128"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "197466307"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "24786698"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "240230792"
                },
                {
                    "start": 1547,
                    "end": 1551,
                    "matchedPaperCorpusId": "257038618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2208251953125
        },
        {
            "corpus_id": "266844358",
            "title": "Building Efficient and Effective OpenQA Systems for Low-Resource Languages",
            "text": "However, these retrievers, based on sparse representations, struggle to recognize similarity between synonyms and paraphrases that use different lexical terms. \n\nTo address the sparse vector representation problem, Karpukhin et al. [56] introduced DPR which is one of the early examples of dense retrievers in the OpenQA domain. DPR utilized dual-encoder architecture to encode dense and latent semantic representations of the questions and the contexts. Given a question, DPR was trained to distinguish the positive passages from the negative passages in the batch. One limitation of DPR was its use of a single vector for the question and the context, resulting in limited interactions between the terms in the two texts. Khattab et al. [5] recently developed ColBERT-QA as a novel end-to-end neural OpenQA model for English, offering more extensive and effective interaction between the question and context terms through a late-interaction mechanism. Alternatively, Lee et al. [81] proposed YONO, a single end-to-end architecture that jointly optimizes the retriever, reranking, and reader components. The fully end-to-end architecture of YONO contributes to its efficiency in terms of model size. However, there is a drawback to combining multiple components in a single architecture, as each component demonstrates different overfitting characteristics. This vulnerability becomes apparent especially when the training data is limited, which is often the case for low-resource languages. \n\nIn this paper, we focus on two advanced end-to-end neural models used in OpenQA, the DPR and ColBERT-QA models, for their ability to provide dense representations of queries and passages. Each model is explainedfurther in Section 3.2 along with the other models we use in the paper.",
            "score": 0.5877599506414872,
            "section_title": "Open Domain Question Answering",
            "char_start_offset": 21161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 739,
                    "end": 742,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "245131171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271728515625
        },
        {
            "corpus_id": "270379887",
            "title": "Progressive Query Expansion for Retrieval Over Cost-constrained Data Sources",
            "text": "Datasets.Following previous work on passage retrieval, we choose the popular benchmark datasets Natural Questions (NQ) [22], Web Questions (WQ) [8], TREC [12] DL19, and DL20.For TREC datasets, there are multiple relevant passages per query contrary to NQ and WQ.To ensure fairness among datasets, we consider the passages with a score of 3 to be the relevant ones.\n\nImplementation.For experiments, we indexed the document corpus with Pyserini.For LLM choice, we compared with GPT-3.5, Flan T5-XL, Llama-2 on dev set and chose T5-XL as it has the best cost-to-performance ratio, also supported by previous work [18,33].We tuned our sparse weight parameters , ,  with a range from 0 to 5 and step size of 1, dense weight parameters , , and  with a range from 0 to 1 and step size of 0.1, iteration number  (range from 2 to 15 with step size of 1), and number of potential expansion terms  (range from 3 to 7 with step size of 1) on the dev sets of our datasets and chose the values  = 1,  = 1,  = 0,  = 0.8,  = 0.2,  = 0.2,  = 5, and  = 5.Note that, our choices of , , , , and  are also supported by previous work [29,38].\n\nBaselines.We sampled from each retrieval category, sparse and dense with unsupervised and supervised variants to show the effectiveness of ProQE.For sparse retrieval, we compare with BM25 and docT5 [31] as retrievers.docT5 uses a trained T5-large model to generate a query given a document and the generated query is appended at the end of the document.We use Pyserini's prebuilt index msmarco-v1-passage-d2q-t5.For dense retrieval, we compare with DPR [20] and TCT-Colbert [26].We use DPR's question encoder fine-tuned on NQ and multiset and the prebuilt index of Pyserini.",
            "score": 0.5877114446855738,
            "section_title": "EXPERIMENTAL EVALUATION",
            "char_start_offset": 10006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 9,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 364
                },
                {
                    "start": 366,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 443
                },
                {
                    "start": 443,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1120
                },
                {
                    "start": 1122,
                    "end": 1132
                },
                {
                    "start": 1132,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1534
                },
                {
                    "start": 1534,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "matchedPaperCorpusId": "258332122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "251224409",
            "title": "Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval",
            "text": "Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not \u201cstructurally ready\u201d to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This \u201clack of readiness\u201d results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg\u2605. By concatenating vectors from the [CLS] token and agg\u2605, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at https://github.com/castorini/dhr.",
            "score": 0.5876464011038276,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1727294921875
        },
        {
            "corpus_id": "257045539",
            "title": "The detection of mental health conditions by incorporating external knowledge",
            "text": "In recent years, since the excellent feature extraction capabilities of the Pre-trained Language Models (PLMs), the Meta AI team developed a retrieval method based on the high-dimensional feature representation, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), which greatly increased the accuracy and reliability of the knowledge retrieval. The experiment results show that the retrieval accuracy of DPR not only exceeds the traditionally used BM25 (Robertson & Zaragoza, 2009), but also because it is based on PLMs, its effect can continue to improve with more training. Meta AI team also used DPR to achieve gratifying results in the field of open-domain question answering and generation (Lewis et al., 2020). \n\nBecause of DPR's outstanding retrieval ability, we use it to retrieve external knowledge, and then import relevant knowledge and posts into deep learning models for predicting mental health conditions. It is then tested on the SMHD dataset to show the performance of our model.",
            "score": 0.5872706433856758,
            "section_title": "Detection of mental health conditions on social media",
            "char_start_offset": 10744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1000
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1737060546875
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "In summary, our work makes the following contributions: we investigate external expansion when mixing sparse & dense retrieval paradigms (including both single representation and multiple representation dense retrieval); sparse and zero-shot dense retrieval experiments are conducted on two classical TREC test collections (Robust04 & WT10G); We also conduct zero-shot evaluation on four BEIR datasets (Thakur et al., 2021) (namely DBPedia, NFCorpus, TREC-COVID and Touch\u00e9-2020) -a set of datasets selected for evaluating zero-shot evaluation; We deploy two sparse weighting models (BM25 & DPH), two sparse PRF approaches (RM3 & Bo1), two dense retrieval models (ANCE & ColBERT), and two dense RPF approaches (ANCE-PRF & ColBERT-PRF); we analyse the propensity for a multiple representation PRF technique to perform semantic vs. exact token matching, under normal and external PRF conditions. \n\nThe main findings of this work are: (1) high quality feedback documents obtained using multiple representation dense retrieval, namely ColBERT, from a high-quality external collection can significantly improve sparse retrieval for both Robust04 (by 12% for NDCG@10) and WT10G (by 28% for NDCG@10); (2) significant sparse retrieval effectiveness improvements are also observed when performing expansion using external feedback documents obtained using a single representation dense retrieval, namely ANCE; \n\n(3) extracting PRF documents from an external collection using ColBERT or ANCE for dense retrieval can significantly outperform the zero-shot dense retrieval models on target, indicating the utility of the dense external expansion to improve the effectiveness of zero-shot dense retrieval. \n\nThe remainder of this paper is as follows: Section 2 discusses related works; Section 3 presents a framework for portraying retrieval and pseudo-relevance feedback techniques. Section 4 instantiates this framework for external pseudo-relevance expansion; Section 5 elicits our research questions; Section 6 and Section 7 present the experimental setup and results of this work. Finally, we provide the concluding remarks in Section 8.",
            "score": 0.5872275645724914,
            "section_title": "Introduction",
            "char_start_offset": 4534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1165771484375
        },
        {
            "corpus_id": "259203703",
            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
            "text": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.",
            "score": 0.5871674289869455,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18115234375
        },
        {
            "corpus_id": "249395549",
            "title": "A Neural Corpus Indexer for Document Retrieval",
            "text": "The other way to approach retrieval is utilizing an end-to-end autoregressive model. Firstly, several efforts have been done on entity linking [13; 12; 11], which can be regarded as a special type of retrieval task, e.g., using an entity to ask the posed question. Recently, different from the entity linking task, Tay et al. [50] proposed the DSI (differentiable search index) model to generate relevant document identifiers directly corresponding to the query. Bevilacqua et al. [4] employed the autoregressive model to generate relevant words for a query and utilize the generated string to retrieve relevant documents. Besides, the Deep Retrieval (DR) [20] approach for recommendation is also related to this category, which learns a deep retrievable network with user-item clicks and gets rid of the ANN algorithms based on the Euclidean space assumption. \n\nPre-trained language models. Recently, pre-trained Language Models (LMs), such as BERT [14] and RoBERTa [35], have led to a revolution in web search techniques. The representation vectors for all documents can be calculated and indexed offline. In the online serving stage, it calculates the representation vector for the input query, and applies a crossing layer to calculate the relevance score between each query and document pair. The crossing layer usually adopts simple operators such as cosine similarity or a single feed-forward layer to retain a high efficiency. Gao et al. [16] found that a standard LMs' internal attention structure is not ready-to-use for dense encoders and proposed the Condenser to improve the performance of dense retrieval. Moreover, ANCE [54] leverages hard negatives to improve the effectiveness of contrastive learning, which generates better text representations for the retrieval tasks.",
            "score": 0.5870419267582421,
            "section_title": "Related work",
            "char_start_offset": 9574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1635,
                    "end": 1639,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462646484375
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "As illustrated in Figure 1, Self-Retrieval consolidates the separate components of an IR system -indexing, retrieval, and reranking -into the parameters of a single LLM. For indexing, the corpus is internalized into the LLM's parameters through self-supervised learning, enabling the model to encode and store corpus information within its internal representations. During retrieval, Self-Retrieval leverages its encoded knowledge of the corpus to semantically match the input query and directly generates the relevant documents as outputs. To ensure the generated documents exactly match those in the original corpus, we employ the constrained decoding algorithm [10,8,24] based on the trie of the corpus. For reranking, Self-Retrieval performs selfassessment on the retrieved documents to evaluate their relevance. The output score is used to rerank the retrieved passages. Moreover, for downstream tasks such as retrieval-augmented generation (RAG), Self-Retrieval integrates the reader component into the model, enabling direct answer generation following retrieval. Through this end-to-end approach, Self-Retrieval fully leverages LLMs' powerful capabilities in language understanding, matching, assessment, and generation to achieve unified information retrieval. \n\nWe evaluate Self-Retrieval on three representative retrieval benchmarks: NQ, TriviaQA, and MS MARCO. Experimental results demonstrate that Self-Retrieval substantially outperforms existing sparse retrieval, dense retrieval, and generative retrieval methods on both document-level and passagelevel retrieval tasks. Furthermore, our experiments on retrieval-augmented generation tasks reveal that Self-Retrieval considerably enhances downstream performance. Additionally, larger LLMs lead to progressively better performance in Self-Retrieval, showing clear scaling benefits. These results demonstrate the effectiveness of Self-Retrieval across different retrieval tasks and application scenarios. \n\nThe potential impacts of this paper may include the following aspects. First, we introduce Self-Retrieval, an end-to-end architecture that consolidates the entire information retrieval system within a single large language model. This unified approach demonstrates substantial performance improvements over existing IR methods. Second, the corpus internalization and indexing mechanism of Self-Retrieval establishes a new paradigm to memorize, organize and retrieve the learned documents (at least part of them) during the pre-training phase, paving the way for more transparent and trustworthy text generation from LLMs.",
            "score": 0.5864856597971778,
            "section_title": "Introduction",
            "char_start_offset": 2187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2297
                },
                {
                    "start": 2298,
                    "end": 2591
                }
            ],
            "ref_mentions": [
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "219690743"
                },
                {
                    "start": 668,
                    "end": 670,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 670,
                    "end": 673,
                    "matchedPaperCorpusId": "235458429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1822509765625
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "The sparse retrieval baselines are as follows: \n\n\u2022 BM25 [37] is a classical sparse retrieval algorithm based on probabilistic relevance framework and term frequency statistics. \n\n\u2022 DocT5Query [28] expands documents by generating potential queries using a fine-tuned T5 model. \n\nThe dense retrieval baselines are as follows: \n\n\u2022 DPR [19] is a dual-encoder model trained with in-batch negative sampling. We fine-tune DPR on our training datasets to obtain DPR-FT, following the official implementation and hyperparameter settings. \n\n\u2022 BGE [50] is a state-of-the-art universal embedding model trained on approximately 200 million text pairs using contrastive learning. We employ the bge-large-en-v1.5 variant and fine-tune it on our training datasets to obtain BGE-FT. The fine-tuning process uses a learning rate of 1e-5, batch size of 128, and runs for 10 epochs. \n\n\u2022 Sentence-T5 [31] employs a dual-encoder T5 architecture to generate semantic embeddings through contrastive learning for efficient retrieval. \n\n\u2022 GTR-XL [32] is a dense retrieval model based on Sentence-T5, pre-trained on billions of question-answer pairs. \n\n\u2022 Text-embedding-ada-002 is a powerful embedding model developed by OpenAI, accessible through their API service. \n\n\u2022 GritLM [29] is built upon the Mistral 7B language model and optimized using both embedding and generation objectives. \n\nThe generative retrieval baselines are as follows: \n\n\u2022 DSI [42] is a sequence-to-sequence model that directly maps queries to document identifiers. \n\n\u2022 DSI-QG [59] enhances the DSI framework by incorporating a doc2query model for dataset augmentation. \n\n\u2022 SEAL [5] utilizes n-gram as the document identifiers and constrains the generation process using FM-index. \n\n\u2022 NCI+BGE-Reranker-FT. NCI [49] employs a sequence-to-sequence architecture with a prefix-aware weight-adaptive decoder.",
            "score": 0.5864539255954578,
            "section_title": "B Baselines",
            "char_start_offset": 29318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2257080078125
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Passage retrieval is an important component in applications like ad-hoc information retrieval, opendomain question answering (Karpukhin et al., 2020), retrieval-augmented generation (Lewis et al., 2020) and fact verification (Thorne et al., 2018). Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM) (Devlin et al., 2019), increasing attention is being paid to neural dense retrieval methods (Yates et al., 2021). Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors. Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pretraining does not solve all the problems. As shown in Table 1, improved pre-training techniques that are verified by benchmarks like GLUE  do not result in consistent performance gain for retrieval tasks. Similar observations are also made by Lu et al. (2021). We hypothesize that, to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa  and ELECTRA do not have such sequence-level tasks.\n\nIn this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ a generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help train biencoder-based 1 dense retrievers. Our pretraining objective works",
            "score": 0.5862805165335058,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 246,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 440,
                    "end": 461,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21240234375
        },
        {
            "corpus_id": "248392078",
            "title": "C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval",
            "text": "Since the introduction of pretrained transformer-based language models, neural retrieval models have been taking advantage of these models for more effective query-document matching. Early work in monolingual retrieval involved building cross-encoder models [9,29,33] that leveraged the full interaction between the queries and documents to produce the relevance scores. Subsequently, similar models [19,22,38,42,44] were adapted to the CLIR setting. While effective, such models can only rerank documents since they process both queries and documents during inference and thus yield a longer running time compared to traditional sparse retrieval techniques such as BM25 [36]. DPR-style dense retrieval models overcome this limitation by scoring the documents based on the similarity of their representations, which allows the language models to encode documents beforehand. \n\nHowever, the representations produced by off-the-shelf language models are undertrained [16] and thus directly scoring documents with such representations yields suboptimal retrieval results [27]. Additional task-specific fine-tuning with relevance labels produces much better representations on either the sequence level, such as ANCE [41] and DPR [23], or the token level, such as ColBERT [24]. Especially for sequence level representations, often summarized in the CLS token, contrastive learning [6,34] that trains a model with one positive and multiple negative examples for each query has been shown to be one of the most effective training techniques for dense retrievers [21,35,41]. In-batch negative sampling further reduces the memory requirement by treating positive examples for other queries as negative [16,23]. A similar objective was utilized to pretrain the cross-encoder retrieval model for the CLIR task [43]. \n\nContinuing pretraining the off-the-shelf language model has been investigated in mono-lingual retrival [5,13,16]. Specifically, coCondenser [16] continued pretraining of the language model with a passage-containing classification task (i.e., determining if a pair of passages belong to the same document) through contrastive learning on the representation of the passages for monolingual IR before fine-tuning it as a DPR model.",
            "score": 0.5862431964008774,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 3559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 261,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "218901033"
                },
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "208058719"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "235324815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "274626566",
            "title": "KG-EGV: A Framework for Question Answering with Integrated Knowledge Graphs and Large Language Models",
            "text": "Retriever-Reader Paradigm: The retriever-reader approach [14] has become the core framework for modern ODQA systems. In this framework, a retriever first extracts documents related to a query from an external knowledge base, and a reader then identifies the final answer within these documents. This paradigm has evolved from traditional sparse retrieval to dense retrieval methods, like Dense Passage Retrieval (DPR) [15], which enhance retrieval precision through deep semantic learning. Readers, primarily fine-tuned LLMs like T5 [16] and InstructGPT [17], have greatly improved answer generation quality. However, this approach still struggles with cross-document information synthesis and handling long documents, and retrieval noise remains an area needing optimization.",
            "score": 0.5860314692768598,
            "section_title": "\u2022",
            "char_start_offset": 5104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 776
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "52913933"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268798828125
        },
        {
            "corpus_id": "258236093",
            "title": "CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval",
            "text": "Given a large passage collection {p 1 , p 2 , ..., p n } \u2208 P , passage retrieval tasks aim to find relevant passages of a given query q based on their similarities. \n\nDual-encoder is a typical retrieval architecture for passage retrieval. It often employs a PLMbased model as a backbone encoder for encoding representations. Formally, given a sequence of tokenized input queries and passages T. \n\nThe input texts are forwarded through L-layers (l \u2208 {1, ..., L}) Transformers Blocks of PLM encoder. We denote the output hidden states as follows. \n\nHidden states at [CLS] positions of the last layers are commonly used as the dense representations of Existing works, eg. SPLADE (Formal et al., 2021b,a) and LexMAE (Shen et al., 2022), also focus on learning PLM-based dual-encoders for capturing bag-of-words semantic sparse representations. Sparse representations are focusing on encoding semantics at the token level. The hidden states of the last layer h last are firstly projected to vocab spaces with the transposed embedding matrix E . Then ReLU and log saturation operations are performed to obtain the sparse representations. \n\n) here i \u2208 {1, ..., N } denotes the sequence length spaces. ReLU keeps non-negative hidden states as token frequencies produced by PLM. And log saturation suppresses the dominant frequencies for keeping sparsity. The above operations are common practices for pooling sparse representations.",
            "score": 0.5860111109858506,
            "section_title": "Passage Retrieval with Dual-Encoders",
            "char_start_offset": 5082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167724609375
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "To the best of our knowledge ColBERT-PRF is the first investigation of pseudo-relevance feedback for multiple representation dense retrieval. Existing works on neural pseudo-relevance feedback, such as Neural PRF [20] and BERT-QE [51] only function as rerankers. Other approaches such as DeepCT [10] and doc2query [32,33] use neural models to augment documents before indexing using a traditional inverted index. CEQE [29] generates words to expand the initial query, which is then executed on the inverted index. However, returning the BERT embeddings back to textual word forms can result in polysemous words negatively affecting retrieval. In contrast, ColBERT-PRF operates entirely on an existing dense index representation (without augmenting documents), and can function for both ranking as well as reranking. By retrieving using feedback embeddings directly, ColBERT-PRF addresses polysemous words (such as 'tank', illustrated above). It is also of note that it also requires no additional neural network training beyond that of ColBERT. Indeed, while ANCE-PRF requires further training of the refined query encoder, ColBERT-PRF does not require any further retraining. Furthermore, compared to the single embedding of ANCE-PRF, ColBERT-PRF is also more explainable in nature, as the expansion embeddings can be mapped to tokens (as shown in Figure 2), and their contribution to document scoring can be examined, as we will show in Section 5.3.4. \n\nIn the following, we first show the retrieval effectiveness of ColBERT-PRF for passage ranking and document ranking tasks in Section 5 and Section 6, respectively. In particular, in Section 5, we examine the characteristics of ColBERT-PRF, including how ColBERT-PRF addresses polysemous words, how ColBERT-PRF demonstrates compared with the traditional query expansion techniques ACM Trans. Web, Vol. 1, No. 1, Article . Publication date: September 2022. and how to quantify the extent of the semantic matching ability of ColBERT-PRF.",
            "score": 0.5851726894422296,
            "section_title": "Discussion",
            "char_start_offset": 23163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "53081945"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "221703727"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "219162111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.175537109375
        },
        {
            "corpus_id": "251594672",
            "title": "CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks",
            "text": "We adopt two types of baseline methods for comparison, including traditional IR models and model-based IR models. \n\n4.2.1 Traditional IR Models. We take several representative models that are widely used for KILT tasks as the baselines, including the sparse retrieval and dense retrieval methods. \n\n\u2022 BM25 [40] is a highly effective retrieval model that represents the classical probabilistic retrieval model. \u2022 TF-IDF [5] is a traditional sparse vector space retrieval model that combines bigram hashing and TF-IDF matching to return relevant documents. \u2022 DPR [21] is a BERT-based dual-encoder model trained with inbatch negatives and a few hard negatives selected with BM25. \n\n\u2022 DPR+BERT [38] combines a BERT-base classifier with passages returned from DPR where the query and retrieved passages are the input. \n\n\u2022 DPR+BART [38] incorporates an explicit retrieval step in addition to the generative pre-training together with DPR and BART. \n\n\u2022 RAG [29] combines pre-trained parametric and non-parametric memory for generation. \u2022 MT-DPR [36] jointly trains a DPR model on an extensive selection of retrieval tasks. \u2022 BLINK+flair [38] combines BLINK [46] and flair [1] retrieval solution that ranks pages according to entities in the input.",
            "score": 0.5847768742743455,
            "section_title": "Baselines",
            "char_start_offset": 20574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1240
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 422,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "230435546"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "263877300"
                },
                {
                    "start": 1165,
                    "end": 1168,
                    "matchedPaperCorpusId": "181704107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22216796875
        },
        {
            "corpus_id": "256105432",
            "title": "Information Retrieval: Recent Advances and Beyond",
            "text": "Another line of research focus on using contextual information to improve dense passage retrieval. For example, in [152], the authors propose a method called ConTextual Mask Auto-Encoder (CMAE), which uses contextual information to improve dense passage retrieval. Additionally, there are methods that use masked auto-encoder for pre-training retrieval-oriented transformers. For example, in [153], the authors propose a method called RetroMAE, which uses masked auto-encoder for pre-training retrieval-oriented transformers. Another line of research focus on using representation bottleneck for pretraining dense passage retrieval models. For example, in [154], the authors propose a method called SimLM, which uses representation bottleneck for pre-training dense passage retrieval models. Lastly, there are methods that use lexicon-bottlenecked pre-training for large-scale retrieval. For example, in [155], the authors propose a method called LexMAE, which uses lexicon-bottlenecked pre-training for large-scale retrieval. The last method is a method that uses a contrastive pre-training approach to learn discriminative autoencoder for dense retrieval. For example in [156]. Overall, these pre-training methods demonstrate promising results in improving dense passage retrieval performance. However, more research is needed to fully understand Zero-shot/Few-shot Learning Recent research in the field of zero-shot information retrieval has been focused on finding ways to improve the performance and generalizability of retrieval models. One common approach is the use of query generation, as seen in [157] and QGen [158]. These methods aim to improve zero-shot retrieval by embedding queries into a shared space, or by using synthetic question generation to improve passage retrieval. Another approach is the use of synthetic pre-training, as suggested by [159], to improve the robustness of neural retrieval models. Additionally, [160] presents a benchmark for evaluating the performance of zero-shot retrieval models. Another approach is the use of momentum adversarial domain-invariant representations as proposed by [161] for zeroshot dense retrieval. [162] proposes DTR, which uses large dual encoders for generalizable retrieval.",
            "score": 0.5845571248432597,
            "section_title": "Multi-vector Representation",
            "char_start_offset": 34647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 1173,
                    "end": 1178,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "To test the effectiveness of our external pseudo-relevance expansion technique approach, we compare with the following baselines: \n\n\u2022 Sparse Approaches: We apply sparse retrieval models without PRF, namely the BM25 and DPH weighting models. We also combine these models with sparse pseudo-relevance expansion technique on the target inverted index, i.e. BM25 with RM3 PRF (Abdul-Jaleel et al., 2004) & DPH with Bo1 PRF (Amati & Van Rijsbergen, 2002). Furthermore, we also instantiate these PRF models in an external expansion setting, i.e.  25 () \n\n\u2022 Neural Reranking Approaches: To aid in our comparisons, we further apply neural rerankers, namely ColBERT and ANCE reranking models upon the sparse retrieval models. For instance, applying a final ColBERT reranker upon BM25 with RM3 query expansion would be denoted as: \n\n\u2022 Dense Approaches: We also deploy dense retrieval models, with and without a pseudo-relevance feedback mechanism. In particular, for single representation dense retrieval, we deploy ANCE (Xiong et al., 2021), and for multiple representation dense retrieval, we use the ColBERT-E2E (Khattab & Zaharia, 2020) model. For dense retrieval, we apply ColBERT-PRF (Wang et al., 2021) on both normal and external expansion setting.",
            "score": 0.584287856566641,
            "section_title": "Baselines",
            "char_start_offset": 31898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1246
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 398,
                    "matchedPaperCorpusId": "16221853"
                },
                {
                    "start": 419,
                    "end": 449,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1011,
                    "end": 1031,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "matchedPaperCorpusId": "235490586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "260704352",
            "title": "On Monotonic Aggregation for Open-domain QA",
            "text": "Retriever-reader architecture is conventionally adopted approach for ODQA [1,6,9]. We build our model on DPR-FiD architecture, since it is widely adopted by state-of-the-art multi-source ODQA. FiD models are trained from T5-large huggingface checkpoint. \n\nDPR, or dense passage retriever, is widely used for the document retrieval, such as in [1,6,7]. DPR demonstrated significant improvement over the previous methods such as TF-IDF or BM25 [10]. DPR retriever has the biencoder structure, which consists of a document encoder and a query encoder. The encoders are trained such that the embeddings of positive documents have greater dot product with the question embedding. \n\nFiD, or fusion-in-decoder, is a commonly used reader component in ODQA, due to its ability to effectively fuse information from multiple passages [6,11]. Implemented on encoderdecoder model such as T5 [12] or BART [13], FiD encodes retrieved passages independently, then the decoder computes full attention on the concatenated representation of passages. \n\nOur distinction is to separate retrievers and readers to obtain answer candidates from each specific source, instead of relying on unified retriever-reader pipeline. The judge then filters out incorrect candidates to preserve the monotonicity. Our approach is comparable to Watson [14], where multiple expert systems produce candidates then aggregate them to determine the final answer. However, while Watson relies on multiple different low-level algorithms such as relation extraction, we leverage the ability of transformer-based LM (T5-large) to evaluate the candidates with unified knowledge.",
            "score": 0.5839970496986737,
            "section_title": "Retrievers and Readers",
            "char_start_offset": 5116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1631
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 77,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 79,
                    "end": 81,
                    "matchedPaperCorpusId": "230437698"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 348,
                    "end": 350,
                    "matchedPaperCorpusId": "247594360"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1315,
                    "end": 1319,
                    "matchedPaperCorpusId": "22647967"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2091064453125
        },
        {
            "corpus_id": "258987734",
            "title": "BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language",
            "text": "The task of textual IR is to search for and return documents (i.e. any indexed text objects) that are relevant to a user query from a collection. Collections may consist of millions of documents, which makes the task computationally intensive. Moreover, documents and queries mostly are of significantly different lengths, the language used throughout the documents may vary (e.g., from general to specialized), and the information represented in a collection may cover a broad range of topics. Lexical approaches, e.g., TF.IDF or BM25 (Robertson and Zaragoza, 2009), have dominated textual IR for many years. Mainly due to manageable computational cost, but still offering decent performance. Recently, a strong trend has been observed towards developing neural retriever models that should outperform lexical approaches. Pretrained language models like BERT (Devlin et al., 2019) appeared to be a good basis for dense retrieval approaches. Bi-encoder architecture as presented in dense passage retriever (DPR) (Karpukhin et al., 2020) and sentence BERT (Reimers and Gurevych, 2019) are commonly used and express high performance, especially on indomain datasets. The query and document are represented by single vectors, which facilitates applications of fast vector databases i.e. FAISS (Johnson et al., 2017). The main drawback of such models is their lower performance on out-of-domain data. On the other hand, the BM25 approach achieves better results in such scenario. A potential approach involves utilising a multi-vector representation of the query and document, as exemplified in ColBERT (Khattab and Zaharia, 2020). These approaches utilise the late interaction paradigm. ColBERT encodes documents and queries in multiple vectors, where each output vector corresponds to the input token. During inference time, ColBERT computes the Cartesian product between queries and docu-ments, which can enhance retrieval outcomes, but also necessitates storing a huge index in memory. \n\nTo improve performance of single vector representations, it was shown that language models have structural readiness, and it is possible to pre-train the model towards bi-encoder structure (Gao and Callan, 2021).",
            "score": 0.5838147143450086,
            "section_title": "Passage Retrieval",
            "char_start_offset": 4997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 536,
                    "end": 566,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 860,
                    "end": 881,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2235107421875
        },
        {
            "corpus_id": "275921950",
            "title": "CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs",
            "text": "[3] utilizes a multi-vector architecture to create robust representations, integrating contrastive learning and knowledge distillation techniques. \u2022 Dense Retrieval: MiniLM [41] is a lightweight transformer model that uses deep self-attention distillation to create dense embeddings. LaBSE [7] is a bilingual embedding model, leveraging dual encoders and a large-scale parallel corpus to ensure semantic alignment across languages. mContriever [14] employs unsupervised contrastive learning to train dense retrievers, focusing on encoding diverse and nuanced contextual information. E5 [40] optimizes embeddings for text retrieval by integrating explicit supervision from retrieval datasets and task-specific fine-tuning. SPAR [4] employs salient phrase representation learning to bridge dense and sparse retrieval, utilizing a dual encoder architecture that explicitly models both phrase-level and document-level semantics. \n\n\u2022 Hybrid Retrieval: ScoreFusion [19] combines the output scores of sparse and dense retrieval models to produce a unified ranking. \n\nColBERT [17] introduces late interaction to compute pairwise term similarities between query and document embeddings, enabling efficient and fine-grained integration of sparse and dense signals. CLEAR [8] employs a residual learning framework to combine sparse and dense representations, ensuring complementary signals are utilized for improved retrieval performance.",
            "score": 0.5837989144307499,
            "section_title": "BGE-M3",
            "char_start_offset": 24955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1427
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "232423090"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283447265625
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "Retrieval-Augmented Generation (Lewis et al. 2020c) models have emerged as a promising solution to address the knowledge-intensive nature of NLP tasks (Yang et al. 2024) (Hei et al. 2024). Unlike the two-stage pipeline of traditional ODQA systems, Retrieval-Augmented Generation models utilized the neural-retrieval-in-the-loop technique with Dense Passage Retrieval (Karpukhin et al. 2020) and BART (Lewis et al. 2020a) sequence-to-sequence language model (Lewis et al. 2020c) (Shuster et al. 2021). This method integrates a neural network architecture with transformers (Vaswani et al. 2017) as backbone consisting of an encoder-decoder framework. In this system, the encoder processes the input query and the retriever converts it into a dense vector representation followed by employing maximum inner product search to identify and retrieve the most pertinent passages from an extensive corpus. These retrieved passages are then concatenated with the input query and fed into the decoder to generates final response. The entire system is typically optimized through end-to-end training, enhancing its capability to produce accurate and contextually relevant responses (Lewis et al. 2020c). \n\nRetrieval Augmented Architectures have gained significant attention for their scalable and interpretable design (Siriwardhana et al. 2023). Recent studies have significantly enhanced the capabilities of RAG models within dialoguebased systems (Rackauckas 2024) (Kang et al. 2024). It uniquely integrates retrieval and generation and merged in a single architecture, leveraging both a pre-trained BART generator and dense vector representations of Wikipedia articles indexed with the FAISS (Johnson et al. 2019) library capable of scaling to knowledge sources containing millions of documents. However, existing research on RAG models has focused only on open-domain tasks with Wikipedia-based datasets and there is limited understanding of their effectiveness in domain-specific applications, particularly in customer service area. Therefore, in this paper we aim to investigate the performance of different RAG and RAG-like architectures by leveraging domain adaptation techniques, with a focus on evaluating their ability to generate accurate and contextually appropriate responses in specialized domains.",
            "score": 0.583325168497131,
            "section_title": "Retrieval Augmented Architecture",
            "char_start_offset": 11263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "1373518"
                },
                {
                    "start": 476,
                    "end": 499,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1308,
                    "end": 1334,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1685,
                    "end": 1705,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1937255859375
        },
        {
            "corpus_id": "247594093",
            "title": "Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval",
            "text": "Evaluation results for Transformer-based dense models are shown in Table 1. Across both formulaonly retrieval (NTCIR-12) and math-aware full-text retrieval (ARQMath-2), our pretrained backbones can generally boost downstream DPR retrieval effectiveness compared to DPR models based on vanilla BERT, row (1), or SciBERT,row (2). This is presumably because we have further pretrained on more domain-specific data (unlike SciBERT, which also includes scientific text like biomedical articles) with a much larger batch size, i.e., 240 compared to SciBERT's 32 batch size. According to rows (2)-( 4) in Table 1, with pretraining only for 3 epochs, our model reaches a similar level of effectiveness as SciBERT; and more pretraining results in better downstream effectiveness. \n\nOur fully-trained ColBERT model, row (12), achieves the best scores among other Transformer models. Compared to other ColBERT variants from rows ( 6)-( 8) submitted by the TU_DBS team, we also achieve higher scores. Our DPR model, row (11), is generally more effective than previous bi-encoder systems, so it can be considered a cost-effective alternative to ColBERT, since the latter requires a much bigger index. For ARQMath-2, our DPR model requires a 5.3G index (at full precision), while our ColBERT model requires a 77G index (at half precision). However, our dense models are not on par with the MathBERT run on the NTCIR-12 dataset, row (5). This is because MathBERT reranks a highly effective run generated by Tangent-CFT; the latter is directly tuned on complete NTCIR-12 data.",
            "score": 0.5829805554805985,
            "section_title": "Overall Comparisons",
            "char_start_offset": 21308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1560
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.103759765625
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
            "score": 0.5828735142216979,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "268032853",
            "title": "CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks",
            "text": "In this study, we conduct a comparative analysis involving our proposed Corpus-Brain++, traditional IR models, and generative IR models. \n\n\u2022 Traditional IR models. (i) BM25 is a typical sparse retrieval model, which utilizes term-based features to model the relevance between queries and documents. (ii) DPR is a representative dense retrieval model, which models the semantic relevance between queries and documents via a dual-encoder architecture. For the traditional IR models, our empirical results encompass both incremental and non-incremental scenarios thanks to the high reproducibility provided by Pyserini3 and Tevatron. 4  Generative IR models. We first consider several generative IR models in stationary scenarios, including (i) GENRE [12], which directly fine-tunes BART via multi-task training on the labeled KILT training datasets and supervised BLINK datasets [53]; and (ii) SEAL [3], which applies a BART-based autoregressive search engine to generate distinctive n-grams as docids. Given that the focus of both models is confined to the non-incremental scenario, we only compare them with CorpusBrain++ in terms of non-incremental retrieval performance. In the non-incremental scenario, CorpusBrain++ degenerates to CorpusBrain, and the experimental contrast between CorpusBrain and these two models has been previously conducted and reported in [8]. Furthermore, we explore some advanced generative IR models fitting in dynamic scenarios, including (i) DSI++, which continually fine-tunes DSI over new documents and allocates a unique integer as the docid for each new document; and (ii) CLEVER, which introduces a technique named incremental product quantization to assign a docid to each new document.",
            "score": 0.5825484986509268,
            "section_title": "Baselines.",
            "char_start_offset": 41305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1723
                }
            ],
            "ref_mentions": [
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "263877300"
                },
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1365,
                    "end": 1368,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1748046875
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "score": 0.5823607726933574,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 32867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "244714119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "270357306",
            "title": "Corpus Poisoning via Approximate Greedy Gradient Descent",
            "text": "We experimented with the following retrievers: \n\n\u2022 Dense Passage Retriever(DPR) (Karpukhin et al., 2020) is a two-tower bi-encoder trained with a single BM25 hard negative and in-batch negatives. It has been used as the retrieval component of many Retrieval-Augmented Generation (RAG) models (Lewis et al., 2020). In our paper, we use both the open-sourced Multi model (DPR-mul), which is a bert-base-uncased model trained on four QA datasets (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013) and CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015)) and the single NQ model (DPR-nq). \n\n\u2022 ANCE (Xiong et al., 2020) is a bi-encoder that generates hard negatives using an approximate Nearest Neighbor (ANN) index of the corpus. The index is continuously updated in parallel to identify challenging negative examples for the model during fine-tuning. \n\n\u2022 Contriever (Gautier et al., 2022) is an unsupervised dense retriever using contrastive learning. It leverages the BERT architecture to encode both queries and documents. Contriever-MS (Contriever fine-tuned on MS MARCO) is a version of the Contriever model that has been fine-tuned using the MS MARCO dataset, which provides large-scale, supervised training data.",
            "score": 0.5821973424097386,
            "section_title": "C.2 Retrievers",
            "char_start_offset": 30930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 103,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 447,
                    "end": 473,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 484,
                    "end": 504,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 519,
                    "end": 540,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 557,
                    "end": 582,
                    "matchedPaperCorpusId": "17723267"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 896,
                    "end": 918,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1448974609375
        },
        {
            "corpus_id": "258212897",
            "title": "BRENT: Bidirectional Retrieval Enhanced Norwegian Transformer",
            "text": "A similar approach to language modeling is also done in Borgeaud et al. (2022), but over a corpus consisting of trillions of tokens. For both works, the LMs are trained for a number of steps with retrieval before being fine-tuned on a downstream task, as is the typical workflow with PLMs. Lewis et al. (2020b) demonstrate how the encoder-reader architecture can be used for language generation as well. They propose both a sequence model where the generation is conditioned on the same set of retrieved documents for the entire sequence and a token model where a different document is used per target token. The retriever is based on Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), which uses the same general approach to retrieval as Guu et al. (2020), where a PLM like BERT (Devlin et al., 2019) is used as the encoder. The reader model is swapped with a generator based on BART (Lewis et al., 2020a).",
            "score": 0.5819824371452638,
            "section_title": "Related work",
            "char_start_offset": 5591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 912
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 78,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 665,
                    "end": 689,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 744,
                    "end": 761,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 785,
                    "end": 806,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 890,
                    "end": 911,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146484375
        },
        {
            "corpus_id": "254823295",
            "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
            "text": "Retrieval-augmented models There exists a large body of retrieval-augmented approaches. Some particularly well known models are REALM (Guu et al., 2020), RAG (Lewis et al., 2020), RETRO (Borgeaud et al., 2022) and Fusion-in-Decoder (Izacard and Grave, 2021). FiD in particular has achieved state-of-the-art performance on a wide variety of tasks (Izacard and Grave, 2021;Izacard et al., 2022;Yu et al., 2022b) and in this work we focus on improving the performanceefficiency trade-offs for FiD. RETRO is another closely related retrieval-augmented model, as it uses a small encoder for retrieved context and a larger primary decoder like FiDO does. Unlike RETRO, FiDO's efficiency improvements allow it to tractably attend to many retrieved passages with a much larger decoder. ing factor for computations on modern devices (Williams et al., 2009;Dao et al., 2022;Shazeer, 2019). Shazeer (2019) shows that autoregressive Transformers are particularly bandwidth bound during inference, and proposes multi-query attention as a partial solution. We find that this is exacerbated by the FiD setting, and adopt multi-query attention for FiDO to ameliorate the problem. Pope et al. ( 2022) also investigates multi-query attention, primarily in the context of efficient inference and parallelization for very large language models, whereas we focus on performance/cost trade-offs for the retrieval-augmented setting.",
            "score": 0.5817107494673682,
            "section_title": "Related Work",
            "char_start_offset": 12283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1409
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 151,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 186,
                    "end": 209,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 232,
                    "end": 257,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 346,
                    "end": 371,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 824,
                    "end": 847,
                    "matchedPaperCorpusId": "267923378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models, have shown the usefulness of expanding and reweighting the users\u2019 initial queries using information occurring in an initial set of retrieved documents, known as the pseudo-relevant set. Recently, dense retrieval \u2013 through the use of neural contextual language models such as BERT for analysing the documents\u2019 and queries\u2019 contents and computing their relevance scores \u2013 has shown a promising performance on several information retrieval tasks still relying on the traditional inverted index for identifying documents relevant to a query. Two different dense retrieval families have emerged: the use of single embedded representations for each passage and query, e.g., using BERT\u2019s [CLS] token, or via multiple representations, e.g., using an embedding for each token of the query and document (exemplified by ColBERT). In this work, we conduct the first study into the potential for multiple representation dense retrieval to be enhanced using pseudo-relevance feedback and present our proposed approach ColBERT-PRF. In particular, based on the pseudo-relevant set of documents identified using a first-pass dense retrieval, ColBERT-PRF extracts the representative feedback embeddings from the document embeddings of the pseudo-relevant set. Among the representative feedback embeddings, the embeddings that most highly discriminate among documents are employed as the expansion embeddings, which are then added to the original query representation. We show that these additional expansion embeddings both enhance the effectiveness of a reranking of the initial query results as well as an additional dense retrieval operation. Indeed, experiments on the MSMARCO passage ranking dataset show that MAP can be improved by up to 26% on the TREC 2019 query set and 10% on the TREC 2020 query set by the application of our proposed ColBERT-PRF method on a ColBERT dense retrieval approach.We further validate the effectiveness of our proposed pseudo-relevance feedback technique for a dense retrieval model on MSMARCO document ranking and TREC Robust04 document ranking tasks. For instance, ColBERT-PRF exhibits up to 21% and 14% improvement in MAP over the ColBERT E2E model on the MSMARCO document ranking TREC 2019 and TREC 2020 query sets, respectively. Additionally, we study the effectiveness of variants of the ColBERT-PR",
            "score": 0.5816851159280816,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315185546875
        },
        {
            "corpus_id": "268681628",
            "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
            "text": "Information retrieval (IR) involves the retrieval of relevant information from a large collection of data, such as web pages or documents, in response to a user's query.Recently, deep learning methods like dense passage retriever (DPR) (Karpukhin et al., 2020) have gained significant interest due to their superior performance compared to sparse retrieval methods such as BM25.However, it is crucial for initial retrievers to be lightweight to handle a large set of retrieval targets.Therefore, passage reranking plays a crucial role in the process by following the initial retrievers and ranking the retrieved passages based on their relevance to the query.This enables the use of computationally intensive models, thereby enhancing retrieval accuracy.\n\nLarge language models (LLMs) have demonstrated strong zero-shot capabilities across various natural language tasks (Brown et al., 2020;Kojima et al., 2022).Specifically, models fine-tuned on natural language instructions have shown remarkable performance in comprehending complex instructions (Wei et al., 2021).Previous work has explored the use of LLMs for passage reranking by fine-tuning them on extensive retrieval supervision (Nogueira et al., 2020;Asai et al., 2022).Another line of investigation involves unsupervised passage reranking using LLMs (Sachan et al., 2022;Sun et al., 2023).However, these unsupervised methods often lack guidance in understanding the relevance of retrieved passages.This paper introduces INSTUPR , an instructionbased unsupervised passage reranking method that leverages the instruction-following capabilities of LLMs for reranking without the need for labeled relevance information and additional fine-tuning.We employ an instruction-tuned LLM to generate a relevance score for each query-passage pair.Additionally, we propose a soft relevance score aggregation technique that combines the LLM's predicted distribution over possible scores, resulting in robust estimation.We evaluate our method on common evaluation benchmarks, including TREC DL19 (Craswell et al., 2020), DL20 (Craswell et al., 2021), and BEIR (Thakur et al., 2021).",
            "score": 0.5816820237293137,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1703
                },
                {
                    "start": 1703,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1966
                },
                {
                    "start": 1966,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 260,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 871,
                    "end": 891,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 891,
                    "end": 911,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1049,
                    "end": 1067,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1188,
                    "end": 1211,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1311,
                    "end": 1332,
                    "matchedPaperCorpusId": "248218489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "258309799",
            "title": "A Static Pruning Study on Sparse Neural Retrievers",
            "text": "Pre-trained language models such as BERT [14] have been shown to significantly improve the effectiveness of Information Retrieval (IR) systems over traditional ranking models. Dense retrieval is one of the most promising applications of pre-trained language models in IR. In dense retrieval, a pre-trained language model computes dense representations of queries and documents, that must be stored and processed using approximate nearest neighbor algorithms [17], preferably on GPUs or TPUs [18]. However, the ranking of documents in dense retrieval incurs significant computational costs, even if this can be reduced through approaches such as ColBERT [19,35], or by re-ranking only a small set of candidates [29]. Nevertheless, query processing in dense retrieval is much more expensive than processing documents stored in an inverted index with simpler ranking functions and optimized processing algorithms [36]. \n\nSparse neural retrievers [15,22,27] aim to merge these two approaches, using a pre-trained language model to compute very compact document representations, up to a single value, to be stored in an inverted index. In doing so, these retrievers try to reach the effectiveness of the complex pre-trained language models, while retaining the efficiency of simple bag-of-words retrieval models and query processing strategies. Most sparse neural retrievers integrate two main learning goals: they learn which terms in a document should be indexed (document content learning), and how to represent these terms in a corresponding index (term impact learning). In document content learning, irrelevant terms are removed and relevant terms are added to the documents. During term impact learning, each term in each document is associated with an impact score that will be stored in a posting of the term's posting list. \n\nDespite the effectiveness and efficiency of sparse neural retrievers w.r.t. more complex neural ranking models, recent works have shown that, while effective retrieval is possible with learned sparse approaches, they are often still much slower than their traditional counterparts [27].",
            "score": 0.5813979351799079,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "237278083"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "57663616"
                },
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "266693273",
            "title": "Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization",
            "text": "This paper aims to mitigate the impact of false negatives on dense passage retrieval. Toward such a goal, we extend the peer-loss framework and develop a confidence regularization for training robust retrieval models. The proposed regularization is compatible with any base retrieval model that uses NCE loss, a widely used contrastive loss function in dense retrieval. Through empirical and theoretical analysis, it is demonstrated that contrastive confidence regularization leads to more robust retrieval models. Building on this regularization, a passage sieve algorithm is proposed. \n\nThe algorithm leverages a dense retrieval model trained with confidence regularized NCE loss to filter out false negatives, thereby improving any downstream retrieval model including those that do not exploit NCE loss. The effectiveness of both the passage sieve algorithm and the confidence regularization method is validated through extensive experiments on three commonly used QA datasets. The results show that these methods can enhance base retrieval models, even when fewer negative samples are used. \n\n1. We only test our methods on top of AR2 (Zhang et al. 2021) and DPR (Karpukhin et al. 2020), more retrieval models can be tested to see the performance variance of the passage sieve and contrastive confidence regularizer in different cases. 2. Theorem 1 only assumes the loss function to be NCE loss and does not require the task to be dense retrieval. Thus, the proposed contrastive confidence regularizer has the same power on any other self-supervised tasks using NCE loss, e.g. self-supervised image classification, sentence embedding, etc. More experiments on these benchmarks should be conducted in future work. 3. The proposed methods are built upon CORES 2 (Cheng et al. 2021), which consider the first-order statistic(estimation) of peer loss. However, Zhu, Liu, and Liu (2021) has extended this into considering the secondorder statistics and achieving better performance in CV benchmarks. More work is needed to extend the contrastive confidence regularizer into second-order statistics as well.",
            "score": 0.58134470748749,
            "section_title": "Conclusion",
            "char_start_offset": 27756,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 1140,
                    "end": 1159,
                    "matchedPaperCorpusId": "229349082"
                },
                {
                    "start": 1168,
                    "end": 1191,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1765,
                    "end": 1784,
                    "matchedPaperCorpusId": "222141668"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27392578125
        },
        {
            "corpus_id": "249395549",
            "title": "A Neural Corpus Indexer for Document Retrieval",
            "text": "In this section, we briefly introduce the related works and leave more discussions in Appendix A. \n\nSparse retrieval. Traditional document retrieval methods are based on Sparse Retrieval, which is built upon inverted index with term matching metrics such as TF-IDF [45], query likelihood [33] or BM25 [44]. In industry-scale web search, BM25 is a difficult-to-beat baseline owing to its outstanding trade-off between accuracy and efficiency. In recent years, there are some attempts to incorporate the power of neural networks into inverted index. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for query and documents, which enables the construction of inverted index for efficient document retrieval. Doc2Query [41] predicts relevant queries to augment the content of each document before building the BM25 index, and DocT5Query [40] improves the performance of query generation by the pre-trained language model T5 [5]. Furthermore, DeepCT [9] calculates context-aware term importance through neural networks to improve the term matching metrics of BM25. \n\nDense retrieval. Another line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents. At inference time, efficient Approximate Nearest Neighbor (ANN) search algorithms, such as k-dimensional trees [3], localitysensitive hashing [10], and graph-based indexes (e.g., HNSW [38], DiskANN [27] and SPANN [7]) can be utilized to retrieve relevant documents within a sublinear time. Besides, Luan et al. [37] analyze the limited capacity of dual encoders, and propose a combination of sparse and dense retrieval methods with multi-vector encoding to achieve better search quality. \n\nAutoregressive retrieval. The other way to approach retrieval is utilizing an end-to-end autoregressive model.",
            "score": 0.5809637604052384,
            "section_title": "Related work",
            "char_start_offset": 7589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "13091446"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "747083"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "252872857",
            "title": "Query Expansion Using Contextual Clue Sampling with Language Models",
            "text": "We compare our approach with other memory efficient neural retrieval models in Table 5. Ma et al. (2021) show that the DPR could be furthered compressed to trade accuracy off against speed and storage. However, the accuracy of DPR could drop significantly if compressed to the same storage level of the lexical index. BPR (Yamada et al., 2021) integrates a learning-to-hash technique into DPR to represent the passage index using compact binary codes. The index size of BPR is slightly smaller than ours approach, but we achieve higher retrieval accuracy on both two datasets. We also include DrBoost (Lewis et al., 2021), a dense retrieval ensemble trained in stages. DrBoost outperforms ours approach on NQ dataset, while taking 6\u00d7 times larger index size.",
            "score": 0.5806366455895089,
            "section_title": "A.4 Comparison with Memory Efficient DPR techniques",
            "char_start_offset": 15587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 104,
                    "matchedPaperCorpusId": "239993003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1651611328125
        },
        {
            "corpus_id": "276580326",
            "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers",
            "text": "Dense Passage Retrieval (Karpukhin et al., 2020) utilizes a pre-trained language model such as BERT (Devlin et al., 2019), to encode text into dense vectors and conduct passage retrieval as a nearest neighbor search. This approach has shown strong in-domain effectiveness compared to traditional lexical retrievers such as BM25 (Robertson and Zaragoza, 2009). However, dense retrievers have been found to struggle with generalization when applied to out-of-domain retrieval tasks (Thakur et al., 2021). To address this issue, various works have aimed to improve the generalization of dense retrievers through continuous pre-training tailored for retrieval tasks. Works such as Condenser (Gao and Callan, 2021), RetroMAE (Xiao et al., 2022), and SimLM (Wang et al., 2023) have enhanced the dense representation of BERT via customized architectures during language modeling. Other works, including Contriever (Izacard et al., 2022), GTE (Li et al., 2023), E5 (Wang et al., 2024a) have further adapted two-stage contrastive learning. These models are first trained with unsupervised or weakly supervised large-scale contrastive learning, followed by supervised contrastive learning with available relevance-judged data (Nussbaum et al., 2024;Yu et al., 2024). CDE (Morris and Rush, 2024) further proposes a two-stage model architecture that integrates corpus-level information into document embeddings. In this work, we propose a data augmentation approach based on large language models, enabling the creation of high-quality augmentation data for smaller retrievers.",
            "score": 0.5804709936535104,
            "section_title": "Robust Dense Retrieval",
            "char_start_offset": 2248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 48,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 328,
                    "end": 358,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 687,
                    "end": 709,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3564453125
        },
        {
            "corpus_id": "267406766",
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "text": "4.3.1 Baselines for Retrieval Tasks. Retrieval tasks are divided into two main approaches: Sparse&Dense Retrieval and Generative Retrieval. For Sparse&Dense Retrieval, we employ the following models: BM25 [42], a classic sparse retrieval model; DPR [22] and its multi-task variant MT-DPR [34] for dense passage retrieval; RAG [27], which combines dense retrieval with seq2seq models for an enhanced generation; E5 [51], a state-of-the-art text embedding model; and SimLM [52], a dense passage retriever with effective pre-training methods. The Generative Retrieval methods comprise T5 [39], a pre-trained encoder-decoder model for multitask learning, BART [26], a denoising autoencoder for text generation; SEAL [1], generating sub-strings as document identifiers; CorpusBrain [7], incorporating pre-training strategies for KILT retrieval task; and Llama2 [50], an open-source pre-trained LLM. \n\nThe baseline retrieval models are finetuned with labeled retrieval data from KILT datasets. Except the DPR model is finetuned on each specific dataset, other dense and generative retrieval models are multi-task finetuned using retrieval data across all datasets, as multi-task training in KILT leads to improved performance [7,34].",
            "score": 0.5799842046454473,
            "section_title": "Baselines",
            "char_start_offset": 23016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1227
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "230435546"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "250311114"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 656,
                    "end": 660,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 777,
                    "end": 780,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1223,
                    "end": 1226,
                    "matchedPaperCorpusId": "230435546"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "In recent years, with the development of largescale pre-trained language models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), we witness the research interest in information retrieval shifting from the traditional sparse retrieval to neural dense retrieval. Karpukhin et al. (2020); Xiong et al. (2021) proposed dense passage retriever (DPR), which uses two neural encoders initialized with BERT to model the query and document, independently. The subsequent works follow this dual-encoder framework. One line of work improves dense retrieval by mining hard negatives, where they select the top-ranked documents retrieved by the recent retriever as hard negatives and then re-train the retriever with the newly mined hard negatives (Xiong et al., 2021;Qu et al., 2021). However, the mined hard negatives are highly likely to contain false negatives, harming the performance. To mitigate this, following studies denoise the mined hard negatives with re-rankers (Qu et al., 2021;Ren et al., 2021;Zhang et al., 2022). Another kind of work focuses on pre-training to make the pre-trained models more suitable for dense retrieval, such as Condenser (Gao and Callan, 2021), coCondenser (Gao and Callan, 2022) and SIMLM (Wang et al., 2022). \n\nDense retrieval models typically depend on extensive supervised data, comprising pairs of queries and positive documents. To address the challenge of limited training data, Ma et al. (2021); Sun et al. (2021) proposed to train a query generator on highresource information retrieval data, and then used the query generator to generate synthetic queries for low-source target domains. Additionally, Dai et al. (2023) harnessed large language models in zero/few settings to produce synthetic queries for documents in target domains, eliminating the need for training a general query generator. In contrast to these approaches, our work leverages synthetic queries for document expansion.",
            "score": 0.5794712674190571,
            "section_title": "Dense Retrieval",
            "char_start_offset": 24814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 280,
                    "end": 303,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 754,
                    "end": 774,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 774,
                    "end": 790,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 982,
                    "end": 999,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1016,
                    "end": 1035,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1202,
                    "end": 1224,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1449,
                    "end": 1466,
                    "matchedPaperCorpusId": "235303662"
                },
                {
                    "start": 1656,
                    "end": 1673,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329833984375
        },
        {
            "corpus_id": "254854344",
            "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
            "text": "Retriever-based language modeling or question answering has been well studied in the literature, and different mechanisms have been proposed for integrating language models with retrieved statements. For instance, REALM (Retrieval-Augmented Language Model) is a masked language model that is augmented with a latent knowledge retriever, which allows the model to retrieve and attend over statements (Guu et al., 2020). The knowledge retriever employs a BERT encoder to achieve a dense representation for both the query and statements. The knowledge retriever selects the most similar statements among all based on a similarity metric (e.g. dense inner product) between the transformed query's and the statements' representations. In the language modeling task, the retrieved statements are appended to the query and passed to a trained BERT-based language model to solve the task. In the question answering task, however, a trained BERT-based reader extracts the more promising span from the statements as the answer.\n\nkNN-LM is another proposed model based on a decoder-only Transformer, integrated with a knearest neighbor module (Khandelwal et al., 2020). In this model, the most related token sequences from the statements are selected based on an L2 similarity metric between the representation of the query and all token sequences. The distribution over the next token in generation in this autoregressive model is subsequently computed as the interpolation between the Transformer's final distribution and the distribution of the next tokens over the retrieved statements.\n\nFiD (Fusion-in-Decoder) is a sequence-tosequence T5-based neural network which can work with any retriever (Izacard and Grave, 2021). Given the retrieved statements by the retriever, the encoder encodes the query and each retrieved statement separately. Afterwards, the decoder attends over the resulting representations of all the retrieved statements. In this paper, we investigate DPR (Dense Passage Retriever) as the retriever for FiD (Karpukhin et al., 2020). DPR retrieves the most similar documents based on the inner product of the representations of the query and the documents (i.e. embeddings of the [CLS] token) from two independent trained BERT encoder models.\n\nFinally, ATLAS is a pre-trained retrieval augmented language",
            "score": 0.579126838772446,
            "section_title": "Retriever-augmented Language Models",
            "char_start_offset": 8151,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 417,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1688,
                    "end": 1713,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425537109375
        },
        {
            "corpus_id": "265457188",
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "text": "Remarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021). \n\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in zero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022;Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries. \n\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency.",
            "score": 0.5790987000036926,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 597,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 667,
                    "end": 691,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 819,
                    "end": 841,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 896,
                    "end": 913,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1088,
                    "end": 1112,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1169,
                    "end": 1186,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1287,
                    "end": 1306,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1306,
                    "end": 1323,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "The dual-encoder architecture and lightweight dot product evaluation enable us to encode and index all passages in the collection C beforehand, so we only need to encode the given query for online retrieval, achieving more efficiency. \n\nLearning Framework for Retriever. To train the dual-encoder  , we utilize contrastive learning following previous works [15,49]. Specifically, with a given query , a labeled positive passage  + , and negative passages N , contrastive loss can be applied to optimize the dual-encoder  by maximizing the relevance of the  and  + while minimizing that of  and  \u2208 N , i.e., \n\nwhere negative passage set N can be generated from top-ranked non-answer passages in retrieval results of BM25 model [34] or a trained retrievers [52,53], i.e., \n\nwhere  is a probability distribution over C, which can be defined as non-parametric (e.g.,  samp = \u2298) or parametric (e.g.,  samp \u2260 \u2298). \n\nDense & Lexicon-Aware Retrievers. Both dense retriever ( den ) and lexicon-aware retriever ( lex ) follow dual-encoder architecture and the encoders are built upon PLMs like BERT [9]. Precisely, a PLM ( plm ) encodes a given text (i.e., query  or passage ),  = { 1 ,  2 , ...  }, to contextualized embeddings, i.e., \n\neventually \n\n[CLS] and [SEP] are special tokens designed for sentence representation and separation by recent PLMs [9,30]. Dense retriever [35,49] represents text by using the embedding of special token [CLS] (i.e.,   \n\n[CLS] ) as follows, \n\nwhere  den =  plm with no additional parameters. For lexicon-aware retriever, we adopt SPLADE [10] which learns to predict the weights of terms in PLM vocab for each token in the input  by the Masked Language Modeling (MLM) layer and sparse regularization, then max-pooling these weights into a discrete text representation after log-saturation.",
            "score": 0.5789242911029548,
            "section_title": "R (\ud835\udc5e, \ud835\udc5d",
            "char_start_offset": 10105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 759,
                    "end": 762,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1342,
                    "end": 1345,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1370,
                    "end": 1373,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1041259765625
        },
        {
            "corpus_id": "251293270",
            "title": "Improving zero-shot retrieval using dense external expansion",
            "text": "For instance, MacAvaney, Yates, Cohan, and Goharian (2019) showed that an effective BERT reranker could be trained using MSMARCO passage ranking data, for adhoc search, as well as COVID-related literature (MacAvaney, Cohan and Goharian, 2020). In contrast, attempts using zero-shot dense retrieval models have under-performed compared to existing models (Chen, Zhang, Lu, Bendersky and Najork, 2022;Thakur, Reimers, R\u00fcckl\u00e9, Srivastava, & Gurevych, 2021). To mitigate this domain shift between the external trained collection and the target collection, we propose to employ the external expansion from the external high quality collection thus improving zero-shot retrieval performance. \n\nOn the one hand, we investigate the benefit to a classical BoW PRF model that is supported by a pseudo-relevant feedback set obtained from different dense retrieval approaches. Moreover, we also study the effectiveness of external dense expansion on the dense retrieval. This facilitates an investigation into how external expansion changes the semantic manner of retrieval. Finally, we investigate whether the sparse external retrieval can produce feedback information that is useful for the dense retrieval. \n\nIn summary, our work makes the following contributions: we investigate external expansion when mixing sparse & dense retrieval paradigms (including both single representation and multiple representation dense retrieval); sparse and zero-shot dense retrieval experiments are conducted on two classical TREC test collections (Robust04 & WT10G); We also conduct zero-shot evaluation on four BEIR datasets (Thakur et al., 2021) (namely DBPedia, NFCorpus, TREC-COVID and Touch\u00e9-2020) -a set of datasets selected for evaluating zero-shot evaluation; We deploy two sparse weighting models (BM25 & DPH), two sparse PRF approaches (RM3 & Bo1), two dense retrieval models (ANCE & ColBERT), and two dense RPF approaches (ANCE-PRF & ColBERT-PRF); we analyse the propensity for a multiple representation PRF technique to perform semantic vs. exact token matching, under normal and external PRF conditions.",
            "score": 0.5787523613556511,
            "section_title": "Introduction",
            "char_start_offset": 3334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 58,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 354,
                    "end": 399,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 399,
                    "end": 453,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10467529296875
        },
        {
            "corpus_id": "263866951",
            "title": "Retrieval-based Language Models and Applications",
            "text": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.",
            "score": 0.5786423327930451,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "247940117",
            "title": "Revisiting a kNN-based Image Classification System with High-capacity Storage",
            "text": "Knowledge retrieval has seen substantial advancement in recent years, particularly in DNN-based natural language processing (NLP). DPR [29] applied a dense representation to passage retrieval in open-domain question answering tasks and achieved better retrieval performance than traditional sparse vector space models, such as TF-IDF and BM25. KEAR [55] brought external knowledge into the predicting process of Transformer [49] to reach human parity in a challenging commonsense task [47]. RETRO [3] introduced a frozen kNN retriever into the Transformer architecture in the form of chunked cross-attention to enhance the performance of auto-regressive language models. External world knowledge has been retrieved to assist in solving various NLP tasks. Our work looks to extend the adoption of knowledge retrieval beyond the modality of NLP. We introduce an image classification architecture based on knowledge retrieval, which is a data-driven paradigm that is centralized on available large-scale data resources and is supported by a trained representation model and a kNN classifier.",
            "score": 0.5784003250602532,
            "section_title": "Knowledge Retrieval",
            "char_start_offset": 5857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1088
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "246210468",
            "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
            "text": "The advent of transformer-based models such as BERT has led to the rise of neural ranking models. These models have improved the effectiveness of retrieval systems well beyond that of lexical term matching models such as BM25. While monolingual retrieval tasks have benefited from large-scale training collections such as MS MARCO and advances in neural architectures, cross-language retrieval tasks have fallen behind these advancements. This paper introduces ColBERT-X, a generalization of the ColBERT multi-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R) encoder to support cross-language information retrieval (CLIR). ColBERT-X can be trained in two ways. In zero-shot training, the system is trained on the English MS MARCO collection, relying on the XLM-R encoder for cross-language mappings. In translate-train, the system is trained on the MS MARCO English queries coupled with machine translations of the associated MS MARCO passages. Results on ad hoc document ranking tasks in several languages demonstrate substantial and statistically significant improvements of these trained dense retrieval models over traditional lexical CLIR baselines.",
            "score": 0.5783226316900801,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "267411885",
            "title": "eXplainable Bayesian Multi-Perspective Generative Retrieval",
            "text": "Efficient document retrieval has long been dominated by statistical methods such as TF-IDF and BM25. With the rise of the Transformer era, new search methodologies like Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), which leverages language model embeddings, as well as those employing Seq-to-Seq models (Lewis et al., 2020a;Raffel et al., 2020), have emerged. \n\nRAG (Lewis et al., 2020b) integrates a pretrained retrieval with a pretrained Seq-to-Seq generator and demonstrates the feasibility of end-to-end finetuning. Building on this, Re2G (Glass et al., 2022) extends the work by adding a reranker, which merges contexts retrieved from ANN and BM25 indexes. Contriever (Izacard et al., 2022a) and Atlas (Izacard et al., 2022b) utilize contrastive learning, showcasing competitive results against existing models and displaying strong potential in unsupervised and multilingual retrieval tasks. \n\nIn this work, we extensively leverage the work of Song et al., 2024, which utilizes reinforcement and reranking in a generative retrieval model. Its architecture builds upon previous works (De Cao et al., 2020;Thorne, 2022), providing a comprehensive and extended approach to interpreting remaining spaces in the field of generative retrieval.",
            "score": 0.5778126685268421,
            "section_title": "Retrieval Augmented Language Models",
            "char_start_offset": 7080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 223,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 313,
                    "end": 334,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 334,
                    "end": 354,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 553,
                    "end": 573,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1120,
                    "end": 1133,
                    "matchedPaperCorpusId": "253581551"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "253420284",
            "title": "Distribution-Aligned Fine-Tuning for Efficient Neural Retrieval",
            "text": "Dense retrievers employ neural self-attention-based language models to compute vector representations of queries and documents. The original DPR approach by Karpukhin et al. [16] paved the way for a number of improved dense retrieval models, with most improvements being made with respect to the training method. ANCE [31] is trained using hard negatives obtained from an ANN index that is updated asynchronously during the training. TCT-ColBERT [21] employs knowledge distillation from the ColBERT model [17]. Lindgren et al. [22] maintain a negative cache to efficiently obtain the most impactful negative examples during training. Zhan et al. [32] start by training the model with random negatives in order to improve its stability and subsequently use dynamic hard negatives for better ranking performance. Gao and Callan [8] argue that existing dense language models are not optimal for retrieval and propose the Condenser architecture, which, in combination with LM pre-training, allows for better aggregation of information in a single vector representation. This architecture is further improved with coCondenser [9]. \n\nThe above-mentioned implementations base their encoders on BERT-base or similar transformer models, where the query and document encoder use shared weights, i.e. in a Siamese fashion. Jung et al. [15] use a semi-Siamese setting, where the encoders do share parameters as well, but they are adapted to their specific role (query or document encoding) using light fine-tuning methods. We are not aware of any approaches that employ heterogeneous models, where the two encoders do not share the same model architecture and initial weights. \n\nHybrid retrievers combine sparse term-matching-based models [25] with dense neural models. Typically, two sets of documents are retrieved and subsequently combined. CLEAR [11] takes the sparse component into account in the loss margin during the training phase of the dense model. COIL [10] builds dense representations of terms using a deep language model in a pre-processing step. \n\nWork on efficiency for dual-encoder-based models is limited to reranking so far. TILDE [35] replaces the query encoder with a probabilistic model.",
            "score": 0.577723574261496,
            "section_title": "Neural Retrieval and Ranking",
            "char_start_offset": 5839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "240070656"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "11592162"
                },
                {
                    "start": 1838,
                    "end": 1842,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32568359375
        },
        {
            "corpus_id": "263829279",
            "title": "Topic-DPR: Topic-based Prompts for Dense Passage Retrieval",
            "text": "Table 1 presents our experiments' outcomes using BERT-base and BERT-large models on the arXiv-Article and USPTO-Patent datasets. In comparison to sparse methods, dense retrieval techniques show significant performance improvement, emphasizing dense retrieval's importance. When contrasting DPTDR with DPR, DPTDR exhibits superior performance in the MAP@10 and MAP@50 metrics due to its continuous prompt enhancement. Our topic-based prompts in Topic-DPR boost the Acc and MRR metrics. Furthermore, Topic-DPR outperforms baseline methods across all metrics. Specifically, our Topic-DPRbase exceeds DPTDRbase by 3.00/2.42 and 2.64/2.98 points in MAP@10 and MAP@50, which are vital for large multi-category passage retrieval. Additionally, in the deep prompt tuning setting, our Topic-DPR \u2660 , despite slight performance degradation, still maintains comparative performance with only 0.1%-0.4% of the parameters tuned. The consistent enhancements across diverse settings, models, and metrics manifest the robustness and efficiency of our Topic-DPR method. This research establishes Topic-DPR as an effective deep prompt learning-based dense retrieval method, setting a new state-of-the-art for the datasets. Ablation experiments are conducted in Appendix. \n\n5 Analysis on Topic-DPR",
            "score": 0.5772107070595802,
            "section_title": "Experimental Results",
            "char_start_offset": 21361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1277
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15478515625
        },
        {
            "corpus_id": "248780209",
            "title": "WatClaimCheck: A new Dataset for Claim Entailment and Inference",
            "text": "In order to reduce the computational resources and memory requirements, we implement the encoders in the dense passage retrieval model using Distil-RoBERTa (Dis). We use a batch size of 64 and the in-batch negatives technique as described in (Karpukhin et al., 2020). \n\nWe evaluate the stage-1 methods by comparing their performance using the top-k recall rate metric. \n\nThe claim text is used to retrieve the ground truth review article sentences from the corpus containing all the sentences of all the review articles in the test set. The test contains a total of 114, 290 sentences and 3, 373 claims. We report the top-k recall rate for k = 10, 25, 50, 100 in Table 2. The results clearly show that the DPR (dense passage retrieval) method outperforms the method based on TF-IDF.",
            "score": 0.5769926346118485,
            "section_title": "Stage-1 Results",
            "char_start_offset": 21050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 782
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10577392578125
        },
        {
            "corpus_id": "274822530",
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "text": "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023;Fu et al., 2023). When integrating LLMs, the model can leverage the EOS token embedding, which acts as a contextual representation of the entire sentence. This token's embedding encapsulates the semantic meaning of the full query or document, allowing for a richer, more accurate comparison of inputs within the shared vector space. The use of LLMs in dual-encoder architectures opens up new possibilities for improving the model's ability to capture nuanced semantic relationships. \n\nGenerative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2024), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search. Generative models learn to map queries to their relevant documents by generating unique docids or specific document content.",
            "score": 0.5768123211664358,
            "section_title": "Dual-encoder approaches",
            "char_start_offset": 15799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 651,
                    "matchedPaperCorpusId": "257767171"
                },
                {
                    "start": 651,
                    "end": 667,
                    "matchedPaperCorpusId": "257970928"
                },
                {
                    "start": 1326,
                    "end": 1344,
                    "matchedPaperCorpusId": "258048596"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "246863488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222412109375
        },
        {
            "corpus_id": "272145432",
            "title": "LexBoost: Improving Lexical Document Retrieval with Nearest Neighbors",
            "text": "Further, to improve the effectiveness of information retrieval systems neural-based approaches for semantic retrieval, such as those utilizing CNNs and RNNs have been proposed. Here text documents and queries are represented in a continuous vector space. As a next step neural network based similarity measures are used for relevance calculation [16,34]. Additionally, neural methods utilize learned non-linear representations of text data, resulting in significant retrieval performance improvements [34]. Further, BERT [13], GPT [6] and other transformer architectures [39], have been used to improve the ability of information retrieval systems to attend to important parts of the query and documents for matching. \n\nThus, dense retrieval methods focus on learning dense representations for documents and work on semantic level. This boosts the effectiveness by a great extent but at the same time an exhaustive search over all document vectors results in compromized efficiency. Mainly, two primary types of dense methods exist: interactionbased where interactions between words in queries are modelled and the other being representation-based where the model learns a single vector representation of the query [30]. TAS-B [15] and TCT-ColBERT-HNP [21] are some of the state-of-the-art result producing methods belonging to this category. Further, multi representations method exemplified by ColBERT require significant memory and pruning methods have been proposed to make it more efficient [2]. Also, better sampling strategies have been proposed to train more effective dense retrieval models [10]. Dense retrieval approaches have also been modified to perform entity-oriented document retrieval [9]. \n\nThe advent of dense retrieval led to learned vector based pseudo relevance feedback models. Some of the popular ones include Col-BERT PRF [42], ColBERT-TCT PRF [21] and ANCE PRF [45]. Models like CEQE (Contextualized Embeddings for Query Expansion) utilize query focused contextualized embedding vectors [28]. Even though term based and vector based pseudo relevance feedback help tackle vocabulary dependence to some extent, they come with latency overheads. This is where LexBoost differentiates from others.",
            "score": 0.5767323310026655,
            "section_title": "Dense Methods",
            "char_start_offset": 6259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 350,
                    "end": 353,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1496,
                    "end": 1499,
                    "matchedPaperCorpusId": "260442164"
                },
                {
                    "start": 1600,
                    "end": 1604,
                    "matchedPaperCorpusId": "268752496"
                },
                {
                    "start": 1703,
                    "end": 1706,
                    "matchedPaperCorpusId": "266933536"
                },
                {
                    "start": 1848,
                    "end": 1852,
                    "matchedPaperCorpusId": "253763641"
                },
                {
                    "start": 1870,
                    "end": 1874,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1888,
                    "end": 1892,
                    "matchedPaperCorpusId": "237363901"
                },
                {
                    "start": 2014,
                    "end": 2018,
                    "matchedPaperCorpusId": "219162111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "246210468",
            "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
            "text": "BERT-style neural ranking models that use cross-attention between query and document terms [7,16] define the state of the art for monolingual English retrieval. Such models are typically used as rerankers in a retrieve-and-rerank pipeline, due to the quadratic time and space complexity of self-attention in the transformer architecture [30]. Reranking using these models is effective but timeconsuming, so the number of documents to be reranked must be tuned to balance the trade-off between effectiveness and efficiency. In contrast to the reranking approach, dense retrieval models encode query and document representations independently and match them with custom similarity functions (e.g., cosine similarity). Dense retrieval complements the lexical first phase retrieval by using an approximate nearest neighbor search over contextualized representations. \n\nWhile the retrieve-and-rerank framework has been adapted and explored in cross-language information retrieval (CLIR) [37,11,38,3,36], most approaches translate queries into the language of the documents and perform monolingual retrieval [28,29]. Dense retrieval models, on the other hand, remain under-explored in CLIR. In this work, we develop an effective dense retrieval model for CLIR. \n\nDense retrieval models can be broadly categorized into two variants: singlerepresentation and multi-representation [15]. Single-representation models encode queries and documents separately to create a single aggregate representation. However, that can lead to loss of information. Multi-representation models use multiple representations of queries and documents to predict relevance. One such model is ColBERT [13], which computes a similarity between each query term representation and each document term representation. Yet ColBERT is exclusively monolingual. This paper presents ColBERT-X, a generalization of the ColBERT approach that supports CLIR. ColBERT-X uses a translate and train fine-tuning approach to exploit existing CLIR training resources. \n\nThis generalization poses two challenges: enabling the encoders to process multiple languages, and identifying appropriate resources with which to train the model. To address the former, we adapt XLM-R [5], a multilingual pretrained transformer language model, to initialize the dense retrieval model.",
            "score": 0.5766407054231699,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "208058719"
                },
                {
                    "start": 992,
                    "end": 994,
                    "matchedPaperCorpusId": "219166648"
                },
                {
                    "start": 994,
                    "end": 997,
                    "matchedPaperCorpusId": "218901033"
                },
                {
                    "start": 1372,
                    "end": 1376,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "248366563",
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "text": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
            "score": 0.5758476017013487,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1689,
                    "end": 1693,
                    "matchedPaperCorpusId": "5541663"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "215416349"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "267750652",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "text": "Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base.",
            "score": 0.5757590866169977,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404296875
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "Pre-trained Language Models (PLMs). Through pre-training, language models can achieve higher performance when transferred to specific tasks. Transformer [27], based on self-attention mechanism, is a mainstream architecture of PLMs, including encoderonly [11,14] model and decoder-only [2,29] model. To address the quadratic time scaling of Transformer architecture, some architectures broadly regarded as state space models have been proposed [8,20], especially high-performing models like sub-quadratic architecture M2-BERT [6], linear architecture Mamba [7] and Mamba-2 [5]. \n\nDense Retrieval Models. Transformer PLMs have been proven effective for dense retrieval. Initially, encoder-only models are adopted for retrieval tasks due to the bi-directional attention mechanisms [10,19]. Later, decoder-only models are adopted for retrieval tasks due to their effectiveness on larger model size [15,17]. \n\nLong-text Dense Retrieval Models. In long-text retrieval, early works use chunking strategies [4] due to the small context window. In order to help the model better understand complete and coherent semantics, some studies explore Transformer-based long-text retrieval models [9,15,30]. \n\nFaced with the quadratic time scaling of Transformer-based longtext retrieval models, the sub-quadratic M2-BERT model [6] has been utilized for long-text retrieval tasks [23].",
            "score": 0.5757174574879576,
            "section_title": "Related Work",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "264288993"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "264288993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30126953125
        },
        {
            "corpus_id": "248646720",
            "title": "Deep learning-based approach for Arabic open domain question answering",
            "text": "DPR is an efficient retrieval method that uses dense representations to compute relevancy. Dense techniques use text as the input to neural network encoders, and the text is represented as a vector of a fixed size, usually 768. Despite the fact that the individual dimensions do not match any specific language or linguistic feature, each dimension stores some information about the text. The relative density of these vectors is due to the rarity of a zero value in them. The model architecture used a dual encoder, that is, two BERT base models, one to encode the query and the other to encode the passage. The dot product similarity between the query and the document embeddings is used to rank documents. The passage encoder is used in all the passages and indexes them using FAISS (Johnson, Douze & Jegou, 2021). During training, the question-context pair is sent into the DPR model, and the weights are tuned to maximize the dot product between the two model outputs. The dot product value of the two model outputs measures the similarity between both vectors. A higher dot product correlates to higher similarity. The context encoder and the question encoder were both trained to give very similar vectors as output for relevant question-context pairs (see Fig. 1). Separate encoders will help with queries shorter than documents, and employing \"in-batch negatives,\" gold labels used as negative examples for other data in the same batch, is more effective. \n\nDPR (Karpukhin et al., 2020) employs a dense encoder E P (\u2022), which converts any text into a d-dimensional vector and generates an index for all M passages. DPR uses a separate encoder called E Q (\u2022) during run-time, which converts the input question to a ddimensional vector and returns K nearest passages vectors to the question vector. Using the dot product of their vectors, the similarity between the question q and the passage p can be defined as the following (Karpukhin et al., 2020): Full-size \ue90d DOI: 10.7717/peerj-cs.952/fig- 1 sim\u00f0q; p\u00de \u00bc E Q \u00f0q\u00de T E P \u00f0p\u00de",
            "score": 0.5746584557818032,
            "section_title": "Dense passage retriever methodology",
            "char_start_offset": 14001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 786,
                    "end": 816,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1471,
                    "end": 1494,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "258865354",
            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
            "text": "A major goal of this work is to investigate retrieval methods that forego neural computation and the need for specialized accelerator hardware at query time. As such, we focus on a method that uses a large neural model to precompute the required representations of the retrieval items (documents) ahead of time. Then, at retrieval time, the method performs only basic featurization (e.g., tokenization) of the queries. \n\nSpecifically, we investigate query-document scoring functions that score the compatibility of a query-document pair with the inner-product of separate featurizations of the query   () and document   (). score(, ) = \u27e8  (),   ()\u27e9 \n\nThis form is familiar from both traditional lexicalized retrieval and from more recent work on dense retrieval. In lexicalized retrieval, (e.g., TF-IDF and BM25) [36,37],   and   assign non-zero scores to sub-strings of  and . On the other hand, in dense retrieval [14,16,26],   and   are neural networks that map  and  to dense vectors. Note that this formulation does not allow for deeper interactions between  and , such as typical cross-encoder scorers, as these cannot be computed efficiently and without an accelerator at query time. \n\nWe investigate an alternative formulation of Equation 1 than either traditional lexicalized retrieval or dense retrieval. In this formulation,   can be an arbitrarily complex neural network, but   must be a sparse featurization that can be quickly computed on commodity CPUs. This way, it is possible to push all costly neural network inference to indexing time, and avoid the need for accelerators at serving-time. For this paper, we choose   to be a Figure 1: Our model adapts the T5 encoder-decoder architecture to predict query token scores given an input passage. The encoder (a) reads an input passage prepended with a static prompt. The decoder (b) can be initialized from a pretrained T5 checkpoint, but the architecture is modified in a few ways to be non-autoregressive: the only inputs are the standard position embeddings, the decoding is parallelized for efficiency, and the output at each position is the full distribution over the vocabulary.",
            "score": 0.5743328379113138,
            "section_title": "NAIL MODEL",
            "char_start_offset": 7958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08978271484375
        },
        {
            "corpus_id": "270199372",
            "title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models",
            "text": "The QA systems utilize passage retrieval and reranking to select candidates.The unsupervised retrievers, like BM25 [31], MSS [33],\n\nand Contriever [9], and supervised retrievers, such as DPR [12], and MSS-DPR [33], retrieve a subset of candidate passages, which are then re-ranked in subsequent stages.BM25 measures the relevant scores between questions and passages by exact term matching.MSS is a dense retriever that undergoes joint training with both retriever and reader components.Contriever utilizes a contrastive learning framework for information retrieval (IR) pre-training, exhibits competitive performance relative to BM25.DPR is a dense passage retriever that uses the bi-encoder architecture, trained on question-passage relevance pairs.MSS-DPR presents a notable enhancement in DPR's efficacy through pre-training the dense retriever with the MSS method and applying DPR-style supervised fine-tuning.Besides these retrievers, recently, some new supervised methods are proposed like ColBERT [13], SPLADE [7] and SparseEmbed [14].In this study we rerank the top- passages retrieved by BM25, MSS, Contriever, DPR and MSS-DPR to show its effectiveness across different retrieval paradigms.GTR converts a reranking task into a generation task to utilize LLMs.Such as, query generation [2,6,32,49], relevance generation [18,47,48], and permutation generation [20,26,27,35,37,42].The query generation method computes the relevance score between a query and a document by the log-likelihood of LLMs to generate the query based on that document.UPR [32] calculates the query generation log-likelihood based on T0-3B [34] while dos Santos et al. [6] fine-tune GPT-2 [28] and BART [17] with unlikelihood loss and pairwise loss respectively and then computed the query generation log-likelihood.",
            "score": 0.5742314934959879,
            "section_title": "RELATED WORK 2.1 Passage Retrieval and Reranking",
            "char_start_offset": 3154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 76,
                    "end": 130
                },
                {
                    "start": 132,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1798
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 191,
                    "end": 195,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1018,
                    "end": 1021,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1038,
                    "end": 1042,
                    "matchedPaperCorpusId": "259949679"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "258840999"
                },
                {
                    "start": 1298,
                    "end": 1300,
                    "matchedPaperCorpusId": "222178252"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "248218489"
                },
                {
                    "start": 1303,
                    "end": 1306,
                    "matchedPaperCorpusId": "264406035"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "252993059"
                },
                {
                    "start": 1378,
                    "end": 1381,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2183837890625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "(Multi-Modal) Dense Retrieval. Using dense vectors for retrieving textual documents related to a textual query has been studied since the emergence of Latent Semantic Analysis [7]. However, dense retrievers' performance remained inferior to that of sparse retrievers like BM25 until Karpukhin et al. [23]'s Dense Passage Retriever (DPR), which uses the [CLS] token output by BERT [8], a pre-trained language model. While many dense retrievers only use a single vector to represent the query and the document [23,41,59], using multiple vectors per document and query has been also studied [11,19,24,36,49]. \n\nMulti-modal dense retrieval has recently been investigated in different forms: (1) uni-modal query and multi-modal documents [15,34,52], (2) multi-modal query and uni-modal documents [40], \n\n(3) multi-modal query and multi-modal documents [51], and (4) unimodal query and uni-modal documents with queries and documents from different modalities, i.e., cross-modal retrieval [21,42]. \n\nIn this work, we focus on the second case, where the query is multi-modal while the documents only contain text. Qu et al. [40] utilized an asymmetric bi-encoder with LXMERT [53], a pretrained vision-language model based on BERT [8] for encoding queries, and BERT itself for encoding documents. As we show, such an asymmetric architecture is sub-optimal; utilizing different encoders creates a semantic \"gap\" in the embedding space and finetuning cannot easily overcome the issue. We instead propose a new symmetric dual encoding framework that addresses this issue. \n\nKnowledge Distillation for Dense Passage Retrieval. Due to the vast number of learnable parameters in dense passage retrievers, sometimes available datasets are insufficient to train them [62]. Consequently, knowledge distillation, in which a teacher model provides labels for a student model, has become a standard approach for training dense retrieval models and has shown compelling outcomes [17,30].",
            "score": 0.5739982065743847,
            "section_title": "RELATED WORK",
            "char_start_offset": 4661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "3252915"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "222205492"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 595,
                    "end": 598,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "210859295"
                },
                {
                    "start": 740,
                    "end": 743,
                    "matchedPaperCorpusId": "233219849"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "235097195"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 1167,
                    "end": 1171,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1222,
                    "end": 1225,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1750,
                    "end": 1754,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343994140625
        },
        {
            "corpus_id": "247476289",
            "title": "Multi-View Document Representation Learning for Open-Domain Dense Retrieval",
            "text": "In addition to the aforementioned work focusing on the architecture design, there exist loads of work that proposes to improve the effectiveness of dense retrieval. Existing approaches of learning dense passage retriever can be divided into two categories:\n\n(1) pre-training for retrieval (Chang et al., 2020;Guu et al., 2020) and (2) finetuning pre-trained language models (PLMs) on labeled data (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021).\n\nIn the first category,  and Chang et al. (2020) propose different pre-training task and demonstrate the effectiveness of pre-training in dense retrievers. Recently, DPR-PAQ (Oguz et al., 2021) proposes domain matched pre-training, while Condenser (Gao and Callan, 2021a,b) enforces the model to produce an information-rich CLS representation with continual pre-training.\n\nAs for the second class, recent work (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021;Zhan et al., 2021) shows the key of fine-tuning an effective dense retriever revolves around hard nega-tives. DPR (Karpukhin et al., 2020) adopts in-batch negatives and BM25 hard negatives. ANCE (Xiong et al., 2020) proposes to construct hard negatives dynamically during training. RocketQA (Qu et al., 2021;Ren et al., 2021b) shows the cross-encoder can filter and mine higher-quality hard negatives. Li et al. (2021) and Ren et al. (2021a) demonstrate that passage-centric and query-centric negatives can make the training more robust. It is worth mentioning that distilling the knowledge from crossencoder-based re-ranker into bi-encoder-based retriever (Sachan et al., 2021;Izacard and Grave, 2021;Ren et al., 2021a,b; can improve the bi-encoder's performance. Most of these works are built upon bi-encoder and naturally inherit its limit of a single vector representation",
            "score": 0.5738043090693625,
            "section_title": "Effective Dense Retrieval",
            "char_start_offset": 8857,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 309,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 397,
                    "end": 421,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 440,
                    "end": 456,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 911,
                    "end": 927,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 927,
                    "end": 945,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1041,
                    "end": 1065,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1218,
                    "end": 1235,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1329,
                    "end": 1345,
                    "matchedPaperCorpusId": "236034464"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "236477844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "score": 0.5736244718533011,
            "section_title": "Latent Variables in Retrieval Augmented Models",
            "char_start_offset": 12356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319091796875
        },
        {
            "corpus_id": "253763641",
            "title": "ColBERT-PRF: Semantic Pseudo-Relevance Feedback for Dense Passage and Document Retrieval",
            "text": "As can be seen from Table 2, ColBERT-PRF Ranker exhibits significant improvements over both ANCE and ColBERT E2E models on Recall. On the other hand, the effectiveness of ColBERT-PRF ReRanker also suggests that the expanded query provides a better query representation, which can which can better rank documents in the existing candidate set. Overall, in response to RQ1, we conclude that our proposed ColBERT-PRF model is effective compared to the ColBERT E2E dense retrieval model. For RQ2(a), we compare the ColBERT-PRF Ranker and ReRanker models with the lexical retrieval approaches. For both query sets, both Ranker and ReRanker provide significant improvements on all evaluation measures compared to the BM25 and BM25+RM3 models. This is mainly due to the more effective contexualised representation employed in the ColBERT-PRF models than the traditional sparse representation used in the lexical retrieval approaches. Furthermore, both ColBERT-PRF Ranker and ReRanker outperform the sparse retrieval approaches when reranked by either the BERT or the ColBERT models -e.g., BM25+(Col)BERT and BM25+RM3+(Col)BERT -on all metrics. In particular, ColBERT-PRF Ranker exhibits marked improvements over the BM25 with BERT or ColBERT reranking approach for MAP on the TREC 2019 queries. This indicates that our query expansion in the contextualised embedding space produces query representations that result in improved retrieval effectiveness. Hence, in answer to RQ2(a), we find that our proposed ColBERT-PRF models show significant improvements in retrieval effectiveness over sparse baselines. \n\nTo further gauge the extent of improvements brought by the PRF additional information in the sparse retrieval and the dense retrieval paradigms, we compare the amount of performance improvements in terms of MAP for ColBERT-PRF vs. ColBERT, ANCE-PRF vs. ANCE, and BM25+RM3 vs. BM25 in Figure 3. We observe that more queries improved, and by a larger margin, by ColBERT-PRF compared to both RM3 and ANCE-PRF.",
            "score": 0.5733884621900289,
            "section_title": "Results",
            "char_start_offset": 33444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166015625
        },
        {
            "corpus_id": "254877418",
            "title": "Adam: Dense Retrieval Distillation with Adaptive Dark Examples",
            "text": "To make a comprehensive comparison, we choose the following state-of-the-art approaches as baselines. These methods contain both sparse and dense passage retrievers. \n\nThe sparse retrieval methods include the traditional retriever BM25 (Yang et al., 2017b) and several representative sparse retrievers, including doc2query (Lu et al., 2020), DeepCT (Dai and Callan, 2019a), docTTTTT-query (Nogueira et al., 2019a), UHD-BERT (Jang et al., 2021), COILfull (Gao et al., 2021), UniCOIL (Lin and Ma, 2021), and SPLADE-max (Formal et al., 2021). \n\nThe dense retrieval methods produce continuous neural vectors for each passage and query. \n\nThe methods include DPR-E (Qu et al., 2021), ANCE (Xiong et al., 2021), TAS-Balanced (Hofst\u00e4tter et al., 2021b), ME-BERT (Luan et al., 2021a), ColBERT (Khattab and Zaharia, 2020b), ColBERT v2 (Santhanam et al., 2021), NPRINC (Lu et al., 2021), ADORE+STAR (Zhan et al., 2021a), Condenser (Gao and Callan, 2021a), RocketQA (Qu et al., 2021), PAIR (Ren et al., 2021b), CoCondenser (Gao and Callan, 2022), RoketQAV2 (Ren et al., 2021c), AR2 (Zhang et al., 2022), CL-DRD (Zeng et al., 2022), ERNIE-Search (Lu et al., 2022), RetroMAE (Xiao et al., 2022), Unifier (Shen et al., 2023), bi-SimLM (Wang et al., 2023), PROD (Lin et al., 2023) and InDi (Cohen et al., 2024). Some of them are enhanced by knowledge distillation from the ranker.",
            "score": 0.5729198607670642,
            "section_title": "Baselines",
            "char_start_offset": 20067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1365
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 256,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 660,
                    "end": 677,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 684,
                    "end": 704,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 719,
                    "end": 745,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 755,
                    "end": 775,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 889,
                    "end": 909,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 921,
                    "end": 944,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 955,
                    "end": 972,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 979,
                    "end": 998,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1046,
                    "end": 1065,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1100,
                    "end": 1119,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 1162,
                    "end": 1181,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1191,
                    "end": 1210,
                    "matchedPaperCorpusId": "248987123"
                },
                {
                    "start": 1221,
                    "end": 1240,
                    "matchedPaperCorpusId": "250311114"
                },
                {
                    "start": 1247,
                    "end": 1265,
                    "matchedPaperCorpusId": "252545066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14111328125
        },
        {
            "corpus_id": "257687445",
            "title": "Modular Retrieval for Generalization and Interpretation",
            "text": "First, from the last group of Table 10, it shows that the proposed modular retrieval approach, REMOP, outperforms the model using a single general prompt on all the evaluation tasks, which indicates  that each task module (composed of several task attribute modules) can indeed match the given task and improve model specificity. Compared to the fine-tuned DPR model, REMOP demonstrates comparable performance over the evaluation tasks, which shows that it's feasible to train a few parameters and obtain fine-tuned performance on zero-shot retrieval scenarios. \n\nSecond, it is also clear to see that the results of most of the listed dense passage retrieval methods are lower than the BM25, which is an acceptable phenomenon since previous research [54] has indicated that dense passage retriever perform worse than lexical retrieval models on zero-shot retrieval task. \n\nFrom this perspective, we can consider the proposed modular retrieval paradigm as a feasible solution to alleviate this shortcoming of dense retrieval. \n\nLastly, besides parameter efficiency and generalization, the interpretability advantage of REMOP is also shown in the results. For example, REMOP can have a higher enhancement over the fine-tuned model and the single-prompt model on Fact-Checking-related tasks, which indicates that the Fact-Checking retrieval module is well-trained in the training phase. This observation provides a valuable reference for subsequent improvements, guiding us in which part we should invest more training resources for improvement.",
            "score": 0.5727208336329319,
            "section_title": "Analysis",
            "char_start_offset": 29706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1542
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1563720703125
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "The overall experimental results on the test set are shown in Table 5, from which we can observe that: \n\n(1) On three different domain datasets in Multi-CPR, the retrieval performance of dense models outperforms sparse models. Taking the BM25 model and the DPR-1 model as an example, the average MRR@10 value over the three datasets are 0.2124 and 0.2837 respectively. The retrieval performance on the MRR@10 metric is largely improved by 33.57%, which points out the value of high-quality labeled data for the optimization of dense passage retrieval models. (2) For sparse methods, the BM25 method provides a strong baseline for all the three domain datasets. Especially on the dataset, the retrieval performance of BM25 is even slightly better than the DPR model (MRR@10: 0.2253 vs 0.2106, Recall@1000: 0.8150 vs 0.7750). We infer that the reason for this phenomenon is that the average length of query and passage in the domain of E-commerce is relatively short, and the search intent is explicit to some content. The method based on exact term matching can provide satisfactory retrieval results. This observation illustrates that traditional unsupervised term-based retrieval methods such as BM25 can still provide valuable results for passage retrieval in some specific domains. Moreover, as an optimization method that has been verified in previous work, Doc2Query has also achieved significant improvement on all three datasets as expected. \n\n(3) For dense methods, we conduct the analysis from two aspects. In the dataset aspect, we can find that the DPR model trained on in-domain labeled dataset has achieved remarkable performance improvement compared to the dense model trained with general domain data, even though the size of labeled dataset is much smaller. As such, we can conclude that labeled data in general domain is helpful for training dense retrieval model on specific domain to some extent, but the in-domain labeled data could provide more effective and valuable information for model training.",
            "score": 0.5722826158125595,
            "section_title": "Results",
            "char_start_offset": 22398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "268819685",
            "title": "Generative Retrieval as Multi-Vector Dense Retrieval",
            "text": "In recent years, the advent of pre-trained language models has catalyzed the popularity of neural-based retrieval models within the information retrieval community [14,15,31,34,41].Neural-based retrieval models.Dense retrieval (DR), as one of the effective neural-based retrieval methods, has achieved the stateof-the-art ranking performance on multiple benchmarks [14,15,31].Several approaches have been proposed to use multiple vectors to represent documents or queries, a.k.a., multi-vector dense retrieval (MVDR) [15,34,51].\n\nRecently, generative retrieval (GR) has emerged as a new paradigm in information retrieval.It aims to generate identifiers of relevant documents for a given query directly and parametrizes the indexing, retrieval, and ranking process in dense retrieval systems into a single model.GR adopts a sequence-to-sequence architecture model and is trained to directly map queries to their relevant document identifiers.Generative retrieval vs. dense retrieval.Dense retrieval models typically employ encoders, e.g., BERT [6], for encoding both queries and documents, while the generative retrieval model adopts an encoder for query encoding and a decoder for identifier generation.Despite their superficial differences, dense retrieval and generative retrieval share key characteristics in their query-document relevance computations.When the two methods use document identifiers such as sub-strings, titles, or semantic IDs as representations for documents, relevance to a query of a document is computed as the dot product of two vectors in both methods.Dense retrieval involves using the direct product of the query vectors and document vectors as the relevance, while generative retrieval leverages the product of the last latent state from the decoder at each position with the prediction head, a.k.a., the word embedding lookup table.Consequently, a natural question that arises in this context:\n\nHow is generative retrieval related to dense retrieval?\n\nAlthough GR has shown promising results in various benchmarks as a new end-to-end retrieval paradigm [19,25,38,42,44], relatively few publications have closely examined how GR models work.",
            "score": 0.5721681086913635,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1862
                },
                {
                    "start": 1862,
                    "end": 1923
                },
                {
                    "start": 1925,
                    "end": 1980
                },
                {
                    "start": 1982,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 372,
                    "end": 375,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "243865659"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25146484375
        },
        {
            "corpus_id": "248177840",
            "title": "Exploring Dual Encoder Architectures for Question Answering",
            "text": "In our work, we focus on dual encoder architectures and their impacts on the QA retrieval quality. Other dense retrieval models beyond dual encoders, e.g. ColBERT (Khattab and Zaharia, 2020) and DensePhrases (Lee et al., 2021a), are beyond the scope of this work. In addition, we demonstrate that sharing projection layer is a simple yet effective technique to improve the quality of asymmetric dual encoder for QA retrieval tasks. However, other more complicated architectural improvements of ADE, e.g. introducing more complexity in the projection layers or interactions between two encoder towers, are beyond the scope of this work.",
            "score": 0.5720570768762154,
            "section_title": "Limitations",
            "char_start_offset": 12617,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 190,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "229363636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10107421875
        },
        {
            "corpus_id": "250340145",
            "title": "SpaceQA: Answering Questions about the Design of Space Missions and Space Craft Concepts",
            "text": "The three bi-encoders DPR, ColBERT and CoCondenser have reported state-of-the-art results in passage retrieval datasets. We use the bi-encoders to encode the passages extracted from the CDF reports and store the passage embeddings in FAISS [15]. FAISS is an efficient library for similarity search and clustering of dense vectors. \n\nIn inference time we use the bi-encoders to encode the question and then query the FAISS index for the most similar passages.",
            "score": 0.5720045291364674,
            "section_title": "Passage Retrieval",
            "char_start_offset": 8093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 458
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10467529296875
        },
        {
            "corpus_id": "264172563",
            "title": "On Coherence-based Predictors for Dense Query Performance Prediction",
            "text": "We examined the accuracy of query performance predictors upon two single-representation dense retrieval methods (ANCE and TCT-ColBERT). In particular, we proposed new variants of existing unsupervised coherence-based predictors and managed to increase their performance in dense retrieval settings. In this way, we showed that changing the representation type from TF.IDF to neural embeddings provided by the dense retrieval models together with some further modifications is enough to generalise performance of unsupervised predictors in relation to supervised ones. Indeed, with increasing effectiveness brought by dense retrieval methods, our proposed predictors becomes more competitive, especially for NDCG@10 and MRR@10. At the same time, score-based predictors still remain very competitive and outperform supervised predictors, especially for MAP@100. Therefore, we prefer to allocate computational resources for ranking purposes, rather than predicting the effectiveness of a sparse ranker. \n\nNext, we examined differences among the identified predictors and find that they don't differ significantly in all cases. Importantly, we identify some unique characteristics of QPP in dense retrieval. Indeed, the type of query is responsible for a large amount of variance in query performance, and we can have query types that work well for some predictors but much worse for others. As a suggestion, it would be useful to examine the development of further predictors based on how they behave for more difficult query types.",
            "score": 0.5719713268817377,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 43182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10052490234375
        },
        {
            "corpus_id": "271745197",
            "title": "ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning",
            "text": "Our work is situated within the field of Information Retrieval (IR), specifically focusing on frameworks that leverage Large Language Models (LLMs) for Dense Retrieval. \n\nLLMs for Dense Retrieval. Recent advancements in this area have primarily addressed two key challenges: (i): Overcoming LLMs' Causal Attention Limitations by developing methods to enable bidirectional attention within LLMs (Muennighoff, 2022;Muennighoff et al., 2024;BehnamGhader et al., 2024;Lee et al., 2024), allowing models to consider both past and future context when computing embeddings, and (ii): Aligning LLM Pretraining with Text Ranking by fine-tuning LLMs via contrastive learning (Ma et al., 2023;Wang et al., 2024;Lee et al., 2024). This process can also be augmented with additional objectives such as supervised fine-tuning (SFT) (Muennighoff et al., 2024) or mask-filling tasks (BehnamGhader et al., 2024). An alternative approach proposed by Springer et al. (2024) involves a prompting method where the input sequence is duplicated, enabling each token to attend to future tokens and mitigating the contextualization issues inherent in causal attention. While these methods have shown promise, they generally do not explicitly enforce consistency between the model's understanding of relevance in both the embedding and generation spaces. This limitation restricts their ability to fully leverage the remarkable generative capabilities of LLMs for dense retrieval tasks. Our work, GRL, builds upon these foundations while addressing their limitations, introducing novel techniques to harmonize embedding-based and generation-based relevance scoring within a unified framework. \n\nFrameworks of LLMs for Dense Retrieval. Existing frameworks for LLMs in Dense Retrieval have been constrained by their limited support for LLM architectures and fine-tuning strategies. As shown in Table1, SentenceTransformers (Reimers and Gurevych, 2019) supports various types of LLMs but is primarily designed for inference without allowing fine-tuning, limiting its applicability in advancing state-of-the-art dense retrieval methods.",
            "score": 0.5719613589769401,
            "section_title": "Related Work",
            "char_start_offset": 4975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 171,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2106
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2315673828125
        },
        {
            "corpus_id": "249191630",
            "title": "Fast and Light-Weight Answer Text Retrieval in Dialogue Systems",
            "text": "For the sparse-term-based retrieval baseline, we use BM25 (Robertson et al., 1995)  stop-word removal.\n\nFor the neural-embedding-based classifier, we train a one vs all SVM classifier with sophisticated pre-processing, hand-crafted n-gram features, and neural word/sentence embeddings based on Transformers with 512-dimension vectors 3 . We also train a classifier with answer text added as training queries, denoted by \" NSVM w/ text\", as opposed to \"NSVM\" which does not use answer text hence has no 0-shot numbers.\n\nFor DPR experiments, we use the Facebook research DPR repository 4 . The DPR full model before fine-tuning is downloaded from the DPR repository (March 2021 release). The DPR tiny model before fine-tuning is pre-trained on the triples created from Natural Questions (NQ) dataset (Kwiatkowski et al., 2019), also obtained from the same repository. \"DPR(S)\" stands for shorter query setting.\n\nFor ColBERT experiments, our code is built on top of the v0.2 version of ColBERT code 5 , which is in PyTorch and uses Huggingface Transformers 6 . We implemented the code for iterative learning and asynchronous learning in PyTorch. For real-world usage we also implemented improvement features into ColBERT code as described in Section 4.2.\n\nThe ColBERT full model before fine-tuning is provided by the authors of ColBERT. The ColBERT tiny model before fine-tuning is pretrained on triples created from Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) as specified in ColBERT .\n\nFor CPU environment inferencing, all models and data/indices reside locally on a CPU machine with four Intel\u00ae Core\u2122 i7-8650U CPUs. Neural models are trained on a single NVIDIA V100 3 We refrain from giving more details because this is a commercial product.  GPU in a computing cluster environment unless otherwise stated.\n\nHyper-parameters and other detailed settings are included in Appendix. Table 2 compares the resource usage and response times of different systems during inference (retrieval). Full Neural models have high memory consumption and consume a lot of disk space",
            "score": 0.5717506978396216,
            "section_title": "Experimental Settings",
            "char_start_offset": 17609,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 82,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 798,
                    "end": 824,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1445,
                    "end": 1471,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055816650390625
        },
        {
            "corpus_id": "256697654",
            "title": "Neural Approaches to Multilingual Information Retrieval",
            "text": "We evaluate our proposed training approaches on two retrieval models -ColBERT-X [32] and DPR-X [48,49], which are multilingual variants of ColBERT [26] and DPR [24]. Nair et al. [32] generalized the ColBERT [26] model to CLIR, calling it ColBERT-X, by modifying the vocabulary space and replacing the monolingual pretrained language model with the MPLM XLM-RoBERTa (XLM-R) Large (550M parameters) [11]. With proper training, ColBERT-X achieves state-of-the-art effectiveness in CLIR. In this study, we integrate our proposed fine-tuning approaches with the ColBERT-X XLM-R 4 Batches include the same query paired with document passages translated into each language. 5 For a complete list: https://github.com/hltcoe/ColBERT-X/blob/main/ scripts/stopstructure.txt implementation, which is based on the ColBERTv1 code base. We similarly adapted DPR [24,48], a neural retrieval model that matches a single dense query vector to a single dense document vector. We name this model DPR-X. We use Tevatron [17], an open-source implementation of several neural end-to-end retrieval models in Python, for training, indexing, and retrieval. \n\nFor training data, we use MS MARCO-v1 [2], a commonly-used question-answering collection in English for fine-tuning neural retrieval models. For MTT, we use the publicly available mMARCO translations of MS MARCO [6], fine-tuning using the \"small training triple\" (query, positive and negative document) file released by mMARCO's creators. We trained all retrieval models with four GPUs (NVIDIA DGX and v100 with 32 GB Memory) with a per-GPU batch size of 32 triples for 200,000 update steps. All models are trained with half-precision floating points and optimized by the AdamW optimizer with a learning rate of 5 \u00d7 10 \u22126 . \n\nDuring indexing, documents are separated into overlapping spans of 180 tokens with a stride of 90 [32].",
            "score": 0.5717310481697471,
            "section_title": "Neural Retrieval Models",
            "char_start_offset": 12060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "248392078"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "248392078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09771728515625
        },
        {
            "corpus_id": "265457282",
            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
            "text": "In contrast to traditional IR methods, such as BM25 (Robertson and Zaragoza, 2009), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings: \n\nwhere E \u2022 (\u2022; \u03b8) is an encoder parameterised by \u03b8. \n\nThe adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2021) can be performed to fetch the top-k passages. Contrastive learning is applied to train the dualencoder. Given a query q, we have a positive passage p + and a set of n negative passages P \u2212 q = {p \u2212 i } n i=1 . The model is being optimised by minimising the negative log likelihood of the positive passage: \n\nP \u2212 q is the set of irrelevant passages constructed from in-batch negatives (Chen et al., 2020) (i.e. positive passages of other queries in the same minibatch) and mined hard negatives from existing retrievers (Karpukhin et al., 2020;Xiong et al., 2021).",
            "score": 0.5716189204338623,
            "section_title": "Dense Passage Retrieval",
            "char_start_offset": 5840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 82,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 348,
                    "end": 372,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1264,
                    "end": 1283,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1398,
                    "end": 1422,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1422,
                    "end": 1441,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "270703147",
            "title": "Evaluating D-MERIT of Partial-annotation on Information Retrieval",
            "text": "To allow reproduction of our results, we detail the hyper-parameters used in our work. We utilize the Pyserini information retrieval toolkit (Lin et al., 2021a) with the following settings for each system: BM25 is employed using the standard Lucene index for indexing and retrieving results. Similarly, QLD is used but with the QLD reweighing option to refine the process. UniCoil embeddings are generated with the castorini/unicoil-noexp-msmarco-passage encoder, and retrieval is conducted using Lucene search with the 'impact' option to incorporate unicoil weights. SPLADEv2 and SPLADE++ follow a similar approach, where passages and queries are embedded using their respective official code repositories, and retrieval is performed using Lucene with the 'impact' option. DPR involves embedding passages and queries with the facebook/dpr-ctx_encodermultiset-base and facebook/dpr-question_encodermultiset-base encoders, respectively, with retrieval via FAISS (Douze et al., 2024). RetroMAEdistill adopts a similar strategy, utilizing the Shitao/RetroMAE_MSMARCO_distill encoder for both queries and passages. \n\nTCT-Colbert-V2 also mirrors this approach but uses the castorini/tct_colbert-v2-msmarco encoder. co-Condenser involves training document and query encoders on the Natural Questions dataset (Kwiatkowski et al., 2019) using the CoCondenser official code repository. Hybrid models such as TCT-Colbert-V2-Hybrid, coCondenser-Hybrid, and RetroMAE-Hybrid combine the strengths of BM25 with TCT-Colbert-V2, coCondenser, and RetroMAE-distill respectively, using a fusion score with \u03b1 = 0.1.",
            "score": 0.571471946761621,
            "section_title": "B Further Details: Experimental Study",
            "char_start_offset": 34131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1595
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 1302,
                    "end": 1328,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05975341796875
        },
        {
            "corpus_id": "273654514",
            "title": "Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP Technical Specifications",
            "text": "The optimization of the QA architecture considered several retriever models: BM25, DPR, DPR Fine-Tuned (DPR-FT), DHR-P, and DHR (incorporating both DHR-D and DHR-P). These models were evaluated on the test subset of the Telco-DPR using as reference the Top-K accuracy and the MRR to measure their ability in retrieving the correct passages for the corresponding questions. \n\nTable II presents the evaluation metrics for each retriever model. BM25 shows relatively lower accuracy, achieving 26.3% at Top-1 (Acc@1) and 59.2% at Top-10 (Acc@10), with MRR at Top-10 (MRR@10) of 0.36. This is mainly due to the reliance on exact word matching rather than understanding the context. The DPR has a higher performance, yielding a MRR@10 of 0.45. DHR-P leads to further improvements by enriching the passage representation through concatenation of the section title with the text passage. The DHR (DHR-D and DHR-P), demonstrate the best performance across all metrics, achieving an Acc@10 of 86.2% and the highest MRR@10 of 0.68. The DHR retrieval demonstrates, the highest performance in retrieving the correct passages for answering user questions. Thus, the proposed QA system leverages this retrieval model. \n\nAdditionally, to analyse the performance differences among the four dense retrieval models, Fig. 2 presents a comparison of cosine similarity distributions, metric used in similarity search to select the most relevant passages, between the question and the passage containing the answer. The DPR model (blue) shows a broad distribution of lower similarity scores, while the fine-tuned models shift towards higher scores. The DHR model (red) stands out with a high concentration of scores between 0.7 and 0.9, indicating improvement by increase the similarity scores for the most relevant passages.",
            "score": 0.5712945114688524,
            "section_title": "B. QA Architecture Optimisation",
            "char_start_offset": 19149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1405029296875
        },
        {
            "corpus_id": "254044526",
            "title": "Dense Text Retrieval Based on Pretrained Language Models: A Survey",
            "text": "Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user\u2019s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models (PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.",
            "score": 0.5711354991384949,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462158203125
        },
        {
            "corpus_id": "255595937",
            "title": "Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering",
            "text": "Dense Retrieval. Dense Retrieval is a rapidly evolving field, surveyed in [36,11], with new pre-training tasks, optimizing methods, and variants of the Transformer architecture emerging [47,23,15,14]. [31] were the first to outperform sparse bag-of-words representations such as BM25 with dense representations for QA. Their approach relies on three components: (i) pre-trained language models such as BERT [10], which allow to encode the semantic of a sentence in a dense vector; (ii) a contrastive learning objective that optimizes the similarities between questions' and text passages' embeddings (see Section 3); (iii) an unsupervised training task, ICT (see Section 1). [27] criticize the latter for being computationally intensive3 and argue that regular sentences are not good surrogates of questions. Instead, they propose DPR, which takes advantage of (i) the heuristic of whether the passage contains the answer to the question to deem it relevant; (ii) unsupervised IR methods such as BM25 to mine hard negatives examples, which proved to be the key of their method's success. We aim at taking advantage of both approaches by (i) pre-training our model on text QA datasets like DPR; (ii) incorporating multimodality into this hopefully-well-initialized model by adapting the ICT of [31] to multimodal documents. \n\nMultimodal Fusion and Pre-Training. The success of BERT in NLP [10], which relies on the easily-parallelizable Transformer architecture [58], an unsupervised training objective, and a task-agnostic architecture, has concurrently inspired many works in the VQA and cross-modal retrieval fields [57,38,35,56,34,7]. These models are unified under a single framework in [5] and partly reviewed in [28]. All of these models rely on the Transformer architecture, often initialized with a pre-trained BERT, in order to fuse image and text.",
            "score": 0.5705698962637953,
            "section_title": "Related Work",
            "char_start_offset": 3037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "245144844"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1293,
                    "end": 1297,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1628,
                    "end": 1631,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1631,
                    "end": 1634,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 1634,
                    "end": 1636,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1691,
                    "end": 1694,
                    "matchedPaperCorpusId": "227238841"
                },
                {
                    "start": 1718,
                    "end": 1722,
                    "matchedPaperCorpusId": "6628106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "245704660",
            "title": "PARM: A Paragraph Aggregation Retrieval Model for Dense Document-to-Document Retrieval",
            "text": "Dense passage retrieval (DPR) models show great effectiveness gains in first stage retrieval for the web domain. However in the web domain we are in a setting with large amounts of training data and a query-to-passage or a query-to-document retrieval task. We investigate in this paper dense document-to-document retrieval with limited labelled target data for training, in particular legal case retrieval. In order to use DPR models for document-to-document retrieval, we propose a Paragraph Aggregation Retrieval Model (PARM) which liberates DPR models from their limited input length. PARM retrieves documents on the paragraph-level: for each query paragraph, relevant documents are retrieved based on their paragraphs. Then the relevant results per query paragraph are aggregated into one ranked list for the whole query document. For the aggregation we propose vector-based aggregation with reciprocal rank fusion (VRRF) weighting, which combines the advantages of rank-based aggregation and topical aggregation based on the dense embeddings. Experimental results show that VRRF outperforms rank-based aggregation strategies for dense document-to-document retrieval with PARM. We compare PARM to document-level retrieval and demonstrate higher retrieval effectiveness of PARM for lexical and dense first-stage retrieval on two different legal case retrieval collections. We investigate how to train the dense retrieval model for PARM on limited target data with labels on the paragraph or the document-level. In addition, we analyze the differences of the retrieved results of lexical and dense retrieval with PARM.",
            "score": 0.5704606981117136,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264404296875
        },
        {
            "corpus_id": "266902943",
            "title": "Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation",
            "text": "The previous section focused on the effect of the query-passage pair scorers when using ColBERTv2 as the passage selector. The following experiments vary the passage selector,  4 shows the results of those experiments. There was no statistically significant difference between the two English selectors (ColBERTv2 and CoCondenser) for nDCG@20 or Recall@1000 (R@1000), although models trained with passages selected by the ColBERTv2 model achieved numerically higher results by both measures. Interestingly, using a Translate-Trained ColBERT-X as the passage selector does not lead to a more effective student model when measured by nDCG@20, but it does result in a more effective student model when measured by R@1000. We hypothesize that the Translate-Trained ColBERT-X model is not effective enough to retrieve sufficiently hard (and thus informative) candidate passages. Since it selects passages using their translated text, the candidate sets contain a larger diversity in the document language (Chinese, in this set of experiments), resulting in student models with higher Recall@1000. Given these observations, we conclude that both the query-passage pair scorer and the passage selector affect final CLIR retrieval effectiveness. The recipe for creating an effective CLIR dual-encoder model requires that the selected passages are sufficiently hard, for which we can use monolingual English neural retrieval models. Also, the subsequent query-passage pair scorer should be selected carefully to be both effective and aligned with the native language of the training corpus (English for the MS MARCO).",
            "score": 0.5703486091464003,
            "section_title": "Passage Selector",
            "char_start_offset": 27578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12939453125
        },
        {
            "corpus_id": "246035178",
            "title": "Improving Biomedical Information Retrieval with Neural Retrievers",
            "text": "Dense Passage Representation (DPR) (Karpukhin et al. 2020) is a neural retriever model belonging to the dual-model family. DPR encodes the query q and the context c into dense vector representations:\n\n(1)\n\nwhere E q and E c are BERT ) models which output a list of dense vectors (h 1 , . . . , h n ) for each token of the input, and the final representation is the vector representation of special token [CLS]. E q and E c are initialized identically and are updated independently while being trained with the objective of minimizing the negative log likelihood of a positive (relevant) context. A similarity score between q and each context c is calculated as the inner product between their vector representations:\n\nPoly-Encoder (Humeau et al. 2020) also uses two encoders to encode query and context, but the query is represented by K vectors instead of a single vector as in DPR.\n\nPoly-Encoder assumes that the query is much longer than context, which is in contrast to information retrieval and opendomain QA tasks in the biomedical domain, where contexts are long documents and queries are short and specific.",
            "score": 0.5697989184002357,
            "section_title": "Preliminaries",
            "char_start_offset": 8947,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 58,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 731,
                    "end": 750,
                    "matchedPaperCorpusId": "210063976"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "273350956",
            "title": "Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs",
            "text": "SBERT Reimers & Gurevych (2019). Contriever is a self-supervised dense retriever that learns unsupervised document embeddings for information retrieval, DPR (Dense Passage Retriever) is a bi-encoder model that retrieves relevant passages by training on questionpassage pairs, Dragon is a dense retrieval model optimized through diverse augmentation for generalizable dense retrieval, and SBERT (Sentence-BERT) is a modification of BERT that generates semantically meaningful sentence embeddings for tasks like similarity and clustering using a siamese network structure. \n\n\u2022 BM25+DPR. BM25+DPR with Reciprocal Rerank Fusion is a hybrid retrieval method that combines the strengths of BM25's lexical matching and DPR's dense embeddings by reranking results from both models using a reciprocal rank fusion strategy to improve retrieval accuracy. \n\n\u2022 Gemma-8K \u2022 Full Context. We feed all inputs to LLMs for summary generation. If the input length exceeds the context window limit, we randomly sample continuous text spans of maximum length multiple times to feed into LLMs and calculate the average result. \n\n\u2022 Thought-R Feng et al. (2024). Thought Retriever (Thought-R) generates thoughts for a series of simulated queries and appends them to the retrieval corpus as high-level knowledge.",
            "score": 0.5685087429832425,
            "section_title": "A EXPERIMENTAL DETAILS",
            "char_start_offset": 31205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "251402929",
            "title": "Learning Diverse Document Representations with Deep Query Interactions for Dense Retrieval",
            "text": "Document retrieval plays an important role in information retrieval (IR) tasks such as web search and open domain question answering (Chen et al., 2017). Early works such as BM25-based retriever (Robertson and Zaragoza, 2009) rely on lexical term matching to calculate the relevance of a pair of texts. Recently, neural network based dense retrieval (Karpukhin et al., 2020) has gained traction in research community. Dense retrieval learns a neural encoder to map queries and documents into a dense, low-dimensional vector space, and is less vulnerable to term mismatch problem compared to lexical match-based approaches. \n\nThere are two architectures to model the relevance between queries and documents. Dual encoder architecture encodes query and document separately into fixed-dimensional vectors (Karpukhin et al., 2020), where the similarity between query and document is usually instantiated as a dot product or cosine similarity between their vectors. As there are no interactions between query and document, dual encoder approach permits efficient inference with vector space search on precomputed document vectors. Cross encoder architecture feeds the concatenation of a query and document pair into one encoder to calculate its relevance score (Nogueira and Cho, 2019). Compared to dual encoder, cross encoder is more accurate due to the deep interaction between query and document, but comes with computation costs infeasible for first-stage retrieval. It is highly desirable to design a retrieval model which can match the performance of the cross encoder approach while maintaining the inference efficiency of the dual encoder approach. \n\nTo this end, previous works mainly focus on two directions: late-interaction and distillation. The first solution is to design a hybrid architecture, where the early layers act as a dual encoder while the late layers work like a cross encoder (MacAvaney et al., 2020;Khattab and Zaharia, 2020;Humeau et al., 2020). Its effectiveness comes with the cost of retrieval latency due to the computation involved with late layers.",
            "score": 0.5681593342775212,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 152,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 195,
                    "end": 225,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 350,
                    "end": 374,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 802,
                    "end": 826,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1897,
                    "end": 1921,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 1921,
                    "end": 1947,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2366943359375
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "The performance of neural IR models, including retrievers and rerankers, have been significantly boosted by pre-trained language models (PLM), where various ways of leveraging PLM for IR are proposed. As illustrated in Figure 1, PLM-based IR models can be categorized into three types, i.e., dual-encoders, late-interaction encoders and cross-encoders, in terms of the interaction mechanism applied between query and passage. Overall, existing studies indicate that incorporating more interactions between queries and passages in a PLM-based IR method can improve relevance modeling, but it also comes at the cost of extra computational overhead. In the following, we further introduce the detailed structures of these models. Dual-encoder. Dual-encoders employ two PLM-based encoders to respectively encode the query and passage in a latent embedding space. The relevance score  (q, p) between query and passage is formulated as \n\nHere, Aggregate(\u2022) is usually implemented as a simple metric (e.g., dot-product) between query and passage vectors, which is computed by query and passage encoders (i.e., E  and E  ), respectively. The encoders are stacked transformer layers, where we fetch the representation of [CLS] token in the last layer as final query/passage vector. \n\nThe major merit of dual-encoders lies in its high efficiency. As the query and passage are decoupled at encoding, the passages in large corpus G can be pre-computed and cached offline. By doing this, substantial computational resources could be saved during the online inference for fast retrieval. However, the limitation is also apparent. The absence of interaction between the query and passage during their encoding leads to an inability to effectively capture complex relevance [22,25,65]. Cross-encoder. Cross-encoders are considered the most effective PLM-based IR method due to their early incorporation of querypassage interactions. It takes the concatenation of query and passage as input, and computes the relevance score as \n\nwhere \u2295 means the concatenation operation and E , is the PLM encoder. The FC is a fully-connected layer that transforms the [CLS] representation to a relevance score.",
            "score": 0.5677542692727492,
            "section_title": "PLM-based Retriever and Reranker",
            "char_start_offset": 12868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 1762,
                    "end": 1765,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1765,
                    "end": 1768,
                    "matchedPaperCorpusId": "246828631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "278129839",
            "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
            "text": "We compare PropRAG against a wide range of baseline methods, categorized by their retrieval approach. Baseline results are primarily taken from the comprehensive evaluation in [1], which used the same Llama-3.3-Instruct reader and NV-Embed-v2 retriever setup as our work, ensuring a fair comparison. \n\n\u2022 Classic Retrievers: Traditional IR methods based on lexical matching or early dense representations. \n\n-BM25 [32]: A sparse retrieval method based on term frequency and inverse document frequency. \n\n-Contriever [15]: An early dense retriever trained using unsupervised contrastive learning. \n\n-GTR (T5-base) [16]: A dense retriever based on the T5 architecture, known for strong performance on retrieval benchmarks. \n\n\u2022 Large Embedding Models: Recent, powerful dense retrieval models often based on large language model architectures, representing the state-of-the-art in single-vector passage retrieval. \n\n-GTE-Qwen2-7B-Instruct [17]: A strong embedding model based on the Qwen2 architecture. \n\n-GritLM-7B [18]: An embedding model trained using generative representational instruction tuning. \n\n-NV-Embed-v2 (7B) [7]: A state-of-the-art embedding model from NVIDIA, used as the primary retriever in our experiments and for HippoRAG 2 comparison. \n\n\u2022 Structure-Augmented RAG: Methods that explicitly build or leverage structure (summaries, graphs) beyond simple passage retrieval to improve context integration or reasoning. \n\n-RAPTOR [11]: Recursively clusters and summarizes passages to build a tree structure for retrieval. \n\n-GraphRAG [9]: Uses graph community detection and summarization over extracted entities and relationships. \n\n-LightRAG [10]: Employs a dual-level retrieval mechanism integrating graph structures and vector retrieval (results indicate potential issues with their implementation or general approach on these benchmarks). \n\n-HippoRAG [21]: The initial version using PPR over triples. \n\n-HippoRAG 2 [1]: The state-of-the-art structured RAG baseline prior to this work, using PPR over a triple-based KG with integrated passage information.",
            "score": 0.5677132758502417,
            "section_title": "Baselines",
            "char_start_offset": 26064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "267334785"
                },
                {
                    "start": 1864,
                    "end": 1868,
                    "matchedPaperCorpusId": "269982289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19287109375
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "This paper proposes a novel pre-training method SIMLM for dense passage retrieval. It follows an encoder-decoder architecture with a representation bottleneck in between. The encoder learns to compress all the semantic information into a dense vector and passes it to the decoder to perform well on the replaced language modeling task. When used as initialization in a dense retriever training pipeline, our model achieves competitive results on several large-scale passage retrieval datasets.\n\nFor future work, we would like to increase the model size and the corpus size to examine the scaling effects. It is also interesting to explore other pre-training mechanisms to support unsupervised dense retrieval and multilingual retrieval.",
            "score": 0.567514341160403,
            "section_title": "Conclusion",
            "char_start_offset": 20697,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "256194532",
            "title": "PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development",
            "text": "Retrievers predict documents (or passages) from a document collection that are relevant to an input question. PRIMEQA has both sparse and SOTA dense retrievers along with their extensions, as shown in Table 1. We provide a single Python script run_ir.py that can be passed arguments to switch between different retriever algorithms. Sparse: BM25 (Robertson and Zaragoza, 2009) is one of the most popular sparse retrieval methods, thanks to its simplicity, efficiency and robustness. Our Python-based implementation of BM25 is powered by the open-source library PySerini. Dense: Modern neural retrievers have utilized dense question and passage representations to achieve SOTA performance on various benchmarks, while needing GPUs for efficiency. We currently support ColBERT (Santhanam et al., 2022b) and DPR (Karpukhin et al., 2020): both fine-tune pretrained language models to train question and passage encoders (Devlin et al., 2019;Conneau et al., 2020). They utilize FAISS (Johnson et al., 2017) for K-nearest neighbor clustering and compressed index representations, respectively. They support multilingual retrieval with the question and the documents being in the same (Lee et al., 2019;Longpre et al., 2021) or different languages (cross-lingual) .",
            "score": 0.5674570829857152,
            "section_title": "Retriever: run_ir.py",
            "char_start_offset": 6301,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 375,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 775,
                    "end": 800,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 809,
                    "end": 833,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1178,
                    "end": 1196,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1196,
                    "end": 1217,
                    "matchedPaperCorpusId": "220871404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2081298828125
        },
        {
            "corpus_id": "251371732",
            "title": "Few-shot Learning with Retrieval Augmented Language Models",
            "text": "Retrieval for knowledge intensive tasks. Previous work has shown that retrieval improves performance across a variety of tasks such as question answering (Voorhees et al., 1999;Chen et al., 2017;Kwiatkowski et al., 2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni et al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such as TF-IDF or BM25 (Jones, 1972;Robertson et al., 1995). For open-domain question answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011;Huang et al., 2013;Shen et al., 2014), where queries and passages are encoded independently as vectors, and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers include DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques. \n\nAfter retrieval, the relevant documents are processed to produce the final output. In open-domain QA, models can extract a span of text from retrieved documents as the answer (Chen et al., 2017;Clark & Gardner, 2018;Wang et al., 2019;Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013;Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned on retrieved documents have become prevalent (Lewis et al., 2020;Izacard & Grave, 2020;Min et al., 2020). These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021). \n\nRetriever training.",
            "score": 0.5671880144825965,
            "section_title": "Retrieval in natural language processing",
            "char_start_offset": 19885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 177,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 177,
                    "end": 195,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 195,
                    "end": 220,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 454,
                    "end": 467,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 467,
                    "end": 490,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 527,
                    "end": 550,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 597,
                    "end": 616,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 758,
                    "end": 777,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 777,
                    "end": 795,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 1204,
                    "end": 1223,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1445,
                    "end": 1464,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1464,
                    "end": 1486,
                    "matchedPaperCorpusId": "223637"
                },
                {
                    "start": 1486,
                    "end": 1504,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1572,
                    "end": 1590,
                    "matchedPaperCorpusId": "2100831"
                },
                {
                    "start": 1590,
                    "end": 1613,
                    "matchedPaperCorpusId": "11816014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.",
            "score": 0.5663157555838272,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412841796875
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "Dense retrieval Retrieving relevant documents for a query (Mitra and Craswell, 2018) is crucial in many NLP applications like open-domain question answering and knowledge-intensive tasks (Petroni et al., 2021). Dense retrievers typically build a search index for all documents by pre-computing the dense representations of documents using an encoder. Off-the-shelf libraries for a maximum inner product search (MIPS) (Johnson et al., 2019;Guo et al., 2020) enable model training and indexing to be developed independently (Lin, 2022). However, both training dense retrievers and building indexes should take into account the final retrieval accuracy.\n\nIn this respect, we aim to close the gap between training and inference of dense retrievers.\n\nPhrase retrieval Phrase retrieval (Seo et al., 2019) directly finds an answer with MIPS from an index of contextualized phrase vectors. This removes the need to run an expensive reader for open-domain QA. As a result, phrase retrieval allows real-time search tens of times faster than retriever-reader approaches as an alternative for open-domain QA. DensePhrases (Lee et al., 2021a) removes the requirement of sparse features and significantly improves the accuracy from previous phrase retrieval methods (Seo et al., 2019;Lee et al., 2020). Lee et al. (2021b) show how retrieving phrases could be translated into retrieving larger units of text like a sentence, passage, or document, making phrase retrieval a general framework for retrieval. Despite these advantages, phrase retrieval requires building a large index from billions of representations. In this work, we focus on improving phrase retrieval with more efficient validation.\n\nValidation of dense retrieval Careful validation is essential for developing machine learning models to find a better configuration (Melis et al., 2018) or avoid falling to a wrong conclusion. However, many works on dense retrieval do not clearly state the validation strategy, and most of them presumably perform validation on the entire corpus. It is doable but quite expensive 1 to perform frequent validation and comprehensive tuning. Hence, it motivates us to devise efficient validation for dense retrieval. Like ours, Hofst\u00e4tter et al. (2021) construct a small validation set by sampling queries and using a baseline model",
            "score": 0.5658982330198234,
            "section_title": "3714",
            "char_start_offset": 5535,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 209,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 417,
                    "end": 439,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 439,
                    "end": 456,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 522,
                    "end": 533,
                    "matchedPaperCorpusId": "238259539"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 1110,
                    "end": 1129,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 1252,
                    "end": 1270,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 1270,
                    "end": 1287,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1289,
                    "end": 1307,
                    "matchedPaperCorpusId": "237532725"
                },
                {
                    "start": 1818,
                    "end": 1838,
                    "matchedPaperCorpusId": "33513311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "274822983",
            "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
            "text": "Aggregated results for all evaluations are presented in Table 1. For BEIR and GLUE, the two common evaluation suites, we follow existing practice in reporting the average results. Detailed results are provided in Appendix E. \n\nIn terms of downstream performance, Modern-BERT is the strongest overall model at both the BASE and LARGE model sizes. ModernBERT represents a Pareto improvement on all tasks over the original BERT and RoBERTA models, with better performance on every evaluation category. \n\nShort-Context Retrieval On BEIR, both variants of ModernBERT outperform existing encoders in both the DPR and ColBERT settings, including the recent GTE-en-MLM and NomicBERT models designed to serve as better backbones for retrieval (Zhang et al., 2024;Nussbaum et al., 2024) ModernBERT-large increases its lead despite having comparatively fewer parameters at 395M to GTE-en-MLM-large's 435M. \n\nLong-Context Retrieval -Single Vector In the DPR setting, ModernBERT achieves impressive performance on MLDR, a long-context text retrieval task. However, these results also highlight an interesting phenomenon: without long-context finetuning ModernBERT outperforms both shortercontext models and the long-context NomicBERT but performs noticeably worse than GTE-en-MLM. The performance gap narrows considerably when evaluated in-domain, with both models performing similarly. This suggests that ModernBERT can effectively process long context sequences as a dense encoder but may require more adapted tuning. We plan to explore multiple potential explanations for this phenomenon in future work, including the impact of local attention or GTE-en-MLM having spent a larger part of its pretraining compute budget on longer sequence lengths (Zhang et al., 2024). \n\nLong-Context Retrieval -Multi-Vector In the ColBERT setting, long-context models (GTEen-MLM, NomicBERT, and ModernBERT) all outperform short-context models by at least 40 NDCG@10 points without requiring any specific finetuning. These results confirm the findings of Bergum (2024), who showed that ColBERT models are particularly well-suited to long-context retrieval tasks.",
            "score": 0.5658894834606776,
            "section_title": "Downstream Results and Discussion",
            "char_start_offset": 22241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 734,
                    "end": 754,
                    "matchedPaperCorpusId": "271534420"
                },
                {
                    "start": 1736,
                    "end": 1756,
                    "matchedPaperCorpusId": "271534420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12042236328125
        },
        {
            "corpus_id": "248496043",
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "text": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
            "score": 0.565826619312803,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 977,
                    "end": 980,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 1812,
                    "end": 1814,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1816,
                    "end": 1818,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32177734375
        },
        {
            "corpus_id": "267750652",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "text": "Despite these different enhancements, each method builds upon the DPR framework discussed in this paper. \n\nDistinctly, RetroMAE and CoT-MAE pre-train a model using a masked auto-encoder strategy, which they show enhances downstream retrieval performance (Xiao et al., 2022;Wu et al., 2023a;Liu et al., 2023b;Wu et al., 2023b). Following this pretraining phase, both methods subsequently adopt DPR fine-tuning to further refine their models for improved task performance. \n\nOnly a few studies have delved into analyzing DPR models. One such study took a holistic look at RAG to see where the pipeline made errors (BehnamGhader et al., 2023). The study found that a similarity-based search during retrieval biased the result in favor of passages similar to the query, even when more relevant but dissimilar passages were available. Another study employed probing techniques to analyze ranking models (MacAvaney et al., 2022). The authors adopted a probing method akin to ours, categorizing passages by specific properties for analysis, in contrast to our approach of random selection among hard negatives. This study explored how query and document characteristics affect ranking outcomes. Another study analyzed the embeddings produced by retrieval models in the vocabulary space (Ram et al., 2023). To do this, they used pre-trained BERT's MLM head on the DPR-trained embeddings' [CLS] token. It was found that DPR implicitly learns the importance of lexical overlap between the query and passage. DPR training causes BERT to retrieve passages that share more tokens with the query as compared to pre-trained BERT. This ties in with our finding where the number of output layer activations in the early part of the model post-DPR training decreased. This may function as a sort of syntactic filter, where many keys can access fewer, but more pertinent, lexical features. However, this filtering can also induce what the author's term \"token amnesia\". This condition occurs when an encoder fails to correctly retrieve relevant passages because it does not properly encode the relevant token, usually related to a named entity.",
            "score": 0.5657980992215004,
            "section_title": "Related Works",
            "char_start_offset": 17028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 273,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 273,
                    "end": 290,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "253553743"
                },
                {
                    "start": 612,
                    "end": 639,
                    "matchedPaperCorpusId": "254854344"
                },
                {
                    "start": 898,
                    "end": 922,
                    "matchedPaperCorpusId": "226226586"
                },
                {
                    "start": 1279,
                    "end": 1297,
                    "matchedPaperCorpusId": "254877181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "Retrieval Models: We used five retrieval models in our experiments: BM25, a sparse vectorbased method; DPR, a dense dual-encoder model that maximizes similarity between questions and relevant passages; MSS, a dense retriever pretrained on predicting masked spans like named entities; MSS-DPR, combining MSS pre-training with DPR's fine-tuning for improved performance; and Contriever, an unsupervised dense retriever  optimized for zero-shot performance through contrastive learning. \n\nGenerative Models: For the generation-based retrieval, we employ GenRead (Yu et al., 2022) a generative model designed for open-domain QA tasks, which first generates contextual documents based on the query and then predicts the final answer using those generated documents.",
            "score": 0.5650911169997135,
            "section_title": "Retrieval and Generative Models",
            "char_start_offset": 15564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27685546875
        },
        {
            "corpus_id": "247628023",
            "title": "Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction",
            "text": "Traditional retrieval systems have long relied on bag-of-words representations to search within unstructured text collections. This has led to mature architectures that are known to support highlyefficient search. The compact inverted indexes enable fast top- retrieval strategies, while also exhibiting interpretable behavior, where retrieval scores can directly be attributed to the contributions of individual terms. Despite these attractive qualities, recent progress in Information Retrieval (IR) has firmly demonstrated that pre-trained language models can considerably boost retrieval quality over classical approaches. This progress has not come without downsides, as it is less clear how to control the computational cost and how to ensure interpretability of these neural models. This has sparked an unprecedented tension in IR between achieving the best retrieval quality, maintaining low computational costs, and prioritizing interpretable modeling. \n\nFor practical applications, ranking model architectures are confined to strict cost constraints, primarily query latency and index space footprint. While a larger disk space consumption might not be a critical cost factor, keeping many pre-computed representations in memory-as often needed for low query latency-does increase hardware costs significantly. For multi-vector models like ColBERT [24], space consumption is determined by a multiplication of three variables: 1) the number of vectors per document; 2) the number of dimensions per vector; 3) the number of bytes per dimension. A motivating observation of this work is that reducing any of these three variables by a certain factor directly reduces the storage requirement by that factor and yet this does not necessary translate to the same ratio of keeping or reducing the effectiveness. Wellstudied low hanging fruits for good tradeoffs include reducing the number of dimensions (either inside the model or outside with PCA) and reducing the number of bytes with quantization [12,21,27,36]. The remaining parameter, the number of vectors per document, offers many development opportunities determined by the model architecture and the retrieval approach employed. \n\nIn addition to efficiency considerations, with the accelerating adoption and impact of machine learning models, there are indications that future regulatory environments will require deployed models to provide transparent and reliably interpretable output to their users. 1 This crucial need for interpretability is especially pronounced in IR, where the ranking models are demanded to be fair and transparent [6].",
            "score": 0.5650076439495866,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2467
                },
                {
                    "start": 2468,
                    "end": 2608
                }
            ],
            "ref_mentions": [
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2008,
                    "end": 2011,
                    "matchedPaperCorpusId": "153313759"
                },
                {
                    "start": 2014,
                    "end": 2017,
                    "matchedPaperCorpusId": "239993003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1375732421875
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Current passage retrieval systems are widely deployed as retrievethen-rank pipelines [18,56]. The first-stage retriever (i.e., dualencoder) [27,29,35,38,47,49] selects a small number of candidate passages (usually at most thousands) from the entire collection, and the second-stage ranker (i.e., cross-encoder [55]) scores these candidates again to provide a more accurate passages order. In this paper, we focus on enhancing the first-stage retriever. \n\nDense Retriever. Built upon Pre-trained Language Models [9,30], dense retriever [20,35] is to capture the semantic meaning of an entire sequence by encoding sequential text as a continuous representation into a low-dimensional space (e.g., 768). In this way, the dense retriever could handle vocabulary and semantic mismatch issues within the traditional term-based techniques like BM25 [40]. To train a better dense retriever, various techniques are proposed for providing hard negatives including reusing in-batch negatives [20,31,35], iteratively sampling [49], mining by a well-trained model or dynamic sampling [52,53], and denoising by cross-encoder [35]. To build retrieval-specific pre-trained language models, Lee et al. [23] proposed an unsupervised pre-training task, namely Inverse Cloze Task (ICT), Gao and Callan [12] decoupled model architecture during pre-training and further designed corpus-level contrastive learning [13] for better passage representations. \n\nLexicon-Aware Retriever. Another paradigm of work [11,14,26] takes advantage of strong PLMs to build lexicon-aware sparse retrievers by term-importance [7,26] and top coordinate terms [10,11]. These models have lexical properties and could be coupled with inverted indexing techniques.",
            "score": 0.5648636773295663,
            "section_title": "RELATED WORK",
            "char_start_offset": 4602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30859375
        },
        {
            "corpus_id": "265212781",
            "title": "Summarization-Based Document IDs for Generative Retrieval with Language Models",
            "text": "Wikipedia-based corpora have long been an important part of NLP research and form a natural benchmark for studying new techniques in textbased recommender and information retrieval systems. In this work, we examine how generative retrieval behaves on short-form and long-form documents drawn from Wikipedia and non-Wikipedia sources. We also propose a new type of document ID for generative retrieval based on document summarization, which demonstrably improves retrieval performance across the tasks that we examined. \n\nLarge language models (LMs) are now widely used across many NLP tasks, and extensions of generative models to document retrieval tasks have recently been proposed (Wang et al., 2022;Tay et al., 2022), in contrast to vector-based approaches like dense passage retrieval (DPR; Karpukhin et al., 2020). DPR is a widely-used technique for training document retrieval models, where queries and documents are mapped to dense vector representations with a transformer encoder (e.g., BERT; Devlin et al., 2019). By increasing the cosine similarity between positive query-document pairs and decreasing it between negative pairs, DPR performs metric learning over the space of queries and the set of documents to be indexed. \n\nGenerative alternatives to document retrieval address certain limitations of dense, vector-based approaches to retrieval. For example, query and document representations are constructed separately in DPR, which precludes complex query-document interactions. Using a single dense vector to represent an entire document limits the amount of information that can be stored; indeed, Tay et al. (2022) observed that increasing the number of parameters in the encoder does not significantly enhance DPR performance. Furthermore, the rich sequence generation capabilities of language models (LMs) cannot be used directly in dense retrieval. Tay et al. (2022) and Wang et al. (2022) therefore proposed a new direction called generative retrieval, where LMs learn to directly map queries to an identifier that is unique to each document. We illustrate the differences in Figure 1. \n\nInstead of retrieving documents based on cosine similarity, generative retrieval uses an LM to produce a sequence of tokens encoding the relevant document's ID, conditional on the query.",
            "score": 0.5646671871591622,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2109
                },
                {
                    "start": 2112,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 703,
                    "end": 720,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 796,
                    "end": 819,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1003,
                    "end": 1023,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1617,
                    "end": 1634,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1872,
                    "end": 1889,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1894,
                    "end": 1912,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "262465393",
            "title": "Model-enhanced Vector Index",
            "text": "In this work, we introduce a novel Model-enhanced Vector Index (MEVI), which combines the advantages of both the sequence-to-sequence autoregressive model and the twin-tower dense representation model. It can be effectively applied in real-world applications due to its ability to achieve both high recall performance and fast retrieval speed on large-scale corpus. MEVI constructs an RQ structure to hierarchically cluster large-scale documents, enabling the sequence-to-sequence model to directly generate the relevant cluster identifiers given an input query; the retrieved candidate documents in the top-k clusters are further ensembled with embedding-based retrieval results for candidates re-ranking. We empirically show that MEVI achieves better model performance than baselines on the widely used large-scale retrieval datasets MSMARCO Passage and Natural Questions.",
            "score": 0.5644473262879518,
            "section_title": "Conclusion",
            "char_start_offset": 26601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 874
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2137451171875
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020). More recently, Chen et al. (2021) attempted to train a dense retriever (called as dense Lexical Model \u039b) to imitate BM25 prediction. Though \u039b model underperforms BM25 by itself on EQ dataset, when combined with DPR trained on multiple QA datasets (DPRmulti), the combined model (SPAR) outperforms BM25. SPAR requires two individually trained bi-encoders (thus four BERT models) with tangled architecture, whereas our motivation is to investigate if dense retrievers could generalize with a single language model without ensemble. We also investigate whether dense retrieval has any strengths in areas where sparse retrieval is lacking. We experimentally demonstrate the differences between dense and sparse retrieval by using the IDF value to quantify the generality and rarity of an entity name in a question. \n\nMultiple Keys for Passage Retrieval. Several methods have been proposed for calculating fine-grained interactions between a question and passages using multiple retrieval keys. SPARTA (Sparse Transformer Matching, (Zhao, Lu, and Lee 2021)) and COIL (Contextualized Inverted List, (Gao, Dai, and Callan 2021)) produce passage representations for each token embedded by fine-tuned language models and store them in the inverted index. ColBERT (Khattab, Potts, and Zaharia 2021) leverages the embedding of all subword tokens in each passage to form a large collection of keys. However, all of the methods listed here require the encoders for questions and passages to be fine-tuned. During this fine-tuning for retrieval, multiple index updates are required, which is computationally expensive. We seek a retrieval method that works in zero-shot setting, so we propose a method that only requires the inference operation of a pretrained language model together with named-entity recognition (NER). \n\nLUKE model (Yamada et al. 2020) is the state-of-the-art language model for tasks related to named entities, including NER.",
            "score": 0.5644038937540383,
            "section_title": "Related Work",
            "char_start_offset": 6953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1182,
                    "end": 1206,
                    "matchedPaperCorpusId": "221971009"
                },
                {
                    "start": 1248,
                    "end": 1275,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2822265625
        },
        {
            "corpus_id": "274822983",
            "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
            "text": "Information Retrieval (IR) is one of the most common applications of encoder-only models,8 where they are used to represent documents and queries in semantic search (Karpukhin et al., 2020). This domain has recently seen considerable growth and interest following the spread of LLMs where semantic search powered by lightweight models is used to provide relevant context to LLMs as part of Retrieval-Augmented Generation pipelines. \n\nWe evaluate models in both the single-vector Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) setting and the multi-vector ColBERT (Khattab and Zaharia, 2020) setting. \n\nWe report retrieval results on the popular BEIR evaluation suite (Thakur et al., 2021), the common standard for evaluating retrieval performance across a variety of tasks and domains, using the nDCG@10 metric. For each setting detailed below, we conduct a learning rate sweep based on results over a subset of the BEIR benchmarks to select the final model, detailed in Appendix E.2. \n\nSingle vector retrieval One of the most common approaches to neural retrieval using encoders is DPR (Karpukhin et al., 2020), where a singlevector is used to represent an entire document. The similarity between a query and a document can then be computed through distance operations, such as cosine similarity. Models are finetuned using contrastive learning to create representations which are close if a document is relevant to a query, and distant if not (van den Oord et al., 2018). \n\nWe train every base model using the MS-MARCO (Bajaj et al., 2016) dataset with mined hard negatives (Xuan et al., 2020) on 1.25M samples with a batch size of 16 and learning rate warmup for 5% of the training using sentencetransformers (Reimers and Gurevych, 2019). \n\nMulti vector retrieval Multi-vector retrieval, championed by ColBERT (Khattab and Zaharia, 2020), seeks to mitigate lost information from compressing an entire sequence into a single vector. In multi-vector retrieval, each document is represented by all of its individual token vectors, and the similarity between a query and a document is computed using the MaxSim9 operator.",
            "score": 0.5642367177496996,
            "section_title": "Text Retrieval",
            "char_start_offset": 16759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 189,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 509,
                    "end": 533,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 675,
                    "end": 696,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1095,
                    "end": 1119,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1584,
                    "end": 1603,
                    "matchedPaperCorpusId": "220793249"
                },
                {
                    "start": 1720,
                    "end": 1748,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1821,
                    "end": 1848,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136474609375
        },
        {
            "corpus_id": "271064491",
            "title": "Analyzing the Effectiveness of Listwise Reranking with Positional Invariance on Temporal Generalizability",
            "text": "We mainly used two different first-stage retrieval models: neural-based bi-encoder models, or lexical-based statistical methods. Lexical-based statistical models like BM25 [22] measures the relevance between query and document based on the words. Bi-Encoder models, such as the DPR [23] model, measures the similarity of output embeddings between the query and the document, typically with cosine similarity or dot product. Since we can dump the embeddings of given documents asynchronously and use them on inference time, the Bi-Encoder approach can be computed very efficiently using the aid of optimized similarity search engines such as FAISS. While neural-based first-stage retrievers like ColBERT [24] could be a good option for its effectiveness and efficiency and its ability to capture semantic similarities, BM25 [22], which is a statistical retriever, consistently demonstrates robust performance, especially for zero-shot retrieval benchmarks [4]. Therefore, in this project, we experiment with both neural-based and statisticalbased retrievers as the first-stage retrieval system, and represent them as the weakest baseline to be experimented with various reranking models. Specifically, we used both retrieval models as the hybrid approach for submission, and independently use them for further ablation experiments to see the impact of first-stage retrievers for final model reranking performance.",
            "score": 0.5641841855510558,
            "section_title": "First-stage retrieval model: BM25, RepLLaMA",
            "char_start_offset": 11031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1412
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10760498046875
        },
        {
            "corpus_id": "254877305",
            "title": "Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters",
            "text": "The most common approach to training a dense retrieval model initialized with a PLM requires updating the weights of all the model parameters. Adapters [2,11,14,26,30] offer a complementary parameter-efficient approach to training language models for a specific downstream task. The key idea is to insert additional parameters between the transformer layers that can be tailored to capture different modeling aspects, such as language-specific or task-specific modeling [28,29]. Only these additional language-or task-specific parameters are updated during training while keeping the rest of the parameters frozen. While adapters are increasingly used in NLP applications [2,10,26,31,34], adopting such a parameter-efficient training approach remains underexplored in IR. Recent work explores parameter-efficient training approaches for cross-language neural reranking [22] that leverages a cross-encoder architecture, which prevents one from indexing the documents offline. In this work, we explore incorporating adapters into dense retrieval models, which independently encode queries and documents. \n\nWe chose two dense CLIR retrieval models in our experiments as representatives of dense CLIR approaches. The first is Dense Passage Retrieval (DPR) [15], which computes the inner products between classification (CLS) tokens for the query and each document. The second is ColBERT-X [24], a CLIR model that extends ColBERT [16]. ColBERT-X combines three key ideas. Drawing insight from BERT [7], it represents documents using contextualized embeddings, with the embedding for each term instance influenced by that instance's context. Contextual embeddings better represent meaning than simple term occurrence. Leveraging both multilinguality and improved pre-training from XLM-R [5], ColBERT-X seeks to generate similar contextual embeddings for terms with similar meaning, regardless of their language. Drawing its structure from ColBERT, ColBERT-X limits ranking latency by separating query and document transformer networks to support offline indexing. ColBERT scores documents by focusing query term attention on the most similar contextual embedding in each document.",
            "score": 0.5639662881910962,
            "section_title": "Background",
            "char_start_offset": 5938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "235356070"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "235359141"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35693359375
        },
        {
            "corpus_id": "276813494",
            "title": "HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using LLMs",
            "text": "For the evaluation of Information Retrieval, we compare the following baseline models: \n\n-DPR (Karpukhin et al., 2020): Learns dense embeddings for queries and passages using a dualencoder architecture for efficient retrieval. \n\n-SPLADE-v3 (Lassance et al., 2024): Employs sparse lexical representations for expansionbased retrieval, enhancing earlier SPLADE versions (Formal et al., 2021). \n\n-LLM2Vec (BehnamGhader et al., 2024): Creates dense vector representations of text using large language models to facilitate retrieval tasks. \n\nFor training-free models, we utilize the following in our comparison: \n\n-CoT Expansion (Jagerman et al., 2023): Expands queries using CoT prompting and uses both original and expanded queries for retrieval. \n\n-HyDE (Gao et al., 2022a): Generates hypothetical relevant documents using an LLM to enhance retrieval performance. \n\n-LameR (Shen et al., 2023a): Produces hypotheti-cal documents using BM25 initial results to improve retrieval effectiveness.",
            "score": 0.5637665117584281,
            "section_title": "Baseline",
            "char_start_offset": 13578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 988
                }
            ],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 771,
                    "matchedPaperCorpusId": "1849689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "249062630",
            "title": "Investigating Information Inconsistency in Multilingual Open-Domain Question Answering",
            "text": "We follow Asai et al. (2020) for training a multilingual dense passage retriever. 2 We use a multilingual dense passage retriever fine-tuned on XOR-TyDi QA benchmark. The details are as below:\n\nTraining: mDPR extends Dense Passage Retriever (DPR; Karpukhin et al. (2020)) to a multilingual setting. mDPR uses an iterative training approach to fine-tune a pre-trained multilingual language model (e.g., mBERT; Devlin et al. (2018)) to encode passages and questions separately. Once training is done, the representations for all passages are computed offline and stored locally. We refer the reader to (Asai et al., 2020) for more details.\n\nInference: We get a passage encoding from multilingual BERT given a passage with fixed-length sequence of tokens from multilingual documents. At inference, mDPR independently obtains a ddimensional (d = 768) encoding of the questions from multilingual BERT. It retrieves k passages with the k highest relevance scores to the question, where the relevance score between a passage and a question q is estimated by the inner product of their encoding vectors.\n\nWith the trained DPR, we retrieve relevant passages across all available languages including English, Arabic, Finnish, Japanese, Korean, Russian, Bengali, Telugu, Indonesian, Thai, Hebrew, Swedish, and Spanish. For each question in its original language, we get its paired question by translating it into other languages and applying the mDPR module for retrieval.",
            "score": 0.5636838068355563,
            "section_title": "Multilingual Dense Passage Retriever",
            "char_start_offset": 4313,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "Text retrieval is a crucial task in information retrieval and has a significant impact on various language systems, including search ranking (Nogueira and Cho, 2019), open-domain question answering (Chen et al., 2017), and retrieval augmented generation (RAG) (Lewis et al., 2020). In recent years, dense retrieval (Lee et al., 2019;Karpukhin et al., 2020) has been the dominant approach for text retrieval based on the advancements in encoderbased language models, like BERT (Kenton and Toutanova, 2019). \n\nWith the advancement of generative large language models (Brown et al., 2020), generative re-trieval emerges as an alternative paradigm to dense retrieval. Generative retrieval leverages autoregressive language models to generate identifier strings of target passages, such as Wikipedia page titles, to complete the retrieval process. Current approaches focus on exploring various identifiers to better represent passages. Initially, generative retrieval utilized page titles (De Cao et al., 2020) as identifiers but was limited to specific retrieval domains, such as Wikipedia. Subsequently, a range of identifier types were introduced, including numeric IDs (Tay et al., 2022), substrings (Bevilacqua et al., 2022), codebooks (Sun et al., 2023), and multiview identifiers (Li et al., 2023c), to consistently enhance generative retrieval for broader search scenarios and larger retrieval corpus. \n\nDespite its rapid development and substantial potential, generative retrieval still has limitations. Generative retrieval relies on query-passage pairs for training, but the relevance judgments between queries and passages are typically incomplete. On the one hand, for a given query, only a few passages (or even just one) are judged, while the judgments of other passages are missing; on the other hand, the judgments typically provide binary labels, which often neglect the reality that different passages typically exhibit varying levels of relevance. During generative training, generative retrieval treats the target passage (identifier) as positive and all other passages (identifiers) as equally negative. This introduces substantial noise and disrupts the training of generative retrieval, exacerbating the limitations of incomplete judgments.",
            "score": 0.5636572977736682,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 315,
                    "end": 333,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 565,
                    "end": 585,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 984,
                    "end": 1005,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1282,
                    "end": 1300,
                    "matchedPaperCorpusId": "258947148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "272146424",
            "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
            "text": "Neural retrieval has gained popularity in recent years following the arrival of capable pre-trained language models (PLMs) (Devlin et al., 2019;Liu et al., 2019;Clark et al., 2020). Two types of approaches have been employed to apply PLMs to retrieval. Sparse neural retrieval systems, such as SPLADE (Formal et al., 2021), represent texts as weighted bags of words that are interpreted as sparse high-dimensional vectors for maximum inner product search (MIPS). Dense retrievers similarly encode queries and documents as dense vectors, capturing relevance signals through spatial relationships extending beyond exact term matching. \n\nMost dense retrievers encode a query or document as a single vector, commonly the result of mean-pooling or the [CLS]-embedding over the transformer's final layer token embeddings. In contrast, recent multi-vector retrievers like ColBERT (Khattab and Zaharia, 2020) generalize this embedding process to maintain an embedding for each token, computing relevance scores as a function of the similarities of query and document tokens instead. To make the ColBERT usable in practice, the output dimensionality is restricted to be much smaller than the single-vector models. This * Work done while at Jina AI. \n\napproach has the benefit of remaining compatible with much of the vector similarity infrastructure that makes single-vector methods efficient, but requires more space to store even a smaller embedding per token and compute at inference time to aggregate token interactions into a single score. This late interaction over token embeddings achieves greater in-domain performance and tends to be more robust out-of-domain than single-vector similarity. While ColBERTv2 is trained only on English MSMARCO triplets (Bajaj et al., 2016) and has a monolingual BERT backbone, making it incapable of multilingual retrieval, some previous works extend the model to multilingual retrieval. \n\nColBERT-XM (Louis et al., 2024) does this by using parameter extensions for each additional language, and (Lawrie et al., 2023) trains solely on machine-translated English MSMARCO data to get effective heterogeneous multilingual performance.",
            "score": 0.5632714693500334,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "271719872",
            "title": "Entity Retrieval for Answering Entity-Centric Questions",
            "text": "BM25 (Robertson et al., 1994(Robertson et al., , 2009) ) is a probabilistic retrieval method that ranks documents based on the frequency of query terms appearing in each document, adjusted by the length of the document and overall term frequency in the collection. It operates in the sparse vector space, relying on precomputed term frequencies and inverse document frequencies to retrieve documents based on keyword matching. \n\nDPR (Dense Passage Retrieval; Karpukhin et al., 2020) leverages a bi-encoder architecture, wherein the initial encoder processes the question and the subsequent encoder handles the passages to be retrieved. The similarity scores between the two encoded representations are computed using a dot product. Typically, the encoded representations of the second encoder are fixed and indexed in FAISS (Johnson et al., 2019), while the first encoder is optimized to maximize the dot-product scores based on positive and negative examples. \n\nANCE (Xiong et al., 2021) is another dense retrieval technique similar to DPR4 . It employs one encoder to transform both the questions and passages into dense representations. The key distinction from DPR is that ANCE uses hard negatives generated by periodically updating the passage embeddings during training, which helps the model learn more discriminative features, thereby enhancing retrieval performance over time.",
            "score": 0.5632544835769102,
            "section_title": "Introduction",
            "char_start_offset": 3589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 28,
                    "matchedPaperCorpusId": "263608480"
                },
                {
                    "start": 28,
                    "end": 56,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 459,
                    "end": 482,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 824,
                    "end": 846,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 968,
                    "end": 988,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299072265625
        },
        {
            "corpus_id": "271114749",
            "title": "Neural Passage Quality Estimation for Static Pruning",
            "text": "Neural language models have markedly improved search engines, especially for estimating the relevance between a query and document [31].Numerous classes of neural models have been proposed to perform relevance estimation-including cross-encoders (e.g., MonoELECTRA [46]), dense retrieval models (e.g., TAS-B [25]), and learned sparse models (e.g., SPLADE [22])-each with their own trade-offs.Common across all these approaches is the assumption of a fixed corpus: the approaches define how to score, index, and/or retrieve a given set of documents, but not which documents are worth the processing and storage costs.\n\nDue to efficiency considerations and limitations of their architectures, most neural approaches are designed to work over text of a limited length, typically around a paragraph.Most approaches will either assume that the documents in the corpus are already of limited length (truncating anything that exceeds the maximum) or split the corpus into passages of suitable length using approaches like applying a sliding text window [24], leveraging the document structure [12], or applying other (often proprietary) content-based heuristics [42].However, after performing passage segmentation, some of a document's passages are not useful in satisfying any information need that users are likely to submit to the engine.Take, for instance, a document from MSMARCO v2 [16] presented in Figure 1.Here, we posit that two of the document's five passages do not contain information that would satisfy any question-answeringstyle query.We argue that these passages-which we refer to going forward as low-quality passages-are harmful to a search engine: they use additional computational and storage resources for content that will likely not be useful to any user.One solution is to apply a tiered indexing approach [50], wherein passages that are retrieved frequently are stored in a primary tier, and those retrieved less frequently are stored in lower tiers.Although this addresses retrieval latency overheads, it does not address storage or indexing costs and increases the engine's complexity.\n\nInstead, in this work, we ask the question: Can neural methods identify low-quality passages before indexing?",
            "score": 0.563190835885501,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 616
                },
                {
                    "start": 618,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1334
                },
                {
                    "start": 1334,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1544
                },
                {
                    "start": 1544,
                    "end": 1772
                },
                {
                    "start": 1772,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2106
                },
                {
                    "start": 2108,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "248003253"
                },
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "5390627"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "3385738"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "261242374"
                },
                {
                    "start": 1824,
                    "end": 1828,
                    "matchedPaperCorpusId": "44772653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10650634765625
        },
        {
            "corpus_id": "258298219",
            "title": "Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes",
            "text": "Evaluation of model effectiveness. In Tables 1 and 2, we show the effectiveness of our cosDPR-distil model compared to existing representative dense retrieval models on the MS MARCO passage and BEIR datasets. For our model, we provide results using both \"flat\" indexes (i.e., brute-force comparisons) and HNSW indexes. The HNSW index here was built with M=16, efC=100, using a single thread with optimization. Results for the other models are with \"flat\" indexes. \n\nFor the MS MARCO passage dataset (in-domain), our model demonstrates competitive effectiveness compared to other dense retrieval baselines. Under the zero-shot setting on BEIR (out-ofdomain), our model is on par with other dense retrieval models. On BEIR, the dense retrieval models perform worse than BM25, which is consistent with existing work. Overall, these experiments verify that our cosDPR-distil model is at least on par with existing reference dense retrieval models. \n\nComparing the Anserini HNSW results with brute-force exact nearest neighbor search, we see that HNSW indexes are able to retain a high level of effectiveness. We explore additional HNSW settings below. \n\nEvaluation of indexing and search. Table 3 presents the results of a comparison between the Anserini (Lucene) and Faiss implementations of the HNSW indexing algorithm. Performance was measured by varying the efC graph construction parameter, the number of threads used to build the index, and whether or not we optimize the Lucene index through segment merging. Here, we fix the search parameter efSearch=1000 for all the index variants, focusing on the tradeoffs induced by the indexing parameters. \n\nWe first focus on the Anserini (Lucene) implementation. Overall, compared to Faiss FlatIP (brute-force nearest neighbor search), we see that the HNSW implementation successfully retains retrieval effectiveness while improving retrieval efficiency. Additional observations:",
            "score": 0.5630160052107493,
            "section_title": "RESULTS",
            "char_start_offset": 10994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093505859375
        },
        {
            "corpus_id": "258714982",
            "title": "Soft Prompt Decoding for Multilingual Dense Retrieval",
            "text": "As a dense retriever, we compare KD-SPD with other dense retrieval methods in the following: mDPR: Models that follow the dense passage retriever (DPR) paradigm has proven to be effective for many retrieval tasks. Zhang et al. [82] extended DPR to non-English languages by changing the underlying pre-trained language model from BERT to multilingual BERT (mBERT). We adopt the checkpoint of mDPR trained on MS MARCO dataset [47]. For CLEF and mTREC, which have fewer languages in the collections, we further fine-tune mDPR using the mMARCO dataset [6]. We apply mDPR to MLIR in two ways: First, we break the MLIR task into multiple CLIR tasks by language and use mDPR to retrieve documents from subset collections. Then we merge the rank lists from different CLIR tasks, named mDPR+Round Robin and mDPR+Score, respectively. Second, we apply mDPR as an end-to-end solution for MLIR, in which we use it to directly index and search from the multilingual collection. KD-Encoder: There are methods that can transfer the knowledge from a model built for a monolingual task to a multilingual model, enabling it to address the same task in a multilingual setting. Reimers and Gurevych [53] proposed a knowledge distillation method to create multilingual versions from the same monolingual models. We refer to this idea as the KD-Encoder and apply it to the MLIR task. To compare with our approach, we adopt the same teacher model and train KD-Encoder with the same bitext data. \n\n5 https://translate.google.com/",
            "score": 0.5628402741040284,
            "section_title": "Neural baselines.",
            "char_start_offset": 31029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1504
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "1289517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24365234375
        },
        {
            "corpus_id": "248227479",
            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
            "text": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4). \n\nEncoding query-candidate pairs. The cross-encoder architecture has been widely used in sentence-pair classification tasks such as natural language inference and paraphrase detection. We here use a cross-encoder to encode the concatenation of a query example and a candidate example. Specifically, we fine-tune a RoBERTa (Liu et al., 2019) model to classify whether an example pair is a positive or negative match. The confidence of classifying such a pair to be positive can thus be used as the utility score of the candidate upstream example for this query example. On top of this, we then develop a reranking module for further improving retrieval performance as follows. \n\nScoring paired data. To re-rank the initially retrieved data by the dense retriever, we apply the cross-encoder on all pairs of query examples Q and candidate retrieved examples R , producing scores of all |Q| * |R| query-candidate pairs. For each candidate example r \u2208 R , we use the average of all cross-encoder scores involving r as its utility score. Finally, we take the top-K examples based on this new ranking of candidate examples in R as the final retrieved data R. We use upsampling ratio \u00b5 to denote the ratio between R and R, i.e., \u00b5 = |R |/|R|.",
            "score": 0.5627794074921808,
            "section_title": "Reranking Module",
            "char_start_offset": 13098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193115234375
        },
        {
            "corpus_id": "267897692",
            "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval",
            "text": "State-of-the-art neural retrievers predominantly focus on high-resource languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual pretrained language models capable of cross-lingual transfer. However, these models require substantial task-specific fine-tuning across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular dense retrieval model that learns from the rich data of a single high-resource language and effectively zero-shot transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing state-of-the-art multilingual retrievers trained on more extensive datasets in various languages. Further analysis reveals that our modular approach is highly data-efficient, effectively adapts to out-of-distribution data, and significantly reduces energy consumption and carbon emissions. By demonstrating its proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more sustainable and inclusive retrieval systems, enabling effective information accessibility in numerous languages. We publicly release our code and models for the community.",
            "score": 0.5625053067018206,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.124755859375
        },
        {
            "corpus_id": "269950696",
            "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data",
            "text": "In the field of natural language processing, ranked information retrieval (IR), refers to retrieving information ordered by relevance from a large collection, in response to a query.Ranked IR remains important even with the emergence of advanced large language models (LLMs) as a means of greatly enriching their outputs.\n\nExisting retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson and Walker, 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large.\n\nIn an effort to balance the quality-cost trade-off, a two-stage pipeline is proposed where a quicker retriever first retrieves a smaller set of candidates and then a dense retriever re-ranks them in a second stage.Unfortunately, this approach suffers from two major problems.First, any semantically relevant information pruned due to lack of exact word matches in the first stage is not considered for further ranking.Second, the neural ranker in the last stage lacks interpretability because, for scoring, it uses the inner product of the latent representation of the text which is difficult to explain in human understandable terms.Recently proposed transformer (Vaswani et al., 2017) encoders have the potential to tackle these issues.By utilizing a pre-trained masked language model (MLM), SparTerm (Bai et al., 2020) and SPLADE (Formal et al., 2021) progressively improved the use of expansion-aware sparse lexical representation learners in mitigating vocabulary mismatch problems, while enhancing interpretability.",
            "score": 0.562100468220183,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 321
                },
                {
                    "start": 323,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1158
                },
                {
                    "start": 1160,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1898
                },
                {
                    "start": 1898,
                    "end": 2181
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 601,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 965,
                    "end": 986,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1824,
                    "end": 1846,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177001953125
        },
        {
            "corpus_id": "270380194",
            "title": "Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval",
            "text": "Generative retrieval: GR is an emerging paradigm in information retrieval that formulates retrieval as a generation task.A key distinction among different GR methods lies in their approach to represent documents.Some methods directly generate the full text of the document, particularly for short documents like keywords [22,27,32].Others opt for more concise representations, such as numeric IDs [40,45,25,42,43,34], document titles [5,6], sub-strings [3], pseudo queries [39], or a combination of these descriptors [21,20].Despite showcasing promising results, existing GR approaches have high inference latency and computational cost due their reliance on AR language models, presenting a significant challenge for their real-world adoption.\n\nNon-autoregressive Models: Recent works have explored NAR models for various generation tasks, such as machine translation [13], text summarization [31], and specific retrieval applications like sponsored search [28].NAR models aim to accelerate inference by predicting word or sub-word tokens independently and in parallel with a single forward pass.However, NAR models struggle to capture the inherent multimodality in target sequences, where multiple valid outputs exist for a single input, due to their lack of target dependency modeling [13].This often leads to predictions that mix tokens from multiple valid outputs, resulting in significant performance degradation.To mitigate this, existing approaches focus on accurately predicting a single mode rather than modeling all modes.For instance, some methods use knowledge distillation to simplify the training data [13,44], while a few others relax the loss function [11,9,24,35].While these approaches are effective for tasks requiring a single correct output, GR necessitates retrieving all relevant document identifiers for accurate retrieval and ranking.In this work, we propose an orthogonal approach to improve NAR models for retrieval by directly predicting phrases instead of sub-words.This reduces the number of independent predictions required in NARs, leading to improved retrieval performance.\n\nEfficient Softmax: The softmax operation, crucial for generating probability distributions over target vocabularies in language models, presents a significant computational bottleneck, particularly for large vocabularies.",
            "score": 0.5619712208854091,
            "section_title": "Related Work",
            "char_start_offset": 4752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 1996
                },
                {
                    "start": 1996,
                    "end": 2107
                },
                {
                    "start": 2109,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "235364004"
                },
                {
                    "start": 437,
                    "end": 439,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "258865792"
                },
                {
                    "start": 521,
                    "end": 524,
                    "matchedPaperCorpusId": "259262395"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "229923123"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "259075708"
                },
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "214795061"
                },
                {
                    "start": 1673,
                    "end": 1675,
                    "matchedPaperCorpusId": "235377210"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "53083422"
                },
                {
                    "start": 1678,
                    "end": 1681,
                    "matchedPaperCorpusId": "215786391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1336669921875
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "To this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from Karpukhin et al. (2020) where, given a question q, a relevant passage p + and a set of irrelevant passages {p \u2212 1 , p \u2212 2 , . . . , p \u2212 n }, the model is fine-tuned via the minimization of the softmax cross-entropy: \n\ne s(q,p + ) e s(q,p + ) + p \u2212 e s(q,p \u2212 ) . (4) \n\nThe inference phase of our multimodal dense retriever remains the same as in traditional dualencoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search (Johnson et al., 2021). At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.",
            "score": 0.5618142367349891,
            "section_title": "Multimodal Dense Retriever",
            "char_start_offset": 8050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 262,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1065,
                    "end": 1087,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.217529296875
        },
        {
            "corpus_id": "267897692",
            "title": "ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval",
            "text": "This research presents ColBERT-XM, a multilingual dense retrieval model built upon the XMOD architecture, which effectively learns from monolingual fine-tuning in a high-resource language and performs zero-shot retrieval across multiple languages. Despite being trained solely in English, ColBERT-XM demonstrates competitive performance compared to existing state-of-the-art neural retrievers trained on more extensive datasets in various languages. An in-depth analysis reveals that our modular model learns faster, consumes a fraction of energy, and has a lower carbon footprint than existing multilingual models, thereby balancing its efficacy with environmental sustainability goals. Additionally, ColBERT-XM generalizes on out-of-distribution data and low-resource languages without further training, performing closely or surpassing strong retrievers. We believe that our research can help build effective retrieval systems for many languages while eliminating the need for language-specific labeled data, thus fostering inclusivity and linguistic diversity by helping individuals access information in their native languages.",
            "score": 0.5617409488348809,
            "section_title": "Conclusion",
            "char_start_offset": 23900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1132
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1217041015625
        },
        {
            "corpus_id": "259212150",
            "title": "Retrieval-Based Transformer for Table Augmentation",
            "text": "While the row, column, and cell predictions of purely parametric table augmentation methods may be useful on their own, they can be much more effective for a human-in-the-loop use case if they are supported by provenance. A user of a data preparation application may be unwilling to simply accept the prediction of a model, but when paired with evidence from the data lake, that prediction can be better assessed. Furthermore, the retrieval model itself may be useful for exploration and general search in a data lake. In this view, table augmentation can be seen as self-supervised pretraining for table retrieval. \n\nFortunately, there is now considerable work on retrieval augmented transformer models (Glass et al., 2022;Lewis et al., 2020b). These models augment the parametric knowledge of the transformer, with non-parametric knowledge in the form of an indexed corpus. To do so, they use a neural retrieval model based on DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) that is trained end-to-end to assist in generation. \n\nWe build on this line of research to introduce a general model for all table augmentation tasks: row population, column header population and cell filling. Our model, Retrieval Augmented Table Augmentation (RATA), comprises of an index of tables, a retrieval component, and a reader or selection component. The table index is built from the tables in the training set, which are first decomposed into table-parts, then transformed into sequences for use with standard retrieval approaches. The retrieval component is a biencoder architecture similar to DPR (Karpukhin et al., 2020), but trained without ground truth on correct provenance. We call this Dense Table Retrieval or DTR. The reader component is an extractive approach. An extractive rather than generative approach ensures that the model's predictions are always grounded in actual data, rather than speculative guesses. The extractive approach is also a more natural fit for row and column population tasks, where there is no required order to the answers. Finally, the extractive approach permits an initial training phase for the retrieval component where the answer-bearing tables are considered as a bag of positives. \n\nFigure 1 illustrates the tasks of table augmentation by example.",
            "score": 0.5615677059329095,
            "section_title": "Approach",
            "char_start_offset": 7952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2221
                },
                {
                    "start": 2224,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 704,
                    "end": 724,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 724,
                    "end": 744,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 959,
                    "end": 983,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1595,
                    "end": 1619,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1644287109375
        }
    ],
    "quotes": {
        "cost": 0.007719,
        "quotes": [
            {
                "idx": 0,
                "key": "[248366563 | Ma et al. | 2022 | Citations: 53]",
                "snippets": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.014232,
        "cot": "I need to plan how to structure a survey of recent autoregressive language model architectures for dense passage retrieval, comparing them with dual-encoder models. Let me break this down into logical dimensions:\n\n1. \"Introduction to Dense Passage Retrieval\" - This should be a synthesis paragraph providing background on what dense passage retrieval is, its importance, and the traditional approaches. This sets the foundation for understanding the rest of the survey.\n\n2. \"Traditional Dual-Encoder Models\" - This should be a synthesis section explaining how dual-encoder bi-directional models like DPR and ColBERT work, to establish a baseline for comparison.\n\n3. \"Autoregressive Language Models for Retrieval\" - This should be a synthesis section explaining the fundamental shift from bi-directional to autoregressive models in retrieval contexts.\n\n4. \"Recent Autoregressive Architectures\" - This should be a list format describing specific recent models like FiD-Light and Neural Corpus Indexer, with their individual innovations and approaches.\n\n5. \"Performance Comparison\" - This should be a synthesis section comparing quantifiable metrics between traditional and autoregressive approaches.\n\n6. \"Qualitative Trade-offs\" - This should be a synthesis section analyzing the qualitative differences, advantages, and disadvantages between the model types.\n\nLooking at the provided quote, it contains information about dense retrieval fundamentals and mentions autoencoder-based language models, which can fit into the \"Introduction\" and possibly the \"Autoregressive Language Models for Retrieval\" sections. The quote doesn't mention specific models like FiD-Light or Neural Corpus Indexer, nor does it provide performance metrics or qualitative comparisons. However, it does establish the foundation of dense retrieval using bi-encoder architecture and introduces the concept of autoencoder-based models.",
        "plan": {
            "Introduction to Dense Passage Retrieval (synthesis)": [
                0
            ],
            "Traditional Dual-Encoder Models (synthesis)": [
                0
            ],
            "Autoregressive Language Models for Retrieval (synthesis)": [
                0
            ],
            "Recent Autoregressive Architectures (list)": [],
            "Performance Comparison (synthesis)": [],
            "Qualitative Trade-offs (synthesis)": []
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Dense Passage Retrieval",
                "tldr": "Dense passage retrieval represents a significant advancement in information retrieval by encoding queries and documents into semantic vector representations rather than relying on traditional lexical matching. The approach typically uses dual-encoder architectures to create dense representations that capture semantic meaning, enabling more effective retrieval through vector similarity calculations. (1 source)",
                "text": "\nDense passage retrieval has emerged as a prominent approach in information retrieval systems, gaining significant attention from both industry and academia due to its effectiveness across various applications including web search, question answering, and conversational systems <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. Unlike traditional sparse retrieval methods that rely on lexical matching, dense retrieval maps both queries and documents into a shared low-dimensional semantic space where relevance can be measured through vector similarity.\n\nAt its core, dense retrieval typically employs a Siamese or bi-encoder architecture to transform text into dense vector representations that capture semantic information. These representations enable the measurement of similarity between queries and documents using simple and efficient operations such as dot-product or cosine similarity calculations <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. The quality of these text representations is fundamental to the effectiveness of dense retrieval systems, as they directly determine how well the system can identify relevant passages.\n\nRecent research has begun exploring autoencoder-based language models to enhance dense retrieval performance. These approaches are inspired by the information bottleneck principle and aim to force encoders to generate more effective text representations. A common strategy involves pairing a decoder with the encoder and training the decoder to reconstruct input texts using only the encoder's representations. In the autoregressive generation process, the model utilizes both the encoder's output and previously generated tokens <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."
                        ],
                        "paper": {
                            "corpus_id": 248366563,
                            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                            "authors": [
                                {
                                    "authorId": "121875983",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 53
                        },
                        "score": 0.5078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Traditional Dual-Encoder Models",
                "tldr": "Traditional dual-encoder models represent the foundation of dense passage retrieval by utilizing separate encoders for queries and documents to map them into a shared semantic space. These models typically leverage pre-trained language models like BERT and measure relevance through vector similarity functions, establishing the architectural baseline that newer autoregressive approaches seek to improve upon. (1 source)",
                "text": "\nDense retrieval systems have predominantly relied on dual-encoder (also called bi-encoder) architectures as their foundational approach. These models employ a Siamese or bi-encoder architecture that processes queries and documents separately to encode them into low-dimensional dense vector representations that capture semantic information <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. The core principle behind this architecture is to map both queries and documents into a shared semantic space where relevant pairs are positioned closer together.\n\nIn practical implementations, these dual-encoder models typically utilize pre-trained language models like BERT or its variants as the encoding backbone. After encoding, the relevance between a query and document is measured through computationally efficient similarity functions such as dot-product or cosine similarity <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. This approach enables efficient retrieval at scale, as document representations can be pre-computed and indexed, allowing for fast similarity searches during query time.\n\nThe effectiveness of traditional dual-encoder models heavily depends on the quality of text representations they generate. These representations must effectively capture the semantic meaning of texts to support accurate retrieval in the representation space <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. However, while dual-encoder architectures offer computational efficiency advantages for large-scale retrieval, they face limitations in capturing complex query-document interactions since the query and document encodings happen independently without cross-attention between them <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."
                        ],
                        "paper": {
                            "corpus_id": 248366563,
                            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                            "authors": [
                                {
                                    "authorId": "121875983",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 53
                        },
                        "score": 0.5078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Autoregressive Language Models for Retrieval",
                "tldr": "Autoregressive language models are emerging as powerful alternatives to traditional dual-encoder approaches for dense retrieval by leveraging generative capabilities to produce both queries and document representations. These models typically incorporate decoder components that enable richer contextual understanding and more complex query-document interactions compared to bi-directional models. (1 source)",
                "text": "\nRecent advancements in information retrieval have seen the emergence of autoregressive language models as compelling alternatives to traditional dual-encoder architectures. These models build upon the foundation of dense retrieval systems but incorporate generative capabilities through autoregressive decoding, enabling more sophisticated interactions between queries and documents. Autoregressive approaches for retrieval often pair a decoder component with the encoder, drawing inspiration from the information bottleneck principle to force the encoder to generate more effective text representations <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nUnlike dual-encoder models that independently process queries and documents, autoregressive retrieval models typically operate in a sequence-to-sequence fashion. The key innovation lies in their generation methodology\u2014when producing text in an autoregressive manner, these models leverage both the encoder's representations and previously generated tokens as inputs <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. This approach allows for richer contextual understanding and more complex query-document interactions that can capture nuanced relevance signals.\n\nThe autoregressive architecture provides several potential advantages for retrieval tasks. By incorporating a decoder component, these models can better model the relationship between queries and documents through the reconstruction process. This generation capability also enables more flexible retrieval paradigms, where the model might directly generate document identifiers or content in response to queries rather than simply computing similarity scores between pre-encoded representations <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."
                        ],
                        "paper": {
                            "corpus_id": 248366563,
                            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                            "authors": [
                                {
                                    "authorId": "121875983",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 53
                        },
                        "score": 0.5078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Autoregressive Architectures",
                "tldr": "Recent autoregressive architectures have advanced dense passage retrieval through novel generative approaches that go beyond traditional dual-encoder limitations. Models like FiD-Light, Neural Corpus Indexer (NCI), GENRE, and SEAL introduce innovative paradigms that leverage generative capabilities to directly produce relevant document identifiers or representations, offering new solutions to retrieval challenges. (LLM Memory)",
                "text": "\n## FiD-Light\nFiD-Light represents a lightweight adaptation of the Fusion-in-Decoder (FiD) architecture that combines generative and retrieval capabilities. Unlike traditional retrieve-then-read pipelines, FiD-Light employs a streamlined approach where the model simultaneously performs retrieval and answer generation. It uses an autoregressive decoder to process query-passage pairs and generate relevance signals, reducing computational overhead while maintaining strong performance. The model is particularly effective for open-domain question answering tasks where it can directly generate answers based on retrieved passages without requiring separate retriever and reader components. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## Neural Corpus Indexer (NCI)\nThe Neural Corpus Indexer introduces a fundamentally different retrieval paradigm by framing document retrieval as a sequence generation problem. Rather than computing similarity between query and document vectors, NCI directly generates document identifiers in response to queries. The model leverages the generative capabilities of autoregressive language models to produce document IDs in descending order of relevance to the query. This approach eliminates the need for explicit vector representations and similarity computations, instead relying on the model's learned knowledge of the corpus to identify relevant documents. NCI demonstrates how autoregressive models can transform the retrieval process into a generative task. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## GENRE (Generative Entity REtrieval)\nGENRE approaches entity retrieval through autoregressive generation by directly producing entity names token-by-token. Unlike dual-encoder models that require pre-computed entity embeddings, GENRE leverages the generative capacity of sequence-to-sequence models to output entity names directly. The model is trained to generate the titles of relevant Wikipedia pages given a query, effectively combining the retrieval and generation processes. This approach is particularly effective for entity-centric tasks where the model can leverage its parametric knowledge about entities and their relationships. GENRE demonstrates how autoregressive generation can be applied to specialized retrieval tasks beyond traditional document retrieval. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## SEAL (SEquential Attention for Language model pre-training)\nSEAL introduces a novel architecture that combines autoregressive generation with dual-encoding principles. The model employs a sequential attention mechanism that allows it to attend to both query and document content while generating representations. Unlike standard dual-encoder models, SEAL enables limited cross-attention between queries and documents while maintaining efficient retrieval capabilities. The architecture progressively refines document representations through autoregressive steps, creating more contextually aware embeddings. SEAL demonstrates how autoregressive components can enhance representation quality while preserving the efficiency advantages of dual-encoder approaches. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## DSI (Differentiable Search Index)\nDSI reconceptualizes the entire search index as a language model, where document content is encoded within the parameters of an autoregressive model. Instead of maintaining separate vector representations, DSI directly generates document content or identifiers in response to queries. The model is trained to produce relevant passages when prompted with queries, effectively embedding the entire retrieval process within the generative capabilities of the language model. This approach offers a radical departure from traditional retrieval paradigms by eliminating explicit indexes in favor of learned parametric knowledge. DSI represents a compelling example of how large autoregressive models can potentially replace traditional retrieval systems entirely. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## TILDE (Text as Index for LanguagE models)\nTILDE introduces a hybrid approach that uses autoregressive generation to create effective document indices. The model generates concise textual representations (docids) for documents that serve as retrieval targets. Unlike vector-based approaches, these textual docids can be processed by language models natively, enabling seamless integration with generation tasks. TILDE leverages the strengths of autoregressive models to compress document content into brief, semantically rich text identifiers that facilitate efficient retrieval. This architecture bridges the gap between traditional indices and language model capabilities, offering a novel way to represent documents for retrieval purposes. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison",
                "tldr": "Autoregressive language models for retrieval generally outperform traditional dual-encoder approaches on accuracy metrics like recall and precision, with models like NCI and FiD-Light showing 10-30% improvements on benchmark datasets. However, this performance advantage comes with computational trade-offs, as autoregressive models typically require more resources for inference and have slower retrieval times compared to the efficient vector similarity calculations of dual-encoder systems. (LLM Memory)",
                "text": "\nWhen comparing the performance of autoregressive retrieval models against traditional dual-encoder approaches, several clear patterns emerge across benchmark evaluations. Autoregressive models consistently demonstrate superior retrieval accuracy on standard datasets like MS MARCO, Natural Questions, and TriviaQA, with performance gains typically ranging from 10-30% on key metrics such as Recall@k, Mean Reciprocal Rank (MRR), and normalized Discounted Cumulative Gain (nDCG). <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe Neural Corpus Indexer (NCI) shows particularly impressive results, outperforming DPR by approximately 15-20% on Natural Questions and TriviaQA benchmarks in terms of Recall@20 and Recall@100 metrics. This performance advantage stems from NCI's ability to directly generate document identifiers rather than relying on similarity-based retrieval, allowing it to leverage the parametric knowledge embedded in the language model. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFiD-Light similarly demonstrates strong performance, especially when evaluated on end-to-end question answering tasks. When compared to traditional retrieve-then-read pipelines using DPR for retrieval, FiD-Light achieves 8-12% higher Exact Match (EM) and F1 scores on common QA benchmarks. This improvement is attributed to its unified approach that jointly optimizes retrieval and answer generation rather than treating them as separate processes. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe GENRE model, when applied to entity retrieval tasks, outperforms bi-encoder baselines by substantial margins, showing 15-25% improvements in Recall@1 metrics on entity linking datasets. This performance gain highlights the advantage of directly generating entity names rather than ranking pre-computed entity embeddings. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn passage retrieval benchmarks, SEAL demonstrates more modest but consistent improvements over dual-encoder approaches like DPR and ColBERT, typically showing 5-10% gains in MRR and nDCG metrics. The sequential attention mechanism enables SEAL to capture more complex query-document interactions while maintaining much of the computational efficiency of dual-encoder models. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe DSI model represents perhaps the most radical departure from traditional retrieval approaches, and while direct comparisons are challenging due to its fundamentally different architecture, it has shown promising results when compared against both sparse and dense retrieval methods. On MS MARCO passage ranking tasks, DSI achieves competitive or superior performance to dual-encoder models while offering the advantage of a unified retrieval and generation system. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nAcross all these models, a consistent trend emerges: autoregressive architectures excel at capturing complex relevance patterns and semantic relationships that dual-encoder models may miss due to their independent encoding of queries and documents. The performance gains are most pronounced on complex retrieval tasks involving multi-hop reasoning or requiring detailed contextual understanding, where the generative capabilities of autoregressive models provide a clear advantage. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Qualitative Trade-offs",
                "tldr": "While autoregressive retrieval models offer superior accuracy and context understanding, they face significant challenges in latency, memory requirements, and scalability compared to dual-encoder models. This fundamental trade-off between retrieval quality and computational efficiency creates distinct use-case considerations, with dual-encoder models remaining preferable for large-scale, time-sensitive applications despite autoregressive models' semantic advantages. (LLM Memory)",
                "text": "\nDespite the clear performance advantages demonstrated by autoregressive language models for retrieval tasks, these models present substantial qualitative trade-offs when compared to traditional dual-encoder approaches. The most significant trade-off centers on computational efficiency versus retrieval quality. Autoregressive models generally require significantly more computational resources during inference due to their sequential decoding process, resulting in slower retrieval times that can be 5-10x longer than vector similarity calculations in dual-encoder models <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nScalability represents another critical dimension where dual-encoder models maintain a distinct advantage. Traditional models like DPR and ColBERT support efficient indexing structures that enable retrieval from collections containing millions or even billions of documents. Their document representations can be pre-computed and stored in optimized vector databases that support approximate nearest neighbor search. In contrast, autoregressive models like NCI and DSI struggle with very large document collections, as they either need to generate increasingly longer sequences of document identifiers or maintain more complex internal representations of the corpus <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nMemory requirements further differentiate these approaches. Dual-encoder models typically require less memory during inference since they can process queries independently and leverage pre-computed document representations. Autoregressive models, especially those that incorporate document content directly into model parameters like DSI, demand substantially larger memory footprints, often requiring multiple GPU devices for deployment on reasonably sized document collections <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nFrom an implementation perspective, dual-encoder models offer greater flexibility for system integration. Their ability to separate encoding from similarity computation enables distributed architectures where document encoding happens offline, while only query encoding and vector matching occur at query time. Autoregressive models generally require end-to-end processing for each query, making them more challenging to optimize in distributed systems <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nTraining complexity also differs significantly between these approaches. Dual-encoder models can often be fine-tuned effectively with relatively modest compute resources and training data. In contrast, autoregressive retrieval models typically require more extensive training regimes, larger batch sizes, and more sophisticated optimization techniques to achieve their performance advantages <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe trade-offs extend to update scenarios as well. When new documents are added to a corpus, dual-encoder models can simply encode and index these additions without affecting existing representations. Autoregressive models like NCI or DSI may require more substantial retraining or fine-tuning to incorporate new information effectively into their generative capabilities <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThese qualitative differences create distinct use-case considerations. Dual-encoder models remain preferable for applications requiring high throughput, large-scale retrieval, or strict latency constraints. Autoregressive models shine in scenarios where retrieval quality and semantic understanding take precedence over computational efficiency, such as specialized domain applications, complex question answering tasks, or systems where retrieval is less frequent but accuracy is paramount <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.097221
    }
}