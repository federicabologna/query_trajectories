{
    "query": "How has classifier-free guidance been technically implemented and empirically evaluated when adapted from diffusion models to autoregressive language models and Flow Matching approaches in NLP tasks, including its advantages and observed limitations?",
    "user_id": "lib_user",
    "task_id": "72caeaa9-1a43-4edb-aaa1-b1fdc01c09fb",
    "timestamp": "2025-06-24T00:21:06.444513",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 45,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.476886,
    "decomposed_query": {
        "rewritten_query": "Technical implementation and empirical evaluation of classifier-free guidance when adapted from diffusion models to autoregressive language models and Flow Matching approaches in NLP tasks, including its advantages and observed limitations.",
        "keyword_query": "classifier-free guidance diffusion models autoregressive language models Flow Matching NLP advantages limitations implementation evaluation",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010182,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2409.10494",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.10494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321408469",
                    "name": "Noah Buchanan"
                },
                {
                    "authorId": "2268404815",
                    "name": "Susan Gauch"
                },
                {
                    "authorId": "2308097654",
                    "name": "Quan Mai"
                }
            ],
            "abstract": "This paper presents a diffusion-based recommender system that incorporates classifier-free guidance. Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering. Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items. Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole. In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance. Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets. In particular, our approach demonstrates the potential to provide better recommendations when data is sparse.",
            "corpus_id": 272690217,
            "sentences": [
                {
                    "corpus_id": "272690217",
                    "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
                    "text": "This paper presents classifier-free guidance as a form of personalization and incorporates denoising in diffusion-based recommender frameworks. By restructuring the guidance mechanism by removing partially noised pseudo-guidance and incorporating true guidance through conditioning on pre-noise data, we demonstrated improvements in performance. The effectiveness of our method was evaluated using precision, recall, nDCG, and mean reciprocal rank @ K on held-out user data, revealing clear benefits over previous methods. Additionally, our approach showed potentially enhanced capabilities in fewshot and zero-shot recommendation scenarios. \n\nOur major contributions include: (1) the implementation and evaluation of classifier-free guidance in diffusion-based recommendation tasks; (2) an in-depth discussion on the parameterization and architecture of the denoising model utilized in these tasks; and (3) insights into the applicability of diffusion models beyond the visual domain. Our findings indicate that classifier-free guidance improves performance in recommendation tasks similarly to its success in visual tasks. Additionally, we provide a framework that addresses challenges within diffusion-based recommendation systems while highlighting the newfound capabilities in few-shot and zero-shot scenarios. \n\nThe potential for future work in this area is substantial. One promising direction involves improving the system by considering actual numerical ratings. While our initial attempt at this was inconclusive, a more sophisticated approach, potentially involving triplet loss to account for negative reviews, could yield better results. Cross-attentive conditioning, another feature that did not meet our performance expectations, might benefit from refined architectural innovations for improved guidance. \n\nThe use of transformers in the diffusion process is another avenue worth exploring. Recent advancements in image generation have shown that replacing the U-Net backbone with a Vision Transformer (ViT) can enhance denoising capabilities, and similar benefits might be realized in recommendation systems. Additionally, the few-shot and zero-shot capabilities of diffusion recommender systems trained with classifierfree guidance warrant further investigation. This aspect is particularly relevant for e-commerce, where new users with no interaction history present ongoing challenges. \n\nIn conclusion, our study lays the groundwork for advancing diffusion-based recommendation systems. By addressing the identified shortcomings and exploring the outlined directions for future research, we can further enhance these systems to meet the evolving needs of recommender applications.",
                    "score": 0.4565122985384442,
                    "section_title": "VI. CONCLUSION AND FUTURE WORK",
                    "char_start_offset": 32582,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 641
                        },
                        {
                            "start": 644,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1124
                        },
                        {
                            "start": 1125,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1820
                        },
                        {
                            "start": 1823,
                            "end": 1906
                        },
                        {
                            "start": 1907,
                            "end": 2125
                        },
                        {
                            "start": 2126,
                            "end": 2280
                        },
                        {
                            "start": 2281,
                            "end": 2405
                        },
                        {
                            "start": 2408,
                            "end": 2506
                        },
                        {
                            "start": 2507,
                            "end": 2700
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation\n# Venue: 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)\n# Authors: Noah Buchanan, Susan Gauch, Quan Mai\n## Abstract\nThis paper presents a diffusion-based recommender system that incorporates classifier-free guidance. Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering. Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items. Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole. In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance. Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets. In particular, our approach demonstrates the potential to provide better recommendations when data is sparse.\n## VI. CONCLUSION AND FUTURE WORK\nThis paper presents classifier-free guidance as a form of personalization and incorporates denoising in diffusion-based recommender frameworks. By restructuring the guidance mechanism by removing partially noised pseudo-guidance and incorporating true guidance through conditioning on pre-noise data, we demonstrated improvements in performance. The effectiveness of our method was evaluated using precision, recall, nDCG, and mean reciprocal rank @ K on held-out user data, revealing clear benefits over previous methods. Additionally, our approach showed potentially enhanced capabilities in fewshot and zero-shot recommendation scenarios. \n\nOur major contributions include: (1) the implementation and evaluation of classifier-free guidance in diffusion-based recommendation tasks; (2) an in-depth discussion on the parameterization and architecture of the denoising model utilized in these tasks; and (3) insights into the applicability of diffusion models beyond the visual domain. Our findings indicate that classifier-free guidance improves performance in recommendation tasks similarly to its success in visual tasks. Additionally, we provide a framework that addresses challenges within diffusion-based recommendation systems while highlighting the newfound capabilities in few-shot and zero-shot scenarios. \n\nThe potential for future work in this area is substantial. One promising direction involves improving the system by considering actual numerical ratings. While our initial attempt at this was inconclusive, a more sophisticated approach, potentially involving triplet loss to account for negative reviews, could yield better results. Cross-attentive conditioning, another feature that did not meet our performance expectations, might benefit from refined architectural innovations for improved guidance. \n\nThe use of transformers in the diffusion process is another avenue worth exploring. Recent advancements in image generation have shown that replacing the U-Net backbone with a Vision Transformer (ViT) can enhance denoising capabilities, and similar benefits might be realized in recommendation systems. Additionally, the few-shot and zero-shot capabilities of diffusion recommender systems trained with classifierfree guidance warrant further investigation. This aspect is particularly relevant for e-commerce, where new users with no interaction history present ongoing challenges. \n\nIn conclusion, our study lays the groundwork for advancing diffusion-based recommendation systems. By addressing the identified shortcomings and exploring the outlined directions for future research, we can further enhance these systems to meet the evolving needs of recommender applications.",
            "reference_string": "[272690217 | Buchanan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2022,
            "reference_count": 115,
            "citation_count": 1133,
            "influential_citation_count": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.10789",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.10789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338016295",
                    "name": "Jiahui Yu"
                },
                {
                    "authorId": "2145139570",
                    "name": "Yuanzhong Xu"
                },
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "1821711",
                    "name": "Thang Luong"
                },
                {
                    "authorId": "1396954703",
                    "name": "Gunjan Baid"
                },
                {
                    "authorId": "2331539",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2053781980",
                    "name": "Vijay Vasudevan"
                },
                {
                    "authorId": "31702389",
                    "name": "Alexander Ku"
                },
                {
                    "authorId": "2118771180",
                    "name": "Yinfei Yang"
                },
                {
                    "authorId": "143990191",
                    "name": "Burcu Karagol Ayan"
                },
                {
                    "authorId": "2044655623",
                    "name": "Ben Hutchinson"
                },
                {
                    "authorId": "143911112",
                    "name": "Wei Han"
                },
                {
                    "authorId": "27456119",
                    "name": "Zarana Parekh"
                },
                {
                    "authorId": "2158973314",
                    "name": "Xin Li"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1387994164",
                    "name": "Jason Baldridge"
                },
                {
                    "authorId": "48607963",
                    "name": "Yonghui Wu"
                }
            ],
            "abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
            "corpus_id": 249926846,
            "sentences": [
                {
                    "corpus_id": "249926846",
                    "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                    "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
                    "score": 0.5386939393779961,
                    "section_title": "Classifier-Free Guidance and Reranking",
                    "char_start_offset": 15352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 986
                        },
                        {
                            "start": 989,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1535
                        },
                        {
                            "start": 1538,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1790
                        },
                        {
                            "start": 1791,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2155
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1621,
                            "end": 1624,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 1835,
                            "end": 1838,
                            "matchedPaperCorpusId": "232035663"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu\n## Abstract\nWe present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.\n## Classifier-Free Guidance and Reranking\nClassifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "reference_string": "[249926846 | Yu et al. | 2022 | Citations: 1133]"
        },
        {
            "title": "Flow Matching for Discrete Systems: Efficient Free Energy Sampling Across Lattice Sizes and Temperatures",
            "venue": "",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349541183",
                    "name": "Ping Tuo"
                },
                {
                    "authorId": "2351208777",
                    "name": "ZeZhu Zeng"
                },
                {
                    "authorId": "2349541600",
                    "name": "Jiale Chen"
                },
                {
                    "authorId": "2313482115",
                    "name": "Bingqing Cheng"
                }
            ],
            "abstract": "Generative models have advanced significantly in sampling material systems with continuous variables, such as atomistic structures. However, their application to discrete variables, like atom types or spin states, remains underexplored. In this work, we introduce a Boltzmann generator built on discrete flow matching, specifically tailored for systems with discrete phase-space coordinates (e.g., the Ising model or crystalline compounds). This approach enables a single model to sample free energy surfaces over a wide temperature range with minimal training overhead. In addition, the model generation is scalable to larger lattice sizes than those in the training set. We demonstrate the effectiveness of our approach on the 2D Ising model, showing efficient and reliable free energy sampling. This framework provides a scalable and computationally efficient solution for discrete coordinate systems and can be extended to sample the alchemical degrees of freedom in crystalline compounds.",
            "corpus_id": 276928343,
            "sentences": [
                {
                    "corpus_id": "276928343",
                    "title": "Flow Matching for Discrete Systems: Efficient Free Energy Sampling Across Lattice Sizes and Temperatures",
                    "text": "where c denotes the desired class, and \u03b3 controls the strength of the classifier guidance. \n\nTo remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6)).",
                    "score": 0.39857759446045876,
                    "section_title": "Multi-temperature generation",
                    "char_start_offset": 14241,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 93,
                            "end": 270
                        },
                        {
                            "start": 273,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 450
                        },
                        {
                            "start": 453,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 909
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: Flow Matching for Discrete Systems: Efficient Free Energy Sampling Across Lattice Sizes and Temperatures\n# Venue: \n# Authors: Ping Tuo, ZeZhu Zeng, Jiale Chen, Bingqing Cheng\n## Abstract\nGenerative models have advanced significantly in sampling material systems with continuous variables, such as atomistic structures. However, their application to discrete variables, like atom types or spin states, remains underexplored. In this work, we introduce a Boltzmann generator built on discrete flow matching, specifically tailored for systems with discrete phase-space coordinates (e.g., the Ising model or crystalline compounds). This approach enables a single model to sample free energy surfaces over a wide temperature range with minimal training overhead. In addition, the model generation is scalable to larger lattice sizes than those in the training set. We demonstrate the effectiveness of our approach on the 2D Ising model, showing efficient and reliable free energy sampling. This framework provides a scalable and computationally efficient solution for discrete coordinate systems and can be extended to sample the alchemical degrees of freedom in crystalline compounds.\n## Multi-temperature generation\nwhere c denotes the desired class, and \u03b3 controls the strength of the classifier guidance. \n\nTo remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6)).",
            "reference_string": "[276928343 | Tuo et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2340406432",
                    "name": "Weichen Fan"
                },
                {
                    "authorId": "2323508574",
                    "name": "Amber Yijia Zheng"
                },
                {
                    "authorId": "2240073241",
                    "name": "Raymond A. Yeh"
                },
                {
                    "authorId": "2324070583",
                    "name": "Ziwei Liu"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)",
            "corpus_id": 277510202,
            "sentences": [
                {
                    "corpus_id": "277510202",
                    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
                    "text": "Diffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
                    "score": 0.48434586893320625,
                    "section_title": "Related Work",
                    "char_start_offset": 3527,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1992
                        },
                        {
                            "start": 1995,
                            "end": 2189
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 127,
                            "end": 130,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 315,
                            "end": 319,
                            "matchedPaperCorpusId": "222140788"
                        },
                        {
                            "start": 325,
                            "end": 329,
                            "matchedPaperCorpusId": "249240415"
                        },
                        {
                            "start": 352,
                            "end": 356,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 448,
                            "end": 452,
                            "matchedPaperCorpusId": "252734897"
                        },
                        {
                            "start": 812,
                            "end": 816,
                            "matchedPaperCorpusId": "252111177"
                        },
                        {
                            "start": 822,
                            "end": 825,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 1518,
                            "end": 1521,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                },
                {
                    "corpus_id": "277510202",
                    "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
                    "text": "Diffusion and flow-based models are the state-of-the-art (SOTA) for generating high-quality images and videos, with recent advancements broadly categorized into Score Matching [10,27,30,[34][35][36] and Flow Matching [5,6,20,24,45] approaches. Flow matching directly predicts a velocity, enabling a more interpretable transport process and faster convergence compared with score-based diffusion methods. Hence, recent SOTA in text-to-image/video models increasingly adopt flow matching. In this paper, we follow the unifying perspective on diffusion and flow models presented by Lipman et al. [22]. We broadly use the term flow matching models to refer to any model trained using flow matching, where samples are generated by solving an ordinary differential equation (ODE). \n\nNext, classifier-free guidance (CFG) [9,44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results. \n\nWe empirically analyze the effect of CFG when the learned velocity is underfitted, i.e., inaccurate, using a mixture of Gaussians as the data distribution.",
                    "score": 0.3869335149548918,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2054
                        },
                        {
                            "start": 2057,
                            "end": 2212
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 176,
                            "end": 180,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 180,
                            "end": 183,
                            "matchedPaperCorpusId": "254854389"
                        },
                        {
                            "start": 183,
                            "end": 186,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "222140788"
                        },
                        {
                            "start": 190,
                            "end": 194,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 217,
                            "end": 220,
                            "matchedPaperCorpusId": "268247980"
                        },
                        {
                            "start": 225,
                            "end": 228,
                            "matchedPaperCorpusId": "252111177"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models\n# Venue: arXiv.org\n# Authors: Weichen Fan, Amber Yijia Zheng, Raymond A. Yeh, Ziwei Liu\n## Abstract\nClassifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)\n## Introduction\nDiffusion and flow-based models are the state-of-the-art (SOTA) for generating high-quality images and videos, with recent advancements broadly categorized into Score Matching [10,27,30,[34][35][36] and Flow Matching [5,6,20,24,45] approaches. Flow matching directly predicts a velocity, enabling a more interpretable transport process and faster convergence compared with score-based diffusion methods. Hence, recent SOTA in text-to-image/video models increasingly adopt flow matching. In this paper, we follow the unifying perspective on diffusion and flow models presented by Lipman et al. [22]. We broadly use the term flow matching models to refer to any model trained using flow matching, where samples are generated by solving an ordinary differential equation (ODE). \n\nNext, classifier-free guidance (CFG) [9,44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results. \n\nWe empirically analyze the effect of CFG when the learned velocity is underfitted, i.e., inaccurate, using a mixture of Gaussians as the data distribution.\n\n## Related Work\nDiffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
            "reference_string": "[277510202 | Fan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "On the Design Fundamentals of Diffusion Models: A Survey",
            "venue": "Pattern Recognition",
            "year": 2023,
            "reference_count": 369,
            "citation_count": 56,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2306.04542",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186402069",
                    "name": "Ziyi Chang"
                },
                {
                    "authorId": "2872629",
                    "name": "G. Koulieris"
                },
                {
                    "authorId": "1993718816",
                    "name": "Hubert P. H. Shum"
                }
            ],
            "abstract": "Diffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.",
            "corpus_id": 259095911,
            "sentences": [
                {
                    "corpus_id": "259095911",
                    "title": "On the Design Fundamentals of Diffusion Models: A Survey",
                    "text": "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]. \n\nInstead of removing classifiers, self-guidance [166] reduces or removes the requirement of annotation by using internal values like activations, attention maps, and intermediate representations [198], [199] to compute guidance. Such design helps finer-grained control and is compatible with aforementioned guidance.",
                    "score": 0.37955109953353117,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 44026,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 137
                        },
                        {
                            "start": 138,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 588
                        },
                        {
                            "start": 591,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1264
                        },
                        {
                            "start": 1267,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1582
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: On the Design Fundamentals of Diffusion Models: A Survey\n# Venue: Pattern Recognition\n# Authors: Ziyi Chang, G. Koulieris, Hubert P. H. Shum\n## Abstract\nDiffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.\n## Classifier-Free Guidance\nTo avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]. \n\nInstead of removing classifiers, self-guidance [166] reduces or removes the requirement of annotation by using internal values like activations, attention maps, and intermediate representations [198], [199] to compute guidance. Such design helps finer-grained control and is compatible with aforementioned guidance.",
            "reference_string": "[259095911 | Chang et al. | 2023 | Citations: 56]"
        },
        {
            "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.07359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243157079",
                    "name": "Thomas Kerby"
                },
                {
                    "authorId": "2243003711",
                    "name": "Kevin R. Moon"
                }
            ],
            "abstract": "Training-free guidance methods for continuous data have seen an explosion of interest due to the fact that they enable foundation diffusion models to be paired with interchangable guidance models. Currently, equivalent guidance methods for discrete diffusion models are unknown. We present a framework for applying training-free guidance to discrete data and demonstrate its utility on molecular graph generation tasks using the discrete diffusion model architecture of DiGress. We pair this model with guidance functions that return the proportion of heavy atoms that are a specific atom type and the molecular weight of the heavy atoms and demonstrate our method's ability to guide the data generation.",
            "corpus_id": 272593086,
            "sentences": [
                {
                    "corpus_id": "272593086",
                    "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
                    "text": "Diffusion models are a powerful method for generating data from a given distribution. To enhance their utility, extensive research has focused on developing techniques for guiding the output rather than relying solely on unconditional generation. These guidance methods have significantly evolved since the introduction of classifier guidance [1], which established diffusion models as the state-of-the-art for image generation, surpassing GANs. Classifier guidance introduced a framework allowing the gradients from a classifier to influence the generation process. The main limitation of this method is that it requires the classifier to perform well on the data at all timesteps t. This necessitates training a guidance model with a specific noise scheduler to augment the training data across all timesteps. \n\nClassifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation [2]. During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training. \n\nA new flexible approach for guiding the sampling of diffusion models is training-free guidance, which allows guidance models to be paired with a diffusion model without requiring the guidance model to be trained on noisy data generated by the noise scheduler [3]- [6]. This enables the creation of foundation diffusion models that can be combined with guidance models in a plug-and-play manner. It also simplifies benchmarking and building on other researchers' work, as separate guidance models are unnecessary and can be easily shared. These improvements have made diffusion models the preferred architecture in many domains where control and human feedback are essential. Moreover, unlike autoregressive models, diffusion models learn the joint data distribution directly and do not rely on chaining conditional distributions. Their iterative nature also provides unique opportunities to guide the generation process in ways that autoregressive or purely conditional models cannot replicate. \n\nClassifier guidance [7] and classifier-free guidance [8] have previously been implemented for graph generation using a discrete diffusion model.",
                    "score": 0.5253885306490957,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 85
                        },
                        {
                            "start": 86,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1399
                        },
                        {
                            "start": 1402,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1939
                        },
                        {
                            "start": 1940,
                            "end": 2076
                        },
                        {
                            "start": 2077,
                            "end": 2231
                        },
                        {
                            "start": 2232,
                            "end": 2396
                        },
                        {
                            "start": 2399,
                            "end": 2543
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 343,
                            "end": 346,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 992,
                            "end": 995,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1661,
                            "end": 1664,
                            "matchedPaperCorpusId": "256846836"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Training-Free Guidance for Discrete Diffusion Models for Molecular Generation\n# Venue: arXiv.org\n# Authors: Thomas Kerby, Kevin R. Moon\n## Abstract\nTraining-free guidance methods for continuous data have seen an explosion of interest due to the fact that they enable foundation diffusion models to be paired with interchangable guidance models. Currently, equivalent guidance methods for discrete diffusion models are unknown. We present a framework for applying training-free guidance to discrete data and demonstrate its utility on molecular graph generation tasks using the discrete diffusion model architecture of DiGress. We pair this model with guidance functions that return the proportion of heavy atoms that are a specific atom type and the molecular weight of the heavy atoms and demonstrate our method's ability to guide the data generation.\n## I. INTRODUCTION\nDiffusion models are a powerful method for generating data from a given distribution. To enhance their utility, extensive research has focused on developing techniques for guiding the output rather than relying solely on unconditional generation. These guidance methods have significantly evolved since the introduction of classifier guidance [1], which established diffusion models as the state-of-the-art for image generation, surpassing GANs. Classifier guidance introduced a framework allowing the gradients from a classifier to influence the generation process. The main limitation of this method is that it requires the classifier to perform well on the data at all timesteps t. This necessitates training a guidance model with a specific noise scheduler to augment the training data across all timesteps. \n\nClassifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation [2]. During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training. \n\nA new flexible approach for guiding the sampling of diffusion models is training-free guidance, which allows guidance models to be paired with a diffusion model without requiring the guidance model to be trained on noisy data generated by the noise scheduler [3]- [6]. This enables the creation of foundation diffusion models that can be combined with guidance models in a plug-and-play manner. It also simplifies benchmarking and building on other researchers' work, as separate guidance models are unnecessary and can be easily shared. These improvements have made diffusion models the preferred architecture in many domains where control and human feedback are essential. Moreover, unlike autoregressive models, diffusion models learn the joint data distribution directly and do not rely on chaining conditional distributions. Their iterative nature also provides unique opportunities to guide the generation process in ways that autoregressive or purely conditional models cannot replicate. \n\nClassifier guidance [7] and classifier-free guidance [8] have previously been implemented for graph generation using a discrete diffusion model.",
            "reference_string": "[272593086 | Kerby et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 60,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144306665",
                    "name": "Xiaoming Zhao"
                },
                {
                    "authorId": "2281750850",
                    "name": "Alexander G. Schwing"
                }
            ],
            "abstract": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.",
            "corpus_id": 276961040,
            "sentences": [
                {
                    "corpus_id": "276961040",
                    "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
                    "text": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.",
                    "score": 0.46975744688900084,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                },
                {
                    "corpus_id": "276961040",
                    "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
                    "text": "Conditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
                    "score": 0.41334331526617074,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1269
                        },
                        {
                            "start": 1270,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1596
                        },
                        {
                            "start": 1599,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2179
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 762,
                            "end": 765,
                            "matchedPaperCorpusId": "52889459"
                        },
                        {
                            "start": 765,
                            "end": 768,
                            "matchedPaperCorpusId": "54482423"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "12174018"
                        },
                        {
                            "start": 876,
                            "end": 879,
                            "matchedPaperCorpusId": "271571434"
                        },
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 965,
                            "end": 968,
                            "matchedPaperCorpusId": "235694314"
                        },
                        {
                            "start": 968,
                            "end": 971,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 971,
                            "end": 974,
                            "matchedPaperCorpusId": "5560643"
                        },
                        {
                            "start": 1015,
                            "end": 1019,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 1022,
                            "end": 1025,
                            "matchedPaperCorpusId": "254096299"
                        },
                        {
                            "start": 1062,
                            "end": 1066,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1233,
                            "end": 1237,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "252596091"
                        },
                        {
                            "start": 1476,
                            "end": 1479,
                            "matchedPaperCorpusId": "271903235"
                        },
                        {
                            "start": 1479,
                            "end": 1482,
                            "matchedPaperCorpusId": "272770713"
                        },
                        {
                            "start": 1482,
                            "end": 1485,
                            "matchedPaperCorpusId": "273549917"
                        },
                        {
                            "start": 1773,
                            "end": 1777,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 2075,
                            "end": 2079,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective\n# Venue: arXiv.org\n# Authors: Xiaoming Zhao, Alexander G. Schwing\n## Abstract\nClassifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.\n## Introduction\nConditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
            "reference_string": "[276961040 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Opportunities and challenges of diffusion models for generative AI",
            "venue": "National Science Review",
            "year": 2024,
            "reference_count": 122,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1093/nsr/nwae348",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11562846, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258803874",
                    "name": "Minshuo Chen"
                },
                {
                    "authorId": "2295990628",
                    "name": "Song Mei"
                },
                {
                    "authorId": "2296663941",
                    "name": "Jianqing Fan"
                },
                {
                    "authorId": "2258997595",
                    "name": "Mengdi Wang"
                }
            ],
            "abstract": "ABSTRACT Diffusion models, a powerful and universal generative artificial intelligence technology, have achieved tremendous success and opened up new possibilities in diverse applications. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active control towards task-desired properties. Despite the significant empirical success, theoretical underpinnings of diffusion models are very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models to highlight their sample generation capabilities under various control goals. At the same time, we dive into the unique working flow of diffusion models through the lens of stochastic processes. We identify theoretical challenges in analyzing diffusion models, owing to their complicated training procedure and interaction with the underlying data distribution. To address these challenges, we overview several promising advances, demonstrating diffusion models as an efficient distribution learner and a sampler. Furthermore, we introduce a new avenue in high-dimensional structured optimization through diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded exposure for stimulating forward-looking theories and methods of diffusion models.",
            "corpus_id": 273142392,
            "sentences": [
                {
                    "corpus_id": "273142392",
                    "title": "Opportunities and challenges of diffusion models for generative AI",
                    "text": "Practical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method [104 ] which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ]. \n\nModulating guidance strength in practice.",
                    "score": 0.4141072172127024,
                    "section_title": "Classifier and classifier-free guidance",
                    "char_start_offset": 54606,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 536
                        },
                        {
                            "start": 539,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1116
                        },
                        {
                            "start": 1117,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1424
                        },
                        {
                            "start": 1427,
                            "end": 1452
                        },
                        {
                            "start": 1455,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1634
                        },
                        {
                            "start": 1637,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2028
                        },
                        {
                            "start": 2031,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 244,
                            "end": 250,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96533203125,
            "relevance_judgment_input_expanded": "# Title: Opportunities and challenges of diffusion models for generative AI\n# Venue: National Science Review\n# Authors: Minshuo Chen, Song Mei, Jianqing Fan, Mengdi Wang\n## Abstract\nABSTRACT Diffusion models, a powerful and universal generative artificial intelligence technology, have achieved tremendous success and opened up new possibilities in diverse applications. In these applications, diffusion models provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active control towards task-desired properties. Despite the significant empirical success, theoretical underpinnings of diffusion models are very limited, potentially slowing down principled methodological innovations for further harnessing and improving diffusion models. In this paper, we review emerging applications of diffusion models to highlight their sample generation capabilities under various control goals. At the same time, we dive into the unique working flow of diffusion models through the lens of stochastic processes. We identify theoretical challenges in analyzing diffusion models, owing to their complicated training procedure and interaction with the underlying data distribution. To address these challenges, we overview several promising advances, demonstrating diffusion models as an efficient distribution learner and a sampler. Furthermore, we introduce a new avenue in high-dimensional structured optimization through diffusion models, where searching for solutions is reformulated as a conditional sampling problem and solved by diffusion models. Lastly, we discuss future directions about diffusion models. The purpose of this paper is to provide a well-rounded exposure for stimulating forward-looking theories and methods of diffusion models.\n## Classifier and classifier-free guidance\nPractical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method [104 ] which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ]. \n\nModulating guidance strength in practice.",
            "reference_string": "[273142392 | Chen et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 46,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166847",
                    "name": "Qinqing Zheng"
                },
                {
                    "authorId": "2267723599",
                    "name": "Matt Le"
                },
                {
                    "authorId": "2219927868",
                    "name": "Neta Shaul"
                },
                {
                    "authorId": "3232072",
                    "name": "Y. Lipman"
                },
                {
                    "authorId": "2267723293",
                    "name": "Aditya Grover"
                },
                {
                    "authorId": "2253976277",
                    "name": "Ricky T. Q. Chen"
                }
            ],
            "abstract": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "corpus_id": 265351587,
            "sentences": [
                {
                    "corpus_id": "265351587",
                    "title": "Guided Flows for Generative Modeling and Decision Making",
                    "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
                    "score": 0.5139842026081904,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Guided Flows for Generative Modeling and Decision Making\n# Venue: arXiv.org\n# Authors: Qinqing Zheng, Matt Le, Neta Shaul, Y. Lipman, Aditya Grover, Ricky T. Q. Chen\n## Abstract\nClassifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.\n",
            "reference_string": "[265351587 | Zheng et al. | 2023 | Citations: 46]"
        },
        {
            "title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2337542688",
                    "name": "Hui Wang"
                },
                {
                    "authorId": "2334437685",
                    "name": "Shujie Liu"
                },
                {
                    "authorId": "2308630482",
                    "name": "Lingwei Meng"
                },
                {
                    "authorId": "2280887661",
                    "name": "Jinyu Li"
                },
                {
                    "authorId": "2135622500",
                    "name": "Yifan Yang"
                },
                {
                    "authorId": "2275765829",
                    "name": "Shiwan Zhao"
                },
                {
                    "authorId": "2338021437",
                    "name": "Haiyang Sun"
                },
                {
                    "authorId": "2290439718",
                    "name": "Yanqing Liu"
                },
                {
                    "authorId": "2275584632",
                    "name": "Haoqin Sun"
                },
                {
                    "authorId": "2275780366",
                    "name": "Jiaming Zhou"
                },
                {
                    "authorId": "2337241715",
                    "name": "Yan Lu"
                },
                {
                    "authorId": "2275736194",
                    "name": "Yong Qin"
                }
            ],
            "abstract": "To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.",
            "corpus_id": 276409208,
            "sentences": [
                {
                    "corpus_id": "276409208",
                    "title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching",
                    "text": "In this framework, the coarse flow-matching model predicts a vector field v c t (x i,c , z i ; \u03b8 c FM ) by conditioning on linguistic features z i extracted from the language model. In the fine stage, the model refines this approximation by recovering details x i,f , represented as the residual between the original frame x i and the upsampled coarse component Upsample(x i,c ). A secondary flow-matching model predicts the vector field v f t (x i,f , z i , x i,c ; \u03b8 f FM ), governing this process by leveraging both the features z i and the coarse component (with ground-truth coarse features x i,c during training and predicted values xi,c during inference) as conditional inputs. This hierarchical conditioning allows the fine model to focus on local details while preserving global coherence from the coarse stage. \n\nFor step i, the training objective combines losses from both stages: \n\nwhere u c t and u f t represent the true conditional vector fields for the coarse and fine components, respectively, and t \u223c U[0, 1]. The initial states x i,c 0 and x i,f 0 are similarly initialized using the prior from Equation 4, applying the corresponding sampling operations. By decoupling low-resolution structure learning from high-detail refinement, this coarse-to-fine approach generates high-fidelity mel-spectrograms while maintaining temporal consistency through autoregressive dependencies. \n\nClassifier-Free Guidance Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol andDhariwal, 2021]. In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale.",
                    "score": 0.4177398305914986,
                    "section_title": "Coarse-to-Fine Flow Matching",
                    "char_start_offset": 14552,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 820
                        },
                        {
                            "start": 823,
                            "end": 891
                        },
                        {
                            "start": 894,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1396
                        },
                        {
                            "start": 1399,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1931
                        },
                        {
                            "start": 1932,
                            "end": 2004
                        },
                        {
                            "start": 2007,
                            "end": 2199
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1614,
                            "end": 1629,
                            "matchedPaperCorpusId": "231979499"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching\n# Venue: arXiv.org\n# Authors: Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin\n## Abstract\nTo advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.\n## Coarse-to-Fine Flow Matching\nIn this framework, the coarse flow-matching model predicts a vector field v c t (x i,c , z i ; \u03b8 c FM ) by conditioning on linguistic features z i extracted from the language model. In the fine stage, the model refines this approximation by recovering details x i,f , represented as the residual between the original frame x i and the upsampled coarse component Upsample(x i,c ). A secondary flow-matching model predicts the vector field v f t (x i,f , z i , x i,c ; \u03b8 f FM ), governing this process by leveraging both the features z i and the coarse component (with ground-truth coarse features x i,c during training and predicted values xi,c during inference) as conditional inputs. This hierarchical conditioning allows the fine model to focus on local details while preserving global coherence from the coarse stage. \n\nFor step i, the training objective combines losses from both stages: \n\nwhere u c t and u f t represent the true conditional vector fields for the coarse and fine components, respectively, and t \u223c U[0, 1]. The initial states x i,c 0 and x i,f 0 are similarly initialized using the prior from Equation 4, applying the corresponding sampling operations. By decoupling low-resolution structure learning from high-detail refinement, this coarse-to-fine approach generates high-fidelity mel-spectrograms while maintaining temporal consistency through autoregressive dependencies. \n\nClassifier-Free Guidance Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol andDhariwal, 2021]. In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale.",
            "reference_string": "[276409208 | Wang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.11797",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.11797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186281333",
                    "name": "Nisha Huang"
                },
                {
                    "authorId": "1443761295",
                    "name": "Fan Tang"
                },
                {
                    "authorId": "40441149",
                    "name": "Weiming Dong"
                },
                {
                    "authorId": "39945992",
                    "name": "Tong-Yee Lee"
                },
                {
                    "authorId": "2155590336",
                    "name": "Changsheng Xu"
                }
            ],
            "abstract": "Image manipulation under the guidance of textual descriptions has recently received a broad range of attention. In this study, we focus on the regional editing of images with the guidance of given text prompts. Different from current mask-based image editing methods, we propose a novel region-aware diffusion model (RDM) for entity-level image editing, which could automatically locate the region of interest and replace it following given text prompts. To strike a balance between image fidelity and inference speed, we design the intensive diffusion pipeline by combing latent space diffusion and enhanced directional guidance. In addition, to preserve image content in non-edited regions, we introduce regional-aware entity editing to modify the region of interest and preserve the out-of-interest region. We validate the proposed RDM beyond the baseline methods through extensive qualitative and quantitative experiments. The results show that RDM outperforms the previous approaches in terms of visual quality, overall harmonization, non-editing region content preservation, and text-image semantic consistency. The codes are available at https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model.",
            "corpus_id": 257102889,
            "sentences": [
                {
                    "corpus_id": "257102889",
                    "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
                    "text": "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance [17] to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier. \n\nIn order to provide classifier-free guidance, the tag  in a class-conditional diffusion model   (  | ) is replaced with a null tag \u2205 throughout the training process. The output of the model is further extended in the direction of   (  | ) \n\nand away from   (  | \u2205) during sampling: \n\nThe recommended guidance scale is  = 5. This equation was inspired by the classifier, \n\nThe function of the true scores is used to represent the gradient  * , \n\nThe modified prediction \u03b5 is subsequently employed to guide us toward the target text prompts  2 , as demonstrated in Algorithm 1:",
                    "score": 0.4227106823263234,
                    "section_title": "Manuscript submitted to ACM",
                    "char_start_offset": 13072,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 31,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 902
                        },
                        {
                            "start": 905,
                            "end": 945
                        },
                        {
                            "start": 948,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1033
                        },
                        {
                            "start": 1036,
                            "end": 1106
                        },
                        {
                            "start": 1109,
                            "end": 1239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 162,
                            "end": 166,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Region-Aware Diffusion for Zero-shot Text-driven Image Editing\n# Venue: arXiv.org\n# Authors: Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, Changsheng Xu\n## Abstract\nImage manipulation under the guidance of textual descriptions has recently received a broad range of attention. In this study, we focus on the regional editing of images with the guidance of given text prompts. Different from current mask-based image editing methods, we propose a novel region-aware diffusion model (RDM) for entity-level image editing, which could automatically locate the region of interest and replace it following given text prompts. To strike a balance between image fidelity and inference speed, we design the intensive diffusion pipeline by combing latent space diffusion and enhanced directional guidance. In addition, to preserve image content in non-edited regions, we introduce regional-aware entity editing to modify the region of interest and preserve the out-of-interest region. We validate the proposed RDM beyond the baseline methods through extensive qualitative and quantitative experiments. The results show that RDM outperforms the previous approaches in terms of visual quality, overall harmonization, non-editing region content preservation, and text-image semantic consistency. The codes are available at https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model.\n## Manuscript submitted to ACM\nEnhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance [17] to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier. \n\nIn order to provide classifier-free guidance, the tag  in a class-conditional diffusion model   (  | ) is replaced with a null tag \u2205 throughout the training process. The output of the model is further extended in the direction of   (  | ) \n\nand away from   (  | \u2205) during sampling: \n\nThe recommended guidance scale is  = 5. This equation was inspired by the classifier, \n\nThe function of the true scores is used to represent the gradient  * , \n\nThe modified prediction \u03b5 is subsequently employed to guide us toward the target text prompts  2 , as demonstrated in Algorithm 1:",
            "reference_string": "[257102889 | Huang et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 24,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.04302",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145915052",
                    "name": "Yuechen Zhang"
                },
                {
                    "authorId": "152230789",
                    "name": "Shengju Qian"
                },
                {
                    "authorId": "2272493196",
                    "name": "Bohao Peng"
                },
                {
                    "authorId": "25059098",
                    "name": "Shu Liu"
                },
                {
                    "authorId": "2273012826",
                    "name": "Jiaya Jia"
                }
            ],
            "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.",
            "corpus_id": 266053531,
            "sentences": [
                {
                    "corpus_id": "266053531",
                    "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                    "text": "In conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
                    "score": 0.4550665595867623,
                    "section_title": "Token-Level Highlight Guidance",
                    "char_start_offset": 10683,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 979
                        },
                        {
                            "start": 982,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1894
                        },
                        {
                            "start": 1895,
                            "end": 1966
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                },
                {
                    "corpus_id": "266053531",
                    "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                    "text": "Prompt engineering and interactions. Based on the autoregressive property of LLMs, users aim to control the generation results by modifying the input contexts. This largely determines the test-time interactions with LLMs, primarily executed through prompt engineering. Representative methods such as CoT [17] introduce demonstrations in the context to enhance reasoning ability. Other multi-branch designs like ToT and GoT [16,18,19,33,34] have been proposed for rich and reliable context generation and self-checking. Aside from prompt engineering, human-model interactions have not been extensively explored in VLMs. Methods like Kosmos-2 [31], LLaVAInteractive [35], LISA [36], and AlphaCLIP [37] enable grounding perception tasks such as detection, segmentation, caption, and image editing through interaction with LLMs. These task-oriented interactions require additional data collection and task-specific tuning. In contrast, Prompt Highlighter is plug-and-play for general text generation in pretrained models. Classifier-free guidance and controllable generation. Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation. Besides, methods in Diffusion Models [38,39] achieve finegrained control over image generation using text prompts by emphasizing areas within cross-attention maps. Finegrained control over autoregressive generation in LLMs and VLMs is still challenging. Later concurrent works CRG and MARINE [40,41], adopt CFG in VLMs for grounding and mitigating hallucination, but employ a different design for positive-negative pairs compared to our approach.",
                    "score": 0.4002901212050553,
                    "section_title": "Interactions with Multi-Modal LLMs",
                    "char_start_offset": 7961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 36
                        },
                        {
                            "start": 37,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1384
                        },
                        {
                            "start": 1385,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1698
                        },
                        {
                            "start": 1699,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 304,
                            "end": 308,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1736,
                            "end": 1740,
                            "matchedPaperCorpusId": "258108187"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.96142578125,
            "relevance_judgment_input_expanded": "# Title: Prompt Highlighter: Interactive Control for Multi-Modal LLMs\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia\n## Abstract\nThis study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.\n## Interactions with Multi-Modal LLMs\nPrompt engineering and interactions. Based on the autoregressive property of LLMs, users aim to control the generation results by modifying the input contexts. This largely determines the test-time interactions with LLMs, primarily executed through prompt engineering. Representative methods such as CoT [17] introduce demonstrations in the context to enhance reasoning ability. Other multi-branch designs like ToT and GoT [16,18,19,33,34] have been proposed for rich and reliable context generation and self-checking. Aside from prompt engineering, human-model interactions have not been extensively explored in VLMs. Methods like Kosmos-2 [31], LLaVAInteractive [35], LISA [36], and AlphaCLIP [37] enable grounding perception tasks such as detection, segmentation, caption, and image editing through interaction with LLMs. These task-oriented interactions require additional data collection and task-specific tuning. In contrast, Prompt Highlighter is plug-and-play for general text generation in pretrained models. Classifier-free guidance and controllable generation. Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation. Besides, methods in Diffusion Models [38,39] achieve finegrained control over image generation using text prompts by emphasizing areas within cross-attention maps. Finegrained control over autoregressive generation in LLMs and VLMs is still challenging. Later concurrent works CRG and MARINE [40,41], adopt CFG in VLMs for grounding and mitigating hallucination, but employ a different design for positive-negative pairs compared to our approach.\n\n## Token-Level Highlight Guidance\nIn conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
            "reference_string": "[266053531 | Zhang et al. | 2023 | Citations: 24]"
        },
        {
            "title": "On Distillation of Guided Diffusion Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 535,
            "influential_citation_count": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.03142",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.03142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "83262128",
                    "name": "Chenlin Meng"
                },
                {
                    "authorId": "9659905",
                    "name": "Ruiqi Gao"
                },
                {
                    "authorId": "1726807",
                    "name": "Diederik P. Kingma"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                },
                {
                    "authorId": "2887364",
                    "name": "Tim Salimans"
                }
            ],
            "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL.E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet $64\\times 64$ and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet $256\\times 256$ and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2\u20134 denoising steps.",
            "corpus_id": 252762155,
            "sentences": [
                {
                    "corpus_id": "252762155",
                    "title": "On Distillation of Guided Diffusion Models",
                    "text": "Denoising diffusion probabilistic models (DDPMs) [4,37,39,40] have achieved state-of-the-art performance on image generation [22,[26][27][28]31], audio synthesis [11], molecular generation [44], and likelihood estimation [10]. Classifierfree guidance [6] further improves the sample quality of diffusion models and has been widely used in large-scale diffusion model frameworks including GLIDE [23], Stable Diffusion [28], DALL\u00b7E 2 [26], and Imagen [31]. However, one key limitation of classifier-free guidance is its low sampling efficiency-it requires evaluating two diffusion models tens to hundreds of times to generate one sample. This limitation has hindered the application of classifier-free guidance models in real-world settings. Although distillation approaches have been proposed for diffusion models [33,38], these approaches are not directly applicable to classifier-free guided diffusion models. To deal with this issue, we propose a two-stage distillation approach to improving the sampling efficiency of classifier-free guided models. In the first stage, we introduce a single student model to match the combined output of the two diffusion models of the teacher. In the second stage, we progressively distill the model learned from the first stage to a fewer-step model using the approach introduced in [33]. Using our approach, a single distilled model is able to handle a wide range of different guidance strengths, allowing for the trade-off between sample quality and diversity efficiently. To sample from our model, we consider existing deterministic samplers in the literature [33,38] and further propose a stochastic sampling process.\n\nOur distillation framework can not only be applied to standard diffusion models trained on the pixel-space [4,36,39], but also diffusion models trained on the latent-space of an autoencoder [28,35] (e.g., Stable Diffusion [28]). For diffusion models directly trained on the pixel-space, our experiments on ImageNet 64x64 and CIFAR-10 show that the proposed distilled model can generate samples visually comparable to that of the teacher using only 4 steps and is able to achieve",
                    "score": 0.3781926412736706,
                    "section_title": "Introduction",
                    "char_start_offset": 1206,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 52,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 55,
                            "end": 58,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 58,
                            "end": 61,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 125,
                            "end": 129,
                            "matchedPaperCorpusId": "231979499"
                        },
                        {
                            "start": 133,
                            "end": 137,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 137,
                            "end": 141,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 141,
                            "end": 144,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 162,
                            "end": 166,
                            "matchedPaperCorpusId": "221818900"
                        },
                        {
                            "start": 417,
                            "end": 421,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 449,
                            "end": 453,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 817,
                            "end": 820,
                            "matchedPaperCorpusId": "222140788"
                        },
                        {
                            "start": 1605,
                            "end": 1608,
                            "matchedPaperCorpusId": "222140788"
                        },
                        {
                            "start": 1768,
                            "end": 1771,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 1771,
                            "end": 1774,
                            "matchedPaperCorpusId": "14888175"
                        },
                        {
                            "start": 1774,
                            "end": 1777,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 1851,
                            "end": 1855,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1855,
                            "end": 1858,
                            "matchedPaperCorpusId": "235421901"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96142578125
                }
            ],
            "relevance_judgement": 0.96142578125,
            "relevance_judgment_input_expanded": "# Title: On Distillation of Guided Diffusion Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans\n## Abstract\nClassifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL.E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet $64\\times 64$ and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet $256\\times 256$ and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2\u20134 denoising steps.\n## Introduction\nDenoising diffusion probabilistic models (DDPMs) [4,37,39,40] have achieved state-of-the-art performance on image generation [22,[26][27][28]31], audio synthesis [11], molecular generation [44], and likelihood estimation [10]. Classifierfree guidance [6] further improves the sample quality of diffusion models and has been widely used in large-scale diffusion model frameworks including GLIDE [23], Stable Diffusion [28], DALL\u00b7E 2 [26], and Imagen [31]. However, one key limitation of classifier-free guidance is its low sampling efficiency-it requires evaluating two diffusion models tens to hundreds of times to generate one sample. This limitation has hindered the application of classifier-free guidance models in real-world settings. Although distillation approaches have been proposed for diffusion models [33,38], these approaches are not directly applicable to classifier-free guided diffusion models. To deal with this issue, we propose a two-stage distillation approach to improving the sampling efficiency of classifier-free guided models. In the first stage, we introduce a single student model to match the combined output of the two diffusion models of the teacher. In the second stage, we progressively distill the model learned from the first stage to a fewer-step model using the approach introduced in [33]. Using our approach, a single distilled model is able to handle a wide range of different guidance strengths, allowing for the trade-off between sample quality and diversity efficiently. To sample from our model, we consider existing deterministic samplers in the literature [33,38] and further propose a stochastic sampling process.\n\nOur distillation framework can not only be applied to standard diffusion models trained on the pixel-space [4,36,39], but also diffusion models trained on the latent-space of an autoencoder [28,35] (e.g., Stable Diffusion [28]). For diffusion models directly trained on the pixel-space, our experiments on ImageNet 64x64 and CIFAR-10 show that the proposed distilled model can generate samples visually comparable to that of the teacher using only 4 steps and is able to achieve",
            "reference_string": "[252762155 | Meng et al. | 2022 | Citations: 535]"
        },
        {
            "title": "Stay on topic with Classifier-Free Guidance",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 55,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.17806",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.17806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056723344",
                    "name": "Guillaume Sanchez"
                },
                {
                    "authorId": "2072838294",
                    "name": "Honglu Fan"
                },
                {
                    "authorId": "51444076",
                    "name": "Alexander Spangher"
                },
                {
                    "authorId": "34490455",
                    "name": "Elad Levi"
                },
                {
                    "authorId": "1451644426",
                    "name": "Pawan Sasanka Ammanamanchi"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
            "corpus_id": 259308807,
            "sentences": [
                {
                    "corpus_id": "259308807",
                    "title": "Stay on topic with Classifier-Free Guidance",
                    "text": "We have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
                    "score": 0.43075533964675494,
                    "section_title": "Conclusion",
                    "char_start_offset": 20190,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 888
                        },
                        {
                            "start": 891,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1236
                        },
                        {
                            "start": 1239,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2243
                        },
                        {
                            "start": 2246,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.95947265625,
            "relevance_judgment_input_expanded": "# Title: Stay on topic with Classifier-Free Guidance\n# Venue: arXiv.org\n# Authors: Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, Stella Biderman\n## Abstract\nClassifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n## Conclusion\nWe have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
            "reference_string": "[259308807 | Sanchez et al. | 2023 | Citations: 55]"
        },
        {
            "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109850978",
                    "name": "Yulai Zhao"
                },
                {
                    "authorId": "2279021651",
                    "name": "Masatoshi Uehara"
                },
                {
                    "authorId": "19205151",
                    "name": "Gabriele Scalia"
                },
                {
                    "authorId": "6785594",
                    "name": "Tommaso Biancalani"
                },
                {
                    "authorId": "2286303828",
                    "name": "Sergey Levine"
                },
                {
                    "authorId": "2300473822",
                    "name": "Ehsan Hajiramezanali"
                }
            ],
            "abstract": "Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\\textbf{CTRL}$ ($\\textbf{C}$onditioning pre-$\\textbf{T}$rained diffusion models with $\\textbf{R}$einforcement $\\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL.",
            "corpus_id": 270562252,
            "sentences": [
                {
                    "corpus_id": "270562252",
                    "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
                    "text": "Classfier guidance. Dhariwal and Nichol (2021); Song et al. (2020) introduced classifier guidance, a method that entails training a classifier and incorporating its gradients to guide inference (while freezing pre-trained models). However, a notable drawback of this technique lies in the classifier's accuracy in predicting y from intermediate x t , resulting in cumulative errors during the diffusion process. To mitigate this issue, several studies propose methods to circumvent it through reconstruction, referred to as reconstruction guidance in this work. Specifically, they employ certain approximations that map intermediate states x t back to the original input space x 0 , allowing the classifier to be learned solely from x 0 to y (Ho et al., 2022;Han et al., 2022;Chung et al., 2022;Finzi et al., 2023;Bansal et al., 2023). In contrast to these works, our approach focuses on fine-tuning the diffusion model itself rather than relying on an inference-time technique. While the strict comparison between model fine-tuning and inference-time techniques is not feasible, we theoretically elucidate the distinctions and connections of our approach with classifier guidance in Section 5.1. \n\nClassfier-free guidance. Classifier-free guidance (Ho and Salimans, 2022) is a method that directly conditions the generative process on both data and context, bypassing the need for explicit classifiers. This methodology has been widely and effectively applied, for example, in text-to-image models (Nichol et al., 2021;Saharia et al., 2022;Rombach et al., 2022). While the original research does not explore classifier-free guidance within the scope of fine-tuning pre-trained diffusion models, several subsequent studies address fine-tuning scenarios Zhang et al. (2023); Xie et al. (2023). As elucidated in Section 5.2, compared to classifier-free guidance, our approach can improve sample efficiency and leverage conditional independence to facilitate offline dataset construction. \n\nFine-tuning via RL. Several previous studies have addressed the fine-tuning of diffusion models by optimizing relevant reward functions.",
                    "score": 0.3832814172581209,
                    "section_title": "RELATED WORKS",
                    "char_start_offset": 2298,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1196
                        },
                        {
                            "start": 1199,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1985
                        },
                        {
                            "start": 1988,
                            "end": 2007
                        },
                        {
                            "start": 2008,
                            "end": 2124
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 46,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 759,
                            "end": 776,
                            "matchedPaperCorpusId": "249674651"
                        },
                        {
                            "start": 795,
                            "end": 814,
                            "matchedPaperCorpusId": "259145376"
                        },
                        {
                            "start": 814,
                            "end": 834,
                            "matchedPaperCorpusId": "256846836"
                        },
                        {
                            "start": 1520,
                            "end": 1541,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1541,
                            "end": 1562,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1774,
                            "end": 1791,
                            "matchedPaperCorpusId": "258108246"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: Adding Conditional Control to Diffusion Models with Reinforcement Learning\n# Venue: arXiv.org\n# Authors: Yulai Zhao, Masatoshi Uehara, Gabriele Scalia, Tommaso Biancalani, Sergey Levine, Ehsan Hajiramezanali\n## Abstract\nDiffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\\textbf{CTRL}$ ($\\textbf{C}$onditioning pre-$\\textbf{T}$rained diffusion models with $\\textbf{R}$einforcement $\\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL.\n## RELATED WORKS\nClassfier guidance. Dhariwal and Nichol (2021); Song et al. (2020) introduced classifier guidance, a method that entails training a classifier and incorporating its gradients to guide inference (while freezing pre-trained models). However, a notable drawback of this technique lies in the classifier's accuracy in predicting y from intermediate x t , resulting in cumulative errors during the diffusion process. To mitigate this issue, several studies propose methods to circumvent it through reconstruction, referred to as reconstruction guidance in this work. Specifically, they employ certain approximations that map intermediate states x t back to the original input space x 0 , allowing the classifier to be learned solely from x 0 to y (Ho et al., 2022;Han et al., 2022;Chung et al., 2022;Finzi et al., 2023;Bansal et al., 2023). In contrast to these works, our approach focuses on fine-tuning the diffusion model itself rather than relying on an inference-time technique. While the strict comparison between model fine-tuning and inference-time techniques is not feasible, we theoretically elucidate the distinctions and connections of our approach with classifier guidance in Section 5.1. \n\nClassfier-free guidance. Classifier-free guidance (Ho and Salimans, 2022) is a method that directly conditions the generative process on both data and context, bypassing the need for explicit classifiers. This methodology has been widely and effectively applied, for example, in text-to-image models (Nichol et al., 2021;Saharia et al., 2022;Rombach et al., 2022). While the original research does not explore classifier-free guidance within the scope of fine-tuning pre-trained diffusion models, several subsequent studies address fine-tuning scenarios Zhang et al. (2023); Xie et al. (2023). As elucidated in Section 5.2, compared to classifier-free guidance, our approach can improve sample efficiency and leverage conditional independence to facilitate offline dataset construction. \n\nFine-tuning via RL. Several previous studies have addressed the fine-tuning of diffusion models by optimizing relevant reward functions.",
            "reference_string": "[270562252 | Zhao et al. | 2024 | Citations: 7]"
        },
        {
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 35,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110872233",
                    "name": "Hyungjin Chung"
                },
                {
                    "authorId": "2109216792",
                    "name": "Jeongsol Kim"
                },
                {
                    "authorId": "153118937",
                    "name": "Geon Yeong Park"
                },
                {
                    "authorId": "2268758810",
                    "name": "Hyelin Nam"
                },
                {
                    "authorId": "2254155658",
                    "name": "Jong Chul Ye"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "corpus_id": 270391454,
            "sentences": [
                {
                    "corpus_id": "270391454",
                    "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
                    "text": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
                    "score": 0.3933625054004025,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models\n# Venue: arXiv.org\n# Authors: Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, Jong Chul Ye\n## Abstract\nClassifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.\n",
            "reference_string": "[270391454 | Chung et al. | 2024 | Citations: 35]"
        },
        {
            "title": "Understanding and Improving Training-free Loss-based Diffusion Guidance",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 14,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.12404, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152966656",
                    "name": "Yifei Shen"
                },
                {
                    "authorId": "2268347004",
                    "name": "Xinyang Jiang"
                },
                {
                    "authorId": "2115738764",
                    "name": "Yezhen Wang"
                },
                {
                    "authorId": "2124527879",
                    "name": "Yifan Yang"
                },
                {
                    "authorId": "2268676735",
                    "name": "Dongqi Han"
                },
                {
                    "authorId": "2268313028",
                    "name": "Dongsheng Li"
                }
            ],
            "abstract": "Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free loss-based guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of training-free guidance, as well as overcome its limitations. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free guidance is more susceptible to adversarial gradients and exhibits slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.",
            "corpus_id": 268531795,
            "sentences": [
                {
                    "corpus_id": "268531795",
                    "title": "Understanding and Improving Training-free Loss-based Diffusion Guidance",
                    "text": "A unique advantage of training-free guidance is that it can be applied to universal control formats, such as style, layout, and FaceID [2,46,35] without any additional training efforts.Furthermore, these algorithms have been successfully applied to offline reinforcement learning, enabling agents to achieve novel goals not previously encountered during training [42].In contrast to classifier guidance and classifier-free guidance, it is proved in [25] that training-free guidance does not offer an approximation to the exact conditional energy.Therefore, from a theoretical perspective, it is intriguing to understand how and when these methods succeed or fail.From an empirical standpoint, it is crucial to develop algorithms that can address and overcome these limitations.\n\nThis paper seeks to deepen the understanding of training-free guidance by examining its mechanisms and inherent limitations, as well as overcoming these limitations.Specifically, our major contributions can be summarized as follows:\n\n\u2022 How does training-free guidance work?Although exact conditional energy is difficult to approximate in a training-free manner, from the optimization standpoint, we show that training-free guidance can effectively decrease the guidance loss function.The optimization perspective clarifies the mystery of why the guidance weights should be meticulously designed in relation to the guidance function and diffusion time, as observed in [46].\u2022 When does training-free guidance not work?We theoretically identify the susceptibility of training-free guidance to adversarial gradient issues and slower convergence rates.We attribute these challenges to a decrease in the smoothness of the guidance network in contrast to the classifier guidance.\u2022 Improving training-free guidance: We introduce random augmentation to alleviate the adversarial gradient and Polyak step size scheduling to improve convergence.The efficacy of these methods is empirically confirmed across various diffusion models (i.e., image diffusion and motion diffusion) and under multiple conditions (i.e., segmentation, sketch, text, object avoidance, and targeting)2 .",
                    "score": 0.4000511360962318,
                    "section_title": "Introduction",
                    "char_start_offset": 2070,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 185,
                            "end": 368
                        },
                        {
                            "start": 368,
                            "end": 546
                        },
                        {
                            "start": 546,
                            "end": 663
                        },
                        {
                            "start": 663,
                            "end": 777
                        },
                        {
                            "start": 779,
                            "end": 944
                        },
                        {
                            "start": 944,
                            "end": 1011
                        },
                        {
                            "start": 1013,
                            "end": 1052
                        },
                        {
                            "start": 1052,
                            "end": 1263
                        },
                        {
                            "start": 1263,
                            "end": 1451
                        },
                        {
                            "start": 1451,
                            "end": 1495
                        },
                        {
                            "start": 1495,
                            "end": 1626
                        },
                        {
                            "start": 1626,
                            "end": 1751
                        },
                        {
                            "start": 1751,
                            "end": 1913
                        },
                        {
                            "start": 1913,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 135,
                            "end": 138,
                            "matchedPaperCorpusId": "256846836"
                        },
                        {
                            "start": 363,
                            "end": 367,
                            "matchedPaperCorpusId": "266162507"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Understanding and Improving Training-free Loss-based Diffusion Guidance\n# Venue: Neural Information Processing Systems\n# Authors: Yifei Shen, Xinyang Jiang, Yezhen Wang, Yifan Yang, Dongqi Han, Dongsheng Li\n## Abstract\nAdding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free loss-based guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of training-free guidance, as well as overcome its limitations. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free guidance is more susceptible to adversarial gradients and exhibits slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.\n## Introduction\nA unique advantage of training-free guidance is that it can be applied to universal control formats, such as style, layout, and FaceID [2,46,35] without any additional training efforts.Furthermore, these algorithms have been successfully applied to offline reinforcement learning, enabling agents to achieve novel goals not previously encountered during training [42].In contrast to classifier guidance and classifier-free guidance, it is proved in [25] that training-free guidance does not offer an approximation to the exact conditional energy.Therefore, from a theoretical perspective, it is intriguing to understand how and when these methods succeed or fail.From an empirical standpoint, it is crucial to develop algorithms that can address and overcome these limitations.\n\nThis paper seeks to deepen the understanding of training-free guidance by examining its mechanisms and inherent limitations, as well as overcoming these limitations.Specifically, our major contributions can be summarized as follows:\n\n\u2022 How does training-free guidance work?Although exact conditional energy is difficult to approximate in a training-free manner, from the optimization standpoint, we show that training-free guidance can effectively decrease the guidance loss function.The optimization perspective clarifies the mystery of why the guidance weights should be meticulously designed in relation to the guidance function and diffusion time, as observed in [46].\u2022 When does training-free guidance not work?We theoretically identify the susceptibility of training-free guidance to adversarial gradient issues and slower convergence rates.We attribute these challenges to a decrease in the smoothness of the guidance network in contrast to the classifier guidance.\u2022 Improving training-free guidance: We introduce random augmentation to alleviate the adversarial gradient and Polyak step size scheduling to improve convergence.The efficacy of these methods is empirically confirmed across various diffusion models (i.e., image diffusion and motion diffusion) and under multiple conditions (i.e., segmentation, sketch, text, object avoidance, and targeting)2 .",
            "reference_string": "[268531795 | Shen et al. | 2024 | Citations: 14]"
        },
        {
            "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
            "venue": "International Society for Music Information Retrieval Conference",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 9,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312327383",
                    "name": "Yun-Han Lan"
                },
                {
                    "authorId": "37188394",
                    "name": "Wen-Yi Hsiao"
                },
                {
                    "authorId": "2312341186",
                    "name": "Hao-Chung Cheng"
                },
                {
                    "authorId": "2312339261",
                    "name": "Yi-Hsuan Yang"
                }
            ],
            "abstract": "Existing text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted rhythm and chords as the condition signal. During inference, the condition can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets -- one derived from extracted features and the other from user-created inputs -- demonstrates that MusiConGen can generate realistic backing track music that aligns well with the specified conditions. We open-source the code and model checkpoints, and provide audio examples online, https://musicongen.github.io/musicongen_demo/.",
            "corpus_id": 271328459,
            "sentences": [
                {
                    "corpus_id": "271328459",
                    "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
                    "text": "Classifier-free guidance [22] is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits.",
                    "score": 0.3832814172581209,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 6690,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 205,
                            "end": 391
                        },
                        {
                            "start": 391,
                            "end": 424
                        },
                        {
                            "start": 426,
                            "end": 525
                        },
                        {
                            "start": 525,
                            "end": 618
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.947265625,
            "relevance_judgment_input_expanded": "# Title: MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation\n# Venue: International Society for Music Information Retrieval Conference\n# Authors: Yun-Han Lan, Wen-Yi Hsiao, Hao-Chung Cheng, Yi-Hsuan Yang\n## Abstract\nExisting text-to-music models can produce high-quality audio with great diversity. However, textual prompts alone cannot precisely control temporal musical features such as chords and rhythm of the generated music. To address this challenge, we introduce MusiConGen, a temporally-conditioned Transformer-based text-to-music model that builds upon the pretrained MusicGen framework. Our innovation lies in an efficient finetuning mechanism, tailored for consumer-grade GPUs, that integrates automatically-extracted rhythm and chords as the condition signal. During inference, the condition can either be musical features extracted from a reference audio signal, or be user-defined symbolic chord sequence, BPM, and textual prompts. Our performance evaluation on two datasets -- one derived from extracted features and the other from user-created inputs -- demonstrates that MusiConGen can generate realistic backing track music that aligns well with the specified conditions. We open-source the code and model checkpoints, and provide audio examples online, https://musicongen.github.io/musicongen_demo/.\n## Classifier-Free Guidance\nClassifier-free guidance [22] is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits.",
            "reference_string": "[271328459 | Lan et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "venue": "",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278618341",
                    "name": "Yuzhe Liang"
                },
                {
                    "authorId": "2358111037",
                    "name": "Wenzhe Liu"
                },
                {
                    "authorId": "2358041541",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "2229877177",
                    "name": "Zhikang Niu"
                },
                {
                    "authorId": "2324996330",
                    "name": "Yushen Chen"
                },
                {
                    "authorId": "2116609277",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2278584538",
                    "name": "Wenxi Chen"
                },
                {
                    "authorId": "2358116915",
                    "name": "Nan Li"
                },
                {
                    "authorId": "2358098456",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2321881822",
                    "name": "Xie Chen"
                }
            ],
            "abstract": "Flow matching has demonstrated strong generative capabilities and has become a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality speech synthesis, Classifier-Free Guidance (CFG) is widely used during the inference of flow-matching-based TTS models. However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios. In this paper, we explore removing CFG from flow-matching-based TTS models to improve inference efficiency, while maintaining performance. Specifically, we reformulated the flow matching training target to directly approximate the CFG optimization trajectory. This training method eliminates the need for unconditional model evaluation and guided tuning during inference, effectively cutting the computational overhead in half. Furthermore, It can be seamlessly integrated with existing optimized sampling strategies. We validate our approach using the F5-TTS model on the LibriTTS dataset. Experimental results show that our method achieves a 9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving comparable speech quality. We will release the code and models to support reproducibility and foster further research in this area.",
            "corpus_id": 278171703,
            "sentences": [
                {
                    "corpus_id": "278171703",
                    "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                    "text": "However, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.",
                    "score": 0.4256204288669282,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 1902,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1545
                        },
                        {
                            "start": 1548,
                            "end": 1592
                        },
                        {
                            "start": 1595,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94580078125
                },
                {
                    "corpus_id": "278171703",
                    "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                    "text": "When w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1  for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival. V. CONCLUSION Given the rising adoption of flow matching in modern text-tospeech (TTS) models, optimizing the inference efficiency of these models has become increasingly important, as they typically require multiple sampling steps to achieve satisfactory performance. While most prior work has focused on reducing the number of sampling steps, another critical issue lies in the use of Classifier-Free Guidance (CFG), which requires two forward passes and introduces additional computational overhead. In this paper, we address this issue by exploring how to remove CFG from flow-matching-based TTS models, enabling a single forward pass per sampling step. By reformulating the flow-matching training objective to directly approximate the output of CFG inference, we eliminate the need for unconditional predictions and guidance scale adjustments during inference. This approach significantly reduces computational cost and simplifies the inference procedure. Using F5-TTS as a case study, we demonstrate that inference cost can be effectively halved without sacrificing speech quality. In future work, we plan to extend this method to other flow-matching-based TTS models and evaluate its effectiveness on larger datasets.",
                    "score": 0.42795721917493756,
                    "section_title": "D. Evaluation Results",
                    "char_start_offset": 15873,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 63
                        },
                        {
                            "start": 64,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1816
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                }
            ],
            "relevance_judgement": 0.94580078125,
            "relevance_judgment_input_expanded": "# Title: Towards Flow-Matching-based TTS without Classifier-Free Guidance\n# Venue: \n# Authors: Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, Zhikang Niu, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen\n## Abstract\nFlow matching has demonstrated strong generative capabilities and has become a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality speech synthesis, Classifier-Free Guidance (CFG) is widely used during the inference of flow-matching-based TTS models. However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios. In this paper, we explore removing CFG from flow-matching-based TTS models to improve inference efficiency, while maintaining performance. Specifically, we reformulated the flow matching training target to directly approximate the CFG optimization trajectory. This training method eliminates the need for unconditional model evaluation and guided tuning during inference, effectively cutting the computational overhead in half. Furthermore, It can be seamlessly integrated with existing optimized sampling strategies. We validate our approach using the F5-TTS model on the LibriTTS dataset. Experimental results show that our method achieves a 9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving comparable speech quality. We will release the code and models to support reproducibility and foster further research in this area.\n## I. INTRODUCTION\nHowever, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.\n\n## D. Evaluation Results\nWhen w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1  for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival. V. CONCLUSION Given the rising adoption of flow matching in modern text-tospeech (TTS) models, optimizing the inference efficiency of these models has become increasingly important, as they typically require multiple sampling steps to achieve satisfactory performance. While most prior work has focused on reducing the number of sampling steps, another critical issue lies in the use of Classifier-Free Guidance (CFG), which requires two forward passes and introduces additional computational overhead. In this paper, we address this issue by exploring how to remove CFG from flow-matching-based TTS models, enabling a single forward pass per sampling step. By reformulating the flow-matching training objective to directly approximate the output of CFG inference, we eliminate the need for unconditional predictions and guidance scale adjustments during inference. This approach significantly reduces computational cost and simplifies the inference procedure. Using F5-TTS as a case study, we demonstrate that inference cost can be effectively halved without sacrificing speech quality. In future work, we plan to extend this method to other flow-matching-based TTS models and evaluate its effectiveness on larger datasets.",
            "reference_string": "[278171703 | Liang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.08942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129786387",
                    "name": "Elvis Nava"
                },
                {
                    "authorId": "51194506",
                    "name": "Seijin Kobayashi"
                },
                {
                    "authorId": "2023392128",
                    "name": "Yifei Yin"
                },
                {
                    "authorId": "50191333",
                    "name": "Robert K. Katzschmann"
                },
                {
                    "authorId": "48117063",
                    "name": "B. Grewe"
                }
            ],
            "abstract": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
            "corpus_id": 256416107,
            "sentences": [
                {
                    "corpus_id": "256416107",
                    "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
                    "text": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
                    "score": 0.4263703525073206,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9423828125
                },
                {
                    "corpus_id": "256416107",
                    "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
                    "text": "equivalent conditioning modalities for adaptation (Figure 1). What we show is that popular techniques for the image domain, such as CLIP-based guidance (Gal et al., 2021;Patashnik et al., 2021), denoising diffusion models (Ho et al., 2020), and classifier-free guidance (Dhariwal & Nichol, 2021;Ho & Salimans, 2021;Nichol et al., 2022) can be repurposed for the meta-learning setting to generate adapted neural network weights. Using multi-step adaptation instead of traditionally conditioning the model on the natural language task information allows our models to achieve higher performance on each task by breaking down computations into multiple steps. \n\nWe approach the generation of neural network weights in two separate phases. In the unconditional pre-training phase, we train a generative hypernetwork (Ha et al., 2016;Sch\u00fcrholt et al., 2022) to map from its latent space to the weight space of a base model (Figure 2.A). In the guidance phase, we learn language-conditioned models that can be used to traverse the hypernetwork latent space and find zeroshot adapted weights with high performance on our task (Figure 2.B and 2.C). Our methods can thus benefit from large scale data through the pre-training phase, even when natural language descriptions are not available for all tasks. \n\nWe summarise our contributions as follows: \n\n1) We introduce HyperCLIP, a contrastive learning method equivalent to Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), producing CLIP embeddings of fine-tuned neural network weights. Using HyperCLIP as a guidance model then allows us to find task-adapted networks in the latent space of a hypernetwork model (Figure 2.B). \n\n2) We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifierfree guidance (Ho & Salimans, 2021) improves the performance of generated base networks.",
                    "score": 0.4081522265376403,
                    "section_title": "Introduction",
                    "char_start_offset": 4086,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 61
                        },
                        {
                            "start": 62,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 656
                        },
                        {
                            "start": 659,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1296
                        },
                        {
                            "start": 1299,
                            "end": 1341
                        },
                        {
                            "start": 1344,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1688
                        },
                        {
                            "start": 1691,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2057
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 222,
                            "end": 239,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 270,
                            "end": 295,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9423828125,
            "relevance_judgment_input_expanded": "# Title: Meta-Learning via Classifier(-free) Diffusion Guidance\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Elvis Nava, Seijin Kobayashi, Yifei Yin, Robert K. Katzschmann, B. Grewe\n## Abstract\nWe introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.\n## Introduction\nequivalent conditioning modalities for adaptation (Figure 1). What we show is that popular techniques for the image domain, such as CLIP-based guidance (Gal et al., 2021;Patashnik et al., 2021), denoising diffusion models (Ho et al., 2020), and classifier-free guidance (Dhariwal & Nichol, 2021;Ho & Salimans, 2021;Nichol et al., 2022) can be repurposed for the meta-learning setting to generate adapted neural network weights. Using multi-step adaptation instead of traditionally conditioning the model on the natural language task information allows our models to achieve higher performance on each task by breaking down computations into multiple steps. \n\nWe approach the generation of neural network weights in two separate phases. In the unconditional pre-training phase, we train a generative hypernetwork (Ha et al., 2016;Sch\u00fcrholt et al., 2022) to map from its latent space to the weight space of a base model (Figure 2.A). In the guidance phase, we learn language-conditioned models that can be used to traverse the hypernetwork latent space and find zeroshot adapted weights with high performance on our task (Figure 2.B and 2.C). Our methods can thus benefit from large scale data through the pre-training phase, even when natural language descriptions are not available for all tasks. \n\nWe summarise our contributions as follows: \n\n1) We introduce HyperCLIP, a contrastive learning method equivalent to Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), producing CLIP embeddings of fine-tuned neural network weights. Using HyperCLIP as a guidance model then allows us to find task-adapted networks in the latent space of a hypernetwork model (Figure 2.B). \n\n2) We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifierfree guidance (Ho & Salimans, 2021) improves the performance of generated base networks.",
            "reference_string": "[256416107 | Nava et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Diffusion Models for Intelligent Transportation Systems: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 194,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293615241",
                    "name": "Mingxing Peng"
                },
                {
                    "authorId": "2267078966",
                    "name": "Kehua Chen"
                },
                {
                    "authorId": "2293665950",
                    "name": "Xusen Guo"
                },
                {
                    "authorId": "2294891908",
                    "name": "Qiming Zhang"
                },
                {
                    "authorId": "2255592398",
                    "name": "Hongliang Lu"
                },
                {
                    "authorId": "2255501442",
                    "name": "Hui Zhong"
                },
                {
                    "authorId": "2321889100",
                    "name": "Di Chen"
                },
                {
                    "authorId": "2257227452",
                    "name": "Meixin Zhu"
                },
                {
                    "authorId": "2298579285",
                    "name": "Hai Yang"
                }
            ],
            "abstract": "Intelligent Transportation Systems (ITS) are vital in modern traffic management and optimization, significantly enhancing traffic efficiency and safety. Recently, diffusion models have emerged as transformative tools for addressing complex challenges within ITS. In this paper, we present a comprehensive survey of diffusion models for ITS, covering both theoretical and practical aspects. First, we introduce the theoretical foundations of diffusion models and their key variants, including conditional diffusion models and latent diffusion models, highlighting their suitability for modeling complex, multi-modal traffic data and enabling controllable generation. Second, we outline the primary challenges in ITS and the corresponding advantages of diffusion models, providing readers with a deeper understanding of the intersection between ITS and diffusion models. Third, we offer a multi-perspective investigation of current applications of diffusion models in ITS domains, including autonomous driving, traffic simulation, trajectory prediction, and traffic safety. Finally, we discuss state-of-the-art diffusion model techniques and highlight key ITS research directions that warrant further investigation. Through this structured overview, we aim to provide researchers with a comprehensive understanding of diffusion models for ITS, thereby advancing their future applications in the transportation domain.",
            "corpus_id": 272831727,
            "sentences": [
                {
                    "corpus_id": "272831727",
                    "title": "Diffusion Models for Intelligent Transportation Systems: A Survey",
                    "text": "Following this work, many studies related to traffic trajectory generation and motion planning have designed various classifiers to controllably generate traffic scenarios that comply with traffic rules and ensure trajectory smoothness. For example, the cumulative rewards learned through reinforcement learning [46], motion planning cost function [47], STL formulas based on traffic rules [48], language-based loss function [49], and driving behavior classes [50] have been designed as classifier to generate task-conditioned samples. \n\nClassifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], [50], [53], [54], [55], [34]. This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly. \n\n2) Latent Diffusion Models: \n\nThe latent diffusion models (LDMs) [38] incorporate pretrained perceptual compression models, VQGAN [56], which consist of an encoder E and a decoder D, as illustrated in the Fig. 3. This approach enables diffusion models to leverage a lower-dimensional latent space, thereby reducing the computational burden during training and speeding up inference while maintaining high fidelity in generated outputs. Following this work, Blattmann et al. [57] extended LDM to the video latent diffusion model (VLDM) by introducing temporal layers and finetuning the autoencoder of pre-trained LDM using video data. \n\nLDMs have gained attention in intelligent transportation systems due to their ability to model complex traffic patterns and generate realistic traffic scenarios.",
                    "score": 0.39970881183856566,
                    "section_title": "1) Conditional Diffusion Models:",
                    "char_start_offset": 18929,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 535
                        },
                        {
                            "start": 538,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1182
                        },
                        {
                            "start": 1185,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1413
                        },
                        {
                            "start": 1414,
                            "end": 1534
                        },
                        {
                            "start": 1537,
                            "end": 1564
                        },
                        {
                            "start": 1567,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2170
                        },
                        {
                            "start": 2173,
                            "end": 2334
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 348,
                            "end": 352,
                            "matchedPaperCorpusId": "260191316"
                        },
                        {
                            "start": 390,
                            "end": 394,
                            "matchedPaperCorpusId": "253237771"
                        },
                        {
                            "start": 425,
                            "end": 429,
                            "matchedPaperCorpusId": "259137395"
                        },
                        {
                            "start": 460,
                            "end": 464,
                            "matchedPaperCorpusId": "262217065"
                        },
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "262217065"
                        },
                        {
                            "start": 1390,
                            "end": 1394,
                            "matchedPaperCorpusId": "257921535"
                        },
                        {
                            "start": 1408,
                            "end": 1412,
                            "matchedPaperCorpusId": "258298040"
                        },
                        {
                            "start": 1602,
                            "end": 1606,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1667,
                            "end": 1671,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 2011,
                            "end": 2015,
                            "matchedPaperCorpusId": "258187553"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94140625
                }
            ],
            "relevance_judgement": 0.94140625,
            "relevance_judgment_input_expanded": "# Title: Diffusion Models for Intelligent Transportation Systems: A Survey\n# Venue: arXiv.org\n# Authors: Mingxing Peng, Kehua Chen, Xusen Guo, Qiming Zhang, Hongliang Lu, Hui Zhong, Di Chen, Meixin Zhu, Hai Yang\n## Abstract\nIntelligent Transportation Systems (ITS) are vital in modern traffic management and optimization, significantly enhancing traffic efficiency and safety. Recently, diffusion models have emerged as transformative tools for addressing complex challenges within ITS. In this paper, we present a comprehensive survey of diffusion models for ITS, covering both theoretical and practical aspects. First, we introduce the theoretical foundations of diffusion models and their key variants, including conditional diffusion models and latent diffusion models, highlighting their suitability for modeling complex, multi-modal traffic data and enabling controllable generation. Second, we outline the primary challenges in ITS and the corresponding advantages of diffusion models, providing readers with a deeper understanding of the intersection between ITS and diffusion models. Third, we offer a multi-perspective investigation of current applications of diffusion models in ITS domains, including autonomous driving, traffic simulation, trajectory prediction, and traffic safety. Finally, we discuss state-of-the-art diffusion model techniques and highlight key ITS research directions that warrant further investigation. Through this structured overview, we aim to provide researchers with a comprehensive understanding of diffusion models for ITS, thereby advancing their future applications in the transportation domain.\n## 1) Conditional Diffusion Models:\nFollowing this work, many studies related to traffic trajectory generation and motion planning have designed various classifiers to controllably generate traffic scenarios that comply with traffic rules and ensure trajectory smoothness. For example, the cumulative rewards learned through reinforcement learning [46], motion planning cost function [47], STL formulas based on traffic rules [48], language-based loss function [49], and driving behavior classes [50] have been designed as classifier to generate task-conditioned samples. \n\nClassifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], [50], [53], [54], [55], [34]. This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly. \n\n2) Latent Diffusion Models: \n\nThe latent diffusion models (LDMs) [38] incorporate pretrained perceptual compression models, VQGAN [56], which consist of an encoder E and a decoder D, as illustrated in the Fig. 3. This approach enables diffusion models to leverage a lower-dimensional latent space, thereby reducing the computational burden during training and speeding up inference while maintaining high fidelity in generated outputs. Following this work, Blattmann et al. [57] extended LDM to the video latent diffusion model (VLDM) by introducing temporal layers and finetuning the autoencoder of pre-trained LDM using video data. \n\nLDMs have gained attention in intelligent transportation systems due to their ability to model complex traffic patterns and generate realistic traffic scenarios.",
            "reference_string": "[272831727 | Peng et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Compress Guidance in Conditional Diffusion Sampling",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.11194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153473846",
                    "name": "Anh-Dung Dinh"
                },
                {
                    "authorId": "51023221",
                    "name": "Daochang Liu"
                },
                {
                    "authorId": "2288626806",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "We found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue. By distributing a small amount of guidance over a large number of sampling timesteps, we observe a significant improvement in image quality and diversity while also reducing the required guidance timesteps by nearly 40%. This approach addresses a major challenge in applying guidance effectively to generative tasks. Consequently, our proposed method, termed Compress Guidance, allows for the exclusion of a substantial number of guidance timesteps while still surpassing baseline models in image quality. We validate our approach through benchmarks on label-conditional and text-to-image generative tasks across various datasets and models.",
            "corpus_id": 271915699,
            "sentences": [
                {
                    "corpus_id": "271915699",
                    "title": "Compress Guidance in Conditional Diffusion Sampling",
                    "text": "Classifier-free guidance is a different form of guidance from classifier guidance. Although classifierfree guidance does not use an explicit classifier for guidance, the diffusion model serves as an implicit classifier inside the model as discussed in Appendix F. We hypothesize that classifier-free guidance also suffers from a similar problem with classifier guidance. We apply the Compress Guidance technique on classifier-free guidance (CompCFG) and demonstrate the results in Table 4.",
                    "score": 0.4310919816026011,
                    "section_title": "Classifier-free guidance",
                    "char_start_offset": 19893,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 82
                        },
                        {
                            "start": 83,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 489
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Compress Guidance in Conditional Diffusion Sampling\n# Venue: arXiv.org\n# Authors: Anh-Dung Dinh, Daochang Liu, Chang Xu\n## Abstract\nWe found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue. By distributing a small amount of guidance over a large number of sampling timesteps, we observe a significant improvement in image quality and diversity while also reducing the required guidance timesteps by nearly 40%. This approach addresses a major challenge in applying guidance effectively to generative tasks. Consequently, our proposed method, termed Compress Guidance, allows for the exclusion of a substantial number of guidance timesteps while still surpassing baseline models in image quality. We validate our approach through benchmarks on label-conditional and text-to-image generative tasks across various datasets and models.\n## Classifier-free guidance\nClassifier-free guidance is a different form of guidance from classifier guidance. Although classifierfree guidance does not use an explicit classifier for guidance, the diffusion model serves as an implicit classifier inside the model as discussed in Appendix F. We hypothesize that classifier-free guidance also suffers from a similar problem with classifier guidance. We apply the Compress Guidance technique on classifier-free guidance (CompCFG) and demonstrate the results in Table 4.",
            "reference_string": "[271915699 | Dinh et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2297729854",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "2212307989",
                    "name": "Nicolas Dufour"
                },
                {
                    "authorId": "2142463387",
                    "name": "Nefeli Andreou"
                },
                {
                    "authorId": "1710314",
                    "name": "Marie-Paule Cani"
                },
                {
                    "authorId": "2297672100",
                    "name": "Victoria Fern\u00e1ndez Abrevaya"
                },
                {
                    "authorId": "145897899",
                    "name": "David Picard"
                },
                {
                    "authorId": "1881509",
                    "name": "Vicky Kalogeiton"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.",
            "corpus_id": 269283056,
            "sentences": [
                {
                    "corpus_id": "269283056",
                    "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
                    "text": "Diffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Ho et al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al., 2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored in numerous works (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022), and is achieved in its simplest form by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence of the condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearly combine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively, Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditional models, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. \n\nIn both cases, a weighting parameter \u03c9 controls the importance of the generative and guidance terms and is directly applied at all timesteps. Varying \u03c9 is a trade-off between fidelity and condition reliance, as an increase in condition reliance often results in a decline in both fidelity and diversity. In some recent literature, the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023) observed that a linearly increasing guidance weight could enhance performance and potentially increase diversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmann et al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterized cosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despite the recent appearance of this topic in the literature, none of the referenced studies has conducted any empirical experiments or analyses to substantiate the use of a guidance weight scheduler.",
                    "score": 0.4087311276359673,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 973
                        },
                        {
                            "start": 976,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 2030
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 117,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 126,
                            "end": 144,
                            "matchedPaperCorpusId": "257532642"
                        },
                        {
                            "start": 163,
                            "end": 183,
                            "matchedPaperCorpusId": "253581601"
                        },
                        {
                            "start": 199,
                            "end": 218,
                            "matchedPaperCorpusId": "254408910"
                        },
                        {
                            "start": 335,
                            "end": 357,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 357,
                            "end": 375,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 788,
                            "end": 809,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1388,
                            "end": 1408,
                            "matchedPaperCorpusId": "255372955"
                        },
                        {
                            "start": 1660,
                            "end": 1677,
                            "matchedPaperCorpusId": "257767316"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: Analysis of Classifier-Free Guidance Weight Schedulers\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fern\u00e1ndez Abrevaya, David Picard, Vicky Kalogeiton\n## Abstract\nClassifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.\n## Introduction\nDiffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Ho et al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al., 2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored in numerous works (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022), and is achieved in its simplest form by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence of the condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearly combine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively, Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditional models, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. \n\nIn both cases, a weighting parameter \u03c9 controls the importance of the generative and guidance terms and is directly applied at all timesteps. Varying \u03c9 is a trade-off between fidelity and condition reliance, as an increase in condition reliance often results in a decline in both fidelity and diversity. In some recent literature, the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023) observed that a linearly increasing guidance weight could enhance performance and potentially increase diversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmann et al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterized cosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despite the recent appearance of this topic in the literature, none of the referenced studies has conducted any empirical experiments or analyses to substantiate the use of a guidance weight scheduler.",
            "reference_string": "[269283056 | Wang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "28000727",
                    "name": "Weiting Tan"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2248152247",
                    "name": "Lingfeng Shen"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                },
                {
                    "authorId": "2264196844",
                    "name": "Philipp Koehn"
                }
            ],
            "abstract": "Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about +7 ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over 14x speedup for En-Es and 5x speedup for En-Fr translations compared to autoregressive baselines.",
            "corpus_id": 269983348,
            "sentences": [
                {
                    "corpus_id": "269983348",
                    "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
                    "text": "In this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations.",
                    "score": 0.4328768178525553,
                    "section_title": "F Ablation on Classifier-free Guidance",
                    "char_start_offset": 35200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 716
                        },
                        {
                            "start": 719,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1133
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation\n# Venue: Neural Information Processing Systems\n# Authors: Weiting Tan, Jingyu (Jack) Zhang, Lingfeng Shen, Daniel Khashabi, Philipp Koehn\n## Abstract\nNon-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about +7 ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over 14x speedup for En-Es and 5x speedup for En-Fr translations compared to autoregressive baselines.\n## F Ablation on Classifier-free Guidance\nIn this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations.",
            "reference_string": "[269983348 | Tan et al. | 2024 | Citations: 0]"
        },
        {
            "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "reference_count": 87,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2401.11078",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.11078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152175923",
                    "name": "Mingyuan Zhou"
                },
                {
                    "authorId": "12212948",
                    "name": "Rakib Hyder"
                },
                {
                    "authorId": "2119019",
                    "name": "Ziwei Xuan"
                },
                {
                    "authorId": "2268757799",
                    "name": "Guo-Jun Qi"
                }
            ],
            "abstract": "Recent advances in 3D avatar generation have gained significant attention. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. How-ever, SDS often generates over-smoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works gen-erate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR)textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors, so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.",
            "corpus_id": 267069249,
            "sentences": [
                {
                    "corpus_id": "267069249",
                    "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
                    "text": "Typically, classifier guidance can be applied to the score-based models by utilizing a distinct classifier. Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n( \n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale.",
                    "score": 0.4064586197702927,
                    "section_title": "Preliminaries",
                    "char_start_offset": 10339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 393
                        },
                        {
                            "start": 396,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 468
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 118,
                            "end": 122,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guo-Jun Qi\n## Abstract\nRecent advances in 3D avatar generation have gained significant attention. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. How-ever, SDS often generates over-smoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works gen-erate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR)textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors, so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments.\n## Preliminaries\nTypically, classifier guidance can be applied to the score-based models by utilizing a distinct classifier. Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n( \n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale.",
            "reference_string": "[267069249 | Zhou et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303409806",
                    "name": "Alexia Jolicoeur-Martineau"
                },
                {
                    "authorId": "14398916",
                    "name": "A. Baratin"
                },
                {
                    "authorId": "2311115257",
                    "name": "Kisoo Kwon"
                },
                {
                    "authorId": "49934559",
                    "name": "Boris Knyazev"
                },
                {
                    "authorId": "49889702",
                    "name": "Yan Zhang"
                }
            ],
            "abstract": "Generating novel molecules is challenging, with most representations leading to generative models producing many invalid molecules. Spanning Tree-based Graph Generation (STGG) is a promising approach to ensure the generation of valid molecules, outperforming state-of-the-art SMILES and graph diffusion models for unconditional generation. In the real world, we want to be able to generate molecules conditional on one or multiple desired properties rather than unconditionally. Thus, in this work, we extend STGG to multi-property-conditional generation. Our approach, STGG+, incorporates a modern Transformer architecture, random masking of properties during training (enabling conditioning on any subset of properties and classifier-free guidance), an auxiliary property-prediction loss (allowing the model to self-criticize molecules and select the best ones), and other improvements. We show that STGG+ achieves state-of-the-art performance on in-distribution and out-of-distribution conditional generation, and reward maximization.",
            "corpus_id": 271161847,
            "sentences": [
                {
                    "corpus_id": "271161847",
                    "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
                    "text": "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain.",
                    "score": 0.5898888599939833,
                    "section_title": "CLASSIFIER-FREE GUIDANCE",
                    "char_start_offset": 13008,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 713
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees\n# Venue: arXiv.org\n# Authors: Alexia Jolicoeur-Martineau, A. Baratin, Kisoo Kwon, Boris Knyazev, Yan Zhang\n## Abstract\nGenerating novel molecules is challenging, with most representations leading to generative models producing many invalid molecules. Spanning Tree-based Graph Generation (STGG) is a promising approach to ensure the generation of valid molecules, outperforming state-of-the-art SMILES and graph diffusion models for unconditional generation. In the real world, we want to be able to generate molecules conditional on one or multiple desired properties rather than unconditionally. Thus, in this work, we extend STGG to multi-property-conditional generation. Our approach, STGG+, incorporates a modern Transformer architecture, random masking of properties during training (enabling conditioning on any subset of properties and classifier-free guidance), an auxiliary property-prediction loss (allowing the model to self-criticize molecules and select the best ones), and other improvements. We show that STGG+ achieves state-of-the-art performance on in-distribution and out-of-distribution conditional generation, and reward maximization.\n## CLASSIFIER-FREE GUIDANCE\nTo enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain.",
            "reference_string": "[271161847 | Jolicoeur-Martineau et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Guiding image captioning models toward more specific captions",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.16686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135550613",
                    "name": "Simon Kornblith"
                },
                {
                    "authorId": "2111917831",
                    "name": "Lala Li"
                },
                {
                    "authorId": "2331539",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2167669221",
                    "name": "Thao Nguyen"
                }
            ],
            "abstract": "Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance [14] for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing p(caption|image) and p(image|caption). Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption\u2192image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.",
            "corpus_id": 260334572,
            "sentences": [
                {
                    "corpus_id": "260334572",
                    "title": "Guiding image captioning models toward more specific captions",
                    "text": "Let x be an image caption and y be the corresponding image. A standard captioning model aims to model the likelihood p(x|y), factorized autoregressively in terms of the probability of each token given previous tokens \n\nThe network is trained so that its output distribution \n\nAt inference time, one typically uses beam search or greedy decoding to produce a caption that has a particularly high probability. In this work, we use greedy decoding because it is the more common choice and it is also simpler to implement. \n\nClassifier-free guidance (CFG) [14] aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans [14] originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12,47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . .",
                    "score": 0.4184348723672907,
                    "section_title": "Classifier-free guidance for image captioning",
                    "char_start_offset": 7766,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 216
                        },
                        {
                            "start": 219,
                            "end": 273
                        },
                        {
                            "start": 276,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 518
                        },
                        {
                            "start": 521,
                            "end": 631
                        },
                        {
                            "start": 634,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1413
                        },
                        {
                            "start": 1416,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1823
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 552,
                            "end": 556,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1048,
                            "end": 1052,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: Guiding image captioning models toward more specific captions\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen\n## Abstract\nImage captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance [14] for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing p(caption|image) and p(image|caption). Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption\u2192image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.\n## Classifier-free guidance for image captioning\nLet x be an image caption and y be the corresponding image. A standard captioning model aims to model the likelihood p(x|y), factorized autoregressively in terms of the probability of each token given previous tokens \n\nThe network is trained so that its output distribution \n\nAt inference time, one typically uses beam search or greedy decoding to produce a caption that has a particularly high probability. In this work, we use greedy decoding because it is the more common choice and it is also simpler to implement. \n\nClassifier-free guidance (CFG) [14] aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans [14] originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12,47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . .",
            "reference_string": "[260334572 | Kornblith et al. | 2023 | Citations: 15]"
        },
        {
            "title": "Erasing Conceptual Knowledge from Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52017367",
                    "name": "Rohit Gandikota"
                },
                {
                    "authorId": "2140009998",
                    "name": "Sheridan Feucht"
                },
                {
                    "authorId": "2225941937",
                    "name": "Samuel Marks"
                },
                {
                    "authorId": "2284996653",
                    "name": "David Bau"
                }
            ],
            "abstract": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "corpus_id": 273098800,
            "sentences": [
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "This approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023).",
                    "score": 0.37955109953353117,
                    "section_title": "Transformer Next Token Prediction: A Classification Perspective",
                    "char_start_offset": 12637,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 342
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: Erasing Conceptual Knowledge from Language Models\n# Venue: arXiv.org\n# Authors: Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau\n## Abstract\nIn this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info\n## Transformer Next Token Prediction: A Classification Perspective\nThis approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023).",
            "reference_string": "[273098800 | Gandikota et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 100,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347677379",
                    "name": "Yimin Zhu"
                },
                {
                    "authorId": "2347797817",
                    "name": "Linlin Xu"
                }
            ],
            "abstract": "Data augmentation effectively addresses the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC). While most methodologies extend features in the latent space, few leverage text-driven generation to create realistic and diverse samples. Recently, text-guided diffusion models have gained significant attention due to their ability to generate highly diverse and high-quality images based on text prompts in natural image synthesis. Motivated by this, this paper proposes Txt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis method to address the ISSD in HSIC. The proposed approach uses a denoising diffusion model, which iteratively removes Gaussian noise to generate hyperspectral samples conditioned on textual descriptions. First, to address the high-dimensionality of hyperspectral data, a universal variational autoencoder (VAE) is designed to map the data into a low-dimensional latent space, which provides stable features and reduces the inference complexity of diffusion model. Second, a semi-supervised diffusion model is designed to fully take advantage of unlabeled data. Random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are used to simulate the varying degrees of mixing. Third, the VAE decodes HSI from latent space generated by the diffusion model with the language conditions as input. In our experiments, we fully evaluate synthetic samples' effectiveness from statistical characteristics and data distribution in 2D-PCA space. Additionally, visual-linguistic cross-attention is visualized on the pixel level to prove that our proposed model can capture the spatial layout and geometry of the generated data. Experiments demonstrate that the performance of the proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models, state-of-the-art CNNs, and semi-supervised methods.",
            "corpus_id": 276647345,
            "sentences": [
                {
                    "corpus_id": "276647345",
                    "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model",
                    "text": "where \u03f5 t \u223c N (0, I). Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9.",
                    "score": 0.41045944639441756,
                    "section_title": "1) Forward Diffusion Process:",
                    "char_start_offset": 22391,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 799
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model\n# Venue: arXiv.org\n# Authors: Yimin Zhu, Linlin Xu\n## Abstract\nData augmentation effectively addresses the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC). While most methodologies extend features in the latent space, few leverage text-driven generation to create realistic and diverse samples. Recently, text-guided diffusion models have gained significant attention due to their ability to generate highly diverse and high-quality images based on text prompts in natural image synthesis. Motivated by this, this paper proposes Txt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis method to address the ISSD in HSIC. The proposed approach uses a denoising diffusion model, which iteratively removes Gaussian noise to generate hyperspectral samples conditioned on textual descriptions. First, to address the high-dimensionality of hyperspectral data, a universal variational autoencoder (VAE) is designed to map the data into a low-dimensional latent space, which provides stable features and reduces the inference complexity of diffusion model. Second, a semi-supervised diffusion model is designed to fully take advantage of unlabeled data. Random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are used to simulate the varying degrees of mixing. Third, the VAE decodes HSI from latent space generated by the diffusion model with the language conditions as input. In our experiments, we fully evaluate synthetic samples' effectiveness from statistical characteristics and data distribution in 2D-PCA space. Additionally, visual-linguistic cross-attention is visualized on the pixel level to prove that our proposed model can capture the spatial layout and geometry of the generated data. Experiments demonstrate that the performance of the proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models, state-of-the-art CNNs, and semi-supervised methods.\n## 1) Forward Diffusion Process:\nwhere \u03f5 t \u223c N (0, I). Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9.",
            "reference_string": "[276647345 | Zhu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Diffusion Model for Data-Driven Black-Box Optimization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 111,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.13219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258812689",
                    "name": "Zihao Li"
                },
                {
                    "authorId": "2279340788",
                    "name": "Hui Yuan"
                },
                {
                    "authorId": "2242535459",
                    "name": "Kaixuan Huang"
                },
                {
                    "authorId": "103688818",
                    "name": "Chengzhuo Ni"
                },
                {
                    "authorId": "2292682601",
                    "name": "Yinyu Ye"
                },
                {
                    "authorId": "2258803874",
                    "name": "Minshuo Chen"
                },
                {
                    "authorId": "2258997595",
                    "name": "Mengdi Wang"
                }
            ],
            "abstract": "Generative AI has redefined artificial intelligence, enabling the creation of innovative content and customized solutions that drive business practices into a new era of efficiency and creativity. In this paper, we focus on diffusion models, a powerful generative AI technology, and investigate their potential for black-box optimization over complex structured variables. Consider the practical scenario where one wants to optimize some structured design in a high-dimensional space, based on massive unlabeled data (representing design variables) and a small labeled dataset. We study two practical types of labels: 1) noisy measurements of a real-valued reward function and 2) human preference based on pairwise comparisons. The goal is to generate new designs that are near-optimal and preserve the designed latent structures. Our proposed method reformulates the design optimization problem into a conditional sampling problem, which allows us to leverage the power of diffusion models for modeling complex distributions. In particular, we propose a reward-directed conditional diffusion model, to be trained on the mixed data, for sampling a near-optimal solution conditioned on high predicted rewards. Theoretically, we establish sub-optimality error bounds for the generated designs. The sub-optimality gap nearly matches the optimal guarantee in off-policy bandits, demonstrating the efficiency of reward-directed diffusion models for black-box optimization. Moreover, when the data admits a low-dimensional latent subspace structure, our model efficiently generates high-fidelity designs that closely respect the latent structure. We provide empirical experiments validating our model in decision-making and content-creation tasks.",
            "corpus_id": 268536735,
            "sentences": [
                {
                    "corpus_id": "268536735",
                    "title": "Diffusion Model for Data-Driven Black-Box Optimization",
                    "text": "Diffusion models successfully incorporate diverse guidance in practical applications.For image generation, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be externally trained, i.e., classifier guidance [Dhariwal and Nichol, 2021] or implicitly specified by a conditioned diffusion model, i.e., classifier-free guidance [Ho and Salimans, 2022].Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022].Other types of guidance are also explored in Nichol et al. [2021], Graikos et al. [2022], Bansal et al. [2023].Similar ideas have been explored in sequence modeling problems.In offline reinforcement learning, Decision Diffuser [Ajay et al., 2023] is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills.For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step.The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.These appealing empirical performance raises condensed curiosity of their theoretical underpinnings.",
                    "score": 0.38704866882156796,
                    "section_title": "Guided Diffusion Models",
                    "char_start_offset": 10336,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 85
                        },
                        {
                            "start": 85,
                            "end": 504
                        },
                        {
                            "start": 504,
                            "end": 681
                        },
                        {
                            "start": 681,
                            "end": 792
                        },
                        {
                            "start": 792,
                            "end": 855
                        },
                        {
                            "start": 855,
                            "end": 1111
                        },
                        {
                            "start": 1111,
                            "end": 1280
                        },
                        {
                            "start": 1280,
                            "end": 1399
                        },
                        {
                            "start": 1399,
                            "end": 1499
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 616,
                            "end": 637,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 771,
                            "end": 791,
                            "matchedPaperCorpusId": "256846836"
                        },
                        {
                            "start": 908,
                            "end": 927,
                            "matchedPaperCorpusId": "254044710"
                        },
                        {
                            "start": 1150,
                            "end": 1167,
                            "matchedPaperCorpusId": "249192356"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9169921875
                }
            ],
            "relevance_judgement": 0.9169921875,
            "relevance_judgment_input_expanded": "# Title: Diffusion Model for Data-Driven Black-Box Optimization\n# Venue: arXiv.org\n# Authors: Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, Mengdi Wang\n## Abstract\nGenerative AI has redefined artificial intelligence, enabling the creation of innovative content and customized solutions that drive business practices into a new era of efficiency and creativity. In this paper, we focus on diffusion models, a powerful generative AI technology, and investigate their potential for black-box optimization over complex structured variables. Consider the practical scenario where one wants to optimize some structured design in a high-dimensional space, based on massive unlabeled data (representing design variables) and a small labeled dataset. We study two practical types of labels: 1) noisy measurements of a real-valued reward function and 2) human preference based on pairwise comparisons. The goal is to generate new designs that are near-optimal and preserve the designed latent structures. Our proposed method reformulates the design optimization problem into a conditional sampling problem, which allows us to leverage the power of diffusion models for modeling complex distributions. In particular, we propose a reward-directed conditional diffusion model, to be trained on the mixed data, for sampling a near-optimal solution conditioned on high predicted rewards. Theoretically, we establish sub-optimality error bounds for the generated designs. The sub-optimality gap nearly matches the optimal guarantee in off-policy bandits, demonstrating the efficiency of reward-directed diffusion models for black-box optimization. Moreover, when the data admits a low-dimensional latent subspace structure, our model efficiently generates high-fidelity designs that closely respect the latent structure. We provide empirical experiments validating our model in decision-making and content-creation tasks.\n## Guided Diffusion Models\nDiffusion models successfully incorporate diverse guidance in practical applications.For image generation, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be externally trained, i.e., classifier guidance [Dhariwal and Nichol, 2021] or implicitly specified by a conditioned diffusion model, i.e., classifier-free guidance [Ho and Salimans, 2022].Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022].Other types of guidance are also explored in Nichol et al. [2021], Graikos et al. [2022], Bansal et al. [2023].Similar ideas have been explored in sequence modeling problems.In offline reinforcement learning, Decision Diffuser [Ajay et al., 2023] is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills.For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step.The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.These appealing empirical performance raises condensed curiosity of their theoretical underpinnings.",
            "reference_string": "[268536735 | Li et al. | 2024 | Citations: 13]"
        },
        {
            "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 76,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02358, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112225563",
                    "name": "Ziyan Guo"
                },
                {
                    "authorId": "2343790499",
                    "name": "Zeyu Hu"
                },
                {
                    "authorId": "2343735032",
                    "name": "Na Zhao"
                },
                {
                    "authorId": "2258965291",
                    "name": "De Wen Soh"
                }
            ],
            "abstract": "Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.",
            "corpus_id": 276107584,
            "sentences": [
                {
                    "corpus_id": "276107584",
                    "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
                    "text": "Classifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks [Peng et al. 2024;Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling.",
                    "score": 0.41045944639441756,
                    "section_title": "E CLASSIFIER FREE GUIDANCE FOR EACH TASK",
                    "char_start_offset": 35315,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 327
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 112,
                            "matchedPaperCorpusId": "274023942"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91162109375
                }
            ],
            "relevance_judgement": 0.91162109375,
            "relevance_judgment_input_expanded": "# Title: MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm\n# Venue: arXiv.org\n# Authors: Ziyan Guo, Zeyu Hu, Na Zhao, De Wen Soh\n## Abstract\nHuman motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.\n## E CLASSIFIER FREE GUIDANCE FOR EACH TASK\nClassifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks [Peng et al. 2024;Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling.",
            "reference_string": "[276107584 | Guo et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.12030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143807221",
                    "name": "Jiachun Pan"
                },
                {
                    "authorId": "143619442",
                    "name": "Hanshu Yan"
                },
                {
                    "authorId": "123200208",
                    "name": "J. Liew"
                },
                {
                    "authorId": "2255521016",
                    "name": "Jiashi Feng"
                },
                {
                    "authorId": "2067120710",
                    "name": "Vincent Y. F. Tan"
                }
            ],
            "abstract": "Training-free guided sampling in diffusion models leverages off-the-shelf pre-trained networks, such as an aesthetic evaluation model, to guide the generation process. Current training-free guided sampling algorithms obtain the guidance energy function based on a one-step estimate of the clean image. However, since the off-the-shelf pre-trained networks are trained on clean images, the one-step estimation procedure of the clean image may be inaccurate, especially in the early stages of the generation process in diffusion models. This causes the guidance in the early time steps to be inaccurate. To overcome this problem, we propose Symplectic Adjoint Guidance (SAG), which calculates the gradient guidance in two inner stages. Firstly, SAG estimates the clean image via $n$ function calls, where $n$ serves as a flexible hyperparameter that can be tailored to meet specific image quality requirements. Secondly, SAG uses the symplectic adjoint method to obtain the gradients accurately and efficiently in terms of the memory requirements. Extensive experiments demonstrate that SAG generates images with higher qualities compared to the baselines in both guided image and video generation tasks.",
            "corpus_id": 266362385,
            "sentences": [
                {
                    "corpus_id": "266362385",
                    "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
                    "text": "Diffusion models are powerful generative models that exhibit impressive performances across different modality generation, including image [5,11,12], video [22,34,39] and audio generation [16]. Guided sampling, including classifier guidance [5] and classifier-free guidance [11], has been widely used in diffusion models to realize controllable generation, such as text-to-image generation [29], imageto-image generation [24,28], and ControlNet [37]. Guided sampling controls the outputs of generative models by conditioning on various types of signals, such as descriptive text, class labels, and images. \n\nA line of guidance methods involves task-specific training of diffusion models using paired data, i.e., targets and conditions. For instance, classifier guidance [5] combines the score estimation of diffusion models with the gradients of the image classifiers to direct the generation process to produce images corresponding to a particular class. In this way, several image classifiers need to be trained on the noisy states of intermediate generation steps of diffusion models. Alternatively, classifier-free guidance [11] directly trains a new score estimator with conditions and uses a linear combination of conditional and unconditional score estimators for sampling. Although this line of methods can effectively guide diffusion models to generate data satisfying certain properties, they are not sufficiently flexible to adapt to any type of guiding due to the cost of training and the feasibility of collecting paired data. \n\nTo this end, another line of training-free guidance methods has been explored [2,14,36]. In training-free guided sampling, at a certain sampling step t, the guidance function is usually constructed as the gradients of the loss function obtained by the off-the-shelf pre-trained models, such as face-ID detection or aesthetic evaluation models. More specifically, the guidance gradients are computed based on the one-step approximation of denoised images from the noisy samples at certain steps t. Then, gradients are added to corresponding sampling steps as guidance to direct the generation process to the desired results. This line of methods offers greater flexibility by allowing the diffusion models to adapt to a broad spectrum of guidance.",
                    "score": 0.47446150636302303,
                    "section_title": "Introduction",
                    "char_start_offset": 1371,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 605
                        },
                        {
                            "start": 608,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1539
                        },
                        {
                            "start": 1542,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1885
                        },
                        {
                            "start": 1886,
                            "end": 2038
                        },
                        {
                            "start": 2039,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2288
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 139,
                            "end": 142,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 142,
                            "end": 145,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 188,
                            "end": 192,
                            "matchedPaperCorpusId": "256390486"
                        },
                        {
                            "start": 241,
                            "end": 244,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 274,
                            "end": 278,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 390,
                            "end": 394,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 421,
                            "end": 425,
                            "matchedPaperCorpusId": "256616002"
                        },
                        {
                            "start": 425,
                            "end": 428,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 445,
                            "end": 449,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 770,
                            "end": 773,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1620,
                            "end": 1623,
                            "matchedPaperCorpusId": "256846836"
                        },
                        {
                            "start": 1626,
                            "end": 1629,
                            "matchedPaperCorpusId": "257622962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method\n# Venue: arXiv.org\n# Authors: Jiachun Pan, Hanshu Yan, J. Liew, Jiashi Feng, Vincent Y. F. Tan\n## Abstract\nTraining-free guided sampling in diffusion models leverages off-the-shelf pre-trained networks, such as an aesthetic evaluation model, to guide the generation process. Current training-free guided sampling algorithms obtain the guidance energy function based on a one-step estimate of the clean image. However, since the off-the-shelf pre-trained networks are trained on clean images, the one-step estimation procedure of the clean image may be inaccurate, especially in the early stages of the generation process in diffusion models. This causes the guidance in the early time steps to be inaccurate. To overcome this problem, we propose Symplectic Adjoint Guidance (SAG), which calculates the gradient guidance in two inner stages. Firstly, SAG estimates the clean image via $n$ function calls, where $n$ serves as a flexible hyperparameter that can be tailored to meet specific image quality requirements. Secondly, SAG uses the symplectic adjoint method to obtain the gradients accurately and efficiently in terms of the memory requirements. Extensive experiments demonstrate that SAG generates images with higher qualities compared to the baselines in both guided image and video generation tasks.\n## Introduction\nDiffusion models are powerful generative models that exhibit impressive performances across different modality generation, including image [5,11,12], video [22,34,39] and audio generation [16]. Guided sampling, including classifier guidance [5] and classifier-free guidance [11], has been widely used in diffusion models to realize controllable generation, such as text-to-image generation [29], imageto-image generation [24,28], and ControlNet [37]. Guided sampling controls the outputs of generative models by conditioning on various types of signals, such as descriptive text, class labels, and images. \n\nA line of guidance methods involves task-specific training of diffusion models using paired data, i.e., targets and conditions. For instance, classifier guidance [5] combines the score estimation of diffusion models with the gradients of the image classifiers to direct the generation process to produce images corresponding to a particular class. In this way, several image classifiers need to be trained on the noisy states of intermediate generation steps of diffusion models. Alternatively, classifier-free guidance [11] directly trains a new score estimator with conditions and uses a linear combination of conditional and unconditional score estimators for sampling. Although this line of methods can effectively guide diffusion models to generate data satisfying certain properties, they are not sufficiently flexible to adapt to any type of guiding due to the cost of training and the feasibility of collecting paired data. \n\nTo this end, another line of training-free guidance methods has been explored [2,14,36]. In training-free guided sampling, at a certain sampling step t, the guidance function is usually constructed as the gradients of the loss function obtained by the off-the-shelf pre-trained models, such as face-ID detection or aesthetic evaluation models. More specifically, the guidance gradients are computed based on the one-step approximation of denoised images from the noisy samples at certain steps t. Then, gradients are added to corresponding sampling steps as guidance to direct the generation process to the desired results. This line of methods offers greater flexibility by allowing the diffusion models to adapt to a broad spectrum of guidance.",
            "reference_string": "[266362385 | Pan et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268399939",
                    "name": "Prin Phunyaphibarn"
                },
                {
                    "authorId": "2328609307",
                    "name": "Phillip Y. Lee"
                },
                {
                    "authorId": "2292419803",
                    "name": "Jaihoon Kim"
                },
                {
                    "authorId": "2292259803",
                    "name": "Minhyuk Sung"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.",
            "corpus_id": 277321603,
            "sentences": [
                {
                    "corpus_id": "277321603",
                    "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
                    "text": "Guidance in Diffusion Models. Classifier-Free Guidance (CFG) [28] has become the de facto guidance technique for conditional generation with diffusion models, leading to notable improvements in both condition alignment and image quality. However, recent research has highlighted some of its limitations. Kynk\u00e4\u00e4nniemi et al. [41] have shown that the specific timesteps at which CFG is applied significantly impact image diversity, and proposed to restrict CFG to certain intervals. \n\nAnother line of work [1,31] addresses the limited applicability of CFG for text-based conditions when using off-the-shelf diffusion models like Stable Diffusion [52]. These approaches introduce a guidance technique that extends to a broader range of generation tasks, including unconditional generation, inverse problems, and conditional generation with non-text conditions (e.g., depth maps [67]). Recently, Karras et al. [38] propose Autoguidance which uses the noise estimate from an under-trained version of itself, instead of unconditional noise, to resolve inherent issues of the entangled guidance for condition alignment and image quality. A more detailed discussion on autoguidance can be found in the Appendix. However, previous works have not explored how the dynamics of CFG shift when a diffusion model is fine-tuned for a specific task [7,46,64]. In this work, we address the critical issue of unconditional noise degradation that occurs during fine-tuning and pro-pose a novel solution by combining noise predictions from multiple diffusion models. \n\nMerging Diffusion Models. Aligned with the mixtureof-experts [8] and model merging [65] literature on foundation models, there is growing research on methods for merging diffusion models to enable effective composition of multiple conditions. Diffusion Soup [3] directly merges weights of different diffusion models, Mix-of-Show [25] combines the weights of LoRA adapters [32], and Max-Fusion [48] merges intermediate model features. Notably, leveraging the iterative denoising process of diffusion models, merging their noise estimates has emerged as a simple yet powerful technique for composing conditions.",
                    "score": 0.3852037787878375,
                    "section_title": "Related Works",
                    "char_start_offset": 5639,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1546
                        },
                        {
                            "start": 1549,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 61,
                            "end": 65,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 504,
                            "end": 507,
                            "matchedPaperCorpusId": "268692048"
                        },
                        {
                            "start": 507,
                            "end": 510,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 644,
                            "end": 648,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 875,
                            "end": 879,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 1333,
                            "end": 1336,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 1336,
                            "end": 1339,
                            "matchedPaperCorpusId": "257631738"
                        },
                        {
                            "start": 1339,
                            "end": 1342,
                            "matchedPaperCorpusId": "253523371"
                        },
                        {
                            "start": 1807,
                            "end": 1810,
                            "matchedPaperCorpusId": "270391477"
                        },
                        {
                            "start": 1942,
                            "end": 1946,
                            "matchedPaperCorpusId": "269148434"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90869140625
                }
            ],
            "relevance_judgement": 0.90869140625,
            "relevance_judgment_input_expanded": "# Title: Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models\n# Venue: arXiv.org\n# Authors: Prin Phunyaphibarn, Phillip Y. Lee, Jaihoon Kim, Minhyuk Sung\n## Abstract\nClassifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.\n## Related Works\nGuidance in Diffusion Models. Classifier-Free Guidance (CFG) [28] has become the de facto guidance technique for conditional generation with diffusion models, leading to notable improvements in both condition alignment and image quality. However, recent research has highlighted some of its limitations. Kynk\u00e4\u00e4nniemi et al. [41] have shown that the specific timesteps at which CFG is applied significantly impact image diversity, and proposed to restrict CFG to certain intervals. \n\nAnother line of work [1,31] addresses the limited applicability of CFG for text-based conditions when using off-the-shelf diffusion models like Stable Diffusion [52]. These approaches introduce a guidance technique that extends to a broader range of generation tasks, including unconditional generation, inverse problems, and conditional generation with non-text conditions (e.g., depth maps [67]). Recently, Karras et al. [38] propose Autoguidance which uses the noise estimate from an under-trained version of itself, instead of unconditional noise, to resolve inherent issues of the entangled guidance for condition alignment and image quality. A more detailed discussion on autoguidance can be found in the Appendix. However, previous works have not explored how the dynamics of CFG shift when a diffusion model is fine-tuned for a specific task [7,46,64]. In this work, we address the critical issue of unconditional noise degradation that occurs during fine-tuning and pro-pose a novel solution by combining noise predictions from multiple diffusion models. \n\nMerging Diffusion Models. Aligned with the mixtureof-experts [8] and model merging [65] literature on foundation models, there is growing research on methods for merging diffusion models to enable effective composition of multiple conditions. Diffusion Soup [3] directly merges weights of different diffusion models, Mix-of-Show [25] combines the weights of LoRA adapters [32], and Max-Fusion [48] merges intermediate model features. Notably, leveraging the iterative denoising process of diffusion models, merging their noise estimates has emerged as a simple yet powerful technique for composing conditions.",
            "reference_string": "[277321603 | Phunyaphibarn et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270764731",
                    "name": "C. Wu"
                },
                {
                    "authorId": "2239102325",
                    "name": "Fernando De la Torre"
                }
            ],
            "abstract": "Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.",
            "corpus_id": 267770589,
            "sentences": [
                {
                    "corpus_id": "267770589",
                    "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
                    "text": "Text-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
                    "score": 0.43616445865871134,
                    "section_title": "Related Work",
                    "char_start_offset": 3897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 33,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 431
                        },
                        {
                            "start": 434,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1446
                        },
                        {
                            "start": 1449,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1836
                        },
                        {
                            "start": 1839,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 1970
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 171,
                            "end": 174,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 174,
                            "end": 177,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 177,
                            "end": 180,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 180,
                            "end": 182,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 197,
                            "end": 201,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "248097655"
                        },
                        {
                            "start": 225,
                            "end": 229,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 252,
                            "end": 256,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 278,
                            "end": 281,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 296,
                            "end": 300,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 310,
                            "end": 313,
                            "matchedPaperCorpusId": "235212350"
                        },
                        {
                            "start": 313,
                            "end": 315,
                            "matchedPaperCorpusId": "248476190"
                        },
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "249926846"
                        },
                        {
                            "start": 594,
                            "end": 597,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 604,
                            "end": 608,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 631,
                            "end": 635,
                            "matchedPaperCorpusId": "246411364"
                        },
                        {
                            "start": 635,
                            "end": 638,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 778,
                            "end": 782,
                            "matchedPaperCorpusId": "245117331"
                        },
                        {
                            "start": 782,
                            "end": 785,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 809,
                            "end": 813,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 909,
                            "end": 913,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1071,
                            "end": 1075,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1075,
                            "end": 1078,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "249375227"
                        },
                        {
                            "start": 1284,
                            "end": 1287,
                            "matchedPaperCorpusId": "251402961"
                        },
                        {
                            "start": 1307,
                            "end": 1311,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 1582,
                            "end": 1585,
                            "matchedPaperCorpusId": "236950721"
                        },
                        {
                            "start": 1585,
                            "end": 1588,
                            "matchedPaperCorpusId": "245704504"
                        },
                        {
                            "start": 1588,
                            "end": 1591,
                            "matchedPaperCorpusId": "247476275"
                        },
                        {
                            "start": 1591,
                            "end": 1594,
                            "matchedPaperCorpusId": "252815928"
                        },
                        {
                            "start": 1699,
                            "end": 1703,
                            "matchedPaperCorpusId": "251252882"
                        },
                        {
                            "start": 1703,
                            "end": 1706,
                            "matchedPaperCorpusId": "252918469"
                        },
                        {
                            "start": 1706,
                            "end": 1709,
                            "matchedPaperCorpusId": "252815928"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: C. Wu, Fernando De la Torre\n## Abstract\nText-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.\n## Related Work\nText-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
            "reference_string": "[267770589 | Wu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "History-Guided Video Diffusion",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 18,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345339612",
                    "name": "Kiwhan Song"
                },
                {
                    "authorId": "8786274",
                    "name": "Boyuan Chen"
                },
                {
                    "authorId": "2243186471",
                    "name": "Max Simchowitz"
                },
                {
                    "authorId": "2344972384",
                    "name": "Yilun Du"
                },
                {
                    "authorId": "2263905014",
                    "name": "Russ Tedrake"
                },
                {
                    "authorId": "2280906248",
                    "name": "Vincent Sitzmann"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance",
            "corpus_id": 276249479,
            "sentences": [
                {
                    "corpus_id": "276249479",
                    "title": "History-Guided Video Diffusion",
                    "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance (Ho et al., 2022b;Wang et al., 2023). For frame conditioning, \"first frame\" guidance is commonplace in image-to-video models (Blattmann et al., 2023a;  Yang et al., 2024), or \"fixed set of few frames\" (Blattmann et al., 2023b;Gupta et al., 2023;Watson et al., 2024), likewise in multi-view diffusion models (Gao et al., 2024). \n\nOur work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score -similar to prior works in compositional generative models (Du & Kaelbling, 2024;Liu et al., 2022;Du et al., 2023), we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance. Diffusion Forcing. Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF) (Chen et al., 2024) proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid (Yin et al., 2024) builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model.",
                    "score": 0.3792469027442187,
                    "section_title": "Preliminaries and Related Work",
                    "char_start_offset": 6172,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 862
                        },
                        {
                            "start": 865,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1671
                        },
                        {
                            "start": 1672,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1925
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 737,
                            "end": 762,
                            "matchedPaperCorpusId": "258187553"
                        },
                        {
                            "start": 1127,
                            "end": 1144,
                            "matchedPaperCorpusId": "249375227"
                        },
                        {
                            "start": 1144,
                            "end": 1160,
                            "matchedPaperCorpusId": "257078922"
                        },
                        {
                            "start": 1558,
                            "end": 1577,
                            "matchedPaperCorpusId": "270869622"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: History-Guided Video Diffusion\n# Venue: arXiv.org\n# Authors: Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann\n## Abstract\nClassifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance\n## Preliminaries and Related Work\nClassifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance (Ho et al., 2022b;Wang et al., 2023). For frame conditioning, \"first frame\" guidance is commonplace in image-to-video models (Blattmann et al., 2023a;  Yang et al., 2024), or \"fixed set of few frames\" (Blattmann et al., 2023b;Gupta et al., 2023;Watson et al., 2024), likewise in multi-view diffusion models (Gao et al., 2024). \n\nOur work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score -similar to prior works in compositional generative models (Du & Kaelbling, 2024;Liu et al., 2022;Du et al., 2023), we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance. Diffusion Forcing. Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF) (Chen et al., 2024) proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid (Yin et al., 2024) builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model.",
            "reference_string": "[276249479 | Song et al. | 2025 | Citations: 18]"
        },
        {
            "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 58,
            "influential_citation_count": 16,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2124211700",
                    "name": "Hannes St\u00e4rk"
                },
                {
                    "authorId": "2064253810",
                    "name": "Bowen Jing"
                },
                {
                    "authorId": "2283429637",
                    "name": "Chenyu Wang"
                },
                {
                    "authorId": "1630330616",
                    "name": "Gabriele Corso"
                },
                {
                    "authorId": "2271178326",
                    "name": "Bonnie Berger"
                },
                {
                    "authorId": "2254308896",
                    "name": "R. Barzilay"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                }
            ],
            "abstract": "Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at https://github.com/HannesStark/dirichlet-flow-matching.",
            "corpus_id": 267547708,
            "sentences": [
                {
                    "corpus_id": "267547708",
                    "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
                    "text": "Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at https://github.com/HannesStark/dirichlet-flow-matching.",
                    "score": 0.48081812765337906,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.904296875
                }
            ],
            "relevance_judgement": 0.904296875,
            "relevance_judgment_input_expanded": "# Title: Dirichlet Flow Matching with Applications to DNA Sequence Design\n# Venue: International Conference on Machine Learning\n# Authors: Hannes St\u00e4rk, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, R. Barzilay, T. Jaakkola\n## Abstract\nDiscrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at https://github.com/HannesStark/dirichlet-flow-matching.\n",
            "reference_string": "[267547708 | Stark et al. | 2024 | Citations: 58]"
        },
        {
            "title": "Text2Data: Low-Resource Data Generation with Textual Control",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284725554",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2284730919",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "2284685690",
                    "name": "Tian Lan"
                },
                {
                    "authorId": "2211974297",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "2284676271",
                    "name": "Yu Bai"
                },
                {
                    "authorId": "2279097262",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2258793468",
                    "name": "Huan Wang"
                },
                {
                    "authorId": "2256976968",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2238207181",
                    "name": "Silvio Savarese"
                }
            ],
            "abstract": "Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines.",
            "corpus_id": 267750410,
            "sentences": [
                {
                    "corpus_id": "267750410",
                    "title": "Text2Data: Low-Resource Data Generation with Textual Control",
                    "text": "2.1. Text-to-data diffusion-based generation Diffusion models, notably divided into classifierguided (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022;Yang et al., 2023;Ho et al., 2022;Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2023) for controllable molecule and motion generation, respectively. Furthermore, since natural languages are a prevalent medium for human to communicate with the world, the text-to-data generation paradigm has gained traction, with diffusion models being instrumental in generating high-quality data aligned with textual inputs. The extensive applications encompass text-to-image generation (Ruiz et al., 2023;Zhang & Agrawala, 2023), text-to-speech generation (Huang et al., 2022;Kim et al., 2022), text-to-shape generation (Li et al., 2023;Lin et al., 2023), and more, leveraging the abundant text descriptions for training potent generative models. Despite advancements in generating data from text across various modalities, many other modalities may not satisfy the stringent requirements for sufficient data-text pairs essential for attaining optimal controllability during the training of models.",
                    "score": 0.41045944639441756,
                    "section_title": "Related works",
                    "char_start_offset": 5191,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 4
                        },
                        {
                            "start": 5,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1270
                        },
                        {
                            "start": 1271,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1845
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 101,
                            "end": 126,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 249,
                            "end": 273,
                            "matchedPaperCorpusId": "247839510"
                        },
                        {
                            "start": 307,
                            "end": 327,
                            "matchedPaperCorpusId": "248965384"
                        },
                        {
                            "start": 868,
                            "end": 892,
                            "matchedPaperCorpusId": "247839510"
                        },
                        {
                            "start": 926,
                            "end": 946,
                            "matchedPaperCorpusId": "252595883"
                        },
                        {
                            "start": 1333,
                            "end": 1352,
                            "matchedPaperCorpusId": "251800180"
                        },
                        {
                            "start": 1403,
                            "end": 1423,
                            "matchedPaperCorpusId": "250492984"
                        },
                        {
                            "start": 1423,
                            "end": 1439,
                            "matchedPaperCorpusId": "246430592"
                        },
                        {
                            "start": 1467,
                            "end": 1484,
                            "matchedPaperCorpusId": "254366593"
                        },
                        {
                            "start": 1484,
                            "end": 1501,
                            "matchedPaperCorpusId": "253708074"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Text2Data: Low-Resource Data Generation with Textual Control\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese\n## Abstract\nNatural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines.\n## Related works\n2.1. Text-to-data diffusion-based generation Diffusion models, notably divided into classifierguided (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022;Yang et al., 2023;Ho et al., 2022;Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2023) for controllable molecule and motion generation, respectively. Furthermore, since natural languages are a prevalent medium for human to communicate with the world, the text-to-data generation paradigm has gained traction, with diffusion models being instrumental in generating high-quality data aligned with textual inputs. The extensive applications encompass text-to-image generation (Ruiz et al., 2023;Zhang & Agrawala, 2023), text-to-speech generation (Huang et al., 2022;Kim et al., 2022), text-to-shape generation (Li et al., 2023;Lin et al., 2023), and more, leveraging the abundant text descriptions for training potent generative models. Despite advancements in generating data from text across various modalities, many other modalities may not satisfy the stringent requirements for sufficient data-text pairs essential for attaining optimal controllability during the training of models.",
            "reference_string": "[267750410 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 249,
            "citation_count": 38,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203366502",
                    "name": "Pu Cao"
                },
                {
                    "authorId": "2273950538",
                    "name": "Feng Zhou"
                },
                {
                    "authorId": "2273931256",
                    "name": "Qing Song"
                },
                {
                    "authorId": "39702333",
                    "name": "Lu Yang"
                }
            ],
            "abstract": "In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \\url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}.",
            "corpus_id": 268264822,
            "sentences": [
                {
                    "corpus_id": "268264822",
                    "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
                    "text": "In this section, we spotlight several pivotal and widely utilized text-to-image foundational models.Detailed information regarding these models is systematically compiled and presented in   \u2022 GLIDE [21].To generate images aligned with free-form text prompts, GLIDE intuitively replace the class label in class-conditioned diffusion models (i.e.ADM [18]) with text, formalizing the first text-to-image diffusion model.The authors explore two different guidance for text-conditioning.\n\nFor classifier guidance, GLIDE trains a CLIP model in noisy image space to provide CLIP guidance.Following [20], GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation.For text condition, GLIDE first transforms the input text c into a token sequence via a trainable transformer [36].Subsequently, they replace the class embedding with the pooled text features and further concatenate the projected sequence text features to the attention context at each attention layer in diffusion model.GLIDE trains the diffusion model and text transformer on the same dataset as DALL\u2022E [22].The diffusion model is trained to predict p(x t\u22121 |x t , c) and generate images with CFG.\n\n\u2022 Imagen [24].Following GLIDE, Imagen adopts classifierfree guidance (CFG) for text-to-image generation.Instead of training a task-specified text encoder from scratch in GLIDE, Imagen leverages a pre-trained and frozen large language model (LLM) as its text encoder, aiming to reduce computational demands.The authors conduct a comparative analysis of various LLMs, including those trained on imagetext datasets (e.g., CLIP [39]) and solely on text corpora (e.g., BERT [40], T5 [37]).Their findings suggest that increasing the scale of language models more effectively enhances the fidelity of samples and the congruence between image and text, compared to the enlargement of image diffusion models.Furthermore, Imagen's exploration into different text conditioning methods reveals cross-attention as the most effective technique.",
                    "score": 0.3815300320438647,
                    "section_title": "Text-to-Image Diffusion Models",
                    "char_start_offset": 7043,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 100,
                            "end": 203
                        },
                        {
                            "start": 203,
                            "end": 344
                        },
                        {
                            "start": 344,
                            "end": 417
                        },
                        {
                            "start": 417,
                            "end": 482
                        },
                        {
                            "start": 484,
                            "end": 581
                        },
                        {
                            "start": 581,
                            "end": 855
                        },
                        {
                            "start": 855,
                            "end": 970
                        },
                        {
                            "start": 970,
                            "end": 1176
                        },
                        {
                            "start": 1176,
                            "end": 1265
                        },
                        {
                            "start": 1265,
                            "end": 1354
                        },
                        {
                            "start": 1356,
                            "end": 1370
                        },
                        {
                            "start": 1370,
                            "end": 1460
                        },
                        {
                            "start": 1460,
                            "end": 1662
                        },
                        {
                            "start": 1662,
                            "end": 1840
                        },
                        {
                            "start": 1840,
                            "end": 2055
                        },
                        {
                            "start": 2055,
                            "end": 2186
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 348,
                            "end": 352,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 965,
                            "end": 969,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1260,
                            "end": 1264,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 1365,
                            "end": 1369,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1780,
                            "end": 1784,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1834,
                            "end": 1838,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Controllable Generation with Text-to-Image Diffusion Models: A Survey\n# Venue: arXiv.org\n# Authors: Pu Cao, Feng Zhou, Qing Song, Lu Yang\n## Abstract\nIn the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \\url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}.\n## Text-to-Image Diffusion Models\nIn this section, we spotlight several pivotal and widely utilized text-to-image foundational models.Detailed information regarding these models is systematically compiled and presented in   \u2022 GLIDE [21].To generate images aligned with free-form text prompts, GLIDE intuitively replace the class label in class-conditioned diffusion models (i.e.ADM [18]) with text, formalizing the first text-to-image diffusion model.The authors explore two different guidance for text-conditioning.\n\nFor classifier guidance, GLIDE trains a CLIP model in noisy image space to provide CLIP guidance.Following [20], GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation.For text condition, GLIDE first transforms the input text c into a token sequence via a trainable transformer [36].Subsequently, they replace the class embedding with the pooled text features and further concatenate the projected sequence text features to the attention context at each attention layer in diffusion model.GLIDE trains the diffusion model and text transformer on the same dataset as DALL\u2022E [22].The diffusion model is trained to predict p(x t\u22121 |x t , c) and generate images with CFG.\n\n\u2022 Imagen [24].Following GLIDE, Imagen adopts classifierfree guidance (CFG) for text-to-image generation.Instead of training a task-specified text encoder from scratch in GLIDE, Imagen leverages a pre-trained and frozen large language model (LLM) as its text encoder, aiming to reduce computational demands.The authors conduct a comparative analysis of various LLMs, including those trained on imagetext datasets (e.g., CLIP [39]) and solely on text corpora (e.g., BERT [40], T5 [37]).Their findings suggest that increasing the scale of language models more effectively enhances the fidelity of samples and the congruence between image and text, compared to the enlargement of image diffusion models.Furthermore, Imagen's exploration into different text conditioning methods reveals cross-attention as the most effective technique.",
            "reference_string": "[268264822 | Cao et al. | 2024 | Citations: 38]"
        },
        {
            "title": "Training Unbiased Diffusion Models From Biased Dataset",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 23,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119255215",
                    "name": "Yeongmin Kim"
                },
                {
                    "authorId": "1999215536",
                    "name": "Byeonghu Na"
                },
                {
                    "authorId": "2220816776",
                    "name": "Minsang Park"
                },
                {
                    "authorId": "91586945",
                    "name": "Joonho Jang"
                },
                {
                    "authorId": "51999226",
                    "name": "Dongjun Kim"
                },
                {
                    "authorId": "2287842068",
                    "name": "Wanmo Kang"
                },
                {
                    "authorId": "2288547472",
                    "name": "Il-Chul Moon"
                }
            ],
            "abstract": "With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.",
            "corpus_id": 268230892,
            "sentences": [
                {
                    "corpus_id": "268230892",
                    "title": "Training Unbiased Diffusion Models From Biased Dataset",
                    "text": "The sampling process of the diffusion model involves an iterative update process using a score direction, typically approximated by the score network. When there is a specific purpose for generating data, score correction becomes necessary. There are several methods to adjust this score direction, each tailored to specific purposes. From a technical standpoint, these methods can be divided into two groups: guidance methods and score-matching regularization methods. \n\nGuidance methods introduce additional gradient signals to adjust the update direction. Classifier guidance (Song et al., 2020;Dhariwal & Nichol, 2021) utilizes a gradient signal from a classifier to generate samples that satisfy a condition. Classifier-free guidance (Ho & Salimans, 2021) also aims at conditional generation but relies on both unconditional and conditional scores. Furthermore, various methods have been proposed to enable controllable generation using auxiliary models with a pre-trained unconditional score model (Graikos et al., 2022;Song et al., 2023). On the other hand, discriminator guidance (Kim et al., 2023) serves a different purpose by enhancing the sampling performance of a diffusion model through the use of a discriminator that distinguishes between real images and generated images. EGSDE (Zhao et al., 2022) leverages guidance signals based on energy functions, enhancing unpaired image-to-image translation. Guidance methods have the advantage of utilizing pre-trained score networks without the need for additional training. However, they require separate network training for guidance and additional network evaluation during the sampling process. \n\nThere is a body of work on score-matching regularization for better likelihood estimation (Lu et al., 2022;Zheng et al., 2023b;Lai et al., 2023). Na et al. (2024) propose a regularized conditional scorematching objective to mitigate label noise. The unique benefit of score-matching regularization is that it does not require an additional network at the inference stage.",
                    "score": 0.4139208937215151,
                    "section_title": "B.3 SCORE CORRECTION IN DIFFUSION MODEL",
                    "char_start_offset": 31604,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 469
                        },
                        {
                            "start": 472,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 853
                        },
                        {
                            "start": 854,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1657
                        },
                        {
                            "start": 1660,
                            "end": 1805
                        },
                        {
                            "start": 1806,
                            "end": 1905
                        },
                        {
                            "start": 1906,
                            "end": 2031
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 739,
                            "end": 759,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1004,
                            "end": 1026,
                            "matchedPaperCorpusId": "249889060"
                        },
                        {
                            "start": 1026,
                            "end": 1044,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 1088,
                            "end": 1106,
                            "matchedPaperCorpusId": "254096299"
                        },
                        {
                            "start": 1295,
                            "end": 1314,
                            "matchedPaperCorpusId": "250526607"
                        },
                        {
                            "start": 1750,
                            "end": 1767,
                            "matchedPaperCorpusId": "249712167"
                        },
                        {
                            "start": 1767,
                            "end": 1787,
                            "matchedPaperCorpusId": "258557324"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8564453125
                }
            ],
            "relevance_judgement": 0.8564453125,
            "relevance_judgment_input_expanded": "# Title: Training Unbiased Diffusion Models From Biased Dataset\n# Venue: International Conference on Learning Representations\n# Authors: Yeongmin Kim, Byeonghu Na, Minsang Park, Joonho Jang, Dongjun Kim, Wanmo Kang, Il-Chul Moon\n## Abstract\nWith significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.\n## B.3 SCORE CORRECTION IN DIFFUSION MODEL\nThe sampling process of the diffusion model involves an iterative update process using a score direction, typically approximated by the score network. When there is a specific purpose for generating data, score correction becomes necessary. There are several methods to adjust this score direction, each tailored to specific purposes. From a technical standpoint, these methods can be divided into two groups: guidance methods and score-matching regularization methods. \n\nGuidance methods introduce additional gradient signals to adjust the update direction. Classifier guidance (Song et al., 2020;Dhariwal & Nichol, 2021) utilizes a gradient signal from a classifier to generate samples that satisfy a condition. Classifier-free guidance (Ho & Salimans, 2021) also aims at conditional generation but relies on both unconditional and conditional scores. Furthermore, various methods have been proposed to enable controllable generation using auxiliary models with a pre-trained unconditional score model (Graikos et al., 2022;Song et al., 2023). On the other hand, discriminator guidance (Kim et al., 2023) serves a different purpose by enhancing the sampling performance of a diffusion model through the use of a discriminator that distinguishes between real images and generated images. EGSDE (Zhao et al., 2022) leverages guidance signals based on energy functions, enhancing unpaired image-to-image translation. Guidance methods have the advantage of utilizing pre-trained score networks without the need for additional training. However, they require separate network training for guidance and additional network evaluation during the sampling process. \n\nThere is a body of work on score-matching regularization for better likelihood estimation (Lu et al., 2022;Zheng et al., 2023b;Lai et al., 2023). Na et al. (2024) propose a regularized conditional scorematching objective to mitigate label noise. The unique benefit of score-matching regularization is that it does not require an additional network at the inference stage.",
            "reference_string": "[268230892 | Kim et al. | 2024 | Citations: 23]"
        },
        {
            "title": "Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.16365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "78859465",
                    "name": "Hossein Souri"
                },
                {
                    "authorId": "7364835",
                    "name": "Arpit Bansal"
                },
                {
                    "authorId": "2075283338",
                    "name": "Hamid Kazemi"
                },
                {
                    "authorId": "120165773",
                    "name": "Liam H. Fowl"
                },
                {
                    "authorId": "2056290221",
                    "name": "Aniruddha Saha"
                },
                {
                    "authorId": "2284863781",
                    "name": "Jonas Geiping"
                },
                {
                    "authorId": "2264628126",
                    "name": "Andrew Gordon Wilson"
                },
                {
                    "authorId": "2254122195",
                    "name": "Rama Chellappa"
                },
                {
                    "authorId": "2237426712",
                    "name": "Tom Goldstein"
                },
                {
                    "authorId": "121592562",
                    "name": "Micah Goldblum"
                }
            ],
            "abstract": "Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .",
            "corpus_id": 268681777,
            "sentences": [
                {
                    "corpus_id": "268681777",
                    "title": "Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion",
                    "text": "Diffusion models [Song andErmon, 2019, Ho et al., 2020] in machine learning have seen a remarkable evolution, particularly in their application to image generation tasks.These models, which simulate the process of transforming a random noise distribution into a specific data distribution, have incorporated various guidance mechanisms to direct this transformation process more precisely.\n\nThe concept of condition [Ho and Salimans, 2022, Bansal et al., 2022, Nichol et al., 2021, Whang et al., 2022, Wang et al., 2022a, Li et al., 2023, Zhang and Agrawala, 2023] or guidance [Dhariwal and Nichol, 2021, Kawar et al., 2022, Wang et al., 2022b, Chung et al., 2022a, Lugmayr et al., 2022, Chung et al., 2022b, Graikos et al., 2022, Bansal et al., 2023] in diffusion models is crucial for achieving outputs that adhere to specific characteristics or criteria, a necessity in applications demanding high precision.\n\nInitially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria.\n\nTo address these constraints, the Control Net [Zhang and Agrawala, 2023] approach was introduced, representing a significant development in guided diffusion models.Control Net, though requiring less training than traditional classifier guidance, still necessitates some degree of model training.",
                    "score": 0.4875373876657201,
                    "section_title": "Guidance in Diffusion Models",
                    "char_start_offset": 8100,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 170,
                            "end": 389
                        },
                        {
                            "start": 391,
                            "end": 911
                        },
                        {
                            "start": 913,
                            "end": 1106
                        },
                        {
                            "start": 1108,
                            "end": 1234
                        },
                        {
                            "start": 1234,
                            "end": 1362
                        },
                        {
                            "start": 1362,
                            "end": 1470
                        },
                        {
                            "start": 1470,
                            "end": 1600
                        },
                        {
                            "start": 1600,
                            "end": 1815
                        },
                        {
                            "start": 1817,
                            "end": 1981
                        },
                        {
                            "start": 1981,
                            "end": 2112
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 480,
                            "end": 500,
                            "matchedPaperCorpusId": "244908890"
                        },
                        {
                            "start": 664,
                            "end": 686,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 729,
                            "end": 751,
                            "matchedPaperCorpusId": "256846836"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85205078125
                }
            ],
            "relevance_judgement": 0.85205078125,
            "relevance_judgment_input_expanded": "# Title: Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion\n# Venue: arXiv.org\n# Authors: Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam H. Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum\n## Abstract\nModern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .\n## Guidance in Diffusion Models\nDiffusion models [Song andErmon, 2019, Ho et al., 2020] in machine learning have seen a remarkable evolution, particularly in their application to image generation tasks.These models, which simulate the process of transforming a random noise distribution into a specific data distribution, have incorporated various guidance mechanisms to direct this transformation process more precisely.\n\nThe concept of condition [Ho and Salimans, 2022, Bansal et al., 2022, Nichol et al., 2021, Whang et al., 2022, Wang et al., 2022a, Li et al., 2023, Zhang and Agrawala, 2023] or guidance [Dhariwal and Nichol, 2021, Kawar et al., 2022, Wang et al., 2022b, Chung et al., 2022a, Lugmayr et al., 2022, Chung et al., 2022b, Graikos et al., 2022, Bansal et al., 2023] in diffusion models is crucial for achieving outputs that adhere to specific characteristics or criteria, a necessity in applications demanding high precision.\n\nInitially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria.\n\nTo address these constraints, the Control Net [Zhang and Agrawala, 2023] approach was introduced, representing a significant development in guided diffusion models.Control Net, though requiring less training than traditional classifier guidance, still necessitates some degree of model training.",
            "reference_string": "[268681777 | Souri et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 101,
            "citation_count": 44,
            "influential_citation_count": 10,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4527007",
                    "name": "Hunter M Nisonoff"
                },
                {
                    "authorId": "2304669531",
                    "name": "Junhao Xiong"
                },
                {
                    "authorId": "2304468952",
                    "name": "Stephan Allenspach"
                },
                {
                    "authorId": "2261919343",
                    "name": "Jennifer Listgarten"
                }
            ],
            "abstract": "Generative models on discrete state-spaces have a wide range of potential applications, particularly in the domain of natural sciences. In continuous state-spaces, controllable and flexible generation of samples with desired properties has been realized using guidance on diffusion and flow models. However, these guidance approaches are not readily amenable to discrete state-space models. Consequently, we introduce a general and principled method for applying guidance on such models. Our method depends on leveraging continuous-time Markov processes on discrete state-spaces, which unlocks computational tractability for sampling from a desired guided distribution. We demonstrate the utility of our approach, Discrete Guidance, on a range of applications including guided generation of small-molecules, DNA sequences and protein sequences.",
            "corpus_id": 270214339,
            "sentences": [
                {
                    "corpus_id": "270214339",
                    "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
                    "text": "Conditioning of diffusion models is typically achieved by way of introducing guidance, either in a classifier-free way (Ho & Salimans, 2021), or by using a classifier (Dhariwal & Nichol, 2021). Classifier guidance, in particular, provides the key ability to leverage an already-trained unconditional model without re-training it, to generate conditional samples simply by modulating its generative process with guidance from a classifier (Du et al., 2023). Such a strategy is particularly important in the sciences where one frequently has orders of magnitude more unlabelled data than labeled data; one can therefore invest considerably in one unconditional model and then repurpose it for many different tasks (Ingraham et al., 2023). \n\nHerein, for the first time, we introduce a principled approach, Discrete Guidance, to perform guidance for diffusion and flow models on discrete state-spaces (Figure 1). Discrete Guidance applies not only to continuous-time diffusion and flow models on discrete state-spaces, but also, in principle, to the broad class of generative models on discrete state-spaces realized through CTMCs. Our key insight is that, in continuous time, only a single dimension of the discrete state-space Markov chain can change at any point in time, making guidance exact and tractable. We empirically demonstrate the effectiveness of Discrete Guidance by applying it to a broad set of discrete state-space conditional generation tasks, including small-molecules, DNA sequences, and protein sequences. Because in the sciences, conditioning is often on real-valued properties rather than on satisfying a particular class, we refer to guidance more generally as predictor guidance and predictor-free guidance. \n\nIn summary, our contributions are: \n\n1. We introduce an exact, theoretically rigorous, general framework, Discrete Guidance (DG), for applying both predictor guidance (PG) and predictor-free guidance (PFG) to generative models in discrete state-spaces based on CTMCs, such as diffusion and flow matching. 2. We develop an approximation to our guidance framework that leads to more efficient sampling while maintaining sample quality. 3. We empirically explore and demonstrate the potential utility of our guidance framework across a wide range of problem domains, using either diffusion or flow models.",
                    "score": 0.477114566567828,
                    "section_title": "Diffusion and Flow-based Generative Models for Science",
                    "char_start_offset": 3230,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 736
                        },
                        {
                            "start": 739,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1127
                        },
                        {
                            "start": 1128,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1728
                        },
                        {
                            "start": 1731,
                            "end": 1765
                        },
                        {
                            "start": 1768,
                            "end": 2035
                        },
                        {
                            "start": 2036,
                            "end": 2164
                        },
                        {
                            "start": 2165,
                            "end": 2333
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 119,
                            "end": 140,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 167,
                            "end": 192,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 438,
                            "end": 455,
                            "matchedPaperCorpusId": "260384616"
                        },
                        {
                            "start": 712,
                            "end": 735,
                            "matchedPaperCorpusId": "254222077"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84375
                }
            ],
            "relevance_judgement": 0.84375,
            "relevance_judgment_input_expanded": "# Title: Unlocking Guidance for Discrete State-Space Diffusion and Flow Models\n# Venue: International Conference on Learning Representations\n# Authors: Hunter M Nisonoff, Junhao Xiong, Stephan Allenspach, Jennifer Listgarten\n## Abstract\nGenerative models on discrete state-spaces have a wide range of potential applications, particularly in the domain of natural sciences. In continuous state-spaces, controllable and flexible generation of samples with desired properties has been realized using guidance on diffusion and flow models. However, these guidance approaches are not readily amenable to discrete state-space models. Consequently, we introduce a general and principled method for applying guidance on such models. Our method depends on leveraging continuous-time Markov processes on discrete state-spaces, which unlocks computational tractability for sampling from a desired guided distribution. We demonstrate the utility of our approach, Discrete Guidance, on a range of applications including guided generation of small-molecules, DNA sequences and protein sequences.\n## Diffusion and Flow-based Generative Models for Science\nConditioning of diffusion models is typically achieved by way of introducing guidance, either in a classifier-free way (Ho & Salimans, 2021), or by using a classifier (Dhariwal & Nichol, 2021). Classifier guidance, in particular, provides the key ability to leverage an already-trained unconditional model without re-training it, to generate conditional samples simply by modulating its generative process with guidance from a classifier (Du et al., 2023). Such a strategy is particularly important in the sciences where one frequently has orders of magnitude more unlabelled data than labeled data; one can therefore invest considerably in one unconditional model and then repurpose it for many different tasks (Ingraham et al., 2023). \n\nHerein, for the first time, we introduce a principled approach, Discrete Guidance, to perform guidance for diffusion and flow models on discrete state-spaces (Figure 1). Discrete Guidance applies not only to continuous-time diffusion and flow models on discrete state-spaces, but also, in principle, to the broad class of generative models on discrete state-spaces realized through CTMCs. Our key insight is that, in continuous time, only a single dimension of the discrete state-space Markov chain can change at any point in time, making guidance exact and tractable. We empirically demonstrate the effectiveness of Discrete Guidance by applying it to a broad set of discrete state-space conditional generation tasks, including small-molecules, DNA sequences, and protein sequences. Because in the sciences, conditioning is often on real-valued properties rather than on satisfying a particular class, we refer to guidance more generally as predictor guidance and predictor-free guidance. \n\nIn summary, our contributions are: \n\n1. We introduce an exact, theoretically rigorous, general framework, Discrete Guidance (DG), for applying both predictor guidance (PG) and predictor-free guidance (PFG) to generative models in discrete state-spaces based on CTMCs, such as diffusion and flow matching. 2. We develop an approximation to our guidance framework that leads to more efficient sampling while maintaining sample quality. 3. We empirically explore and demonstrate the potential utility of our guidance framework across a wide range of problem domains, using either diffusion or flow models.",
            "reference_string": "[270214339 | Nisonoff et al. | 2024 | Citations: 44]"
        },
        {
            "title": "CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 50,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.04559",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.04559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2174547685",
                    "name": "Shi-You Xu"
                }
            ],
            "abstract": "Image captioning task has been extensively researched by previous work. However, limited experiments focus on generating captions based on non-autoregressive text decoder. Inspired by the recent success of the denoising diffusion model on image synthesis tasks, we apply denoising diffusion probabilistic models to text generation in image captioning tasks. We show that our CLIP-Diffusion-LM is capable of generating image captions using significantly fewer inference steps than autoregressive models. On the Flickr8k dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our code is available at https://github.com/xu-shitong/diffusion-image-captioning.",
            "corpus_id": 252780170,
            "sentences": [
                {
                    "corpus_id": "252780170",
                    "title": "CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning",
                    "text": "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0.",
                    "score": 0.4185848365485682,
                    "section_title": "classification free guidance",
                    "char_start_offset": 14046,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 695
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8193359375
                }
            ],
            "relevance_judgement": 0.8193359375,
            "relevance_judgment_input_expanded": "# Title: CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning\n# Venue: arXiv.org\n# Authors: Shi-You Xu\n## Abstract\nImage captioning task has been extensively researched by previous work. However, limited experiments focus on generating captions based on non-autoregressive text decoder. Inspired by the recent success of the denoising diffusion model on image synthesis tasks, we apply denoising diffusion probabilistic models to text generation in image captioning tasks. We show that our CLIP-Diffusion-LM is capable of generating image captions using significantly fewer inference steps than autoregressive models. On the Flickr8k dataset, the model achieves 0.1876 BLEU-4 score. By training on the combined Flickr8k and Flickr30k dataset, our model achieves 0.2470 BLEU-4 score. Our code is available at https://github.com/xu-shitong/diffusion-image-captioning.\n## classification free guidance\nThe effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0.",
            "reference_string": "[252780170 | Xu | 2022 | Citations: 12]"
        },
        {
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 9,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260304347",
                    "name": "Jiajun Ma"
                },
                {
                    "authorId": "2112911801",
                    "name": "Tianyang Hu"
                },
                {
                    "authorId": "2259551815",
                    "name": "Wenjia Wang"
                },
                {
                    "authorId": "2136769001",
                    "name": "Jiacheng Sun"
                }
            ],
            "abstract": "Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.",
            "corpus_id": 264172506,
            "sentences": [
                {
                    "corpus_id": "264172506",
                    "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
                    "text": "Diffusion probabilistic model (DPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020b] is a powerful generative model that employs a forward diffusion process to gradually add noise to data and generate new data from noise through a reversed process. DPM's exceptional sample quality and scalability have significantly contributed to the success of Artificial Intelligence Generated Content (AIGC) in various domains, including images [Saharia et al., 2022, Ramesh et al., 2022, 2021, Rombach et al., 2022], videos [Ho et al., 2022b, Singer et al., 2022, Ho et al., 2022a, Molad et al., 2023], and 3D objects [Poole et al., 2022, Lin et al., 2023, Wang et al., 2023]. \n\nConditional generation is one of the core tasks of AIGC. With the diffusion formulation, condition injection, especially the classical class condition, becomes more transparent as it can be modeled as an extra term during the reverse process. To align with the diffusion process, Dhariwal and Nichol [2021] proposed classifier guidance (CG) to train a time/noise-dependent classifier and demonstrated significant quality improvement over the unguided baseline. Ho and Salimans [2022] later proposed classifier-free guidance (CFG) to implicitly implement the classifier gradient with the score function difference and achieved superior performance in the classical class-conditional image generation. However, both CG and CFG require extra training with labeled data, which is not only time-consuming but also practically cumbersome, especially when adapting to new conditions. To reduce computational costs, training-free guidance methods have been proposed [Bansal et al., 2023] that take advantage of pretrained discriminative models. \n\nDespite the improved flexibility, training-free guidance has not demonstrated convincing performance compared to CG & CFG in formal quantitative evaluation of guiding diffusion generation. There seems to be an irreconcilable trade-off between performance and flexibility and the current guidance schemes are still to be desired.",
                    "score": 0.3866509228738399,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 683
                        },
                        {
                            "start": 686,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1385
                        },
                        {
                            "start": 1386,
                            "end": 1562
                        },
                        {
                            "start": 1563,
                            "end": 1722
                        },
                        {
                            "start": 1725,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2053
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 64,
                            "end": 81,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 499,
                            "end": 522,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 548,
                            "end": 569,
                            "matchedPaperCorpusId": "252595919"
                        },
                        {
                            "start": 644,
                            "end": 662,
                            "matchedPaperCorpusId": "253708074"
                        },
                        {
                            "start": 966,
                            "end": 992,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1644,
                            "end": 1665,
                            "matchedPaperCorpusId": "256846836"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81298828125
                }
            ],
            "relevance_judgement": 0.81298828125,
            "relevance_judgment_input_expanded": "# Title: Elucidating The Design Space of Classifier-Guided Diffusion Generation\n# Venue: International Conference on Learning Representations\n# Authors: Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun\n## Abstract\nGuidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.\n## Introduction\nDiffusion probabilistic model (DPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020b] is a powerful generative model that employs a forward diffusion process to gradually add noise to data and generate new data from noise through a reversed process. DPM's exceptional sample quality and scalability have significantly contributed to the success of Artificial Intelligence Generated Content (AIGC) in various domains, including images [Saharia et al., 2022, Ramesh et al., 2022, 2021, Rombach et al., 2022], videos [Ho et al., 2022b, Singer et al., 2022, Ho et al., 2022a, Molad et al., 2023], and 3D objects [Poole et al., 2022, Lin et al., 2023, Wang et al., 2023]. \n\nConditional generation is one of the core tasks of AIGC. With the diffusion formulation, condition injection, especially the classical class condition, becomes more transparent as it can be modeled as an extra term during the reverse process. To align with the diffusion process, Dhariwal and Nichol [2021] proposed classifier guidance (CG) to train a time/noise-dependent classifier and demonstrated significant quality improvement over the unguided baseline. Ho and Salimans [2022] later proposed classifier-free guidance (CFG) to implicitly implement the classifier gradient with the score function difference and achieved superior performance in the classical class-conditional image generation. However, both CG and CFG require extra training with labeled data, which is not only time-consuming but also practically cumbersome, especially when adapting to new conditions. To reduce computational costs, training-free guidance methods have been proposed [Bansal et al., 2023] that take advantage of pretrained discriminative models. \n\nDespite the improved flexibility, training-free guidance has not demonstrated convincing performance compared to CG & CFG in formal quantitative evaluation of guiding diffusion generation. There seems to be an irreconcilable trade-off between performance and flexibility and the current guidance schemes are still to be desired.",
            "reference_string": "[264172506 | Ma et al. | 2023 | Citations: 9]"
        },
        {
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332536645",
                    "name": "Zak Buzzard"
                }
            ],
            "abstract": "Diffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.",
            "corpus_id": 277043967,
            "sentences": [
                {
                    "corpus_id": "277043967",
                    "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
                    "text": "Diffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.",
                    "score": 0.4945227949248744,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.810546875
                }
            ],
            "relevance_judgement": 0.810546875,
            "relevance_judgment_input_expanded": "# Title: Understanding the Quality-Diversity Trade-off in Diffusion Language Models\n# Venue: arXiv.org\n# Authors: Zak Buzzard\n## Abstract\nDiffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.\n",
            "reference_string": "[277043967 | Buzzard | 2025 | Citations: 0]"
        },
        {
            "title": "Semantic Guidance Tuning for Text-To-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.15964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268815667",
                    "name": "Hyun Kang"
                },
                {
                    "authorId": "1875315185",
                    "name": "Dohae Lee"
                },
                {
                    "authorId": "2235816037",
                    "name": "Myungjin Shin"
                },
                {
                    "authorId": "2152633600",
                    "name": "In-Kwon Lee"
                }
            ],
            "abstract": "Recent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in response to prompts. Project page is available at: https://korguy.github.io/",
            "corpus_id": 266551068,
            "sentences": [
                {
                    "corpus_id": "266551068",
                    "title": "Semantic Guidance Tuning for Text-To-Image Diffusion Models",
                    "text": "Meanwhile, when the model fails to accurately render a specific concept, the similarity score for that concept fluctuates and generally remains lower, highlighting the direct impact of concept representation on the quality of the generated image. \n\nConcept Guidance Based on the observation that the cosine similarity k is associated with semantic alignment, we propose a concept guidance term \u03d5 that modulates the noise estimate of the diffusion model based on the similarity. Formally, extended to Eq. (2), we compute, \u03f5 \u03b8 (z t ) + w g s p + \u03d5(z t , S c , s p ), (8) in which S c = {s a , s 1 s , s 2 s , ..., s n s }. The concept guidance \u03d5 is defined as, \n\nwhere w c is the concept guidance scale and s c \u2208 S c . The \u03b3 is a delta function based on the cosine similarity between s p and s c , \n\nThe threshold \u03b7 is a hyperparameter that corresponds to the minimum inclusion of a concept in prompt score. Naturally, larger \u03b7 increases the effect of concept guidance. We empirically found that \u03b7 = 1/(n + 1) generally works well. The guidance direction \u03c8(s p , s c ) is determined as, \n\nFor subject concepts, a lower cosine similarity suggests a divergence of it from the prompt score. In such cases, our approach involves steering the model's guidance towards these concepts to ensure their inclusion. Conversely, the abstract concept is understood as an independent component of the main score that is distinct from the subject concepts. Therefore, a low cosine similarity here indicates that the main score closely resembles the aggregate of subject concepts, potentially leading to a fusion of concepts in the generated results. To counteract this, when the cosine similarity for the abstract concept decreases, we adjust the guidance so that k(s p , s s ) > \u03b7. \n\nNote that our method doesn't need extra training or optimization and can be used with any diffusion model that uses classifier-free guidance [11].",
                    "score": 0.3876258272559843,
                    "section_title": "Concept Extraction",
                    "char_start_offset": 13472,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 246
                        },
                        {
                            "start": 249,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 658
                        },
                        {
                            "start": 661,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 795
                        },
                        {
                            "start": 798,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1765
                        },
                        {
                            "start": 1768,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 565,
                            "end": 568,
                            "matchedPaperCorpusId": "247628171"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8056640625
                }
            ],
            "relevance_judgement": 0.8056640625,
            "relevance_judgment_input_expanded": "# Title: Semantic Guidance Tuning for Text-To-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: Hyun Kang, Dohae Lee, Myungjin Shin, In-Kwon Lee\n## Abstract\nRecent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in response to prompts. Project page is available at: https://korguy.github.io/\n## Concept Extraction\nMeanwhile, when the model fails to accurately render a specific concept, the similarity score for that concept fluctuates and generally remains lower, highlighting the direct impact of concept representation on the quality of the generated image. \n\nConcept Guidance Based on the observation that the cosine similarity k is associated with semantic alignment, we propose a concept guidance term \u03d5 that modulates the noise estimate of the diffusion model based on the similarity. Formally, extended to Eq. (2), we compute, \u03f5 \u03b8 (z t ) + w g s p + \u03d5(z t , S c , s p ), (8) in which S c = {s a , s 1 s , s 2 s , ..., s n s }. The concept guidance \u03d5 is defined as, \n\nwhere w c is the concept guidance scale and s c \u2208 S c . The \u03b3 is a delta function based on the cosine similarity between s p and s c , \n\nThe threshold \u03b7 is a hyperparameter that corresponds to the minimum inclusion of a concept in prompt score. Naturally, larger \u03b7 increases the effect of concept guidance. We empirically found that \u03b7 = 1/(n + 1) generally works well. The guidance direction \u03c8(s p , s c ) is determined as, \n\nFor subject concepts, a lower cosine similarity suggests a divergence of it from the prompt score. In such cases, our approach involves steering the model's guidance towards these concepts to ensure their inclusion. Conversely, the abstract concept is understood as an independent component of the main score that is distinct from the subject concepts. Therefore, a low cosine similarity here indicates that the main score closely resembles the aggregate of subject concepts, potentially leading to a fusion of concepts in the generated results. To counteract this, when the cosine similarity for the abstract concept decreases, we adjust the guidance so that k(s p , s s ) > \u03b7. \n\nNote that our method doesn't need extra training or optimization and can be used with any diffusion model that uses classifier-free guidance [11].",
            "reference_string": "[266551068 | Kang et al. | 2023 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271161847",
            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
            "text": "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain.",
            "score": 0.5898888599939833,
            "section_title": "CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 13008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "On the other hand, we present a continuous-time formulation which allows guidance to be achieved exactly by adjusting the rates, and derived Taylor-approximated guidance to specifically approximate the likelihood ratios of only the transitions that change a single dimension. Furthermore, Discrete Guidance allows the user to adjust the number of time steps at inference time and choose among different sampling algorithms (e.g., those discussed in Appendix B) for the best performance on downstream tasks. Wang et al. (2024) use a similar guidance approach to DiGress in a diffusion language model for protein sequences. \n\nFor autoregressive models, Yang & Klein (2021) propose an approach (FUDGE) of applying guidance by training a predictor on each step of the decoding process and adjusting the decoding probabilities accordingly. This can be viewed as being similar to how a predictor can be trained at various noise scale for predictor guidance in diffusion. In an alternative approach (PPLM), Dathathri et al. (2020) steer the output of a pre-trained language model towards desired attributes by applying gradient ascent on the hidden states using an attribute model. There is also extensive literature on fine-tuning a pre-trained autoregressive models for controllable generations (Schulman et al., 2017;Rafailov et al., 2023), and some of these approaches have been recently applied to tasks in the natural sciences (Widatalla et al., 2024;Chennakesavalu et al., 2024). We note that a key advantage of guidance over these approaches is that guidance can leverage a already-trained unconditional model without further fine-tuning, which can be prohibitively expensive. Furthermore, predictor guidance also allows for different predictors to be combined in a modular fashion.",
            "score": 0.5641779593608185,
            "section_title": "E.1 Additional Discussion",
            "char_start_offset": 61481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1783
                }
            ],
            "ref_mentions": [
                {
                    "start": 651,
                    "end": 670,
                    "matchedPaperCorpusId": "233210709"
                },
                {
                    "start": 1000,
                    "end": 1023,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 1426,
                    "end": 1450,
                    "matchedPaperCorpusId": "269976016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09930419921875
        },
        {
            "corpus_id": "274762839",
            "title": "Simple Guidance Mechanisms for Discrete Diffusion Models",
            "text": "Our guidance results indicate that (1) classifier-free guidance is more useful when paired with diffusion models compared to AR (Table 4) and that (2) our proposed D-CBG is the best classifierbased method for discrete guidance, especially when combined with UDLM (Table 5 & Figure 3). \n\nBaselines For guidance experiments, our primary baseline is the dominant AR approach. We compare to three flavors of guided AR. The first is applying D-CFG to AR models. We also use the established control mechanisms of Plug-and-play language models (PPLM; Dathathri et al. (2019)) and FUDGE (Yang & Klein, 2021). To demonstrate the better performance of our D-CBG method, we compare to Gruver et al. (2024) (NOS), an extension of PPLM to discrete diffusion. (Hoogeboom et al., 2021) 1.39 Discrete Flow \u2020 (Tran et al., 2019) 1.23 Autoregressive \u2020",
            "score": 0.5552540942343969,
            "section_title": "GUIDED DISCRETE DIFFUSION",
            "char_start_offset": 15825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 694,
                    "matchedPaperCorpusId": "258987335"
                },
                {
                    "start": 746,
                    "end": 770,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 792,
                    "end": 811,
                    "matchedPaperCorpusId": "157062310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.576171875
        },
        {
            "corpus_id": "249926846",
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "score": 0.5386939393779961,
            "section_title": "Classifier-Free Guidance and Reranking",
            "char_start_offset": 15352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1835,
                    "end": 1838,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "259165510",
            "title": "A Survey of Diffusion Models in Natural Language Processing",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015b;Ho et al., 2020;Song et al., 2020) have shown remarkable performance in image generation and attracted huge attention in the field of artificial intelligence. Researchers have also adopted the models to the field of natural language processing (NLP) and have just started to explore their generative capabilities in the domain (Fig. 1). To date, diffusion models have been applied to a wide range of generative NLP tasks, such as unconditional text generation, controllable text generation, machine translation, and text simplification. \n\nThe main challenge in incorporating diffusion models into NLP is the discreteness of texts, which contrasts with the continuous space in which diffusion is modeled. To address this challenge, researchers have introduced modifications to the models, and we categorize them into two approaches: \n\n\u2022 Discrete diffusion models built on categorical distributions. This method generalizes diffusion process to the discrete domain by corrupting and refining sentences at the token level. \u2022 Embedding diffusion models encode discrete texts into continuous space and perform Gaussian noising. As part of this method, additional embedding and rounding steps can be used in the forward and reverse processes, respectively, to convert tokens into embeddings. \n\nIn the following sections, we first introduce the general framework of vanilla diffusion models and the modified architecture for discrete state spaces in Section 2. In Section 3, we classify the surveyed architectures into two aforementioned approaches (discrete vs embedding diffusion models), using specific criteria that have been proposed. In Sec-tion 4, we conduct a detailed comparative analysis of diffusion models against other generative models in NLP domain. Based on empirical evidence, we highlight the advantages of diffusion models over autoregressive (AR) models, specifically in terms of parallel generation, text interpolation, token-level control, and robustness. In addition, we explore how various surveyed architectures have incorporated the Transformer with diffusion models for NLP. We highlight algorithms and techniques proposed for diffusion models in NLP in Section 5. Finally, we discuss potential future directions that are both timely and worthy of exploration in Section 6.",
            "score": 0.5268215926296407,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0191192626953125
        },
        {
            "corpus_id": "272593086",
            "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
            "text": "Diffusion models are a powerful method for generating data from a given distribution. To enhance their utility, extensive research has focused on developing techniques for guiding the output rather than relying solely on unconditional generation. These guidance methods have significantly evolved since the introduction of classifier guidance [1], which established diffusion models as the state-of-the-art for image generation, surpassing GANs. Classifier guidance introduced a framework allowing the gradients from a classifier to influence the generation process. The main limitation of this method is that it requires the classifier to perform well on the data at all timesteps t. This necessitates training a guidance model with a specific noise scheduler to augment the training data across all timesteps. \n\nClassifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation [2]. During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training. \n\nA new flexible approach for guiding the sampling of diffusion models is training-free guidance, which allows guidance models to be paired with a diffusion model without requiring the guidance model to be trained on noisy data generated by the noise scheduler [3]- [6]. This enables the creation of foundation diffusion models that can be combined with guidance models in a plug-and-play manner. It also simplifies benchmarking and building on other researchers' work, as separate guidance models are unnecessary and can be easily shared. These improvements have made diffusion models the preferred architecture in many domains where control and human feedback are essential. Moreover, unlike autoregressive models, diffusion models learn the joint data distribution directly and do not rely on chaining conditional distributions. Their iterative nature also provides unique opportunities to guide the generation process in ways that autoregressive or purely conditional models cannot replicate. \n\nClassifier guidance [7] and classifier-free guidance [8] have previously been implemented for graph generation using a discrete diffusion model.",
            "score": 0.5253885306490957,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2396
                },
                {
                    "start": 2399,
                    "end": 2543
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 992,
                    "end": 995,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "265351587",
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "score": 0.5139842026081904,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "259165510",
            "title": "A Survey of Diffusion Models in Natural Language Processing",
            "text": "This survey paper provides a comprehensive review of the use of diffusion models in natural language processing (NLP). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In NLP, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in NLP, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive (AR) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to AR models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures and semantic contents, and robustness. Exploring further permutations of integrating Transformers into diffusion models would be a valuable pursuit. Also, the development of multimodal diffusion models and large-scale diffusion language models with notable capabilities for few-shot learning would be important directions for the future advance of diffusion models in NLP.",
            "score": 0.5117560887898397,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0236053466796875
        },
        {
            "corpus_id": "252917661",
            "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
            "text": "In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition x is also a sequence of words, applying Diffusion-LM can be difficult. The reason is that classifiers are attributes-oriented, and we can not train hundreds-of-thousands classifiers to model the semantic meaning between conditions and generated sentences. \n\nSEQ2SEQ is an essential setting in NLP that covers a wide range of important tasks such as openended sentence generation, dialogue, paraphrasing, and text style transfer. In this paper, we propose DIFFUSEQ, depicted in Figure 1(c), a classifier-free diffusion model that supports SEQ2SEQ text generation tasks. By modeling the conditional probability of the target sentence w given context x using one single model, one advantage of DIFFUSEQ is that this paradigm allows a complete model to fit data distribution and utilize conditional guidance, rather than depending on a separate classifier. \n\nDifferent from canonical generation approaches in an autoregressive (AR) left-to-right manner (Radford et al., 2019), DIFFUSEQ generates text tokens parallelly in the non-autoregressive (NAR) way. \n\nTo corroborate the effectiveness of our DIFFUSEQ, we conduct experiments on four SEQ2SEQ tasks. \n\nCompared to AR and NAR models, which suffer from the \"degeneration\" problem (Holtzman et al., 2019) and rely on decoding strategies, DIFFUSEQ can achieve considerable sentence-level diversity without sacrificing the quality (see \u00a7 4.2). \n\nTo sum up, we make a series of technical and conceptual contributions: (a) we are the first to deploy the diffusion model on SEQ2SEQ text generation, and our proposed DIFFUSEQ as a conditional language model is trained end-to-end in a classifier-free manner; (b) we establish a theoretical connection among AR, NAR and DIFFUSEQ models, and justify DIFFUSEQ as an extension of iterative-NAR models; (c) with strong empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.",
            "score": 0.5115921379576627,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 1307,
                    "end": 1330,
                    "matchedPaperCorpusId": "127986954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "270062517",
            "title": "Glauber Generative Model: Discrete Diffusion Models via Binary Classification",
            "text": "Within this framework, DiffusER (Reid et al., 2022) considers the noising Markov Chain to be Levenshtein edit operations over text for language modeling and evaluates its performance for downstream tasks such as translation and textual style transfer. \n\nWhile continuous diffusion models are learnt by solving the regression task of score matching, another line of work (Lou et al., 2023;Meng et al., 2022) considers its discrete analogue called ratio matching (Hyv\u00e4rinen, 2007;Sun et al., 2022). These works that argue that Argmax flow and D3PM in fact learn these importance ratios of probability distributions indirectly, and propose to learn these directly by solving a class of regression tasks. While (Meng et al., 2022) considers least squares regression in this setting, (Lou et al., 2023) uses a tailored loss function. Directly learning these ratios improves the learning performance as observed in SEDD (Lou et al., 2023), achieving competitive performance to autoregressive models for language modeling via discrete diffusion.",
            "score": 0.5016857940625715,
            "section_title": "RELATED WORK",
            "char_start_offset": 4224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1038
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 406,
                    "matchedPaperCorpusId": "253255468"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "2352990"
                },
                {
                    "start": 478,
                    "end": 495,
                    "matchedPaperCorpusId": "254096040"
                },
                {
                    "start": 707,
                    "end": 726,
                    "matchedPaperCorpusId": "253255468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0127716064453125
        },
        {
            "corpus_id": "277043967",
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "text": "Diffusion models have seen immense success in modelling continuous data across a range of domains such as vision and audio. Despite the challenges of adapting diffusion models to discrete data, recent work explores their application to text generation by working in the continuous embedding space. However, these models lack a natural means to control the inherent trade-off between quality and diversity as afforded by the temperature hyperparameter in autoregressive models, hindering understanding of model performance and restricting generation quality. This work proposes the use of classifier-free guidance and stochastic clamping for manipulating the quality-diversity trade-off on sequence-to-sequence tasks, demonstrating that these techniques may be used to improve the performance of a diffusion language model.",
            "score": 0.4945227949248744,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "278327925",
            "title": "T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models",
            "text": "Recent study [Li et al., 2024b] demonstrates the pivotal role of the inference stage in affecting the performance of diffusion models. Building on this, we explored the sensitivity of the flow matching diffusion model to key inference parameters: classifier-free guidance scales (CFG) and generation time steps,evaluated using MRR@10. Figure 4 shows a heatmap illustrating performance impact, with yellow regions yielding superior results and green areas reflecting suboptimal performance. Notably, the model achieves higher MRR@10 scores within the range of CFG scores between 7  [Kawaguchi et al., 2025] utilize time series features, such as upward trends and peaks, as dictionary entries to generate coherent signals. At the fragment level, several researchers have introduced datasets tailored for time series reasoning tasks [Williams et al., 2024;Chow et al., 2024]. A large-scale, fine-grained, general-purpose text-time series dataset for time series generation tasks remains in its early exploration. \n\nText-Time Series Generation. The general text-to-time series paradigm can be achieved through contrastive learning or generative modeling. Recently, contrastive learning has been employed to facilitate text-to-time series mapping. However, these approaches are primarily focused on retrieval tasks [Ito et al., 2024;Rizhko and Bloom, 2024] and cannot be directly applied to time series generation. In contrast, generative modeling, including variational autoencoders (VAEs) [Desai et al., 2021;Lee et al., 2023], diffusion models [Yuan and Qiao, 2024;Kong et al., 2020;Wen et al., 2023;Narasimhan et al., 2024], and large language models [OpenAI, 2023;Dubey et al., 2024], provides more versatile frameworks for generating time series conditioned on textual descriptions. Among these, conditional diffusion models [Yuan and Qiao, 2024;Cao et al., 2024;Narasimhan et al., 2024] show promise for text-to-time series generation due to their ability to model complex temporal dynamics and generate temporally coherent sequences.",
            "score": 0.4930027725760109,
            "section_title": "Parameter Sensitivity (RQ4)",
            "char_start_offset": 19064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 31,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 830,
                    "end": 853,
                    "matchedPaperCorpusId": "235623791"
                },
                {
                    "start": 1310,
                    "end": 1328,
                    "matchedPaperCorpusId": "255439889"
                },
                {
                    "start": 1486,
                    "end": 1506,
                    "matchedPaperCorpusId": "244130293"
                },
                {
                    "start": 1506,
                    "end": 1523,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 1542,
                    "end": 1563,
                    "matchedPaperCorpusId": "268248137"
                },
                {
                    "start": 1581,
                    "end": 1598,
                    "matchedPaperCorpusId": "235623791"
                },
                {
                    "start": 1598,
                    "end": 1622,
                    "matchedPaperCorpusId": "268709089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "208119272",
            "title": "Efficient Reuse of Natural Language Processing Models for Phenotype-Mention Identification in Free-text Electronic Medical Records: A Phenotype Embedding Approach",
            "text": "Technically, the guidance to model adaptation is composed of two parts: avoid duplicate waste (skip validation/training efforts on cases the model is already familiar with); and avoid imbalance waste (group new language patterns together so that validation/continuous training on each group separately can be more efficient than doing it over the whole corpus). To quantify the guidance effectiveness, the following metrics are introduced. \n\n\u2022  \u2022 Imbalance Waste To achieve convergence performances, an NLP model needs to be trained on a minimal number (denoted as e) of samples from each language pattern. Calling the language pattern set in a new task as C = {C 1 , ..., C k }, the following equation counts the minimal number of samples needed for getting convergent results. \n\nWhen the language patterns are identifiable, the Imbalance Waste that can be avoided is quantified as \n\n\u2022 Accuracies Our approach uses a distance threshold between centroids of clusters and reference vector to determine whether the language patterns of identified clusters are known to the pre-trained NLP model. For those whose distances are within the threshold, we assume the model performs well as they are similar enough. To evaluate whether the approach can really identify familiar patterns, we quantify the accuracy of those within-threshold clusters and also those within-threshold single mentions that are not clustered. Both macro-accuracy (average of all cluster accuracies) and micro-accuracy (overall accuracy) are used -detailed explanations at [32]. \n\nFigure 5 shows the results of our NLP model adaptation guidance on 4 phenotype identification tasks. To simplify the validation, we make it a binary classification task: whether an identified mention is a phenotype mention or not (all contextual phenotype mentions in Table 1 are deemed as a mention and not-a-phenotype-mention is labelled as not-a-mention). For each new phenotype identification task, the NLP model (pre-)trained for the semantically most similar (defined in Equation 2) phenotype was chosen as the reuse model. Models and labelled data for the four pairs of phenotypes were selected from six physical comobidity studies on SLaM CRIS data.",
            "score": 0.49017752559144967,
            "section_title": "Model adaptation guidance evaluation",
            "char_start_offset": 21630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2206
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02838134765625
        },
        {
            "corpus_id": "268681777",
            "title": "Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion",
            "text": "Diffusion models [Song andErmon, 2019, Ho et al., 2020] in machine learning have seen a remarkable evolution, particularly in their application to image generation tasks.These models, which simulate the process of transforming a random noise distribution into a specific data distribution, have incorporated various guidance mechanisms to direct this transformation process more precisely.\n\nThe concept of condition [Ho and Salimans, 2022, Bansal et al., 2022, Nichol et al., 2021, Whang et al., 2022, Wang et al., 2022a, Li et al., 2023, Zhang and Agrawala, 2023] or guidance [Dhariwal and Nichol, 2021, Kawar et al., 2022, Wang et al., 2022b, Chung et al., 2022a, Lugmayr et al., 2022, Chung et al., 2022b, Graikos et al., 2022, Bansal et al., 2023] in diffusion models is crucial for achieving outputs that adhere to specific characteristics or criteria, a necessity in applications demanding high precision.\n\nInitially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria.\n\nTo address these constraints, the Control Net [Zhang and Agrawala, 2023] approach was introduced, representing a significant development in guided diffusion models.Control Net, though requiring less training than traditional classifier guidance, still necessitates some degree of model training.",
            "score": 0.4875373876657201,
            "section_title": "Guidance in Diffusion Models",
            "char_start_offset": 8100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 389
                },
                {
                    "start": 391,
                    "end": 911
                },
                {
                    "start": 913,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1362
                },
                {
                    "start": 1362,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1815
                },
                {
                    "start": 1817,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 500,
                    "matchedPaperCorpusId": "244908890"
                },
                {
                    "start": 664,
                    "end": 686,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 729,
                    "end": 751,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "For example, NOS jointly trains the classifier with the unconditional denoising model by having the classifier rely on one hidden layer of the denoising model. In contrast, Discrete Guidance enables easier re-purposing of already well-developed predictor models that work on the native, discrete input space (e.g., see Section 6.3). Stark et al. (2024) introduce Dirichlet FM, enabling generation of samples in discrete spaces by flow matching on the continuous state-space of the probability simplex; they can perform both predictor-free guidance and predictor-guidance. In contrast, our approach directly operates on the native discrete state-space. Empirically, we find our predictor-guidance approach to yield better results than Dirichlet FM on the same task presented by Stark et al. (Section 6.2). \n\nThe DiGress method of Vignac et al. (2023) performs approximate guidance in a discrete-time, discrete state-space diffusion framework. To do so, similarly to (yet distinctive of) our TAG approach, they use a Taylor series approximation of the predictor model operating on the discrete state-space. \n\nAlthough our TAG approximation bears resemblance to DiGress, DiGress is fundamentally anchored in discrete-time, which has been shown to be limiting in theory and in practice (Campbell et al., 2022;2024;Lou et al., 2023;Gat et al., 2024), and further substantiated in our experimental results. In particular, because DiGress is based on a discrete-time formulation, the normalizing constant required for conditioning (Equation 1) is intractable, therefore requiring additional approximations beyond our TAG approximation (Appendix E). Moreover, with Discrete Guidance, we can perform tractable exact guidance, which at times provides a benefit over TAG, whereas DiGress can only perform approximate guidance. Finally, DiGress is specific to diffusion and not readily generalizable to flow matching, which has been shown to offer several theoretical and practical advantages over diffusion models (Campbell et al., 2024;Gat et al., 2024;Lipman et al., 2023). Further discussion of related work can be found in Appendix E.",
            "score": 0.48724154199624103,
            "section_title": "Related Work",
            "char_start_offset": 19683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 829,
                    "end": 849,
                    "matchedPaperCorpusId": "252595881"
                },
                {
                    "start": 1282,
                    "end": 1305,
                    "matchedPaperCorpusId": "249192370"
                },
                {
                    "start": 2043,
                    "end": 2063,
                    "matchedPaperCorpusId": "252734897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "277510202",
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "text": "Diffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
            "score": 0.48434586893320625,
            "section_title": "Related Work",
            "char_start_offset": 3527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "267547708",
            "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
            "text": "Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at https://github.com/HannesStark/dirichlet-flow-matching.",
            "score": 0.48081812765337906,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "259165510",
            "title": "A Survey of Diffusion Models in Natural Language Processing",
            "text": "This survey paper extensively discusses the formulations, strengths, limitations, and applications of diffusion models in NLP. We conduct a comprehensive comparison between diffusion models and alternative generative models, focusing on autoregressive (AR) models. Additionally, we explore the integration of the Transformer architecture with diffusion models across various architectures. \n\nOur findings demonstrate the significant advantages of diffusion models over AR models. They excel in parallel generation, enabling faster and more efficient text generation. Diffusion models also demonstrate superior performance in sentence interpolation, token-level controls, and robustness to input corruption. Further research on integrating Transformers into diffusion models and developing multimodal and large-scale diffusion language models for few-shot learning is crucial. \n\nIn summary, this survey paper provides a comprehensive overview of diffusion models in NLP, highlighting their benefits, comparative analysis with AR models, and avenues for future research. We hope it can contribute to the understanding and advancement of diffusion models in the field of NLP.",
            "score": 0.4783979637692767,
            "section_title": "Conclusion",
            "char_start_offset": 29377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0228424072265625
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "Conditioning of diffusion models is typically achieved by way of introducing guidance, either in a classifier-free way (Ho & Salimans, 2021), or by using a classifier (Dhariwal & Nichol, 2021). Classifier guidance, in particular, provides the key ability to leverage an already-trained unconditional model without re-training it, to generate conditional samples simply by modulating its generative process with guidance from a classifier (Du et al., 2023). Such a strategy is particularly important in the sciences where one frequently has orders of magnitude more unlabelled data than labeled data; one can therefore invest considerably in one unconditional model and then repurpose it for many different tasks (Ingraham et al., 2023). \n\nHerein, for the first time, we introduce a principled approach, Discrete Guidance, to perform guidance for diffusion and flow models on discrete state-spaces (Figure 1). Discrete Guidance applies not only to continuous-time diffusion and flow models on discrete state-spaces, but also, in principle, to the broad class of generative models on discrete state-spaces realized through CTMCs. Our key insight is that, in continuous time, only a single dimension of the discrete state-space Markov chain can change at any point in time, making guidance exact and tractable. We empirically demonstrate the effectiveness of Discrete Guidance by applying it to a broad set of discrete state-space conditional generation tasks, including small-molecules, DNA sequences, and protein sequences. Because in the sciences, conditioning is often on real-valued properties rather than on satisfying a particular class, we refer to guidance more generally as predictor guidance and predictor-free guidance. \n\nIn summary, our contributions are: \n\n1. We introduce an exact, theoretically rigorous, general framework, Discrete Guidance (DG), for applying both predictor guidance (PG) and predictor-free guidance (PFG) to generative models in discrete state-spaces based on CTMCs, such as diffusion and flow matching. 2. We develop an approximation to our guidance framework that leads to more efficient sampling while maintaining sample quality. 3. We empirically explore and demonstrate the potential utility of our guidance framework across a wide range of problem domains, using either diffusion or flow models.",
            "score": 0.477114566567828,
            "section_title": "Diffusion and Flow-based Generative Models for Science",
            "char_start_offset": 3230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 140,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 167,
                    "end": 192,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 438,
                    "end": 455,
                    "matchedPaperCorpusId": "260384616"
                },
                {
                    "start": 712,
                    "end": 735,
                    "matchedPaperCorpusId": "254222077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "266362385",
            "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
            "text": "Diffusion models are powerful generative models that exhibit impressive performances across different modality generation, including image [5,11,12], video [22,34,39] and audio generation [16]. Guided sampling, including classifier guidance [5] and classifier-free guidance [11], has been widely used in diffusion models to realize controllable generation, such as text-to-image generation [29], imageto-image generation [24,28], and ControlNet [37]. Guided sampling controls the outputs of generative models by conditioning on various types of signals, such as descriptive text, class labels, and images. \n\nA line of guidance methods involves task-specific training of diffusion models using paired data, i.e., targets and conditions. For instance, classifier guidance [5] combines the score estimation of diffusion models with the gradients of the image classifiers to direct the generation process to produce images corresponding to a particular class. In this way, several image classifiers need to be trained on the noisy states of intermediate generation steps of diffusion models. Alternatively, classifier-free guidance [11] directly trains a new score estimator with conditions and uses a linear combination of conditional and unconditional score estimators for sampling. Although this line of methods can effectively guide diffusion models to generate data satisfying certain properties, they are not sufficiently flexible to adapt to any type of guiding due to the cost of training and the feasibility of collecting paired data. \n\nTo this end, another line of training-free guidance methods has been explored [2,14,36]. In training-free guided sampling, at a certain sampling step t, the guidance function is usually constructed as the gradients of the loss function obtained by the off-the-shelf pre-trained models, such as face-ID detection or aesthetic evaluation models. More specifically, the guidance gradients are computed based on the one-step approximation of denoised images from the noisy samples at certain steps t. Then, gradients are added to corresponding sampling steps as guidance to direct the generation process to the desired results. This line of methods offers greater flexibility by allowing the diffusion models to adapt to a broad spectrum of guidance.",
            "score": 0.47446150636302303,
            "section_title": "Introduction",
            "char_start_offset": 1371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "256390486"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1626,
                    "end": 1629,
                    "matchedPaperCorpusId": "257622962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "253237701",
            "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
            "text": "Another advantage of diffusion models is their post-hoc controllability using guidance from auxiliary models such as style classifiers (Dhariwal and Nichol, 2021). However, controllability is hard to achieve without compromises in modularity in diffusion-based LMs for text. To enable diffusion generation into discrete text rather than continuous modalities, prior approaches have employed different approximations, e.g., training with embeddings, character, or byte-level methods Hoogeboom et al., 2021;Austin et al., 2021;Chen et al., 2022). In contrast, existing mainstream LMs and the guidance classifiers they derive often operate at a sub-word level with sub-word representations trained jointly with the language model (Devlin et al., 2019;Raffel et al., 2020). Subsequently, changing the input representations to characters or embeddings requires developing guidance models from scratch, which can be expensive or infeasible in many cases. In this work, we propose a simplex-based solution which enables the diffusion over discrete texts while maintaining the advantages of diffusion models with plug-andcontrol guidance models.\n\nIn sum, to enable diffusion-based LMs for text we present the SSD-LM approach ( \u00a73), addressing the above two challenges. SSD-LM is trained to generate text semi-autoregressively-generating blocks of tokens left-to-right with bidirectional context within the block-which offers the benefits of both AR-LMs and diffusion models. It supports training with and generating variable-length sequences. At the same time, it allows refinement within the token block, in contrast to token-level autoregressive decoding where previously generated tokens cannot be modified at all. SSD-LM uses the same tokenization as popular AR-LMs, representing discrete text via a distribution (or simplex) defined over the input vocabulary and is trained to reconstruct texts from noisy versions of the distributions. Due to its underlying representation, our method also offers an easy and modular way of guided (controlled) generation using off-the-shelf text classifiers under the minimal assumption of shared tokenizer.\n\nOur evaluation experiments show, for the first time to our",
            "score": 0.4735339373730785,
            "section_title": "Introduction",
            "char_start_offset": 1729,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 525,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1195068359375
        },
        {
            "corpus_id": "278000556",
            "title": "Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion",
            "text": "(Campbell et al., 2024) formulated discrete flows using CTMCs, learning scores via cross-entropy and enabling inference-time flexibility by adjusting the rate matrix family without retraining, also unifying multimodal generation. Discrete diffusion principles have also been applied to structured data, such as graphs in DiGress (Vignac et al., 2023), using specific noise transitions, auxiliary features, and classifier guidance. \n\nScaling and Adapting Pre-trained Models for Diffusion Language Modeling Significant recent effort has focused on scaling diffusion models for language generation, often by adapting large pre-trained autoregressive (AR) or masked language models (MLMs). DiffusionBERT (He et al., 2023) integrated BERT into an absorbing-state diffusion framework, leveraging pre-trained weights and exploring novel noise schedules and time conditioning. Ye et al. (2023) adapted pretrained MLMs (like XLM-R) for generative tasks by finetuning with an RDM objective, enabling instruction-following capabilities. AR2Diff (Han et al., 2024) proposed converting pre-trained AR models to diffusion models by enabling bidirectional attention and continuing training with a diffusion objective. DiffuLLaMA (Gong et al., 2024) presented a continual pre-training method to adapt AR models (like LLaMA) into time-embedding-free diffusion models using attention mask annealing. LLaDA (Nie et al., 2025) developed a large masked diffusion model trained with a masking objective, adapting standard pre-training and SFT pipelines for this non-autoregressive paradigm. These works demonstrate the potential of leveraging existing large model architectures and weights to build capable diffusion language models. \n\nGuidance and Control in Discrete Diffusion Controlling the generation process of discrete diffusion models is vital for their application. Several approaches modify the sampling procedure or the model itself. Nisonoff et al. (2024) introduced Discrete Guidance (DG), a principled framework for guidance in CTMC-based models, offering exact predictor guidance (PG), predictor-free guidance (PFG), and an efficient Taylor-Approximated Guidance (TAG) variant by exploiting tractable normalization constants during inference.",
            "score": 0.47073470801854994,
            "section_title": "I. Connection to Continuous Target Score Matching",
            "char_start_offset": 59009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 350,
                    "matchedPaperCorpusId": "252595881"
                },
                {
                    "start": 700,
                    "end": 717,
                    "matchedPaperCorpusId": "254044147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "We deployed Discrete Guidance on three conditional generation tasks spanning problems on small molecules, DNA sequences and protein sequences. On the whole, we found unconditional flow matching training to be more stable than unconditional diffusion, so we trained our own unconditional flow matching models to be used with guidance in all three experiments. To demonstrate that Discrete Guidance also readily applies to continuous-time discrete diffusion models, we also guided a pre-trained discrete diffusion model to produce class-conditional images (Appendix F.1). In addition to training stability, flow matching models, through their partial decoupling of learning and sampling, enable greater sampling flexibility than diffusion models. For example, Campbell et al. (2024) showed that flow matching models in discrete spaces naturally give rise to an additional hyperparameter controlling the stochasticity of the sampling path at inference time, whereas diffusion models correspond to fixing a specific level of stochasticity at training time. Because Discrete Guidance readily applies to flow matching, we can make use of this sampling-time flexibility to enhance sample quality. \n\nOur goals were to investigate if Discrete Guidance worked as expected and to investigate potential benefits offered by our approach. For each task, we compared to the discrete-time, discrete diffusion guidance approach DiGress (Vignac et al., 2023) to illustrate the empirical advantage of Discrete Guidance. In all experiments, DiGress used the same architecture and hyperparameters as Discrete Guidance for both the unconditional and predictor models. For the DNA and protein tasks, we compared to additional task-specific baselines that could yield additional insights into the utility of our approach. Our intent in this section is not to claim optimal sample generation on any particular problem, rather, to demonstrate that Discrete Guidance can be successfully applied to multiple domains. Further details of all experiments can be found in Appendix F.1-F.4.",
            "score": 0.46986655453034093,
            "section_title": "Empirical Investigations",
            "char_start_offset": 21839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 1419,
                    "end": 1440,
                    "matchedPaperCorpusId": "252595881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0999755859375
        },
        {
            "corpus_id": "276961040",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "text": "Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.",
            "score": 0.46975744688900084,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "262460694",
            "title": "Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy",
            "text": "However, finetuning often demands a substantial amount of task-specific data and introduces complexity. A groundbreaking development came with GPT-3 (Brown et al. 2020), a massive autoregressive language model with 175B parameters. It introduced In-Context Learning, allowing it to perform tasks without extensive fine-tuning or model updates. This innovation led to Instruction-Finetuned Language Models, exemplified by GPT-3.5 (Ye et al. 2023), which efficiently adapts to specific tasks using task-specific instructions. This approach overcomes previous limitations, making LLMs a cornerstone in NLP.",
            "score": 0.46923156722838544,
            "section_title": "Related Work",
            "char_start_offset": 10410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 603
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 429,
                    "end": 445,
                    "matchedPaperCorpusId": "257632113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022247314453125
        },
        {
            "corpus_id": "270560691",
            "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
            "text": "The modern autoregressive Large Language Models (LLMs) have achieved outstanding performance on NLP benchmarks, and they are deployed in the real world. However, they still suffer from limitations of the autoregressive training paradigm. For example, autoregressive token generation is notably slow and can be prone to \\textit{exposure bias}. The diffusion-based language models were proposed as an alternative to autoregressive generation to address some of these limitations. We evaluate the recently proposed Score Entropy Discrete Diffusion (SEDD) approach and show it is a promising alternative to autoregressive generation but it has some short-comings too. We empirically demonstrate the advantages and challenges of SEDD, and observe that SEDD generally matches autoregressive models in perplexity and on benchmarks such as HellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference latency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD allows conditioning on tokens at abitrary positions, SEDD appears slightly weaker than GPT-2 for conditional generation given short prompts. Finally, we reproduced the main results from the original SEDD paper.",
            "score": 0.46279015863554906,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01116180419921875
        },
        {
            "corpus_id": "256616213",
            "title": "Representation Deficiency in Masked Language Modeling",
            "text": "Limitations. The focus of our work is on MLM and our analyses do not apply to other pretraining settings not using [MASK] tokens, and we discuss potential implications of our findings on autoregressive language models in Appendix F. While the current large language models are mostly autoregressive models, we believe that text encoder models still have important and wide applications in NLP, including but not limited to (1) Non-generation tasks. Many natural language understanding tasks do not have to be modeled autoregressively, for which encoder-only models are generally more parameter efficient and effective (Zhong et al., 2023). ( 2) Retrieval-augmented text generation (Lewis et al., 2020b), which typically uses an encoder for retrieval to enhance the generator's factualness. \n\n(3) Reward models in reinforcement learning from human feedback (RLHF) can use encoder models (Song et al., 2023). Empirically, we mainly compare with models pretrained by MLM and its simple variants and do not include all state-of-the-art models, as they typically require integrating multiple pretraining strategies and/or architecture changes (He et al., 2023).",
            "score": 0.46272987664337345,
            "section_title": "CONCLUSION",
            "char_start_offset": 26151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 681,
                    "end": 702,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00891876220703125
        },
        {
            "corpus_id": "258967177",
            "title": "Likelihood-Based Diffusion Language Models",
            "text": "Large language models lie at the center of recent advances in artificial intelligence. Shared across nearly all such language models is a common recipe: learn a model that maximizes data likelihoods using an autoregressive, left-to-right factorization. Maximum-likelihood pretraining has been a remarkably successful paradigm, leading to models that perform well on a range of downstream tasks and display complex behaviors like in-context learning [1,28]. \n\nThus far, autoregressive modeling has been a core part of this process due to its computational efficiency and empirical performance. However, this choice carries drawbacks. Autoregressive models generate tokens one at a time, making it difficult to perform long-range planning or controllable generation [17,19,21]. In addition, certain sequence distributions may be fundamentally more difficult to model autoregressively [22]. \n\nGiven the importance of language modeling, these potential drawbacks motivate us to explore alternatives to the autoregressive approach. As a promising candidate, we turn to continuous diffusion models [32,12], which have achieved state-of-the-art results in image modeling [5,30,31]. In language, prior works on diffusion models exist [e.g. 21,10,6], but these optimize non-likelihoodbased objectives. Without the ability to use standard likelihood-based benchmarks [25,14,26], it is difficult to say precisely how these models compare to autoregressive models (see Section 7 for a discussion). Somewhat concerningly, there is no work showing that it is possible for diffusion language models to achieve any nontrivial likelihoods on standard benchmarks. \n\nIn this work, we explore the limits of likelihood-based diffusion language models. Our goal is to train and release a diffusion model which achieves better likelihoods than GPT-2 124M [29], which we consider the smallest widely-adopted autoregressive model today. To achieve this goal, we first",
            "score": 0.4598865289345052,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 771,
                    "end": 774,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1096,
                    "end": 1099,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01078033447265625
        },
        {
            "corpus_id": "271328803",
            "title": "Discrete Flow Matching",
            "text": "We provide here some more details on relevant related works. \n\nContinuous diffusion and flows. Another line of works has been exploring the use of continuous space diffusion for discrete data, typically operating in the logits space (Dieleman et al., 2022;Li et al., 2022;Han et al., 2022;Lin et al., 2022;Chen et al., 2022). An additional body of work has been focusing on the adoption of latent diffusion-like modeling (Lovelace et al., 2022;He et al., 2022). Stark et al. (2024) proposed to learn a continuous Flow Matching on the probability simplex with Dirichlet paths. \n\nAutoregressive modeling. Autoregressive models have been a significant area of focus in recent years, particularly in the context of natural language processing and machine learning (Zhao et al., 2023). Autoregressive modeling, in its most fundamental form, utilizes the chain rule to learn the joint sequence probability by breaking it down into next-token conditional probabilities. GPT-2 (Radford et al., 2019), showcased the power of autoregressive language models in generating coherent and contextually relevant text over long passages. Its successor, GPT-3 (Brown et al., 2020), further pushed the boundaries, demonstrating impressive performance across a range of tasks without task-specific training data. Later models were adapted to other domains such as, code (Roziere et al., 2023;Li et al., 2023;Chen et al., 2021), biology (Zhang et al., 2024;Ferruz and H\u00f6cker, 2022;Madani et al., 2023), math (Romera-Paredes et al., 2024;Imani et al., 2023;Ahn et al., 2024), audio (Kreuk et al., 2022;Copet et al., 2024;Hassid et al., 2024) and more. \n\nMasked generative modeling. Masked generative modeling proposes to mask a variable portion of the input sequence and training a model to predict this masked section.",
            "score": 0.4580107046816899,
            "section_title": "Appendix A Related works, continuation",
            "char_start_offset": 28510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 272,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 421,
                    "end": 444,
                    "matchedPaperCorpusId": "254854302"
                },
                {
                    "start": 444,
                    "end": 460,
                    "matchedPaperCorpusId": "254044147"
                },
                {
                    "start": 1142,
                    "end": 1162,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1350,
                    "end": 1372,
                    "matchedPaperCorpusId": "261100919"
                },
                {
                    "start": 1436,
                    "end": 1460,
                    "matchedPaperCorpusId": "249974508"
                },
                {
                    "start": 1460,
                    "end": 1480,
                    "matchedPaperCorpusId": "256304602"
                },
                {
                    "start": 1487,
                    "end": 1516,
                    "matchedPaperCorpusId": "266223700"
                },
                {
                    "start": 1516,
                    "end": 1535,
                    "matchedPaperCorpusId": "257427208"
                },
                {
                    "start": 1535,
                    "end": 1552,
                    "matchedPaperCorpusId": "268724239"
                },
                {
                    "start": 1580,
                    "end": 1599,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 1599,
                    "end": 1619,
                    "matchedPaperCorpusId": "258832977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00955963134765625
        },
        {
            "corpus_id": "261076029",
            "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning",
            "text": "In this work, we pioneer the study of the scalability of DIFFUSION-LLMs to catch up with the recent advances of LLMs and facilitate the exploration of their potential. Our experiments verify their scalability regarding data, model sizes, and tasks. Further, we showcase positive prospects about their reasoning capabilities such as casual-order generation and implicit planning for further exploitation. \n\nLatest advancement on diffusion language models. After the first release of our study of DIFFUSION-LLM, diffusion language models have attracted broad attention and plenty of closely related studies have emerged. As such, we would like to highlight the latest progress to facilitate more upcoming advancements in this field. \n\n\u2022 Foundation. (2025) extends the flow matching formulation. All these studies confirm the practicality of building capable diffusion language models to serve as an alternative paradigm to autoregressive language models, with specific manners evolving. \n\n\u2022 Scaling validation. The scalability of diffusion language models under masked language modelinglike objectives has rapid progress. Gong et al. (2024) successfully build large-scale diffusion language models by adapting from autoregressive language models, offering another promising routine to gain large diffusion language models with relatively low cost. Nie et al. (2024) investigate the pretraining of diffusion language models from scratch and show a scaling law parallel to autoregressive language models, indicating similar scaling trends of the two paradigms. Nie et al. (2025) further scales up the pretrained diffusion language models to 8B parameters and 2.3T pre-training tokens with up-to-date recipes, with results highlighting the competitiveness of diffusion language models to frontier open-source autoregressive models on well-recognized benchmarks for large language models and showcase a helpful chatbot built upon large diffusion language models. Besides natural language, Wang et al. (2024a) scales diffusion language models to empower generative modeling of proteins. \n\n\u2022 Capabilities and applications. Diffusion language models have a bidirectional receptive field and can perform refinement by nature. For this reason, recent progress has confirmed that diffusion language models show superiority in scenarios where left-to-right generation order is suboptimal.",
            "score": 0.4578545554910621,
            "section_title": "DISCUSSIONS",
            "char_start_offset": 38296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01554107666015625
        },
        {
            "corpus_id": "273234068",
            "title": "EventFlow: Forecasting Continuous-Time Event Data with Flow Matching",
            "text": "Similarly, Shchur et al. (2020a) learn the distribution over the next inter-arrival time via a normalizing flow. Lin et al. (2022) benchmark several choices of generative models, including diffusion, GANs, and VAEs. Despite the flexibility of these models, these approaches are all autoregressive in nature, making them ill-suited for multi-step forecasting tasks. In contrast, L\u00fcdke et al. (2023) propose a diffusion-style model which is able to avoid autoregressive sampling via an iterative refinement procedure. \n\nOur work can be viewed as a novel approach for building flexible decoders for TPPs, extending flow matching to the setting of continuous-time event sequences. In contrast to prior work using generative models, our model is likelihood-free and non-autoregressive, achieving strong performance on long-term forecasting tasks. The work of L\u00fcdke et al. (2023) is perhaps most closely related to ours, but we emphasize that the method of L\u00fcdke et al. (2023) requires an involved training and sampling procedure. In contrast, our method is straightforward to both implement and sample from, while simultaneously outperforming existing approaches.",
            "score": 0.4571199596647234,
            "section_title": "RELATED WORK",
            "char_start_offset": 6190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1158
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 32,
                    "matchedPaperCorpusId": "202889230"
                },
                {
                    "start": 113,
                    "end": 130,
                    "matchedPaperCorpusId": "249494719"
                },
                {
                    "start": 378,
                    "end": 397,
                    "matchedPaperCorpusId": "264935503"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "264935503"
                },
                {
                    "start": 951,
                    "end": 970,
                    "matchedPaperCorpusId": "264935503"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.008514404296875
        },
        {
            "corpus_id": "272690217",
            "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
            "text": "This paper presents classifier-free guidance as a form of personalization and incorporates denoising in diffusion-based recommender frameworks. By restructuring the guidance mechanism by removing partially noised pseudo-guidance and incorporating true guidance through conditioning on pre-noise data, we demonstrated improvements in performance. The effectiveness of our method was evaluated using precision, recall, nDCG, and mean reciprocal rank @ K on held-out user data, revealing clear benefits over previous methods. Additionally, our approach showed potentially enhanced capabilities in fewshot and zero-shot recommendation scenarios. \n\nOur major contributions include: (1) the implementation and evaluation of classifier-free guidance in diffusion-based recommendation tasks; (2) an in-depth discussion on the parameterization and architecture of the denoising model utilized in these tasks; and (3) insights into the applicability of diffusion models beyond the visual domain. Our findings indicate that classifier-free guidance improves performance in recommendation tasks similarly to its success in visual tasks. Additionally, we provide a framework that addresses challenges within diffusion-based recommendation systems while highlighting the newfound capabilities in few-shot and zero-shot scenarios. \n\nThe potential for future work in this area is substantial. One promising direction involves improving the system by considering actual numerical ratings. While our initial attempt at this was inconclusive, a more sophisticated approach, potentially involving triplet loss to account for negative reviews, could yield better results. Cross-attentive conditioning, another feature that did not meet our performance expectations, might benefit from refined architectural innovations for improved guidance. \n\nThe use of transformers in the diffusion process is another avenue worth exploring. Recent advancements in image generation have shown that replacing the U-Net backbone with a Vision Transformer (ViT) can enhance denoising capabilities, and similar benefits might be realized in recommendation systems. Additionally, the few-shot and zero-shot capabilities of diffusion recommender systems trained with classifierfree guidance warrant further investigation. This aspect is particularly relevant for e-commerce, where new users with no interaction history present ongoing challenges. \n\nIn conclusion, our study lays the groundwork for advancing diffusion-based recommendation systems. By addressing the identified shortcomings and exploring the outlined directions for future research, we can further enhance these systems to meet the evolving needs of recommender applications.",
            "score": 0.4565122985384442,
            "section_title": "VI. CONCLUSION AND FUTURE WORK",
            "char_start_offset": 32582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2405
                },
                {
                    "start": 2408,
                    "end": 2506
                },
                {
                    "start": 2507,
                    "end": 2700
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "We introduced and empirically explored a principled and general approach to perform guidance on discrete state-spaces. Our approach, Discrete Guidance, is applicable to a broad class of generative models on discrete state-spaces realized through CTMCs, including continuous-time diffusion and flow models. We evaluated our approach empirically by applying it to guided conditional generation tasks in multiple domains, including small-molecules, DNA sequences and protein sequences. \n\nWhile our work demonstrated the effectiveness of Discrete Guidance in a variety of applications, it also leaves some avenues for further investigations and improvements. Although we have shown that Taylor-approximated guidance works well empirically, it lacks the theoretical guarantees offered by exact guidance, and further investigation into the potential trade-off between efficiency and accuracy would be of interest to the community. Also, guidance requires training predictors on noised samples, and it is unclear what training strategies would lead to the most effective guided generation (Klarner et al., 2024). Finally, it would also be interesting and potentially fruitful to explore Discrete Guidance for controllable text generation of language models. \n\nOur work illustrates that guided conditional generation in discrete state-spaces has the potential to be leveraged across the natural sciences. We expect that future work will more fully realize the potential of guided generation in these and other domains.",
            "score": 0.45595042209075165,
            "section_title": "Discussion",
            "char_start_offset": 36891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "In conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning. Prompt Highlighter extends CFG control in language models in a more generalized manner. The user's selection on the context x is converted into a token-level binary highlight mask m = {m 1 , . . . , m N }. We define m i = 1 if the i-th token x i is highlighted, and m i = 0 otherwise. This mask constructs a two-branch condition: the normal and the unconditional contexts. The normal context operates in the same manner as in vanilla inference.",
            "score": 0.4550665595867623,
            "section_title": "Token-Level Highlight Guidance",
            "char_start_offset": 10683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "257636789",
            "title": "Language Model Behavior: A Comprehensive Survey",
            "text": "al. 2018;Devlin et al. 2019;Zhang et al. 2022b); when possible, examples are not repeated during pre-training (Hoffmann et al. 2022;Chowdhery et al. 2022). Due to high computational costs, relatively few language models are pre-trained from scratch as described here, and they are usually trained in industry labs. In practice, most NLP researchers build applications upon existing pretrained language models, using the approaches described in Section 2.3.\n\nThis survey considers pre-trained language models as described above. Recent language models often contain further non-task-specific fine-tuning stages (particularly autoregressive models; Thoppilan et al. 2022;Ouyang et al. 2022). For example, autoregressive models are sometimes fine-tuned using the language modeling objective on curated human-written examples that demonstrate desirable text outputs (Ouyang et al. 2022) or examples of outputs that correctly follow input instructions (Wei et al. 2022a;Iyer et al. 2022). These approaches are referred to as supervised fine-tuning (SFT) or instruction tuning. Some more recent models are also tuned using reinforcement learn-ing, with predicted human preferences for different responses used as a reward (reinforcement learning from human feedback, or RLHF; Ouyang et al. 2022;OpenAI 2023a). Throughout this survey, we consider non-fine-tuned language models unless otherwise specified. 3 Non-fine-tuned language models still serve as the foundation for more recent language models.",
            "score": 0.4509262079130371,
            "section_title": "Training",
            "char_start_offset": 10624,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 28,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 132,
                    "end": 154,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 669,
                    "end": 688,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 862,
                    "end": 882,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 965,
                    "end": 982,
                    "matchedPaperCorpusId": "255096269"
                },
                {
                    "start": 1270,
                    "end": 1289,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00891876220703125
        },
        {
            "corpus_id": "274762839",
            "title": "Simple Guidance Mechanisms for Discrete Diffusion Models",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) gained widespread adoption in image generation and signal processing in part due to their high controllability via mechanisms such as classifier-based (Dhariwal & Nichol, 2021a) and classifier-free guidance (Nichol et al., 2021;Ho & Salimans, 2022). Tasks where guidance plays a key role include MRI denoising (Song & Ermon, 2019), 3D reconstruction (Poole et al., 2022;Gao et al., 2024), and conditional generation (Saharia et al., 2022;Gokaslan et al., 2024). \n\nHowever, applying controllable diffusion-based generation to tasks where the data is discrete (e.g., molecule design or text generation) presents challenges. First, standard diffusion models and their guidance mechanisms are not directly applicable, since they require taking gradients with respect to the data, and these are not defined in discrete settings. Second, popular discrete extensions of diffusion (Sahoo et al., 2024a;Shi et al., 2024) cannot perform multiple editing passes on generated tokens, hence are not ideal for controllable generation. Third, the performance of discrete diffusion models (measured by perplexity) lags behind autoregressive (AR) models, especially for classes of diffusion that are amenable to control, such as uniform noise (Austin et al., 2021;Lou et al., 2023). \n\nHere, we propose discrete diffusion models and guidance mechanisms that are effective at controllable generation and that address the above challenges. First, we provide straightforward and easy-to-implement adaptations of classifier-based and classifier-free guidance for discrete diffusion models. Second, we revisit uniform noise diffusion language models (UDLM), which undo random token perturbations and are particularly amenable to guidance, since they can repeatedly edit their samples (Austin et al., 2021) and thus correct errors. We address performance issues that plagued Relative to autoregressive models, which make local predictions one token at a time, discrete diffusion models denoise the entire sequence at every iteration, allowing for more guidable outputs.",
            "score": 0.45049659358935434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 501,
                    "end": 523,
                    "matchedPaperCorpusId": "272724289"
                },
                {
                    "start": 1289,
                    "end": 1310,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 1824,
                    "end": 1845,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "258048884",
            "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
            "text": "To overcome the limitations, non-autoregressive text modeling has been proposed (Ghazvininejad et al., 2019;Ren et al., 2020;Gu et al., 2018;Sa-haria et al., 2020;Savinov et al., 2022) through masked language models (Ghazvininejad et al., 2019), iterative sequence alignment (Saharia et al., 2020), insertion and deletion (Gu et al., 2018), or unrolling the generation path (Savinov et al., 2022). Our Masked-Diffusion LM achieves the non-autoregressive generation through gradually recovering the intermediate latent variables in a planned sequence from the forward process. \n\nPlug-and-Play Controllable Generation Our work is also closely related to the line of research about plug-and-play controllable generation methods (Yang and Klein, 2021;Dathathri et al., 2020;Krause et al., 2021;Liu et al., 2021), which modify the outputs based on extra guidance such as classifiers without changing or fine-tuning the pretrained language models. Dathathri et al. (2020) used gradients to edit the autoregressive language model's hidden representations to fulfill the control guidance. Yang and Klein (2021) proposed to reweight the predicted token from the language models while (Krause et al., 2021;Liu et al., 2021) further fine-tuned a smaller LM to reweight the token predictions. In this work, we apply the gradient-based plug-and-play approach to our Masked-Diffusion LM for controllable generation by making classifier-guided gradient updates to the intermediate latent variables during the diffusion. (Li et al., 2022), which are based on continuous diffusion models, we propose to make corruptions in both discrete and continuous space to help modeling the textual data. Specifically, we formulate a novel corruption process as an alternative to Gaussian diffusion (in Section 3.2) and we directly map continuous vectors to discrete inputs in every diffusion step with cross-entropy objectives (in Section 3.3).",
            "score": 0.4500472618754947,
            "section_title": "Related Work",
            "char_start_offset": 7228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 108,
                    "matchedPaperCorpusId": "202538740"
                },
                {
                    "start": 125,
                    "end": 141,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 163,
                    "end": 184,
                    "matchedPaperCorpusId": "245123905"
                },
                {
                    "start": 216,
                    "end": 244,
                    "matchedPaperCorpusId": "202538740"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "215786391"
                },
                {
                    "start": 322,
                    "end": 339,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 374,
                    "end": 396,
                    "matchedPaperCorpusId": "245123905"
                },
                {
                    "start": 725,
                    "end": 747,
                    "matchedPaperCorpusId": "233210709"
                },
                {
                    "start": 747,
                    "end": 770,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 770,
                    "end": 790,
                    "matchedPaperCorpusId": "221655075"
                },
                {
                    "start": 790,
                    "end": 807,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 942,
                    "end": 965,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 1081,
                    "end": 1102,
                    "matchedPaperCorpusId": "233210709"
                },
                {
                    "start": 1175,
                    "end": 1196,
                    "matchedPaperCorpusId": "221655075"
                },
                {
                    "start": 1196,
                    "end": 1213,
                    "matchedPaperCorpusId": "235313967"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "248366293",
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "text": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
            "score": 0.4497079827283006,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0204925537109375
        },
        {
            "corpus_id": "276408237",
            "title": "Continuous Diffusion Model for Language Modeling",
            "text": "Early works approached by fully relaxing the discrete data into continuous space (Han et al., 2022) or embedding the tokens in a latent space (Li et al., 2022;Dieleman et al., 2022), without any constraint. However, continuous relaxation without constraint fails to accurately model the discreteness of the categorical distribution. Recent works utilize the logit space (Hoogeboom et al., 2021;Graves et al., 2023) or the probability simplex (Avdeyev et al., 2023;St\u00e4rk et al., 2024) based on the Dirichlet distribution, which require strong assumptions on the diffusion noising processes. Flow matching has been applied to the probability simplex by using the statistical manifold on categorical distribution (Cheng et al., 2024;Davis et al., 2024) but has limited performance lagging behind discrete diffusion models. We evaluate our Riemannian Diffusion Language Model (RDLM) for text generation tasks on two language benchmarks: Text8 (Mahoney, 2006) and One Billion Words Dataset (Chelba et al., 2013). \n\nBaselines We compare against state-of-the-art autoregressive and diffusion models. Multinomial Diffusion (Hoogeboom et al., 2021), D3PM (Austin et al., 2021), SEDD (Lou et al., 2024), MDLM (Sahoo et al., 2024), MD4 (Shi et al., 2024) (Tran et al., 2019) are flow-based models, and ARDM (Hoogeboom et al., 2022) and MAC (Shih et al., 2022) are any-order autoregressive models. We also compare with transformer AR model (Vaswani et al., 2017). We provide further details on the baselines in Appendix B.1 \n\nImplementation Details For all experiments, we use the same data split and context size following Lou et al. (2024) and Sahoo et al. (2024).",
            "score": 0.4470161410715474,
            "section_title": "Continuous Diffusion Models for Discrete Data",
            "char_start_offset": 22655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1654
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 159,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 370,
                    "end": 394,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 442,
                    "end": 464,
                    "matchedPaperCorpusId": "258762371"
                },
                {
                    "start": 464,
                    "end": 483,
                    "matchedPaperCorpusId": "267547708"
                },
                {
                    "start": 710,
                    "end": 730,
                    "matchedPaperCorpusId": "270062958"
                },
                {
                    "start": 730,
                    "end": 749,
                    "matchedPaperCorpusId": "269982972"
                },
                {
                    "start": 1146,
                    "end": 1167,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 1199,
                    "end": 1219,
                    "matchedPaperCorpusId": "270380319"
                },
                {
                    "start": 1225,
                    "end": 1243,
                    "matchedPaperCorpusId": "270286107"
                },
                {
                    "start": 1244,
                    "end": 1263,
                    "matchedPaperCorpusId": "157062310"
                },
                {
                    "start": 1296,
                    "end": 1320,
                    "matchedPaperCorpusId": "238354021"
                },
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "249152160"
                },
                {
                    "start": 1428,
                    "end": 1450,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004886627197265625
        },
        {
            "corpus_id": "266174534",
            "title": "Equivariant Flow Matching with Hybrid Probability Transport",
            "text": "Flow Matching and Diffusion Models Diffusion models have been studied in various research works such as [12,13,14], and have recently shown success in fields like high-dimensional statistics [15], language modeling [16], and equivariant representations [10]. Loss-rescaling techniques for diffusion models have been introduced in [17], while enhancements to the architecture incorporating classifier guidance are discussed in [18]. Noise schedule learning techniques have also been proposed in [19,20]. Diffusion models suffer from unstable probability dynamics and inefficient sampling, which limits their effectiveness in some scenarios. Flow matching is a relatively new approach that has gained attention recently. Research works such as [11,21,22] have proposed this simulation-free objective for training continuous normalizing flow. It involves other probability paths besides the diffusion path and could potentially offer better sampling efficiency through ODE solving. Furthermore, the follow-ups [23,24] proposed to use the OT couplings to straighten the marginal probability paths. A concurrent work [25] also utilized a similar equivariant optimal transport mapping to guide the training of flow matching, especially Section 4 in their paper and Section 4.2 of our work, and their objective is also referred to as equivariant flow matching.",
            "score": 0.4452642950129249,
            "section_title": "Related Work",
            "char_start_offset": 4826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 191,
                    "end": 195,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "231979499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "264451832",
            "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
            "text": "Continuous Diffusion Models for Text Data. Initially proposed by Li et al. (2022), continuous language diffusion models embed tokens in a latent space, learn a diffusion model there, and take the nearest neighbor to dequantize. While initial versions struggled, these models have achieved significant results by iterating on several empirical components. For example, prior works improve downstream performance with alternative loss functions (moving away from likelihood-based score matching) (Han et al., 2022;Mahabadi et al., 2023) and explicitly encoding conditional information (e.g. inputting an infilling mask) (Gong et al., 2023;Dieleman et al., 2022). Additionally, distribution annealing methods like thresholding (Li et al., 2022) and classifier-free guidance (Ho, 2022) can further improve generation quality, although recent work has shown that methods like self-conditioning (Strudel et al., 2022) and designing a less sparse embedding space (e.g. based on bits) (Chen et al., 2022) can obviate the need for such methods. Finally, Gulrajani & Hashimoto (2023) showed that, with many surgical changes to the training paradigm, it is possible for language diffusion models to begin approaching autoregressive performance for likelihoods. \n\nDiscrete Diffusion Models. Most discrete diffusion works follow the framework set out by D3PM (Austin et al., 2021) which mimics \"mean prediction\" (Ho et al., 2020). These discrete diffusion methods are largely applied to fields other than language (e.g. images), likely due to empirical challenges. Despite this, some works have shown strong performance on language, particularly for seq-to-seq tasks and more efficient generation (Zheng et al., 2023;Chen et al., 2023;Ye et al., 2023). Notably, from these works discrete diffusion has tended to be advantageous over continuous diffusion in reducing network evaluations. \n\nSEDD vs Prior Work. SEDD is a discrete diffusion model that focuses on score matching, the crucial ingredient for continuous diffusions (Song & Ermon, 2019;Ho et al., 2020).",
            "score": 0.44366028937503094,
            "section_title": "Related Work",
            "char_start_offset": 26513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 81,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "252917661"
                },
                {
                    "start": 724,
                    "end": 741,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1045,
                    "end": 1073,
                    "matchedPaperCorpusId": "258967177"
                },
                {
                    "start": 1346,
                    "end": 1367,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 1399,
                    "end": 1416,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.471923828125
        },
        {
            "corpus_id": "267637251",
            "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
            "text": "Our research also intersects with the burgeoning field of controllable text generation within NLP. Existing approaches focus on enforcing lexical constraints by altering the decoding strategy, effectively changing the search space for text generation. \n\nFor instance, Post (Post & Vilar, 2018) integrates constraints directly into the beam search process. GeDi (Krause et al., 2020) and DExperts (Liu et al., 2021a) employ contrastive methods to adjust token distribution autoregressive. On the other hand, PPO-MCTS (Liu et al., 2023b) leverages Monte-Carlo Tree Search for decoding guidance. Recent studies focus on utilizing language models for controllable text-generation tasks. For instance, InstructCTG (Zhou et al., 2023) and COGNACGEN (Chen et al., 2022) utilize natural language instructions for desired text generation. Diffusion-LM (Li et al., 2022) and LatentOps (Liu et al., 2023a) integrate control mechanisms during the diffusion process from embedding and latent spaces, respectively.",
            "score": 0.4392324742933822,
            "section_title": "A.3. Controllable Text Generation",
            "char_start_offset": 34733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1000
                }
            ],
            "ref_mentions": [
                {
                    "start": 843,
                    "end": 860,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 875,
                    "end": 894,
                    "matchedPaperCorpusId": "252280511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023284912109375
        },
        {
            "corpus_id": "276741987",
            "title": "Proteina: Scaling Flow-based Protein Structure Generative Models",
            "text": "In conclusion, we could in theory look at our Prote\u00edna flow models equally as score-based diffusion models. With that in mind, from a pure performance perspective flow matching-based approaches and diffusion-based approaches should in principle perform similarly well when coupling with a Gaussian noise distribution. In practice, performance boils down to choosing the best training objective formulation, the best time sampling distribution to give appropriate relative weight to the objective (see Sec. 3.2), etc.-these aspects dictate model performance, independently of whether one approaches the problem from a diffusion model or a flow matching perspective. \n\nIn fact, we directly leverage the connections between diffusion and flow models when developing our stochastic samplers (see App. I.1) and guidance schemes. Both classifier-free guidance (Ho & Salimans, 2021) and autoguidance (Karras et al., 2024) were proposed for diffusion models, but due to the relations between score and velocity, we can also apply them to our flow models (to the best of our knowledge, our work is the first to demonstrate classifier-free guidance and autoguidance for flow matching of protein backbone generation). Please see App. I.2 for all technical details regarding guidance in Prote\u00edna. \n\nConsidering these relations, why did we overall opt for the flow matching formulation and perspective? (i) Flow matching can be somewhat simpler to implement and explain, as it is based on simple interpolations between data and noise samples. No stochastic diffusion processes need to be considered. (ii) Flow matching offers the flexibility to be directly extended to more complex interpolations, beyond Gaussians and diffusion-like methods. For instance, we may consider optimal transport couplings (Pooladian et al., 2023;Tong et al., 2024) to obtain straighter paths for faster generation or we could explore other, more complex non-Gaussian noise distributions. We plan to further improve Prote\u00edna in the future and flow matching offers more flexibility in that regard. At the same time, when using Gaussian noise, all tricks from the diffusion literature still remain applicable.",
            "score": 0.43903853123107617,
            "section_title": "J ON THE RELATION BETWEEN FLOW MATCHING AND DIFFUSION MODELS",
            "char_start_offset": 79937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 854,
                    "end": 875,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1788,
                    "end": 1812,
                    "matchedPaperCorpusId": "258418096"
                },
                {
                    "start": 1812,
                    "end": 1830,
                    "matchedPaperCorpusId": "259847293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "267770589",
            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
            "text": "Text-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
            "score": 0.43616445865871134,
            "section_title": "Related Work",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 33,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 180,
                    "end": 182,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 313,
                    "end": 315,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 594,
                    "end": 597,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "246411364"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "251402961"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1582,
                    "end": 1585,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 1588,
                    "end": 1591,
                    "matchedPaperCorpusId": "247476275"
                },
                {
                    "start": 1591,
                    "end": 1594,
                    "matchedPaperCorpusId": "252815928"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "251252882"
                },
                {
                    "start": 1703,
                    "end": 1706,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1706,
                    "end": 1709,
                    "matchedPaperCorpusId": "252815928"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "269983348",
            "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
            "text": "In this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations.",
            "score": 0.4328768178525553,
            "section_title": "F Ablation on Classifier-free Guidance",
            "char_start_offset": 35200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1133
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "270560691",
            "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling",
            "text": "In this work, we argue that diffusion models for text are a relevant alternative to autoregressive generation, since SEDD achieves similar generation quality as GPT-2, while offering more flexibility at sampling.Additionally, we present the advantages and challenges of current SEDD models.Most notably, we believe that improving the sampling efficiency of SEDD is crucial to enable its applications.For example, matching the unconditional text quality of GPT-2 with nucleus sampling requires many steps and, hence, is significantly slower than GPT-2 with KV-caching.In parallel to the efficiency aspect, alternative definitions of the forward process operator are relevant for application on reasoning tasks.According to empirical analysis while we are hopeful for the future of diffusion language models, it is still early to say that they will dethrone autoregressive models in the immediate future.",
            "score": 0.4322292749676765,
            "section_title": "Conclusion",
            "char_start_offset": 10334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00787353515625
        },
        {
            "corpus_id": "264451832",
            "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
            "text": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
            "score": 0.4318273238911129,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.011199951171875
        },
        {
            "corpus_id": "261395306",
            "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation",
            "text": "The widespread use of LLMs was established, practically speaking, with the work of Devlin et al. (2019), where a transformer architecture (Vaswani et al., 2017) is pretrained with substantial amounts of unlabelled text with a Masked Language Modelling (MLM) objective. With this architecture, a new paradigm in NLP surfaced, where the adaptation to downstream tasks was conducted by finetuning the pretrained model with supervised data. Later on, GPT-3 (Brown et al., 2020), which is trained with an autoregressive objective, showed competitive results by leveraging few-shot prompting. Nevertheless, given their training objective function, it was difficult for autoregressive LLMs to successfully perform downstream NLP tasks without substantial prompt engineering. Ouyang et al. (2022) propose finetuning GPT-3 using a 3-step approach named Reinforcement Learning through Human Feedback (RLHF). In detail, the model is (1) initially finetuned using supervised data obtained from labelling prompts (SFT); (2) a reward model is trained using ranked responses given a prompt; (3) the policy is optimised against the reward model using the Proximal Policy Optimisation reinforcement learning algorithm (Schulman et al., 2017). As a testament to the power of this approach, ChatGPT took the world by storm in late 2022 thanks to its incredible humanlike generation capabilities. This was achieved by including dialogues in all steps of RLHF.",
            "score": 0.4313310357663612,
            "section_title": "Large Language Models",
            "char_start_offset": 6914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1439
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 103,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 138,
                    "end": 160,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00981903076171875
        },
        {
            "corpus_id": "271915699",
            "title": "Compress Guidance in Conditional Diffusion Sampling",
            "text": "Classifier-free guidance is a different form of guidance from classifier guidance. Although classifierfree guidance does not use an explicit classifier for guidance, the diffusion model serves as an implicit classifier inside the model as discussed in Appendix F. We hypothesize that classifier-free guidance also suffers from a similar problem with classifier guidance. We apply the Compress Guidance technique on classifier-free guidance (CompCFG) and demonstrate the results in Table 4.",
            "score": 0.4310919816026011,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 19893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "259308807",
            "title": "Stay on topic with Classifier-Free Guidance",
            "text": "We have shown that Classifier-Free Guidance, which was originally conceived of in text-to-image applications, can be an effective way of increasing adherence to the prompt in autoregressive language modeling. In contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model. We have shown that CFG can boost performance across an array of canonical benchmarks in NLP that involve variations of the prompt: basic prompting, chain-of-thought prompting, text-to-text prompting and chatbot prompting. Finally, we sought to explain the effects of CFG by showing it decreased sampling entropy, but not in the same ways that Instruction-tuned models do. Ultimately, we leave for future work the exact effects that CFG is having, but we propose qualitative visualizations that confirm our intuitions around prompt adherence. \n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45,73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train. \n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context. It's also possible that CFG might be misused. We have not tested the effects of CFG if used in conjunction with malicious strategies for hacking language models, including but not limited to: prompt injection and prompts aimed at overriding alignment. It's possible that there are unforeseen effects induced by an increased adherence to parts of the prompt. We tried to explore this at length, both quantitatively and qualitatively, and we designed tasks that might reveal such behavior. However, we cannot conclude this method is risk-free. We advocate for standardized benchmarks aimed more squarely at language-model risk (including, possibly, pairs of models along with known prompt injections). Such standardized benchmarks could help us unit-test an advancement like CFG before releasing it into the wild. \n\nStella Biderman supervised the process.",
            "score": 0.43075533964675494,
            "section_title": "Conclusion",
            "char_start_offset": 20190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2243
                },
                {
                    "start": 2246,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "268063305",
            "title": "TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings",
            "text": "Autoregressive (AR) large language models such as GPT-4 (OpenAI 2023) or Llama 3 (Dubey et al. 2024) are the current gold standard in the text generation problem. They are capable of creating high-quality and coherent texts that are practically indistinguishable from the human ones. However, the disadvantage of this approach is the inability of the model to correct its own mistakes made during left-to-right generation. If a mistake occurs, it may spoil the subsequent text. In addition, the autoregressive method of token generation slows down the inference process as it requires performing a single model evaluation for each new token. \n\nDiffusion modeling is currently the state-of-the-art approach for data generation in image (Rombach et al. 2022;Podell et al. 2023), audio (Evans et al. 2024) and video (Blattmann et al. 2023) domains. Researchers now attempt to adapt it also for text generation (Li et al. 2022;Gong et al. 2023;Karimi Mahabadi et al. 2024). Diffusion models are a class of probabilistic generative models that are able to iteratively transfer noise to a representative sample of data. While some of the proposed text diffusion models are autoregressive (Lovelace et al. 2023;Zhang et al. 2023), the majority of them are not and, by design, they have several advantages over AR language models. First, being non-autoregressive (NAR) models, they generate all the tokens simultaneously and can adjust any part of the sequence during the generation process. They also can be faster than AR models because the number of neural function evaluations for diffusion models depends on the number of denoising iterations rather than the length of the sequence. And given the possibility of distillation of diffusion models (Meng et al. 2023), the number of iterations can be greatly reduced. \n\nTo date, a number of text diffusion models have been proposed, each based on substantially new ideas with little overlap with other methods.",
            "score": 0.42936806643061753,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 756,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 907,
                    "end": 923,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 923,
                    "end": 940,
                    "matchedPaperCorpusId": "252917661"
                },
                {
                    "start": 940,
                    "end": 968,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1182,
                    "end": 1204,
                    "matchedPaperCorpusId": "254854302"
                },
                {
                    "start": 1204,
                    "end": 1222,
                    "matchedPaperCorpusId": "259075458"
                },
                {
                    "start": 1742,
                    "end": 1760,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01282501220703125
        },
        {
            "corpus_id": "258762741",
            "title": "Diffusion Language Models Generation Can Be Halted Early",
            "text": "Exploring Large Language Models (LMs) is a dominant research direction in NLP. The two primary methods of training LMs for NLP are autoregressive training (Radford et al., 2019;Raffel et al., 2020;Chowdhery et al., 2022) and masked language modeling (Devlin et al., 2019;He et al., 2020;Liu et al., 2019;Lan et al., 2020). \n\nThe exploration of alternative models, such as diffusion models (Ho et al., 2020;Song et al., 2020), is a promising avenue for research as diffusion allows native non-causal conditioning and simplified controllable generation methods (Nichol et al., 2022). In recent works, with models such as \"Diffusion LM\" and Plaid (Li et al., 2022;Gulrajani and Hashimoto, 2023), Simplex-based Diffusion * These authors contributed equally to this work Language Model (SSD) (Han et al., 2023), GENIE (Lin et al., 2022), and Continuous Diffusion for Categorical Data (CDCD) (Dieleman et al., 2022) being introduced, indicating an emerging interest for using diffusion models in text generation. \n\nA crucial distinction between autoregressive LMs and diffusion language models (DLMs) lies in their modeling approaches. Autoregressive LMs predominantly adhere to the common probabilistic model. In contrast, DLMs exhibit substantial divergence in their application for modeling categorical data. When exploring DLMs, it is essential to consider the lack of connectivity between such models. The majority of comparisons between them have primarily focused on evaluating sample quality (Gulrajani and Hashimoto, 2023;Han et al., 2023). \n\nWhile it is essential to study the sample quality of DLMs, it does not further our understanding of the differences between these DLMs models. This work addresses this issue and evaluates various DLMs with a unified view of their generation process. Given this unified view, we study the dynamics of the generation process within different DLMs and focus on the changes in the samples during that process.",
            "score": 0.42930397740688,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 322
                },
                {
                    "start": 325,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 197,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 304,
                    "end": 321,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 559,
                    "end": 580,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 787,
                    "end": 805,
                    "matchedPaperCorpusId": "253237701"
                },
                {
                    "start": 1525,
                    "end": 1542,
                    "matchedPaperCorpusId": "253237701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00760650634765625
        },
        {
            "corpus_id": "268724317",
            "title": "Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction",
            "text": "Transformer architectures and their training schemes have led to the notable development of language foundation models in the NLP field.In particular, BERT [39] employs a masked language modeling (MLM), which randomly masks a certain percentage of words and trains the model to predict them.GPT-2 [87] uses a causal language modeling (CLM), an autoregressive method for predicting the next token.T5 [89] involves sequence-to-sequence (Seq2Seq) modeling, using an encoder-decoder architecture to generate the output sequence.These unique models stand out in various genera-tive tasks, including machine translation [12,127,136], text generation [30,59,92], and question-answering [8,25,86].\n\nBeyond the NLP field, language foundational models have also exhibited superior performance in vision-language tasks and solving mathematical problems.This includes classification [88,146], generation [24,147], and problemsolving [104,149].These works explore the application of foundational language models, with the goal of extending the scope of the pre-training/fine-tuning paradigm.\n\nMost recently, there have been attempts to incorporate language priors into time-series forecasting [9,76,105].For instance, ForecastQA [38] proposes a QA benchmark with timestamp constraints to verify its forecasting ability regarding future events.Xue et al. [134] study mobility prediction, inferring how people move in cities. Inspired by chatbot applications, PromptCast [131] has made predictions on weather temperature, energy consumption, and customer flow.The most relevant work to ours [45] uses linguistic intermediate representations for trajectory prediction, solving action-related reasoning through language priors.However, they cannot fully take advantage of the linguistic model, in that they inherently use pre-trained tokenizers learned from text data.In particular, their approach is not suitable for trajectory prediction tasks because of the inconsistent analysis of numerical data.Furthermore, when dealing with coordinate sequences, existing numerical regressors are directly utilized as auxiliary modules to language models, inhibiting a higher level of understanding like social interactions.",
            "score": 0.42838394304772354,
            "section_title": "Language-Based Reasoning and Prediction",
            "char_start_offset": 5949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1078
                },
                {
                    "start": 1080,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1545
                },
                {
                    "start": 1545,
                    "end": 1710
                },
                {
                    "start": 1710,
                    "end": 1851
                },
                {
                    "start": 1851,
                    "end": 1984
                },
                {
                    "start": 1984,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "235166182"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "244119627"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "201058686"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "227231240"
                },
                {
                    "start": 648,
                    "end": 651,
                    "matchedPaperCorpusId": "221970785"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "232240435"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 896,
                    "end": 900,
                    "matchedPaperCorpusId": "244714549"
                },
                {
                    "start": 921,
                    "end": 926,
                    "matchedPaperCorpusId": "253801800"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "259738553"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "226262280"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "29150573"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "221978039"
                },
                {
                    "start": 1341,
                    "end": 1346,
                    "matchedPaperCorpusId": "252212309"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "239024571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "254877381",
            "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers",
            "text": "The datasets and baseline models used in our research are publicly available. Diffusion models, previously successful in vision, face challenges in NLP due to discrete token sequences. Promising results have been shown in DiffusionLM (Li et al., 2022) and DiffuSeq (Gong et al., 2022), but both works use encoder-only models and have limitations in scalability and efficiency. This research explores and improves the diffusion-based sequence-to-sequence text generation models. Our work alters to encoder-decoder Transformers which are widely applied in recent LLMs such as FLAN-T5 (Chung et al., 2022) for better scalability, potential, and sampling speed acceleration (Section 6.3). \n\nOur work also incorporates novel techniques like self-conditioning and adaptive noise schedules, outperforming several AR and NAR baselines. SeqDiffuSeq demonstrates the feasibility of encoderdecoder diffusion models for sequence-to-sequence tasks and may serve as a starting point for future exploration of text diffusion models' potential, serving as another method approaching sequenceto-sequence text generation besides widely implemented AR and NAR models. Considering the excellent performance of diffusion models in other domains such as vision, text diffusion models have great potential in generating text sequences with high quality and may be an emerging framework of text generation.",
            "score": 0.42826881585602394,
            "section_title": "B Ethic Statements and Boarder Impact",
            "char_start_offset": 25025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1382
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 251,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0258636474609375
        },
        {
            "corpus_id": "259833814",
            "title": "Prompting language models improves performance in imbalanced setting",
            "text": "Fine tuning language models is a common strategy in Natural Language Processing (NLP), where a classifier head is added on top of the base language model to obtain the desired classification output. This approach has been applied to various NLP models, including RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), and DeBERTa (He et al., 2020), and has demonstrated exceptional performance on benchmark datasets such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019). \n\nAn alternate approach to adapting language models to downstream tasks involves the use of autoregressive text generation or prompt based fine tuning. This technique is commonly used in sequenceto-sequence models such as T5 (Raffel et al., 2019) leading to state-of-the-art performance on Super-GLUE benchmark. This type of fine tuning strategy has an added advantage of multi-task training (Mc-Cann et al., 2018). This technique has also shown to improve models zero shot capability (Puri and Catanzaro, 2019) where we can provide only task description and model is able to classify the input correctly. \n\nWork by Scao and Rush (2021); Schick and Sch\u00fctze (2020); Webson and Pavlick (2021) has shown that prompting language models really helps in few shot learning setting over vanilla fine tuned models. In high data regime setting prompting and vanilla fine tuned language models achieve the similar performance. However, these studies used balanced datasets where the number of examples from each class are equal. \n\nThe issue of class imbalance in machine learning is a well-known challenge, and occurs when the distribution of samples across classes is skewed. These types of problems are encountered in various real world settings like malware detection (Demirkiran et al., 2021), spam detection (Rao et al., 2023), medical domain (Altaf et al., 2023) and many more. Previous work by Buda et al. (2017); Leevy et al. (2018) has shown that if we use general supervised loss then it leads to poor generalization on the minority classes.",
            "score": 0.4280540963064724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 448,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1100,
                    "end": 1120,
                    "matchedPaperCorpusId": "232233408"
                },
                {
                    "start": 1744,
                    "end": 1769,
                    "matchedPaperCorpusId": "245502646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.023284912109375
        },
        {
            "corpus_id": "257504777",
            "title": "Diffusion Models in NLP: A Survey",
            "text": "Natural language processing aims to understand, model and manage human language from different sources (e.g., text or audio). Text generation has become one of the most critical and challenging tasks in natural language processing. Its goal is to write seemingly reasonable and readable text in human language given input data (e.g., sequences and keywords) or random noise. Many methods based on diffusion models have been developed for text generation. \n\nTing Chen [1] proposes a diffusion model for generating discrete data and applies it to the image captioning task. The main idea behind their approach is to first represent discrete data as binary bits and then train a continuous diffusion model that models these bits as real numbers, called simulated bits. \n\nThe natural language processing research group at Stanford University, Xiang Lisa Li [2] et al. exploit the nonautoregressive mechanism of the diffusion model to accomplish the task of controlled text generation by iteratively noise reduction from noise vectors to word vectors with the help of the continuous diffusion property, which is innovative in defining an equation of word embedding unifying the discrete to continuous states in the diffusion process. \n\nJacob Austin [3] team proposed the diffusion-like generative model D3PM for discrete data, which improves the diffusion model for discrete data by defining a new discrete damage process and generalizes the diffusion mode in the diffusion process. \n\nShansan Gong [4] team proposed a diffusion model for sequence-to-sequence (SEQ2SEQ) text generation task design using end-to-end classifier-free guided diffusion condition generation to overcome the problem of discrete nature of text and provided provided a comparative link between the diffusion model and the autoregressive and non-autoregressive models. \n\nGuangyi Liu's [5] team proposed a new method for composable text manipulation in a compact text latent space. The diffusion process is put in charge of the control of text properties on a low-dimensional continuous space only. And after making the latent vectors have the corresponding properties, the latent vectors are then handed over to the decoder to generate the text. The limitation of fixed-length generation of Diffusion-LM is avoided, and the text fluency and model training speed are improved.",
            "score": 0.42796118000013295,
            "section_title": "A. Text Generation",
            "char_start_offset": 6311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1477
                },
                {
                    "start": 1480,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 1244,
                    "end": 1247,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223388671875
        },
        {
            "corpus_id": "278171703",
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "text": "When w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1  for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival. V. CONCLUSION Given the rising adoption of flow matching in modern text-tospeech (TTS) models, optimizing the inference efficiency of these models has become increasingly important, as they typically require multiple sampling steps to achieve satisfactory performance. While most prior work has focused on reducing the number of sampling steps, another critical issue lies in the use of Classifier-Free Guidance (CFG), which requires two forward passes and introduces additional computational overhead. In this paper, we address this issue by exploring how to remove CFG from flow-matching-based TTS models, enabling a single forward pass per sampling step. By reformulating the flow-matching training objective to directly approximate the output of CFG inference, we eliminate the need for unconditional predictions and guidance scale adjustments during inference. This approach significantly reduces computational cost and simplifies the inference procedure. Using F5-TTS as a case study, we demonstrate that inference cost can be effectively halved without sacrificing speech quality. In future work, we plan to extend this method to other flow-matching-based TTS models and evaluate its effectiveness on larger datasets.",
            "score": 0.42795721917493756,
            "section_title": "D. Evaluation Results",
            "char_start_offset": 15873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "273532521",
            "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
            "text": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.",
            "score": 0.42723104646915766,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0309600830078125
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "As an example, Frey et al. ( 2024) generate samples in discrete state-spaces by way of Langevin MCMC in a continuous latent space, using ideas from empirical Bayes (Robbins, 1992;Saremi & Hyv\u00e4rinen, 2019). Some of the approaches proposed to apply guidance to diffusion models on the continuous state-space representations of discrete state-space objects in order to utilize the guidance approaches for continuous state-space diffusion. Li et al. ( 2022) model text by first embedding the discrete text tokens into a continuous word embeddings, perform standard Gaussian diffusion on the embeddings, then decode the embedding back to discrete tokens. This approach requires specialized modeling choices e.g., reparameterization of the loss function and clamping the predictions to the nearest word embedding, which might be challenging to adapt to domains other than natural languages. In an alternative approach, Gruver et al. ( 2024) (NOS) propose to perform guidance in the hidden layers of the unconditional diffusion model. However, NOS requires additional training procedures and hyperparameters, e.g., jointly training the unconditional denoising model and the classifier on one hidden layer of the denoising model, potentially limiting the flexibility and adaptability of such approach. We note that approaches which perform guidance in continuous latent spaces lose the discrete structure of the data during guidance, which can be important for settings where the discrete structures contain information that are useful for guidance (Campbell et al., 2024;Qin et al., 2023). \n\nGuidance has also been explored in the framework of discrete-time, discrete state-space diffusion (Vi-  5). However, since DiGress uses a fixed number of time steps at training time, the gradient approximation is applied to all possible transition states. On the other hand, we present a continuous-time formulation which allows guidance to be achieved exactly by adjusting the rates, and derived Taylor-approximated guidance to specifically approximate the likelihood ratios of only the transitions that change a single dimension.",
            "score": 0.4268795056642635,
            "section_title": "E.1 Additional Discussion",
            "char_start_offset": 59640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 179,
                    "matchedPaperCorpusId": "26161481"
                },
                {
                    "start": 179,
                    "end": 204,
                    "matchedPaperCorpusId": "70350020"
                },
                {
                    "start": 1564,
                    "end": 1581,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10052490234375
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
            "score": 0.4263703525073206,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "278171703",
            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
            "text": "However, another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time. It is important to note that this method was primarily designed for class-conditional tasks, where the model is trained to generate images based on fixed categories; its application to text-to-speech remains underexplored. Inspired by this pioneering work, in this paper, we explore enhancing speech generation by modifying the flow matching training target, specifically for textconditioned and audio-conditioned tasks, such as TTS. Our goal is to enable the model to perform only conditional predictions during inference, eliminating the need for unconditional predictions required by traditional CFG. To validate the feasibility of this approach, we adopt F5-TTS, a representative flow-matching-based TTS model, as a case study. Experimental results show that our method can effectively halve the computational cost per sampling step without degrading the generated speech quality compared to the baseline F5-TTS with CFG. Furthermore, our method can be seamlessly combined with advanced sampling strategies to achieve additional speedup, highlighting its potential for real-time applications. \n\nOur contributions are summarized as follows: \n\n\u2022 We present the first attempt to remove classifier-free guidance from flow-matching-based TTS models at inference time by adopting the model-guidance training to alter the prediction target of flow matching. \u2022 We validate our approach on F5-TTS, effectively halving the inference cost without compromising the quality of the generated speech. Moreover, the proposed method can be seamlessly integrated with existing optimized sampling strategies, resulting in further speedup.",
            "score": 0.4256204288669282,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "266177020",
            "title": "DiffusionSL: Sequence Labeling via Tag Diffusion Process",
            "text": "We provide a complete and detailed review of previous work about applying the diffusion model to language for the potential audiences to get familiar with this promising field. Most existing literature focuses on NLG instead of NLU. \n\n\u2022 Multinomial-Diffusion (Hoogeboom et al., 2021) defines a multinomial diffusion process with categorical data to generate text or a segmentation map. \n\n\u2022 D3PM (Austin et al., 2021) also defines the diffusion model in discrete space and the newly proposed discrete corruption process improves the performance. \n\n\u2022 DiffusionLM (Li et al., 2022b) firstly proposes to embed the text into continuous space to circumvent the discreteness problem and round the final denoised vectors back into words. \n\n\u2022 DiffuSEQ (Gong et al., 2023) firstly employs the diffusion model for seq2sesq text generation setting. And it novelly proposes to concatenate and embed the source and target text and only add noise to the target text embedding while keeping the source text embedding as clean. \n\n\u2022 SSD-LM (Han et al., 2022) iteratively generates text blocks in a semi-autoregressive manner and conducts the diffusion process on the natural vocabulary space, which balances the advantage of Autoregressive and Non-Autoregressive model and allows convenient guidance incorporation. \n\n\u2022 SED (Strudel et al., 2022) casts the discrete language symbols into continuous embeddings and incorporates the self-condition trick into the backward denoising process. \n\n\u2022 CDCD (Dieleman et al., 2022) uses the scorematching framework to solve several language modeling tasks in continuous time and input space. \n\n\u2022 DiffusionBERT (He et al., 2023) uses the diffusion model to improve the masked language model, which combines the advantage of two denoising models. New proposed noise schedule and time embedding injection methods are applied to it.",
            "score": 0.42511529577791274,
            "section_title": "G Detailed Related Work of Diffusion Model with Language",
            "char_start_offset": 28199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 283,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 561,
                    "end": 579,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1631,
                    "end": 1648,
                    "matchedPaperCorpusId": "254044147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00916290283203125
        },
        {
            "corpus_id": "277313413",
            "title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
            "text": "To comprehensively assess the performance of our framework, we compare it against a set of established baselines, including both general-purpose large language models and specialized pre-trained models for scientific and technical domains. \n\nWe consider the following baselines: \n\n(1) General-purpose LLMs: GPT and Claude, widely used for natural language understanding and text generation tasks. \n\n(2) SciBERT [29], a pre-trained language model designed specifically for scientific text processing, which has demonstrated strong performance in scientific literature comprehension and reasoning tasks. \n\n(3) RoBERTa [21], an optimized variant of BERT that enhances training robustness and performance across multiple NLP tasks. \n\n(4) LLaMA 3 [12], the latest iteration in the LLaMA series of large-scale language models, which offers improved efficiency and reasoning capabilities. \n\n(5) Qwen-7B [5], an autoregressive generative language model based on masked language modeling, optimized for diverse text generation and completion tasks. \n\nAll of these models are publicly accessible, allowing for reproducible benchmarking and comparative evaluation of our proposed framework.",
            "score": 0.4232053437532949,
            "section_title": "Baselines",
            "char_start_offset": 28706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279083251953125
        },
        {
            "corpus_id": "277043967",
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "text": "It is therefore essential for language models to possess effective methods for manipulating this trade-off, enabling versatile usage at any chosen level of diversity, as well as potentially unlocking greater generation quality at the cost of diversity. \n\nWhile some works make efforts to evaluate model diversity as well as quality [9], or explore the effect of MBR [27], diffusion language models currently lack effective means of manipulating this trade-off. This project's key contributions are as follows: \n\n\u2022 We propose classifier-free guidance and stochastic clamping as methods for controlling the quality-diversity tradeoff in diffusion language models, and demonstrate their efficacy. \u2022 By applying these techniques to a diffusion language model, we achieve highly competitive results on the paraphrasing task using the Quora Question Pairs (QQP) dataset despite training for just three hours, far less than similar works. \u2022 We open-source our implementation1 which was written from scratch using the diffusers library [26], rather than adapting an existing image diffusion codebase as in many prior works [8,9,14,27], leading to a much smaller repository overall.",
            "score": 0.42291789174385086,
            "section_title": "Introduction",
            "char_start_offset": 1968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "257102889",
            "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
            "text": "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance [17] to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier. \n\nIn order to provide classifier-free guidance, the tag  in a class-conditional diffusion model   (  | ) is replaced with a null tag \u2205 throughout the training process. The output of the model is further extended in the direction of   (  | ) \n\nand away from   (  | \u2205) during sampling: \n\nThe recommended guidance scale is  = 5. This equation was inspired by the classifier, \n\nThe function of the true scores is used to represent the gradient  * , \n\nThe modified prediction \u03b5 is subsequently employed to guide us toward the target text prompts  2 , as demonstrated in Algorithm 1:",
            "score": 0.4227106823263234,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 13072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "273962786",
            "title": "Conditional [MASK] Discrete Diffusion Language Model",
            "text": "Efforts to integrate generative flow models into sequence generation exploit the distribution shift from a source language to a target language through a series of invertible linear transformations (Ma et al., 2019;Zhang et al., 2024). However, as DDPM (Ho et al., 2020a) demonstrate the effectiveness of generating images, diffusion models have been a major topic of interest within the field of generative flow models (Song et al., 2021a,b). To apply such diffusion methodologies to NLP, in order to leverage their strengths in controllability and diversity, recent studies have demonstrated promising performance across various tasks (Li et al., 2022;Gong et al., 2023a;He et al., 2023;Yuan et al., 2023;Lovelace et al., 2023;Chen et al., 2023;He et al., 2023;Lou et al., 2024;Zhou et al., 2024;Shi et al., 2024;Sahoo et al., 2024;Zheng et al., 2024;The et al., 2024;Wang et al., 2024). \n\nAlthough Continuous Diffusion Language Models (CDLMs) such as Diffusion-LM (Li et al., 2022), DiffuSeq-v1, v2 (Gong et al., 2023a,b), and LD4LG (Lovelace et al., 2023) show promising performance, Bansal et al. (2022) argue that such operations do not necessarily have to be governed by stochastic randomness. \n\nBuilding on this rationale, D3PM (Austin et al., 2023) propose the discrete restoration-generation approach and DiffusionBERT (He et al., 2022) adopt pre-trained language models (PLMs) to DDLM. SEDD (Lou et al., 2024) propose score entropy inspired by MLM loss, and outperform existing CDLMs. Recent works by Shi et al. (2024) and Sahoo et al. (2024) extend this idea and obtain better empirical results.",
            "score": 0.4225419727303056,
            "section_title": "Related Works",
            "char_start_offset": 3451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1607
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "268681211"
                },
                {
                    "start": 673,
                    "end": 689,
                    "matchedPaperCorpusId": "254044147"
                },
                {
                    "start": 747,
                    "end": 763,
                    "matchedPaperCorpusId": "254044147"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "258557887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007694244384765625
        },
        {
            "corpus_id": "274281285",
            "title": "KL-geodesics flow matching with a novel sampling scheme",
            "text": "Non-autoregressive language models have emerged as a promising alternative to traditional autoregressive models in natural language processing (NLP) tasks, potentially offering significant advantages during inference by enabling the simultaneous generation of all tokens rather than a sequential process. However, capturing the complex dependencies inherent in discrete textual data without relying on the autoregressive assumption presents substantial challenges. \n\nIn this paper, we investigate a conditional flow matching approach for text generation, which has garnered significant interest recently. Notably, methods such as Discrete Flow Matching (DFM) [10] and Dirichlet flow matching [26] have adapted continuous flow-based models to address the discrete nature of text by representing tokens as one-hot vectors within a V -dimensional simplex, where V denotes the vocabulary size. \n\nThe central concept in flow matching involves initiating a probability path \u03c1 t between a known starting distribution \u03c1 0 and a target distribution \u03c1 1 , which is only accessible through samples. In our context, these samples are provided as discrete sequences, represented as sequences of vectors within a V -dimensional simplex. Previous research has highlighted the limitations of linear interpolation in this framework [26]. In response, we propose the utilization of geodesics under the Kullback-Leibler (KL) divergence, which effectively incorporates the inherent geometry of the simplex. These geodesics correspond to linear interpolation in the logit space, expressed as l t = log x t . \n\nIn the conditional flow matching (CFM) framework for generative modeling, the choice of loss function is flexible. Building upon insights from recent research, we implement a denoiser that maximizes the conditional likelihood P \u03b8 (x 1 | x t , t). While it is feasible to model the joint conditional distribution P \u03b8 (x 1 , x t ), this approach offers no clear advantage over existing autoregressive models; specifically, setting t = 0 would require modeling the unconditional data distribution P (x 1 ). Consequently, we focus on modeling the marginal likelihood of each token in x 1 conditioned on x t , effectively approximating the joint distribution in a simplified manner. Although theoretical justification for this loss function has been established in the context of singletoken predictions, its applicability to general sequences has not been thoroughly investigated.",
            "score": 0.4222960091571258,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2465
                }
            ],
            "ref_mentions": [
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "267547708"
                },
                {
                    "start": 1315,
                    "end": 1319,
                    "matchedPaperCorpusId": "267547708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00847625732421875
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "Natural language generation (NLG)\n\nNatural text generation aims to produce fluent, reasonable and understandable linguistic text from input data (Yu et al., 2022b).This task is more formally known as \"natural language generation\" in the literature.At present, it is one of the most important and challenging subtasks in NLP.NLG has two principal generative methods: autoregressive (AR) and non-autoregressive (NAR), also known as end-to-end generation.With the rise of deep learning in recent years, researchers have proposed various models to realize language generation, including the Transformer (Vaswani et al., 2017), BERT (Devlin et al., 2018), and GPT (Radford et al., 2019), as well as diffusion-based text generative model.In the era of Large Language Models (LLMs), decoder-only models, exemplified by the GPT, have emerged as a pivotal technology in the domain of text generation.Such models generate text exclusively through the decoder, obviating the necessity for a dedicated encoder, and operate in an autoregressive manner, sequentially generating discrete tokens.The introduction of diffusion-based models has steered the evolution of the text generation field towards harnessing both discrete and continuous features more comprehensively across diverse tasks.",
            "score": 0.4217411544340223,
            "section_title": "DEFINITIONS",
            "char_start_offset": 8019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 35,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1277
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 163,
                    "matchedPaperCorpusId": "222272210"
                },
                {
                    "start": 599,
                    "end": 621,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 659,
                    "end": 681,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121002197265625
        },
        {
            "corpus_id": "271874728",
            "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding",
            "text": "Discrete diffusion models. Based on seminal works Austin et al. (2021); Campbell et al. (2022), recent work on masked diffusion models (Lou et al., 2023;Shi et al., 2024;Sahoo et al., 2024) has demonstrated their strong performance in natural language generation. Additionally, they have been applied to biological sequence generation (e.g., DNA, protein sequences in Campbell et al. (2024); Sarkar et al. (2024)). In these cases, the use of diffusion models over autoregressive models is particularly apt, given that many biological sequences ultimately adopt complex three-dimensional structures. We also note that ESM3 (Hayes et al., 2024), a widely recognized foundational model in protein sequence generation, bears similarities to masked diffusion models. \n\nDespite its significance, it cannot be integrated with standard classifier guidance because adding a continuous gradient to a discrete objective is not inherently valid. Unlike standard classifier guidance, our algorithm can be seamlessly applied to discrete diffusion models. \n\nDecoding in autoregressive models with rewards. The decoding strategy, which dictates how sentences are generated from the model, is a critical component of text generation in autoregressive language models (Wu et al., 2016;Chorowski and Jaitly, 2016;Leblond et al., 2021). (2024). While there are similarities between these works and ours, to the best of our knowledge, no prior work has extended such methodologies to diffusion models. Furthermore, our approach leverages characteristics unique to diffusion models that are not present in autoregressive models such as SVDD-PM.",
            "score": 0.4217158473372706,
            "section_title": "Related Works",
            "char_start_offset": 7392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 70,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 72,
                    "end": 94,
                    "matchedPaperCorpusId": "249192370"
                },
                {
                    "start": 392,
                    "end": 412,
                    "matchedPaperCorpusId": "270057410"
                },
                {
                    "start": 622,
                    "end": 642,
                    "matchedPaperCorpusId": "270963342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1756591796875
        },
        {
            "corpus_id": "852013",
            "title": "Improving Data Driven Wordclass Tagging by System Combination",
            "text": "In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpusbased/probabilistic. Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven). \n\nData driven methods appear to be the more popular. This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) wellunderstood. When a data driven method is used, a model is automatically learned from the implicit structure of an annotated training corpus. This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality. \n\nObviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard-and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited. A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. \n\nIn the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.",
            "score": 0.4215847068705514,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 303,
                    "matchedPaperCorpusId": "5216592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017181396484375
        },
        {
            "corpus_id": "258823370",
            "title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion",
            "text": "Bit Diffusion (Chen et al., 2022) uses real numbers to model the bits of data for enabling continuous state diffusion models to generate discrete data. Besides, self-conditioning and asymmetric time intervals that greatly improve the sample quality. Diffusion-LM (Li et al., 2022) maps discrete tokens into continuous latent variable by adding an embedding step and a rounding step to the standard diffusion process with designing a training objective to learn the embedding. It achieves more complex controllable text generation through continuous diffusion. \n\nConditional Text Diffusion DiffuSeq (Gong et al., 2022) adopts partially noising processes with only a single Transformer encoder and trained end-to-end in a classifier-free manner to extend Diffusion-LM for sequence-to-sequence (Seq2Seq) generation tasks. Considering the importance of embedding space for the diffusion process, SED (Strudel et al., 2022) uses a BERT to generate embeddings for diffusion input tokens, with the training objective of Diffusion-LM and self-conditioning skill from Bit Diffusion. Besides, classifier-free guidance (Ho and Salimans, 2022) are performed to allows leveraging both the unconditional and conditional abilities of a model to improve its conditional generations. CDCD (Dieleman et al., 2022) is a framework for continuous diffusion models of categorical data with score interpolation and time warping based on score matching diffusion models (Song and Ermon, 2019;Song et al., 2021c). It adopts an encoder-decoder (ED) architecture for machine translation. \n\nThe potential of applying ED architectures to more diffusion text generation tasks still needs to be explored. It should be noted that a concurrent study SeqDiffuSeq (Yuan et al., 2022) also studies applying encoder-decoder for text diffusion. SeqDiffuSeq extends the continuous text diffusion model to sequence-to-sequence text generation under the encoder-decoder architecture. Two techniques of self-conditioned denoising and token-level adaptive noise schedule are also adopted in SeqDiffuSeq.",
            "score": 0.42102657747069494,
            "section_title": "Unconditional and Controllable Text Diffusion",
            "char_start_offset": 5611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2060
                }
            ],
            "ref_mentions": [
                {
                    "start": 1446,
                    "end": 1468,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "273532521",
            "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
            "text": "However, the adaptation of diffusion models from AR LLMs remains unexplored. \n\nNon-autoregressive Generation Non-autoregressive (NAR) models, introduced by Gu et al. (2018), break free from the left-to-right generation constraint, allowing for new capabilities like planning with future tokens (Wu et al., 2024). Current diffusion language models are a notable part of the NAR family (Gong et al., 2023b). Given the challenges of developing NAR models, researchers often seek to find a trade-off. For instance, SSD-LM (Han et al., 2023) leverages diffusion models to iteratively generate text blocks, facilitating a semi-NAR generation process. Similarly, CLLM (Kou et al., 2024) enhances LLMs by enabling the parallel generation of n tokens, thereby improving decoding speed. FiLM (Shen et al., 2023) adapts language models to generate tokens in any order, which is particularly useful for infilling tasks. Guo et al. (2020) trains a NAR using a curriculum for the attention mask on translation tasks with seq2seq labels. Additionally, Gloeckle et al. (2024) focus on training models to achieve better and faster multi-token predictions as they scale up. These NAR approaches provide compelling alternatives to traditional AR LLMs, yet few have thoroughly explored training large NAR models on large-scale unlabeled data.",
            "score": 0.4204498252293536,
            "section_title": "RELATED WORK",
            "char_start_offset": 33103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1322
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 311,
                    "matchedPaperCorpusId": "268819892"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "252917661"
                },
                {
                    "start": 518,
                    "end": 536,
                    "matchedPaperCorpusId": "253237701"
                },
                {
                    "start": 661,
                    "end": 679,
                    "matchedPaperCorpusId": "268230510"
                },
                {
                    "start": 908,
                    "end": 925,
                    "matchedPaperCorpusId": "208175668"
                },
                {
                    "start": 1037,
                    "end": 1059,
                    "matchedPaperCorpusId": "269457456"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012969970703125
        },
        {
            "corpus_id": "267177800",
            "title": "Harnessing Generative Pre-Trained Transformers for Construction Accident Prediction with Saliency Visualization",
            "text": "Additionally, Kim & Chi [34] developed a system for managing construction accident knowledge, demonstrating notable recall, precision, and F1 score values of 0.71, 0.93, and 0.80, respectively. These research efforts emphasize the importance of employing advanced NLP-based modeling techniques to gain a comprehensive understanding of the various factors influencing accidents and to improve safety protocols. However, as mentioned in the Introduction, current studies have predominantly focused on developing conventional machine and deep learning methods, which entail manually extracting text features and inputting them into a classifier [15,29,36]. These approaches come with inherent limitations, including limited scalability and generalization, difficulties in analyzing free-form text, high dimensionality, and a lack of adaptability to new tasks. While traditional machine and deep learning methods have made strides in NLP within the construction domain, overcoming these limitations requires the incorporation of more innovative, generative, and robust AI models. \n\nRecently, LLMs, particularly those built on the transformer architecture such as GPT, developed by OpenAI, present notable advantages for various NLP tasks, including classification, compared to traditional machine learning and deep learning models [20,37]. These models are trained on vast datasets, enabling them to generate human-like text based on the input they receive. The adaptability and proficiency of GPT models in processing and generating natural language have the potential to significantly impact and facilitate various aspects in diverse fields, ranging from education and customer service to research and industries. \n\nThe capacity of GPT to generate text resembling human writing can be attributed to its deployment of the transformer model. The transformer model employed in GPT is depicted in Figure 1. The model employs a decoder structure with 96 repeated layers. These layers allow the model to progressively refine its understanding of the input text, enabling it to generate coherent and contextually relevant outputs [38]. The repeated stacking of decoder layers, coupled with attention mechanisms and residual connections, ensures the model's proficiency in tasks such as language translation and text generation, particularly classification [39,40]. The transformer demonstrates proficiency in capturing contextual information, implementing end-to-end learning without manual feature engineering, employing attention mechanisms for nuanced comprehension, and leveraging transfer learning to improve performance [41].",
            "score": 0.42033349545800647,
            "section_title": "Research Background",
            "char_start_offset": 10540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2355
                },
                {
                    "start": 2356,
                    "end": 2622
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 28,
                    "matchedPaperCorpusId": "115687171"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "113976312"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "40743699"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "258529854"
                },
                {
                    "start": 2347,
                    "end": 2351,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 2351,
                    "end": 2354,
                    "matchedPaperCorpusId": "248987297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040008544921875
        },
        {
            "corpus_id": "253237701",
            "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015), trained to iteratively refine noised inputs, have recently emerged as powerful tools for generative modeling in several continuous-valued domains such as images (Ho et al., 2020), audio (Kong et al., 2021), video (Ho et al., 2022), among others. Attempts to adapt them for discrete domains such as text data, however, have only had limited success: prior work have shown to be promising on specialized cases and small datasets (Hoogeboom et al., 2021;Austin et al., 2021;Chen et al., 2022), but diffusion models for text still underperform (and thus are not widely adopted) compared to autoregressive language models (AR-LMs) which remain the state-of-the-art general purpose text generators Brown et al., 2020). 1 https://github.com/xhan77/ssd-lm Despite potential advantages of diffusion models for text, there are two key barriers for their adoption. First, diffusion models generate text nonautoregressively, i.e., generating all tokens simultaneously rather than token by token left-to-right. Although non-autoregressive generation can be an advantage since token generation is informed by a broader bi-directional (latent) context (Lee et al., 2018;Ghazvininejad et al., 2019), it requires predefining an output sequence length. This limits the flexibility and applicability of trained models, since a model trained with short sequence lengths is not usable for generating longer sequences. On the other hand, non-autoregressive training with long sequences is expensive and difficult to optimize. In this work, we propose a semi-autoregressive solution which strikes a balance between length flexibility and the ability to alter previously generated tokens.\n\nAnother advantage of diffusion models is their post-hoc controllability using guidance from auxiliary models such as style classifiers (Dhariwal and Nichol, 2021). However, controllability is hard to achieve without compromises in modularity in diffusion-based LMs for text. To enable diffusion generation into discrete",
            "score": 0.4201589569547346,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 499,
                    "end": 519,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 1185,
                    "end": 1203,
                    "matchedPaperCorpusId": "3438497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038116455078125
        },
        {
            "corpus_id": "268537008",
            "title": "Enhancing Traffic Incident Management with Large Language Models: A Hybrid Machine Learning Approach for Severity Classification",
            "text": "BERT-large [27] also employs Masked Language Modeling (MLM) as its training method.As an enlarged version of BERT, it maintains the core principles but scales up the architecture to handle more complex tasks and deliver better performance.\n\nXLNet [28] uses a Generalized Autoregressive Pretraining method.It addresses some limitations of BERT and incorporates features of Transformer-XL, thus allowing for better handling of long-term dependencies in the text.\n\nXLNet-large [28] is a scaled-up version of XLNet, and it also employs Generalized Autoregressive Pretraining.It brings the advantages of XLNet into a larger and more powerful architecture, suitable for more complicated tasks.\n\nRoBERTa (Robustly Optimized BERT) [29] aims to improve upon BERT by optimizing its training process.It employs a modified form of MLM as its training method, removes the next sentence prediction task, and introduces dynamic masking for better performance.\n\nALBERT (\"A Lite BERT\") [30] is a self-supervised transformer-based NLP model that utilizes Masked Language Modeling and Sentence Ordering Prediction for pretraining, utilizes shared-layer architecture for reduced memory footprint.The latest version (2.0) improves upon its predecessor with adjustments like lower dropout rates and additional training data for enhanced performance in downstream tasks.The largest configuration is ALBERT-xxlarge version 2 have been selected as a large model in this study.\n\nIncluding variations of BERT in the analysis is a design choice due to the expectation of higher performance from these models.BERT and its variants are designed to capture complex language nuances and contextual relationships within text, which often result in better performance on tasks involving natural language understanding.Even if current datasets do not show substantial differences between language models, it remains a reasonable approach to test BERT variations, as they have the potential to yield superior results, especially with richer narrative content in the data.\n\nThe model summary table (see Table I) provides a synthesized overview of several prominent NLP models, the technical aspects and functionalities of these models, number of parameters, vector size, primary features, and their relevance to traffic incident severity classification.This comparative analysis helps to understand the capabilities of each model when dealing with incident report texts, thereby informing our selection of models for the study.",
            "score": 0.41963542852600555,
            "section_title": "Full-text Representation",
            "char_start_offset": 16986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 239
                },
                {
                    "start": 241,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 460
                },
                {
                    "start": 462,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 687
                },
                {
                    "start": 689,
                    "end": 789
                },
                {
                    "start": 789,
                    "end": 944
                },
                {
                    "start": 946,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1451
                },
                {
                    "start": 1453,
                    "end": 1580
                },
                {
                    "start": 1580,
                    "end": 1784
                },
                {
                    "start": 1784,
                    "end": 2035
                },
                {
                    "start": 2037,
                    "end": 2316
                },
                {
                    "start": 2316,
                    "end": 2490
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0226287841796875
        },
        {
            "corpus_id": "252780170",
            "title": "CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning",
            "text": "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0.",
            "score": 0.4185848365485682,
            "section_title": "classification free guidance",
            "char_start_offset": 14046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 695
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "260334572",
            "title": "Guiding image captioning models toward more specific captions",
            "text": "Let x be an image caption and y be the corresponding image. A standard captioning model aims to model the likelihood p(x|y), factorized autoregressively in terms of the probability of each token given previous tokens \n\nThe network is trained so that its output distribution \n\nAt inference time, one typically uses beam search or greedy decoding to produce a caption that has a particularly high probability. In this work, we use greedy decoding because it is the more common choice and it is also simpler to implement. \n\nClassifier-free guidance (CFG) [14] aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans [14] originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12,47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . .",
            "score": 0.4184348723672907,
            "section_title": "Classifier-free guidance for image captioning",
            "char_start_offset": 7766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "252917661",
            "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
            "text": "Among existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma & Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020;Nichol & Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song & Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b;Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts. \n\nOn the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021;Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal x) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities p(w|x) to sequences of words w given x). In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition x is also a sequence of words, applying Diffusion-LM can be difficult.",
            "score": 0.4178019886871501,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 63,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 99,
                    "end": 122,
                    "matchedPaperCorpusId": "1687220"
                },
                {
                    "start": 152,
                    "end": 171,
                    "matchedPaperCorpusId": "6610705"
                },
                {
                    "start": 177,
                    "end": 201,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 303,
                    "end": 322,
                    "matchedPaperCorpusId": "8768364"
                },
                {
                    "start": 412,
                    "end": 429,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 429,
                    "end": 453,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 614,
                    "end": 643,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "221818900"
                },
                {
                    "start": 1117,
                    "end": 1141,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 1141,
                    "end": 1161,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "276409208",
            "title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching",
            "text": "In this framework, the coarse flow-matching model predicts a vector field v c t (x i,c , z i ; \u03b8 c FM ) by conditioning on linguistic features z i extracted from the language model. In the fine stage, the model refines this approximation by recovering details x i,f , represented as the residual between the original frame x i and the upsampled coarse component Upsample(x i,c ). A secondary flow-matching model predicts the vector field v f t (x i,f , z i , x i,c ; \u03b8 f FM ), governing this process by leveraging both the features z i and the coarse component (with ground-truth coarse features x i,c during training and predicted values xi,c during inference) as conditional inputs. This hierarchical conditioning allows the fine model to focus on local details while preserving global coherence from the coarse stage. \n\nFor step i, the training objective combines losses from both stages: \n\nwhere u c t and u f t represent the true conditional vector fields for the coarse and fine components, respectively, and t \u223c U[0, 1]. The initial states x i,c 0 and x i,f 0 are similarly initialized using the prior from Equation 4, applying the corresponding sampling operations. By decoupling low-resolution structure learning from high-detail refinement, this coarse-to-fine approach generates high-fidelity mel-spectrograms while maintaining temporal consistency through autoregressive dependencies. \n\nClassifier-Free Guidance Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol andDhariwal, 2021]. In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale.",
            "score": 0.4177398305914986,
            "section_title": "Coarse-to-Fine Flow Matching",
            "char_start_offset": 14552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2004
                },
                {
                    "start": 2007,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 1614,
                    "end": 1629,
                    "matchedPaperCorpusId": "231979499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "218900594",
            "title": "Comparing BERT against traditional machine learning text classification",
            "text": "In this work we have introduced the BERT model and the classical NLP strategy where a machine learning model is trained using the features retrieved with TF-IDF and hypothesize about the behaviour of BERT w.r.t these techniques in the search of a default technique to tackle NLP tasks. We have introduced four different NLP scenarios where we have shown how BERT has outperformed the traditional NLP approach, adding empirical evidence of its superiority in average NLP problems w.r.t. classical methodologies. Furthermore, and of critical interest, implementing BERT has turned out to be far less complicated than implementing the traditional methods. It is also noteworthy the importance of transfer learning. We have been able to obtain this results thanks to pre-training. \n\nTransfer learning has become more apparent in experiment 4.4 (which has the smallest dataset among all the experiments). We are nevertheless aware of the limitations of the BERT model. Although it seems that it is a good default for NLP tasks, its results can be improved. In order to do so, we would like to research in a hyperparameter auto-tuned BERT model for any new NLP task with Bayesian Optimization. We would like to use that auto-tuned BERT to enable classification of language messages for robots [16] [10] showing consciousness correlated behaviours.",
            "score": 0.4170381479165789,
            "section_title": "Conclusions and further work",
            "char_start_offset": 17205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1341
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "258088263",
            "title": "A Survey of Text Representation and Embedding Techniques in NLP",
            "text": "The topic of NLP broadly consists of two main steps: first is the representation of the input text (raw data) into numerical format (vectors or matrix), and second is the design of models for processing the numerical data to achieve a desired goal or task. This paper focuses on the first part and shows how because of the change in the text representation, the NLP field progressed from just being able to comprehend bits and pieces to all the aspects of the text. As shown in Figure 1, we broadly classify the embedding learning techniques into rule-based, statistical, and neural-network-based approaches. \n\nIn hand-based techniques, the rules and features are derived by experts, such as trees, graphs, grammar rules, etc. Statistical and mathematical formulas are used to derive the feature. In neural networks, the neural model learns the features automatically, which are categorized into context-sensitive, context-insensitive, and pre-trained embeddings. \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Sunil Karamchandani . \n\nThe NLP field began with the exact matching technique where Context Free Grammar (CFG) was used for analysis. The search engines were mainly based on the complex nested if-then rules represented by CFG. Further advancements led to approximate matching, where errors up to a certain threshold were ignored. However, because of the ambiguous nature of natural languages, using such techniques and designing complex rules could have been more convenient, time-consuming, and error-prone. More statistical approaches then followed these pattern-matching approaches. \n\nAs the research evolved, statistical approaches were sought, where the focus was given to the frequency of the words. Several techniques, such as One Hot Encoding (OHE), Bag of Words (BoW), Term-Frequency (TF), Inverse-Document-Frequency (IDF), etc., fall under this category. Compared to grammar-based, these techniques were easy to implement and improved the models' accuracy. However, such representations suffered from the curse of dimensionality, and also, because of computational power limitations, their performance suffered on large-scale datasets. As a result, they needed to be more scalable, and their usage was primarily limited to small-scale datasets.",
            "score": 0.41652957606691954,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0087432861328125
        },
        {
            "corpus_id": "273228630",
            "title": "Diversity-Rewarded CFG Distillation",
            "text": "Classifier-free guidance (CFG). Initially introduced for diffusion models (Ho and Salimans, 2022) and later adapted for autoregressive LLMs (Gafni et al., 2022;Sanchez et al., 2023;Wings, 2022), CFG has found widespread application in various generative domains, including image (Nichol et al., 2021;Ramesh et al., 2022;Rombach et al., 2022;Saharia et al., 2022;Yu et al., 2022), video (Blattmann et al., 2023;Ho et al., 2022), and audio (Copet et al., 2023;Kreuk et al., 2023) generation. However, the detrimental impact of CFG on diversity is well-documented (Dhariwal and Nichol, 2021;Kreuk et al., 2023;Nichol et al., 2021), limiting its application when exploration is key. \n\nDistillation. Knowledge distillation (Hinton et al., 2015) is emerging as a powerful technique to train state-of-the-art models (Gemma Team et al., 2024). By transferring knowledge from a teacher, the student can perform better than with standard training on the same data (Gu et al., 2023;Lin et al., 2020;Sanh et al., 2019). Closer to our work, Meng et al. (2023) propose a two-stage offline procedure to distill a CFG-augmented diffusion model. In contrast, we employ a single-stage on-policy distillation procedure (Agarwal et al., 2024) to distill a CFG-augmented LLM, while introducing a novel diversity-promoting RL algorithm and model merging for improved quality-diversity trade-off. \n\nQuality-diversity in LLMs. Zhang et al. (2021) compare the quality-diversity trade-offs of various inference-time strategies for LLMs, including temperature sampling, top-k sampling (Fan et al., 2018), and nucleus sampling (Holtzman et al., 2020).",
            "score": 0.4157199759208486,
            "section_title": "Related work",
            "char_start_offset": 26735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 300,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 320,
                    "end": 341,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 341,
                    "end": 362,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 362,
                    "end": 378,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 386,
                    "end": 410,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 458,
                    "end": 477,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 561,
                    "end": 588,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 588,
                    "end": 607,
                    "matchedPaperCorpusId": "252668761"
                },
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 718,
                    "end": 739,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1028,
                    "end": 1046,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 1200,
                    "end": 1222,
                    "matchedPaperCorpusId": "263610088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "260438429",
            "title": "Curricular Transfer Learning for Sentence Encoded Tasks",
            "text": "Traditional Machine Learning (ML) technology is influenced by the assumption that a difference in data distribution between training and real-world data can result in a degradation of the predictive learner (Shimodaira, 2000). To overcome the effects of data divergence, studies have developed several algorithms that have some regularizing hyper-parameter to mitigate the variance in the hypothesis space and learn computable functions that have great generality to out-of-distribution data (Sarker, 2021). \n\nThese traditional methods have been successfully applied in many practical applications, but they still present limitations for specific real-world scenarios where we have complex data like images or text (Bengio et al., 2007). Deep learning overcame a few of these limitations, acquiring nearhuman performance in various tasks. Whereas traditional ML has an explicit bias to control overfitting, deep models need to acquire its bias from data, which generally could be expensive or even unattainable in some scenarios. \n\nTransfer learning was introduced to achieve high performance in low-resource domains by artificially increasing available data from a different do-main to overcome these limitations (Ruder, 2019). This technique consists in pre-training a model with high variance in domains with several available examples, so those learning models can acquire suitable biases that generalize for other tasks, including tasks where little to no examples are available (Howard and Ruder, 2018). \n\nIn Natural Language Processing (NLP), the standard approach for transfer learning is language modeling, which consists in pre-training a model to denoise a sentence, and then fine-tuning it to the target task where the model is applied (Devlin et al., 2018). This self-supervised sequential transfer learning enables the model to learn high-level linguistic features and statistical properties of language that help the model to generalize for many downstream tasks. Exploiting these capabilities, recent studies encode tasks as a pure sequence-tosequence problem (Raffel et al., 2020) to directly solve it with auto-regressive language modeling task (Brown et al., 2020). \n\nTask-Oriented Dialog Systems (TODS) are conversational agents designed for solving businessrelated tasks by interacting with customers through a conversation in a natural language interface.",
            "score": 0.4156051433606023,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2377
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 225,
                    "matchedPaperCorpusId": "9238949"
                },
                {
                    "start": 492,
                    "end": 506,
                    "matchedPaperCorpusId": "232322114"
                },
                {
                    "start": 1484,
                    "end": 1508,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 2163,
                    "end": 2183,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01085662841796875
        },
        {
            "corpus_id": "273142392",
            "title": "Opportunities and challenges of diffusion models for generative AI",
            "text": "Practical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method [104 ] which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ]. \n\nModulating guidance strength in practice.",
            "score": 0.4141072172127024,
            "section_title": "Classifier and classifier-free guidance",
            "char_start_offset": 54606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2028
                },
                {
                    "start": 2031,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 250,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "268230892",
            "title": "Training Unbiased Diffusion Models From Biased Dataset",
            "text": "The sampling process of the diffusion model involves an iterative update process using a score direction, typically approximated by the score network. When there is a specific purpose for generating data, score correction becomes necessary. There are several methods to adjust this score direction, each tailored to specific purposes. From a technical standpoint, these methods can be divided into two groups: guidance methods and score-matching regularization methods. \n\nGuidance methods introduce additional gradient signals to adjust the update direction. Classifier guidance (Song et al., 2020;Dhariwal & Nichol, 2021) utilizes a gradient signal from a classifier to generate samples that satisfy a condition. Classifier-free guidance (Ho & Salimans, 2021) also aims at conditional generation but relies on both unconditional and conditional scores. Furthermore, various methods have been proposed to enable controllable generation using auxiliary models with a pre-trained unconditional score model (Graikos et al., 2022;Song et al., 2023). On the other hand, discriminator guidance (Kim et al., 2023) serves a different purpose by enhancing the sampling performance of a diffusion model through the use of a discriminator that distinguishes between real images and generated images. EGSDE (Zhao et al., 2022) leverages guidance signals based on energy functions, enhancing unpaired image-to-image translation. Guidance methods have the advantage of utilizing pre-trained score networks without the need for additional training. However, they require separate network training for guidance and additional network evaluation during the sampling process. \n\nThere is a body of work on score-matching regularization for better likelihood estimation (Lu et al., 2022;Zheng et al., 2023b;Lai et al., 2023). Na et al. (2024) propose a regularized conditional scorematching objective to mitigate label noise. The unique benefit of score-matching regularization is that it does not require an additional network at the inference stage.",
            "score": 0.4139208937215151,
            "section_title": "B.3 SCORE CORRECTION IN DIFFUSION MODEL",
            "char_start_offset": 31604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 759,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1004,
                    "end": 1026,
                    "matchedPaperCorpusId": "249889060"
                },
                {
                    "start": 1026,
                    "end": 1044,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 1088,
                    "end": 1106,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1295,
                    "end": 1314,
                    "matchedPaperCorpusId": "250526607"
                },
                {
                    "start": 1750,
                    "end": 1767,
                    "matchedPaperCorpusId": "249712167"
                },
                {
                    "start": 1767,
                    "end": 1787,
                    "matchedPaperCorpusId": "258557324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "265351587",
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "text": "Conditional generative modeling paves the way to numerous machine learning applications such as conditional image generation (Dhariwal and Nichol, 2021;Rombach et al., 2022), text-to-speech synthesis (Wang et al., 2023;Le et al., 2023), and even solving decision making problems (Chen et al., 2021;Janner et al., 2021Janner et al., , 2022;;Ajay et al., 2022). Models that appear ubiquitously across a variety of application domains are diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) and flow-based models (Song et al., 2020b;Lipman et al., 2023;Albergo and Vanden-Eijnden, 2022). Majority of this development has been focused around diffusion models, where multiple forms of conditional guidance (Dhariwal and Nichol, 2021;Ho and Preprint. Table 1.1: The data point x1 and the conditioning variables y on which the conditional guidance will be applied, for the three applications settings that we consider. Salimans, 2022) have been introduced to place larger emphasis on the conditional information. While flow models have been shown to be more efficient alternatives than diffusion models (Lipman et al., 2023;Pooladian et al., 2023) in unconditional generation, requiring less computation to sample, their behavior in conditional generation tasks has not been explored as much. It also remains unclear whether conditional guidance can be applied to and help the performance of flow-based models. \n\nIn this work, we study the behavior of Flow Matching models for conditional generation. We introduce Guided Flows, an adaptation of classifier-free guidance (Ho and Salimans, 2022) to Flow Matching models, showing that an analogous modification can be made to the velocity vector fields, including the optimal transport (Lipman et al., 2023) and cosine scheduling (Albergo and Vanden-Eijnden, 2022) flows used by prior works. \n\nWe experimentally validate Guided Flows on a variety of applications, ranging from generative modeling over multiple modalities to offline reinforcement learning (RL), see Table 1.1.",
            "score": 0.41349007932971327,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 219,
                    "end": 235,
                    "matchedPaperCorpusId": "259275061"
                },
                {
                    "start": 279,
                    "end": 298,
                    "matchedPaperCorpusId": "235294299"
                },
                {
                    "start": 298,
                    "end": 317,
                    "matchedPaperCorpusId": "235313679"
                },
                {
                    "start": 453,
                    "end": 482,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 482,
                    "end": 498,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "276961040",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "text": "Conditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
            "score": 0.41334331526617074,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "235694314"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "5560643"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 1476,
                    "end": 1479,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "272770713"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "273549917"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "254126443",
            "title": "Flow-based network intrusion detection based on BERT masked language model",
            "text": "It is common in practical application of NIDS for there to be a change in the data distribution between its training data and the data it encounters when deployed. Conventional ML algorithms often adapt poorly to such change, which limit their usefulness in real-world scenarios [1]. To address this, Energy-based Flow Classifier (EFC) [3] was proposed as a solution. Despite having good adaptability, EFC produces high false positives rate for domains where the distribution of features of malicious flows overlap with that of benign flows. We theorize that the reason for the limitations of conventional ML algorithms and EFC is the use of singular flows as input data, as the classifier can only model the distribution of features within a flow. This limitation can be overcome with the use of sequences of flows, allowing the classifier to further models Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s  BERT [2] is a transformer-based machine learning technique for NLP developed by Google. The BERT framework is comprised of two steps: pre-training and fine-tuning. In pre-training, the BERT model is trained on unlabeled data. For fine-tuning, the model is first initialized using the pre-trained parameters, and then trained using labeled data from the downstream tasks. BERT is pre-trained with two unsupervised tasks, which are Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, some of the words in a sentence are replaced with a different token. The objective is to predict the original value of the masked words based on other unmasked words in the sentence. In NSP, BERT takes sentence pairs as input. The objective is to predict whether the second sentence in the pair is the next sentence in the document. For fine-tuning, task-specific inputs and outputs are added to a pre-trained BERT model.",
            "score": 0.41311840045438664,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "225153435"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "235406214"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.008880615234375
        },
        {
            "corpus_id": "265213138",
            "title": "UT5: Pretraining Non autoregressive T5 with unrolled denoising",
            "text": "Large language models (LLMs) have revolutionized natural language processing (NLP) by enabling automatic text generation and prediction. Traditionally, language models are autoregressive: they generate a sequence of tokens one by one, conditioning each token on the previously generated ones. While this approach has led to impressive results (OpenAI, 2023;Anil et al., 2023), it suffers from slow inference due to its sequential nature. Several studies (Gu et al., 2018;Ghazvininejad et al., 2019a) have explored the use of non-autoregressive generation for language modeling, where tokens can be generated in parallel, without the need of conditioning on previously generated ones. Nonautoregressive generation has shown promising results in terms of efficiency and speed, and has the potential to be applied to various NLP tasks (Liu et al., 2020). Pretraining has proven the foundational procedure for autoregressive generation (Devlin et al., 2019;Radford et al., 2018). However, few studies have focused on pretraining for non-autoregressive language modeling for efficient language generation . The main advantage of nonautoregressive generation is parallel generation of all tokens, making it faster than auto-regressive generation. However, non-autoregressive generation usually exhibits quality gaps when comparing with similar sized autoregressive models (Gu and Kong, 2020). \n\nIn this paper, we propose a pretraining regime to improve the quality of non-autoregressive generation. To explore the effects of pretraining on decoder-only models, we employed step-unrolled denoising (Savinov et al., 2021) to pretrain the models. In the rest of the paper, we describe our proposed pretraining regime in detail and evaluate its effectiveness in improving the quality of efficientnon-autoregressive text generation. \n\nOur contributions are: \n\n\u2022 Introduce training regime for nonautoregressive models for efficient language generation, \n\n\u2022 We show that the non-autoregressive pretraining with unrolled denoising significantly improves the results on downstream benchmarks compared to training from scratch.",
            "score": 0.4124799541796349,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 932,
                    "end": 953,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0129241943359375
        },
        {
            "corpus_id": "267547708",
            "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
            "text": "A key attribute of iterative generative models is the ability to continuously and gradually bias the generative process towards a class label with user-specified strength, a technique known as guidance (Dhariwal & Nichol, 2021;Ho & Salimans, 2022). Initially proposed in the context of diffusion models, where the generative process follows the score \u015d(x, t; \u03b8) \u2248 \u2207 x log p t (x) of the noisy data distribution, guidance is implemented by taking a linear combination of the unconditional and conditional score models \u015dCFG (x, t, y; \u03b8) = \u03b3\u015d(x, t, y; \u03b8)+(1\u2212\u03b3)\u015d(x, t, \u2205; \u03b8) (18) \n\nwith \u03b3 > 0 and running the generative process with this adjusted score. In the context of flow matching, Dao et al. (2023); Zheng et al. (2023) derived a relationship between the score and marginal flow for certain Gaussian probability paths and implemented flow guidance by propagating the effects of standard score adjustments to the resulting flow fields. We implement guidance for Dirichlet FM by deriving a similar relationship between the score and flow field, detailed below. Note that when \u03b3 = 1, this adjustment precisely mimics the flow that would result from training only on points with class label y; however, similar to prior works, we find \u03b3 > 1 futher enhances the guidance efficacy. \n\nRelationship between flow and score. For the Dirichlet marginal probability path, the score can be obtained from the model posterior via the denoising score-matching identity (Song & Ermon, 2019): \n\nWe can differentiate Equation 14 to obtain a matrix equation \n\nHere, D is a K \u00d7 K diagonal matrix dependent on x, t and \u015d, p \u2208 R n . (Technically, \u015d contains both on-simplex and off-simplex components, the latter of which is ignored.) Meanwhile, the computation of the marginal flow (Equation 10) can also be written as a very similar matrix equation v = Up where the entries of U are given by Equation 15. \n\nCombining these, we obtain \n\nwhere D is invertible since it is diagonal with nonnegative entries.",
            "score": 0.4120617883196379,
            "section_title": "Guidance",
            "char_start_offset": 17171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 227,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "253237701",
            "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
            "text": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM\u2014a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
            "score": 0.4119376474352623,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1529541015625
        },
        {
            "corpus_id": "267035296",
            "title": "Image Translation as Diffusion Visual Programmers",
            "text": "Novelty. Our DVP contributes on three distinct technical levels. Specifically, we first re-consider the design of classifier-free guidance, and propose a conditional-flexible diffusion model which eschews the reliance on the sensitive manual control hyper-parameter w. Second, by decoupling the intricate concepts in feature spaces into simple symbols, we enable a context-free manipulation of contents via visual programming, which is both controllable and explainable. Third, our GPT-anchored programming framework serves as a demonstrable testament to the versatile transferability of Language Model Models, which can be extended seamlessly to a series of critical yet traditional tasks (e.g., detection, segmentation, tracking tasks). We do believe these distinctive technical novelties yield invaluable insights within the community. \n\nLimitation and Future Work. Despite DVP showcases superiority over state-of-the-art methods in both qualitative and quantitative aspects (see \u00a74.2), it also comes with new challenges and unveils some intriguing questions. For example, our approach grapples with obscured objects, attributed mainly to the segment module's limitation in exclusively processing the visible segment of an object, neglecting the occluded portion. This limitation consequently influences the diffusion process to generate base merely on the segmented part. In addition, in scenarios of photometric conditions, DVP and its counterparts falter in accurately aligning with the translated object(s), echoing challenges akin to the aforementioned issue (see \u00a7L). We posit that a specialized, fine-grained dataset tailored for a specific purpose (e.g., occluded items, photometric conditions) might be the future direction to bolster the diffusion model's proficiency. In our study, we integrated instance normalization with the null-text optimization, which is specifically design for text-guided diffusion model. This integration, however, is not directly applicable to a wider range of diffusion-based image generation applications. Nonetheless, the substantial potential of instance normalization has not been fully explored or utilized yet. A promising direction to leverage the power of instance normalization lies in fine-tuning the text-to-image task, which is split into a two-step procedure: image generation followed by image fine-tuning.",
            "score": 0.41179629887638036,
            "section_title": "N DISCUSSION",
            "char_start_offset": 36293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "259376564",
            "title": "MaChAmp at SemEval-2023 tasks 2, 3, 4, 5, 7, 8, 9, 10, 11, and 12: On the Effectiveness of Intermediate Training on an Uncurated Collection of Datasets.",
            "text": "Poth et al. (2021) evaluate a variety of (supervised) approaches to automatically identify which source datasets are beneficial, and conclude that pre-computable sentence representations are efficient for this task, and confirm that within task-type transfer outperforms cross-task-type transfer which is in line with the findings of Padmakumar et al. (2022) who compare transfer across and within task types. \n\nInstead of selecting tasks, recent work has attempted to train on a wide variety of tasks. This is commonly done in the space of generative language models, where training on a variety of tasks is easier because many NLP tasks can be converted to generation tasks, and can then directly be used to (re-)train an autoregressive language model (Aribandi et al., 2022;Sanh et al., 2022;Chung et al., 2022). In this setup it is easier to exploit a large variety of task types and a much higher amount of datasets (~50-1,800 datasets) is used compared to previous work. However, autoregressive language models still lag behind in performance compared to autoencoder language models for many tasks. Work that focused on intermediate training for encoder models and a large variety of task types is mainly done in the biomedical domain (Parmar et al., 2022;Fries et al., 2022). \n\nCompared to previous work on autoencoder models, we have a larger variety of tasks as well as languages. Our selection of datasets is somewhat arbitrary (all SemEval 2023 tasks), leading to a more challenging setup for multi-task learning compared to previous work who usually used a carefully curated set of datasets (with often 1 language and/or task type). The most similar to our setup is van der Goot (2022), as they also use an arbitrary set of tasks (all SemEval 2022 textbased tasks) and also compare joint training with intermediate training on the full collection of the data. However, their results show no clear trend on when one approach outperforms the other.",
            "score": 0.41128248880803675,
            "section_title": "Intermediate Training",
            "char_start_offset": 4286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 358,
                    "matchedPaperCorpusId": "248376935"
                },
                {
                    "start": 754,
                    "end": 777,
                    "matchedPaperCorpusId": "244478674"
                },
                {
                    "start": 777,
                    "end": 795,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 1241,
                    "end": 1262,
                    "matchedPaperCorpusId": "248227545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014007568359375
        },
        {
            "corpus_id": "273228684",
            "title": "CaLMFlow: Volterra Flow Matching using Causal Language Models",
            "text": "Flow Matching and Continuous Normalizing Flows: Flow matching has significantly enhanced the efficiency and scope of continuous normalizing flows (CNFs) (Chen et al., 2018;Papamakarios et al., 2021). Conditional Flow Matching (CFM) (Lipman et al., 2022) allows for precise control over the generative process by optimizing conditional vector fields tailored for specific distribution paths, including those based on Gaussian distributions. Tong et al. (2024) generalized the conditional paths and introduced mini-batch optimal transport and Schr\u00f6dinger bridge CFM, improving the efficiency and performance of CFM models. In Hu et al. (2024), flow matching is applied to text generation in a non-autoregressive manner, showing improvements compared to other diffusion-based text generation models such as DiffSeq (Gong et al., 2023). Our work, however, is primarily concerned with adapting LLMs to generate continuous data conditioned on text. \n\nText-conditional Generation: Text-conditional image generation has made significant strides through the integration of diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021) and large language models (LLMs). State-of-the-art systems like Stable Diffusion (Rombach et al., 2022), DALLE-2 (Ramesh et al., 2022), and DINOv2 (Oquab et al., 2024) leverage LLM embeddings to generate high-quality images from textual descriptions. Recent research (Ding et al., 2021;Yu et al., 2022;Ge et al., 2024;Zhan et al., 2024) has focused on adapting LLMs for multimodal generation, often employing vector quantization (van den Oord et al., 2017;Razavi et al., 2019;Ge et al., 2023) to extend LLM vocabularies with latent tokens representing non-textual data. Our CaLMFlow method introduces a novel approach as the first flow matching-based textconditional generative model that produces continuous tokens, potentially offering greater flexibility and expressiveness compared to discrete token-based methods.",
            "score": 0.4110202152065127,
            "section_title": "RELATED WORK",
            "char_start_offset": 3812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 172,
                    "matchedPaperCorpusId": "49310446"
                },
                {
                    "start": 172,
                    "end": 198,
                    "matchedPaperCorpusId": "208637478"
                },
                {
                    "start": 440,
                    "end": 458,
                    "matchedPaperCorpusId": "259847293"
                },
                {
                    "start": 624,
                    "end": 640,
                    "matchedPaperCorpusId": "268417074"
                },
                {
                    "start": 812,
                    "end": 831,
                    "matchedPaperCorpusId": "252917661"
                },
                {
                    "start": 1081,
                    "end": 1110,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1110,
                    "end": 1126,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1126,
                    "end": 1144,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1226,
                    "end": 1248,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1258,
                    "end": 1279,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 1292,
                    "end": 1312,
                    "matchedPaperCorpusId": "258170077"
                },
                {
                    "start": 1412,
                    "end": 1431,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 1447,
                    "end": 1463,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 1463,
                    "end": 1481,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1574,
                    "end": 1601,
                    "matchedPaperCorpusId": "20282961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01137542724609375
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "In this work, we elucidate the design space of off-the-shelf classifier guidance in diffusion generation. Using our training-free and accessible designs, off-the-shelf classifiers can effectively guide conditional diffusion, achieving state-of-the-art performance in ImageNet 128x128. Our approach is applicable to various diffusion models such as DDPM, EDM, and DiT, as well as text-to-image scenarios. We believe our work contributes significantly to the investigation of the ideal guidance method for diffusion models that may greatly benefit the booming AIGC industry. \n\nThere are multiple directions to extend this work. First, we primarily investigated classifier guidance in diffusion generation while there are more sophisticated discriminative models, e.g., detection models and visual Question Answering models, as well as other types of generative methods, e.g., masked image modeling [Chang et al., 2022[Chang et al., , 2023] ] and autoregressive models [Esser et al., 2021, Yu et al., 2022, 2023]. It would be interesting to explore other types of guided generation. Second, we only considered image generative models, and extending to language models would also be a promising direction. We believe that our proposed designs and calibration methodology hold potential for diverse modalities and we leave this for future work. To replicate the DiT off-the-shelf classifier guided sampling in Table 9: classifier-free scale s = 1.5, Softplus \u03b2 = 6, joint logit temperature \u03c4 1 = 1.1, marginal logit temperature \u03c4 2 = 0.5, classifier guidance schedule added sine factor \u03b3 t = 0.2,",
            "score": 0.410625380074945,
            "section_title": "Discussion",
            "char_start_offset": 24646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1591
                }
            ],
            "ref_mentions": [
                {
                    "start": 966,
                    "end": 985,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "267750410",
            "title": "Text2Data: Low-Resource Data Generation with Textual Control",
            "text": "2.1. Text-to-data diffusion-based generation Diffusion models, notably divided into classifierguided (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022;Yang et al., 2023;Ho et al., 2022;Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2023) for controllable molecule and motion generation, respectively. Furthermore, since natural languages are a prevalent medium for human to communicate with the world, the text-to-data generation paradigm has gained traction, with diffusion models being instrumental in generating high-quality data aligned with textual inputs. The extensive applications encompass text-to-image generation (Ruiz et al., 2023;Zhang & Agrawala, 2023), text-to-speech generation (Huang et al., 2022;Kim et al., 2022), text-to-shape generation (Li et al., 2023;Lin et al., 2023), and more, leveraging the abundant text descriptions for training potent generative models. Despite advancements in generating data from text across various modalities, many other modalities may not satisfy the stringent requirements for sufficient data-text pairs essential for attaining optimal controllability during the training of models.",
            "score": 0.41045944639441756,
            "section_title": "Related works",
            "char_start_offset": 5191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 126,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 307,
                    "end": 327,
                    "matchedPaperCorpusId": "248965384"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 926,
                    "end": 946,
                    "matchedPaperCorpusId": "252595883"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1403,
                    "end": 1423,
                    "matchedPaperCorpusId": "250492984"
                },
                {
                    "start": 1423,
                    "end": 1439,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 1467,
                    "end": 1484,
                    "matchedPaperCorpusId": "254366593"
                },
                {
                    "start": 1484,
                    "end": 1501,
                    "matchedPaperCorpusId": "253708074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "276647345",
            "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model",
            "text": "where \u03f5 t \u223c N (0, I). Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9.",
            "score": 0.41045944639441756,
            "section_title": "1) Forward Diffusion Process:",
            "char_start_offset": 22391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 799
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "276107584",
            "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
            "text": "Classifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks [Peng et al. 2024;Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling.",
            "score": 0.41045944639441756,
            "section_title": "E CLASSIFIER FREE GUIDANCE FOR EACH TASK",
            "char_start_offset": 35315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 327
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 112,
                    "matchedPaperCorpusId": "274023942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "score": 0.410370729783105,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.020172119140625
        },
        {
            "corpus_id": "276574988",
            "title": "Generative Models in Decision Making: A Survey",
            "text": "The first equality is the flow-matching constraint, the second is the reward constraint. Assuming the reward R is nonnegative and not identically zero, those two constraints ensures that the Markov chains reaches s f at some finite time \u03c4 and that the last non-terminal position p \u03c4 \u22121 of the Markov chain follows the distribution obtained by normalizing R i.e. P(p \u03c4 \u22121 = s) = R(s\u2192s f ) \u2211 s \u2032 \u2208S R(s \u2032 \u2192s f ) . The framework has been extended beyond graphs and acyclicity [92], [93], [94], with variation on losses and regularizations [93], [95], [96]. Autoregressive Models (AMs). Autoregressive models are widely used in natural language processing and generative modeling. They are central to sequence-to-sequence models, like the Transformer [97], which generate output sequences based on input or context. These models predict each output element sequentially, conditioned on the preceding ones, as shown in (17): \n\nAt each time step t, the model predicts the next element y t based on the context provided by the preceding elements. \n\nA popular method for autoregressive generation is using autoregressive language models, such as GPT [98], which have proven successful in NLP tasks like text generation, machine translation, and dialogue systems. \n\nIn AMs, the input at each step combines the previous elements and a context vector, typically obtained from an encoder that processes the input sequence and generates a fixed-size representation for the decoder. \n\nDuring training, AMs are optimized using Maximum Likelihood Estimation (MLE) [99], aiming to maximize the likelihood of generating the target sequence given the input or context. This involves calculating the probability of each target element based on preceding elements and updating model parameters to improve predictions [100]. \n\nOverall, AMs are a powerful tool for sequence generation tasks, particularly in NLP [101], significantly advancing generative modeling and remaining an active research area.",
            "score": 0.41018930353395183,
            "section_title": "Generative Models",
            "char_start_offset": 18181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "266551610"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "266551610"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1796,
                    "end": 1801,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0079345703125
        },
        {
            "corpus_id": "273811174",
            "title": "\u03c00: A Vision-Language-Action Flow Model for General Robot Control",
            "text": "Our work builds on recently proposed methods in largescale robot learning, as well as multimodal language models. Our work is most closely related to recently proposed visionlanguage action (VLA) models, which use pre-trained VLMs that are fine-tuned for robot control [7,24,55]. Such models employ autoregressive discretization to represent actions in a manner analogous to text tokens. In contrast, our model employs a novel design that fine-tunes a VLM to produce actions via flow matching [32,28], a variant of diffusion [20,46]. This allows us to handle high-frequency action chunks [57] (up to 50 Hz) and highly dexterous tasks, which we show pose a major challenge for prior autoregressive VLAs [7]. This resembles a number of recent works on diffusion models for action generation [9,60]. In contrast to these works, our model uses a pre-trained VLM backbone [5]. Our contribution is also fundamentally integrative, focusing on a framework for robot foundation models, including not only the model architecture itself but also a pre-training recipe, pre-training and posttraining phases, and a range of real-world experiments. \n\nOutside of robot control, many models have been proposed that combine pre-trained language models with diffusion [40,41,14], including models that specifically hybridize diffusion and autoregressive large language models [19,29,59]. Such models are typically concerned with image generation, but our action generation model builds on a number of previously proposed concepts. Like Zhou et al. [59], we train our model via a diffusion-style (flow matching) loss applied on individual sequence elements, in lieu of the standard cross-entropy loss for decoder-only transformers. Like Liu et al. [29], we use a separate set of weights for the tokens corresponding to diffusion. Incorporating these concepts into a VLA model, we introduce what to our knowledge is the first flow matching VLA that produces high-frequency action chunks for dexterous control. \n\nOur work also builds on a rich history of prior works on large-scale robot learning.",
            "score": 0.40977723473184585,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 9393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "257378658"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0212860107421875
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Diffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Ho et al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al., 2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored in numerous works (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022), and is achieved in its simplest form by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence of the condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearly combine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively, Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditional models, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. \n\nIn both cases, a weighting parameter \u03c9 controls the importance of the generative and guidance terms and is directly applied at all timesteps. Varying \u03c9 is a trade-off between fidelity and condition reliance, as an increase in condition reliance often results in a decline in both fidelity and diversity. In some recent literature, the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023) observed that a linearly increasing guidance weight could enhance performance and potentially increase diversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmann et al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterized cosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despite the recent appearance of this topic in the literature, none of the referenced studies has conducted any empirical experiments or analyses to substantiate the use of a guidance weight scheduler.",
            "score": 0.4087311276359673,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 117,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "257532642"
                },
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "253581601"
                },
                {
                    "start": 199,
                    "end": 218,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 335,
                    "end": 357,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 788,
                    "end": 809,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1660,
                    "end": 1677,
                    "matchedPaperCorpusId": "257767316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "However, RDMs achieve a running speed approximately 10 times faster than a comparable-sized autoregressive baseline like GPT2, owing to the implementation of various optimization techniques.\n\nIn summary, diffusion models generally exhibit higher time complexity because they require multiple iterations to recover text from noise.In contrast, PLMs have lower time complexity as they only need a single forward pass to predict the next word from the context.The choice of an appropriate model depends on specific application scenarios and requirements.For instance, in diffusion models, the generation process typically involves parallel generation of the entire sequence, while autoregressive models must sequentially generate elements at each position.Therefore, when generating long sentences, diffusion models might be more efficient.",
            "score": 0.40861039373900276,
            "section_title": "Time complexity",
            "char_start_offset": 33389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 192,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012725830078125
        },
        {
            "corpus_id": "272987204",
            "title": "Improving Academic Skills Assessment with NLP and Ensemble Learning",
            "text": "Recent advancements in natural language processing (NLP) have significantly improved text analysis and understanding, providing new opportunities for educational assessments. Devlin et al. [1] introduced BERT, which transformed the landscape of language representation models through bidirectional training of Transformer encoders. BERT's ability to capture context from both directions in a text significantly improved performance on various NLP tasks, including text classification and language inference. Building on this, Liu et al. [2] developed RoBERTa, optimizing the training process by using more data and larger batches, leading to further improvements in model performance. Similarly, Lewis et al. [3] introduced BART, which combines bidirectional and autoregressive Transformers, enhancing the model's ability to generate and comprehend text. \n\nWhile these models achieved state-of-the-art results in many benchmarks, their application to educational assessments, particularly for predicting multiple linguistic indicators simultaneously, remained underexplored. Sun et al. [4] demonstrated the potential of fine-tuning BERT for specific tasks, highlighting its adaptability. Raffel et al. [5] extended this further with the introduction of T5, a text-to-text transfer learning framework, showcasing the versatility of transfer learning in handling various NLP tasks. \n\nEnsemble learning approaches, such as those discussed by Sagi and Rokach [6] , have shown promise in combining the strengths of individual models to improve overall performance. Techniques like stacking and blending have been effective in various domains. Krawczyk et al. [7] provided a comprehensive survey on ensemble learning for data stream analysis, emphasizing its robustness and adaptability. \n\nYang et al. [8] introduced XLNet, which further advanced the capabilities of autoregressive pretraining for language understanding, showcasing improvements over BERT in several benchmarks. Howard and Ruder [9] proposed universal language model fine-tuning for text classification, demonstrating significant performance gains in various text classification tasks. Qiu et al. [10] provided a comprehensive survey on pretrained models for NLP, emphasizing their impact on various downstream tasks. \n\nBrown et al. [11] introduced a groundbreaking model that showcased the ability of language models to perform well with few-shot learning, further emphasizing the potential of pretrained models in NLP.",
            "score": 0.408215740406625,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2278
                },
                {
                    "start": 2281,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1455,
                    "end": 1458,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 1654,
                    "end": 1657,
                    "matchedPaperCorpusId": "1372281"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 2158,
                    "end": 2162,
                    "matchedPaperCorpusId": "212747830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0272674560546875
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "equivalent conditioning modalities for adaptation (Figure 1). What we show is that popular techniques for the image domain, such as CLIP-based guidance (Gal et al., 2021;Patashnik et al., 2021), denoising diffusion models (Ho et al., 2020), and classifier-free guidance (Dhariwal & Nichol, 2021;Ho & Salimans, 2021;Nichol et al., 2022) can be repurposed for the meta-learning setting to generate adapted neural network weights. Using multi-step adaptation instead of traditionally conditioning the model on the natural language task information allows our models to achieve higher performance on each task by breaking down computations into multiple steps. \n\nWe approach the generation of neural network weights in two separate phases. In the unconditional pre-training phase, we train a generative hypernetwork (Ha et al., 2016;Sch\u00fcrholt et al., 2022) to map from its latent space to the weight space of a base model (Figure 2.A). In the guidance phase, we learn language-conditioned models that can be used to traverse the hypernetwork latent space and find zeroshot adapted weights with high performance on our task (Figure 2.B and 2.C). Our methods can thus benefit from large scale data through the pre-training phase, even when natural language descriptions are not available for all tasks. \n\nWe summarise our contributions as follows: \n\n1) We introduce HyperCLIP, a contrastive learning method equivalent to Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021), producing CLIP embeddings of fine-tuned neural network weights. Using HyperCLIP as a guidance model then allows us to find task-adapted networks in the latent space of a hypernetwork model (Figure 2.B). \n\n2) We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifierfree guidance (Ho & Salimans, 2021) improves the performance of generated base networks.",
            "score": 0.4081522265376403,
            "section_title": "Introduction",
            "char_start_offset": 4086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 239,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 270,
                    "end": 295,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "237375932",
            "title": "STAMP 4 NLP - An Agile Framework for Rapid Quality-Driven NLP Applications Development",
            "text": "The field of artificial intelligence in general and natural language processing as one of its sub-fields offers tremendous novel business opportunities in a steadily growing market [13]. Recent progress in NLP research shows the potential for business applications, leading to a demand for more advanced NLP applications [7]. \n\nThe state-of-the-art in NLP differs from research to industrial domains. Besides the progress in research, the application of ML-based NLP in many enterprises is severely limited [6]. The black-box behavior of ML models, missing know-how, complex technological landscape, and the decision on an appropriate tool stack discourage enterprises from implementing NLP approaches [1,10]. They discard promising projects due to the combination of high and uncertain effort estimation [23]. \n\nMany ML projects fail because of exceeding budgets, deadlines or they do not meet the business requirements [11]. The late integration of several projects can lead to a services shutdown [27,17]. We minimize the risk of these situations with agile methodology to handle the uncertainties and generate business value and feedback on the application as early as possible. This increases the quality and trust in the software for all involved stakeholders [5]. \n\nWe propose a new process model adjusted for developing NLP applications: Standardized Modeling Process for Natural Language Processing (STAMP 4 NLP). \n\nWith STAMP 4 NLP, we merge software engineering principles with best practices from data science to improve and accelerate the development cycle and integrate prototypes with every iteration into a test or production environment. STAMP 4 NLP provides a transparent development process, including roles, tasks, artifacts, and best practices. \n\nOur main contributions3 are: \n\n-A novel process model for developing NLP applications, with formally specified roles, activities, and artifacts focusing on quality, and early business value. -Usage of predefined environment and software templates based on prior experiences for accelerating the development start.",
            "score": 0.40807020751628637,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2081
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 702,
                    "end": 705,
                    "matchedPaperCorpusId": "52965836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00989532470703125
        },
        {
            "corpus_id": "272421662",
            "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
            "text": "After the release of GPT-2, another model known as GPT-3 [64] was introduced. This paper describes major advances in natural language processing (NLP) by scaling up language models to improve their few-shot learning capabilities. This study introduces GPT-3, an autoregressive language model with 175 billion parameters, which is ten times larger than that of any previous non-sparse model. Unlike typical models that require significant After the release of GPT-2, another model known as GPT-3 [64] was introduced. This paper describes major advances in natural language processing (NLP) by scaling up language models to improve their few-shot learning capabilities. This study introduces GPT-3, an autoregressive language model with 175 billion parameters, which is ten times larger than that of any previous non-sparse model. Unlike typical models that require significant task-specific fine-tuning datasets, the GPT-3 performs well across a wide range of NLP tasks, including translation, question answering, and cloze tasks, with no gradient updates or fine-tuning. It also excels in tasks that require on-the-fly reasoning and domain adaptability, such as unscrambling words and computing three-digit arithmetic tasks. GPT-3 showed a competitive performance on several benchmarks, proving its ability to handle new language tasks with few samples. It creates human-like text samples that are frequently indistinguishable from real articles, thereby demonstrating its potential for use in content generation and other applications. However, this model struggles with specific datasets and experiences difficulties when trained using large online corpora. This study highlights the broader social effects of implementing such powerful language models and their potential limitations. Despite these challenges, the findings indicate that very large language models, such as GPT-3, may be critical in developing adaptable, general-purpose language systems. This is a major advancement in NLP, demonstrating the ability of scaling models to improve performance across various tasks. \n\nAfter the release of GPT-1, researchers at Google introduced Bidirectional Encoder Representations from Transformers (BERT) [66], a new language representation model that significantly enhances NLP by pre-training deep bidirectional representations from unlabeled text.",
            "score": 0.40788533964923535,
            "section_title": "Devlin, J. [66]",
            "char_start_offset": 87867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162200927734375
        },
        {
            "corpus_id": "276961771",
            "title": "Constrained Discrete Diffusion",
            "text": "To empirically demonstrate the advantage provided by CDD, we compare our method against similarly sized autoregressive and diffusion-based language models. Specifically, we benchmark our performance on constrained text generation against autoregressive baselines: GPT-2 and Llama 3.2 [41,42], and discrete diffusion baselines MDLM and UDLM [16,17]. We intentionally select models of similar size to, and up to one magnitude larger than, our discrete diffusion model, noting that we expect the performance of both classes of models to scale similarly with size. As shown by Schiff et al. [17], MDLM outperforms UDLM on the tasks described in Sections 5.1 and 5.2, and therefore we adopt this stronger baseline for comparison in these sections and utilize both in Section 5.3. For this reason, we use MDLM as the base discrete diffusion model for Sections 5.1 and 5.2, and UDLM for Section 5.3. \n\nWe evaluate on three constrained language generation tasks:  1. Toxicity Mitigation: Given a prompt from RealToxicityPrompts [43], the model is tasked with completing the passage with the goal of providing non-toxic responses. 2. Instruction Following: We present lexical constraints in an instruction-tuning format and ask the model to generate outputs that adhere to these specifications. 3. Molecule Generation: Our language model, trained on the molecular grammar 'SMILES' [44], is tasked to generate novel molecule sequences or sequences which satisfy certain properties.",
            "score": 0.40781389567764126,
            "section_title": "Experiments",
            "char_start_offset": 15679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1372,
                    "end": 1376,
                    "matchedPaperCorpusId": "5445756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0120086669921875
        },
        {
            "corpus_id": "221139500",
            "title": "Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview",
            "text": "In both these cases the motivation is to train powerful base models using large amounts of training data, then to adapt these to specific tasks or domains, for which considerably less training data is available. \n\nIn computer vision, the base model is typically a large convolutional network trained to perform image classification or object recognition using the ImageNet database [40], [41]. The ImageNet model is then adapted to a lower resource task, such as computer-aided detection in medical imaging [42]. Kornblith et al [43] have investigated empirically how well ImageNet models transfer to different tasks and datasets. \n\nTransfer learning in NLP differs from computer vision, and from the speech recognition approaches discussed in this paper, in that the base model is trained in an unsupervised fashion to perform language modeling or a related task, typically using web-crawled text data. Base models used for NLP include the bidirectional LSTM [44] and Transformers Encoder Softmax which make use of self-attention [45], [46]. These models are then trained on specific NLP tasks, with supervised training data, which is specified in a common format (e.g. text-totext transfer [46]), often trained in a multi-task setting. Earlier adaptation approaches in NLP focused on feature adaptation (e.g. [47]), but more recently better results have been obtained using model-based adaptation, for instance \"adapter layers\" [46], [48], in which trainable transform layers are inserted into the pretrained base model. \n\nMore broadly there has been extensive work on domain adaptation and transfer learning in machine learning, reviewed by Kouw and Loog [49]. This includes work on few-shot learning [50]- [52] and normalizing flows [53], [54]. Normalizing flows which provide a probabilistic framework for feature transformations, were first developed for speech recognition as Gaussianization [55], and more recently have been applied to speech synthesis [56] and voice transformation [57]. \n\nFinally we provide a meta analysis of experimental studies using the main adaptation algorithms that we have discussed (Sec. XIII).",
            "score": 0.40751054355191707,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 9292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "2930547"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "3333267"
                },
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "53245704"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "67413369"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "211532237"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "12554042"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "208991859"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "53145796"
                },
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "173990774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02606201171875
        },
        {
            "corpus_id": "258686601",
            "title": "TESS: Text-to-Text Self-Conditioned Simplex Diffusion",
            "text": "Diffusion for continuous domains Diffusion models were first proposed by Sohl-Dickstein et al. (2015) and popularized by Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020), which proposed a new parameterization that revealed an equivalence between ground-truth prediction and noise estimation. Song et al. (2021) proposed an alternative stochastic differential equation interpretation of diffusion that involves the Stein score function. Nichol and Dhariwal (2021) proposed a number of modifications to DDPMs, which improved log-likelihood and reduced sampling steps. Ho and Salimans (2021) proposed classifier-free guidance, which allows for highly controllable generation without the need for an external classifier to guide the model score estimates. \n\nContinuous diffusion for discrete domains Following the success of diffusion models on continuous domains, there have been several attempts to apply diffusion on discrete data. Li et al. (2022a) applied diffusion on the latent token embedding space. Their resulting language model relies on word-level tokenization and works mostly on small datasets with a short sequence length of 64 tokens. Strudel et al. (2022) used frozen pretrained word embedding with careful scaling to address the in- stability resulting from the competition between diffusion and reconstruction loss. However, their method does not allow the joint training of word embeddings, and the model was not evaluated on downstream NLP tasks. More recently, Dieleman et al. ( 2022) attempted to learn the embedding and diffusion model jointly, still by performing diffusion in the embedding space. Other recent works have also applied diffusion on word embeddings to tackle sequence-to-sequence problems (Gong et al., 2023;Yuan et al., 2022). Concurrent to our work, Ye et al. (2023) proposed methods for manipulating the noise in the diffusion process during training and inference, yielding improved conditional text generation. Another concurrent work explores variational diffusion models for language modeling in embedding space (Gulrajani and Hashimoto, 2023). However, they compare their models to 8\u00d7 smaller autoregressive models; our method obtains competitive performance with same-size autoregressive models.",
            "score": 0.40738696676622577,
            "section_title": "Related Work",
            "char_start_offset": 18787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 101,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 310,
                    "end": 328,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 454,
                    "end": 480,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1743,
                    "end": 1762,
                    "matchedPaperCorpusId": "252917661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "277781166",
            "title": "Performance of Large Language Models in Supporting Medical Diagnosis and Treatment",
            "text": "While the models evaluated in this study predominantly use autoregressive architectures (generating text sequentially token by token), a newer generation of language models based on diffusion processes is emerging. Inspired by the success of diffusion models in image and video generation (e.g., DALL-E, Midjourney, Sora) [6], this approach, fundamentally different from sequential generation, is being adapted for text tasks [7]. \n\nCompanies like Inception Labs are pioneering commercial-scale diffusion Large Language Models (dLLMs), such as their announced Mercury model [8], claiming significant potential advantages over traditional autoregressive LLMs [9]. Proposed benefits often highlighted include: \n\n\u2022 Speed and Efficiency: Diffusion processes can allow for parallel generation steps, potentially offering substantially faster inference (with claims of 5-10x improvements) and reduced computational cost compared to the inherently sequential nature of autoregressive models. \n\n\u2022 Reasoning and Reliability: It is suggested that some diffusion frameworks might possess built-in mechanisms for iterative refinement or error correction during the generation process. This could potentially lead to improved reasoning capabilities and a reduction in common LLM issues like hallucinations, although this requires rigorous validation. \n\n\u2022 Output Control: The generative process in diffusion models may afford enhanced control over the structure, style, or specific constraints of the generated text, making them potentially well-suited for tasks requiring precise formats, such as function calling or structured data generation (e.g., filling medical templates). \n\n\u2022 Multimodality: The underlying mathematical framework of diffusion models might offer a more unified approach for generative AI across different data types, including text, images, audio, and video, potentially simplifying the development of truly multimodal systems. \n\nAlthough dLLMs were not included in our current evaluation due to their novelty and limited availability/benchmarking at the time of testing, they represent a potentially significant future direction in LLM development. Their distinct generative process could lead to different performance characteristics and trade-offs compared to current models. Further research and independent benchmarking, especially on complex medical reasoning and generation tasks, are required to validate these claims and understand their applicability and safety within clinical contexts.",
            "score": 0.407003885573224,
            "section_title": "Emerging Diffusion-Based Language Models",
            "char_start_offset": 6425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2506
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "252199918"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "267949411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0304412841796875
        },
        {
            "corpus_id": "277043967",
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "text": "By aggregating techniques from recent methods and running several experiments, I was able to train a state-of-the-art language diffusion model for the paraphrasing task on the QQP dataset. This section proposes and evaluates methods for manipulating the model's diversity/quality trade-off. \n\n4.1. Controlling Diversity/Quality Trade-off 4 discuss at length the limitations of comparing language models based on their quality alone. Due to inherent qualitydiversity trade-offs, comparisons must take into account model behaviour across a range of diversities rather than focusing on a single point. The quality-diversity trade-off of autoregressive models may naturally be controlled using sampling temperature, or more sophisticated methods such as stochastic beam search, while similar techniques exist for language GANs such as generator rejection sampling [4]. In this vein, I explore three methods for controlling the diversity/quality trade-off at inference time: 1. Stochastic Clamping. Rather than clamp the final predicted embeddings \u01770 to the closest tokens, a random selection is made from nearby tokens. \n\n2. Classifier-free Guidance. As observed elsewhere in the literature, a higher guidance strength should improve fidelity at the cost of reducing diversity [10]. 3. Combined Methods. I explore methods for combining clamping and classifier-free guidance.",
            "score": 0.4068685329362246,
            "section_title": "Experiments",
            "char_start_offset": 10545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "252917661",
            "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
            "text": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at \\url{https://github.com/Shark-NLP/DiffuSeq}",
            "score": 0.4068632377369526,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0227203369140625
        },
        {
            "corpus_id": "267069249",
            "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
            "text": "Typically, classifier guidance can be applied to the score-based models by utilizing a distinct classifier. Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n( \n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale.",
            "score": 0.4064586197702927,
            "section_title": "Preliminaries",
            "char_start_offset": 10339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 468
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "267740476",
            "title": "Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks",
            "text": "The Natural Language Processing (NLP) research space is now dominated by Large Language Models (LLMs), with a steady influx of different so-called foundation models from major AI companies every few months. The vast majority of recent LLMs are designed for generative tasks and chat-style interactions, reliant on a mixture of autoregressive LM pre-training with follow-up reinforcement learning from human feedback (RLHF) to create the likes of ChatGPT [1,2]. However, the performance of these generative LLMs on classic NLP tasks such as sequence classification, relation models. [16]. Pruning model parameters to reduce the redundant weights for given downstream tasks has also been effective in certain cases [13]. Another approach is to avoid full fine-tuning altogether, opting instead for zero-shot task adaption through prompting (prompt engineering), or by reducing the number of trainable parameters necessary for fine-tuning the LLM for its new task, a process known as Parameter Efficient Fine-tuning (PEFT). Notable PEFT methods include: Prompt tuning [17], Prefix tuning [18], Low Rank Adaptation (LoRA) [19], and Inhibit Activations (IA 3 ) [20]. These PEFT methods have become popular across various NLP tasks, and in this work, we will explore the utility of a select few for differently sized LLMs in the clinical domain.",
            "score": 0.4062133629145482,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0203399658203125
        },
        {
            "corpus_id": "263334000",
            "title": "Generative Design of inorganic compounds using deep diffusion language models",
            "text": "We trained two different diffusion-based language generation models, Diffusion-LM [19] and Diffusion-BERT [20], from scratch based on our formula sequence dataset and then compared the percentage of chemically valid compositions generated by these two models. These two models are different in both adding noise and denoising processes. In the adding noise process, Diffusion-BERT incorporates the initial state, denoted as x 0 , in its calculations when determining x T based on the current state xT \u2212 1. Conversely, in the denoising process, Diffusion-BERT computes the next state sample, x t\u22121 , by taking into account both the current state x t and the time step t associated with the current state. In contrast, the Diffusion-LM model derives x T directly from x T \u22121 by adding Gaussian noises and determines a more clear state x t\u22121 solely based on the current state x t . \n\nDiffusion-LM Most pre-trained large language models are autoregressive models that strictly adhere to the left-toright order. This constraint becomes especially evident in controllable generation settings such as infilling and syntactic control where global information plays a crucial role. In contrast, Diffusion-LM [19] is a non-autoregressive language model that could generate sentences regardless of the order. By iteratively denoises a sequence of Gaussian vectors into word vectors, Diffusion-LM yields a sequence of continuous intermediate latent variables. These continuous latent vectors are then updated by the gradient to make them satisfy the requirements of complex generation tasks. \n\nDiffusion-BERT Diffusion-BERT [20] leverages the capabilities of a pre-trained large language model BERT [13] as its foundational architecture. This utilization ensures the advantages associated with a well-initialized text generation process. Unlike Diffusion-LM, Diffusion-BERT is a discrete diffusion model. Diffusion-BERT introduced an innovative noise schedule for the forward add noise process, this spindle noise schedule controls the degree of noise added at each step based on the word frequency of each token. The more frequently a word shows up in the training dataset, the earlier it becomes a mask.",
            "score": 0.4061161441757855,
            "section_title": "Diffusion language models for material composition generation",
            "char_start_offset": 10968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083160400390625
        },
        {
            "corpus_id": "265221143",
            "title": "Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations",
            "text": "Controlled decoding Mudgal et al. [2023] shows improvements on dialog safety and response length. Diffusion-LM [Li et al., 2022] further extends the method to non-autoregressive language model based on continuous diffusions. All these works demonstrate how to train the classifiers and how to combine the classification scores to guide the decoding towards the preference of the classifiers. \n\nFor the classifier-free guidance, context-aware decoding (CAD) [Shi et al., 2023] shows improvements in summarization and knowledge conflicting tasks; PREADD [Pei et al., 2023] shows improvements in toxic output mitigation, gender bias reduction, and sentiment control. In particular, PREADD showed that the evidence can be an instruction, and you can adjust the guidance scale to positive (negative) value to follow (disobey) the instruction, respectively. \n\nWhile the dynamic adjustments in prediction shows great improvement in attribution, another line of work showed that there exists tradeoffs. In classical decoding algorithms, [Aksitov et al., 2023] showed that increasing sampling temperature promotes diversity while sacrificing the sensibleness and attributions. [Chang et al., 2023] showed the tradeoff curves between diversity and attributions (to the evidence) for the classical top-p, top-k, and temperatures sampling. They also proposed a new sampling algorithm to mitigate the tradeoffs. The discovery may be just a tip of the iceberg. Could other dynamic adjustment algorithms also face certain tradeoffs? \n\nTo cover different kinds of dynamical adjustment algorithms in the analysis, we first reformulate the decoding algorithms as a dynamic programming (DP) problem, similar to Kim et al. [2022], Mudgal et al. [2023], where you can incorporate sequence level preference as a future reward. The classical algorithms that don't care about sequence level preference degenerates the setup. With DP, we lift the decoding algorithm design into a policy optimization problem in the action-state value function space. Surprisingly, it turns out the action-state value function is composed of items with information theoretical interpretations.",
            "score": 0.405699431759686,
            "section_title": "Introduction",
            "char_start_offset": 3844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "214720168",
            "title": "Inductive transfer learning for molecular activity prediction: Next-Gen QSAR Models with MolPMoFiT",
            "text": "The MolPMoFiT method we proposed here is adapted from the ULMFiT (Universal Language Model Fine-Tuning) [45], a transfer learning method developed for any NLP classification tasks. The original implementation of ULMFiT breaks the training process into three stages: \n\n1. Train a general domain language model in the selfsupervised manner on a large corpus (e.g., Wikitext-103 [57]). Language models are a type of model that aim to predict the next word in the sentences given the context precede it. The input and labels of the dataset used to train a language model are provided by the text itself. After training on millions of unlabeled text, the language model captures the extensive and in-depth knowledge [58][59][60] of a language and can provide useful features for other NLP tasks. 2. Fine-tuning the general language model on the task corpus to create a task specific language model. 3. Fine-tuning the task specific language model for downstream classification/regression model. \n\nAs described above, the ULMFiT is a three-stage transfer learning process that includes two types of models: language models and classification/regression models. A language model is a model that takes in a sequence of words and predicts the most likely next word. A language model is trained in a self-supervised manner and no label is required. This means the training data can be generated from a huge amount of unlabeled text data. The classification/regression model is a model that takes a whole sequence and predicts the class/value associated to the sequence, requiring labeled data.",
            "score": 0.4054425264983694,
            "section_title": "ULMFiT",
            "char_start_offset": 7865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0244293212890625
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\n\nOne potential route towards addressing these issues is meta-learning 1 -which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC + 19] attempts to do this via what we call \"in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\n\nWhile it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -for example [RWC + 19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.\n\nAnother recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters [DCLT18], to 1.5 billion parameters [RWC + 19], to 8 billion parameters [SPP + 19], 11 billion parameters [RSR + 19], and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or",
            "score": 0.40530240070131995,
            "section_title": "body",
            "char_start_offset": 2024,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014007568359375
        },
        {
            "corpus_id": "254044531",
            "title": "Continuous diffusion for categorical data",
            "text": "Several diffusion-based and diffusion-inspired approaches have been proposed for non-autoregressive iterative refinement of discrete data, and especially for language in particular (Austin et al., 2021;Chang et al., 2022;Ghazvininejad et al., 2019;Hoogeboom et al., 2021a,b;Reid et al., 2022;Savinov et al., 2021). \n\nReplacing continuous diffusion with a discrete corruption process affords some flexibility, but it also requires forgoing several capabilities associated with the continuous paradigm, such as efficient sampling algorithms based on advanced ODE solvers, or classifier-free guidance. \n\nMore recently, several papers have proposed approaches to apply the continuous diffusion framework to discrete data. It is important to distinguish continuity in the input space from continuity of the time variable of the corruption process; for CDCD, both are continuous. Li et al. (2022), Strudel et al. (2022) and Han et al. (2022) all target language modelling as the primary application and use an embedding-based strategy in combination with discrete-time diffusion. Campbell et al. (2022) and Sun et al. (2022) propose continuous-time models for discrete input, though they do not explore the application to language modelling. Meng et al. (2022) propose concrete score matching, which can be applied to both discrete and continuous inputs. Chen et al. (2022) 2022) also suggested using the cross-entropy loss (in combination with other loss terms).",
            "score": 0.4050046670393793,
            "section_title": "Discrete diffusion",
            "char_start_offset": 31606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1457
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 221,
                    "end": 248,
                    "matchedPaperCorpusId": "202538740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09136962890625
        },
        {
            "corpus_id": "267296436",
            "title": "Prediction model of Wordle report based on GA-BP neural network",
            "text": "Natural Language Processing (NLP) plays a vital role in artificial intelligence, enabling machines to understand and generate human language. This paper proposes a novel approach to improve NLP tasks by integrating a Genetic Algorithm-based BP neural network model. The model combines the strengths of genetic algorithms and neural networks. Genetic algorithms offer effective search capabilities, while the BP neural network provides a flexible learning framework. By integrating these techniques, the proposed model aims to overcome limitations of traditional NLP models. It effectively handles the complexity of natural language data, provides efficient training by avoiding local optima, and exhibits enhanced generalization capabilities for unseen data. Extensive experiments on benchmark NLP datasets validate the effectiveness of the proposed model. Results demonstrate its superiority over state-of-the-art NLP models in terms of accuracy, efficiency, and robustness. This paper presents a novel approach to enhance NLP tasks by integrating a genetic algorithm-based BP neural network model. The model shows promising results in various NLP applications and offers advantages over traditional approaches. This research contributes to the advancement of NLP techniques, facilitating more accurate and efficient language processing systems.",
            "score": 0.4049565273377137,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0165252685546875
        },
        {
            "corpus_id": "276408237",
            "title": "Continuous Diffusion Model for Language Modeling",
            "text": "Discrete diffusion models (Austin et al., 2021;Lou et al., 2024) emerged as a promising competitor to autoregressive models for the generative modeling of discrete data. These models have demonstrated competitive performance on tasks such as language modeling (Shi et al., 2024;Sahoo et al., 2024) and code generation (Gat et al., 2024). Unlike Several efforts were made to adapt continuous diffusion models for discrete data, but their performance falls short of discrete diffusion models, demonstrating a significant gap compared to autoregressive models. Past works applied diffusion models for images to discrete data through continuous relaxation without constraint (Han et al., 2022;Li et al., 2022). Other lines of works (Avdeyev et al., 2023;St\u00e4rk et al., 2024) map discrete data to the probability simplex which exerts a strong prior assumption on Dirichlet distribution, but often fails to model complex patterns. This led to recent works (Cheng et al., 2024;Davis et al., 2024) that apply flow matching to learn the categorical distributions using the structure of the statistical manifold, but these methods are limited to small sequences and categories. In particular, the link between discrete and continuous diffusion remains unclear, hindering the development of a coherent diffusion framework for discrete data. \n\nIn this work, we present Riemannian Diffusion Language Model (RDLM), a continuous diffusion framework for language modeling that incorporates the geometry of the statistical manifold in the diffusion processes. We establish a connection between continuous flow on the statistical manifold and the discrete diffusion process, showing that the trajectory of the transition distribution can be modeled to a conditional flow on the manifold. Based on the analogy, we introduce a simple design of the diffusion processes on the manifold that generalizes previous discrete diffusion models. We further present a simulation-free training scheme using radial symmetry that integrates simple parameterization and maximum likelihood-based training objectives. Through experiments on language modeling tasks, image modeling, and biological sequence design, we validate that our framework outperforms existing discrete diffusion models.",
            "score": 0.4047039463108276,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 47,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 260,
                    "end": 278,
                    "matchedPaperCorpusId": "270286107"
                },
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "270380319"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "271328803"
                },
                {
                    "start": 689,
                    "end": 705,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 728,
                    "end": 750,
                    "matchedPaperCorpusId": "258762371"
                },
                {
                    "start": 750,
                    "end": 769,
                    "matchedPaperCorpusId": "267547708"
                },
                {
                    "start": 949,
                    "end": 969,
                    "matchedPaperCorpusId": "270062958"
                },
                {
                    "start": 969,
                    "end": 988,
                    "matchedPaperCorpusId": "269982972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007205963134765625
        },
        {
            "corpus_id": "270062517",
            "title": "Glauber Generative Model: Discrete Diffusion Models via Binary Classification",
            "text": "Recent works have explored diffusion models for discrete data. Approaches for diffusion on discrete data can be broadly classified into two kinds: (i) discrete diffusion in the token space where a discrete time Markov chain is the denoiser, (ii) continuous diffusion in an embedding space of the discrete data. We briefly discuss these approaches and their applications in this section. \n\nDiscrete diffusion via Discrete Markov Chains: Diffusion models over discrete spaces, analogous to continuous space diffusion, were introduced in (Sohl- Dickstein et al., 2015) for generative modeling. Argmax flows (Hoogeboom et al., 2021) and D3PM (Austin et al., 2021) (and refinements such as (Zheng et al., 2023)) solidified these ideas -by considering the noising process to be a discrete time, discrete space Markov chain. They propose a variational loss function to teach a neural network to reverse this Markov chain. More specifically, D3PM considers character-level language modeling, token-level language modeling on short sequences (128) with relatively small vocabularies (8192), and low-resolution (e.g., 32 \u00d7 32) image generation tasks with small models (e.g., 36M)). On language modeling tasks these models still perform worse than an autoregressive transformer-based model with comparable parameters. Scaling these models to larger vocabularies is an active area of research. MDLM (Sahoo et al., 2024) simplifies and improves upon D3PM by making different engineering choices and considering a simplified training objective for a specific noising process. Using image tokenizers like VQGANs (Esser et al., 2021), VQ-DDM (Hu et al., 2021) and VQ-Diffusion (Gu et al., 2021) are able to apply models similar to Argmax flows and D3PM to generate images with larger resolutions. Within this framework, DiffusER (Reid et al., 2022) considers the noising Markov Chain to be Levenshtein edit operations over text for language modeling and evaluates its performance for downstream tasks such as translation and textual style transfer.",
            "score": 0.4042394557191853,
            "section_title": "RELATED WORK",
            "char_start_offset": 2443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 542,
                    "end": 565,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 604,
                    "end": 628,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 1387,
                    "end": 1407,
                    "matchedPaperCorpusId": "270380319"
                },
                {
                    "start": 1626,
                    "end": 1643,
                    "matchedPaperCorpusId": "244896176"
                },
                {
                    "start": 1661,
                    "end": 1678,
                    "matchedPaperCorpusId": "244714856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00844573974609375
        },
        {
            "corpus_id": "269605094",
            "title": "Unraveling the Dominance of Large Language Models Over Transformer Models for Bangla Natural Language Inference: A Comprehensive Study",
            "text": "During human communication, a lot of information is conveyed.Typically, the receiver learns far more than what the speaker says.This is due to humans' innate inference ability or broad world knowledge.Understanding the relationship between premise and hypothesis is important in NLI.Discordances between them might describe the implication relationship as contradictory [1].\n\nRecent advances in LLMs [2] [3] have ushered in a new era of NLP, bringing unparalleled possibilities.These LLMs, with parameter sizes reaching a hundred billion, use contextual learning to thrive in few-shot and zero-shot learning scenarios, removing the need for task-specific fine-tuning.This innovation not only eliminates dependency on labeled datasets, but also allows the model to perform magnificently on a wide range of tasks for which it was not expressly trained.Despite their impressive capabilities, LLMs are prone to producing erroneous data, necessitating the use of techniques such as Reinforcement Learning from Human Feedback (RLHF) [4] to assure the development of dependable responses.\n\nAutoregressive language models [5], epitomized by the Generative Pre-trained Transformer series like GPT-3, have revolutionized NLP approaches.Operating as a decoder-only transformer, GPT-3 excels at generating text sequentially, leveraging attention mechanisms to focus on pertinent segments of input text.This model demonstrates exceptional versatility, adeptly tackling various language tasks without task-specific fine-tuning and excelling in zero-shot and few-shot learning scenarios.GPT-3's capability enables it to respond effectively to text-based prompts, even with limited examples or instructions, facilitating a more interactive and dynamic text generation process.\n\nChatGPT [6] and another recently proposed LLM, like Gemini 1.5 Pro [7], have garnered significant attention for their utilization of the RLHF technique to overcome various limitations observed in earlier models.While these models are trained across multiple languages, English remains dominant in their training datasets.Despite demonstrating robust zero-shot performance across various NLP tasks in English and other languages, their application in the Bengali language domain, which is widely spoken yet resource-constrained, remains largely unexplored.",
            "score": 0.4039721979204708,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 61,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 374
                },
                {
                    "start": 376,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1572
                },
                {
                    "start": 1572,
                    "end": 1760
                },
                {
                    "start": 1762,
                    "end": 1973
                },
                {
                    "start": 1973,
                    "end": 2083
                },
                {
                    "start": 2083,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "257006599"
                },
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "259376637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0197601318359375
        },
        {
            "corpus_id": "255595489",
            "title": "The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference",
            "text": "Black-box neural network models are important building blocks in state-of-the-art NLP pipelines as depicted in Figure 2. Classical pipeline approaches still have their place in small-data scenarios, which are common in interaction systems, e.g. when interactively selecting document subsets, NLP techniques such as NER, POS, LDA, can provide ondemand descriptive statistics, which in turn can be visualized to characterize the selected subset, in comparison to the training corpus. Furthermore, they can be used to combine several neural systems. Another advantage is that the explicit inputs and outputs of individual models may help to interpret the combined system. We therefore include them in our considerations. \n\nA lot of research interest is currently directed toward large language models (LLM) and how they can be utilized to solve common NLP tasks. In contrast to end-to-end models that are typically trained on specific tasks, these LLMs are trained in a self-supervised or semi-supervised fashion on very large training data and use many trainable parameters. In mid-2022, the largest language model reported has 540 billion parameters (Chowdhery et al., 2022). One of the intriguing aspects of LLMs is that they seem to capture factual knowledge to some extent (Petroni et al., 2019), which makes them very powerful. \n\nTwo main methods have been used recently to apply LLMs for specific NLP tasks: fine-tuning   task-specific models using text encodings generated by LLMs (Devlin et al., 2018) or employing LLMs as few shot learners (Brown et al., 2020) and adapting them to different NLP tasks using a few examples or prompts. \n\nLMs have been shown to have superior performance on many complex tasks. At the same time, they come with important limitations: (1) LLMs require large amounts of data and computing power and are often created in such a way that the training process and the training data are not openly accessible. \n\n(2) The training data often consist of large volumes of texts published online, which can reflect harmful views and biases, which are then propagated to downstream applications. (3) The extremely large size of the models, together with the size of the training data render XNLP a very challenging problem.",
            "score": 0.403266740955411,
            "section_title": "NLP Models and Interpretability",
            "char_start_offset": 3203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 1275,
                    "end": 1297,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1547,
                    "end": 1567,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0141754150390625
        },
        {
            "corpus_id": "230799344",
            "title": "Ask2Transformers: Zero-Shot Domain labelling with Pretrained Language Models",
            "text": "The whole Natural Language Processing (NLP) research area have been accelerated with the advent of the unsupervised pre-trained Language Models. First with ELMo (Peters et al., 2018) and then with BERT (Devlin et al., 2019) the paradigm of using pre-trained Language Models for finetuning on a particular NLP task has became the new standard approach, replacing the more traditional knowledge-based and fully supervised approaches. Currently, as the size of the corpus and models increase, the research community has observed that the Transfer Learning approach has the capacity to work without any or with a very small fine-tuning. Some examples of the strength of this approach are GPT-2 (Radford et al., 2019) or more recently GPT-3 (Brown et al., 2020) that shows the ability of these huge pre-trained Language Models to solve tasks for which have not even trained. \n\nRecently, with the arrival of the GPT-3 new ways to perform zero and few shot approaches have been discovered. These approaches propose the inclusion of a small number of supervised examples in the input as a hint for the model. The model then, just by looking a small set of examples, is able to complete successfully the task at hand. Brown et al. (2020) report that they solve a wide range of NLP tasks just following the previous approach. However, this approach only looks appropriate when the model is large enough. \n\nIn this paper we exploit the domain knowledge already encoded within the existing pretrained Language Models to enrich the WordNet (Miller, 1998) synsets and glosses with domain labels. We explore and evaluate different pretrained Language Models and pattern objectives. For instance, consider the example shown in Table 1. Given a WordNet definition such as the one of <hospital, infirmary> and the knowledge encoded in a pre-trained Language Model, the task is to assess which is its most suitable domain label. Thus, we create an appropriate pattern in natural language adapted to the objective of the Language Model. In the example, we use a Language Model fine-tuned on a general task such as Natural Language Inference (NLI) (Bowman et al., 2015).",
            "score": 0.40265639459255065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 182,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 202,
                    "end": 223,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 690,
                    "end": 712,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.024566650390625
        },
        {
            "corpus_id": "273482143",
            "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
            "text": "In recent years, autoregressive language models (LMs; Bengio et al. 2000) have dominated the landscape of natural language processing and artificial intelligence. Empowered by scaling laws (Kaplan et al., 2020), these models have demonstrated impressive performance across various applications (OpenAI, 2022;Achiam et al., 2023;Anthropic, 2023;Team et al., 2023, inter alia). However, this apparent success masks significant limitations that are becoming increasingly evident. Autoregressive models inherently struggle with tasks requiring complex reasoning, long-term planning, and maintaining global coherence (Bubeck et al., 2023;Valmeekam et al., 2023;2024;Dziri et al., 2024;Kambhampati et al., 2024). These shortcomings represent substantial challenges in developing AI systems capable of robust problem-solving and adaptable cognition (Wu et al., 2022;Zhao et al., 2023;Trinh et al., 2024;Yao et al., 2023;Shinn et al., 2024, inter alia). While autoregressive approaches have driven considerable progress, their limitations suggest that they may not be the optimal solution for all aspects of machine intelligence. As the field evolves, it becomes increasingly important to explore alternative paradigms that can address these inherent drawbacks and potentially offer new avenues for advancement in AI capabilities. \n\nIn response to these limitations, recent research has focused on addressing the inherent constraints of autoregressive models. Various strategies have been explored, including the integration of search algorithms at inference (Yao et al., 2024;Besta et al., 2024) and the incorporation of backtracking supervision during training (Lehnert et al., 2024;Gandhi et al., 2024). However, these approaches are not without their own drawbacks: the former often incurs significant computational costs, while the latter frequently results in verbose inputs and suboptimal performance. \n\nTo address this challenge, we argue for a fundamentally different modeling approach: discrete diffusion models.",
            "score": 0.4024982055363106,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 73,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 633,
                    "end": 656,
                    "matchedPaperCorpusId": "260440590"
                },
                {
                    "start": 661,
                    "end": 680,
                    "matchedPaperCorpusId": "258967391"
                },
                {
                    "start": 842,
                    "end": 859,
                    "matchedPaperCorpusId": "249063032"
                },
                {
                    "start": 877,
                    "end": 896,
                    "matchedPaperCorpusId": "267032902"
                },
                {
                    "start": 896,
                    "end": 913,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1569,
                    "end": 1588,
                    "matchedPaperCorpusId": "261030303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01078033447265625
        },
        {
            "corpus_id": "269050904",
            "title": "TEXT GENERATION MODELS FOR PARAPHRASE ON KAZAKH LANGUAGE",
            "text": "Traditionally, machine learning frameworks for natural language tasks have been geared towards languages with extensive resources, such as English. Nonetheless, the necessity to tailor these sophisticated frameworks to languages with fewer resources is becoming increasingly crucial to broaden the reach of NLP technologies. While there have been initiatives to modify generative pre-trained models for languages with limited resources, the research on tailoring these models for the Kazakh language, specifically for text generation, remains scant. The application of a unified model to evaluate machine translation quality across numerous language pairs has showcased the scalability of this approach [1]. Still, its practical effectiveness and flexibility for adapting to new linguistic contexts, especially for a language with as few resources as Kazakh, are aspects that continue to pose significant inquiries. \n\nThe given study applies diffusion models and transformers to the task of text generation and paraphrasing in Kazakh, contrasting with another research [2] that uses ontologies and parsing trees for sentence classification. Both are centered on enhancing NLP resources for the Kazakh language. However, the distinction lies in the approaches: the given research is geared towards generating new text, while the comparative study focuses on analyzing and understanding existing sentences. This paper aims to bridge this gap by investigating the adaptation of two state-of-the-art generative models -Diffusion models and Transformers -for text generation in Kazakh. Diffusion models like Denoising Diffusion Probabilistic Models have shown promising results in high-quality and diverse text generation for English [2]. The study makes a valuable contribution to the field of natural language processing for the Kazakh and Turkish languages, offering tools for the determination of grammatical categories. Its strengths lie in the use of machine learning algorithms and extensive datasets, which are balanced by the complexities of language processing and potential limitations in the applicability of the algorithms [3]. Similarly, pretrained transformers fine-tuned on downstream tasks have dominated leaderboards across various NLP benchmarks [4]. Although there has been some work on paraphrasing datasets [5]. The research focuses on a fundamental aspect of samplebased machine translation: identifying the degree of similarity between sentences. This involves aligning an input sentence with a corresponding example from a database, selecting segments of this sentence, and subsequently adjusting or paraphrasing them to produce the intended translation [6].",
            "score": 0.40146118995637836,
            "section_title": "Introduction.",
            "char_start_offset": 4304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2465
                },
                {
                    "start": 2466,
                    "end": 2678
                }
            ],
            "ref_mentions": [
                {
                    "start": 1069,
                    "end": 1072,
                    "matchedPaperCorpusId": "222417833"
                },
                {
                    "start": 1729,
                    "end": 1732,
                    "matchedPaperCorpusId": "222417833"
                },
                {
                    "start": 2131,
                    "end": 2134,
                    "matchedPaperCorpusId": "243843104"
                },
                {
                    "start": 2260,
                    "end": 2263,
                    "matchedPaperCorpusId": "124568016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0202178955078125
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "Pre-trained language models The PLMs based on Transformers usually adopt an autoregressive approach (Manning & Schutze, 1999) (see Fig. 6), to generate sentences via a time series forecast technology.A trained language model samples a sequence of discrete words to predict the next possible word based on previous content.\n\nFormally, the model obtains the probability score of word x i by calculating the conditional probability P\u00f0x i jx 1 ; x 2 ; \u00c1 \u00c1 \u00c1 ; x i\u00c01 \u00de (see Eq. ( 5)).After concatenating x i behind the original sequence \u00f0x 0 ; \u00c1 \u00c1 \u00c1 ; x i\u00c01 \u00de to obtain the new representation \u00f0x 0 ; \u00c1 \u00c1 \u00c1 ; x i\u00c01 ; x i \u00de, the model uses the new representation to predict the probability score of next word x i\u00fe1 .In this way, the next word will continuously generate in a loop until , eos .or another constraint token is generated.\n\nDiffusion-based models The generation method of the diffusion model in NLP is different from the traditional autoregressive method.As can be seen from Fig. 7, its",
            "score": 0.40128330252285016,
            "section_title": "Comparison of generation methods",
            "char_start_offset": 24981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 322
                },
                {
                    "start": 324,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 827
                },
                {
                    "start": 829,
                    "end": 960
                },
                {
                    "start": 960,
                    "end": 991
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0060272216796875
        },
        {
            "corpus_id": "275470889",
            "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models",
            "text": "Our experiments demonstrate that generalizing beyond these methods to different choices of potentials, rewards, and samplers provide several improvements, such as higher reward samples. \n\nTDS [Wu et al., 2023b] uses twisted SMC (See Section 3 in Naesseth et al. [2019]) for conditional sampling by targeting p target (x 0 | y) \u221d p \u03b8 (x 0 )p( y | x 0 ). The proposal generator for TDS uses classifier-guidance, restricting guidance to continuous state diffusion models and differentiable reward functions. In contrast, FK STEERING enables guidance with reward functions beyond differentiable likelihoods and generalizes to discrete state-spaces, including for text diffusion models. \n\nMore recently, Li et al. [2024] propose SVDD, a derivative-free approach to guiding diffusion models for reward maximization, by targeting the distribution the limit of lim \u03bb\u2192\u221e \n\nIf there is a single sample x * 0 in the support of p \u03b8 that maximizes r, then this distribution collapses to a point mass on x * 0 . At each time step, SVDD samples k states x i t from a diffusion model and selects one state x i * t with the highest reward and makes k copies of that state, therefore reducing diversity of samples x i 0 . \n\nText Generation and SMC. The sampling procedure for traditional autoregressive language models poses a challenge for SMC approaches, since SMC typically requires estimating the reward of the full sequence given its prefix. Lew et al. [2023] address this challenge by limiting to rewards calculated within a fixed look-ahead window. In contrast, Zhao et al. [2024] learn intermediate twisting potentials to marginalize over the remaining elements of a particular partial sequence. In our work, we demonstrate that intermediate estimates from diffusion models, for attributes like toxicity, can be used even with off-the-shelf reward models by evaluating the reward models on intermediate denoised estimates (see fig. 5).",
            "score": 0.4007071826074552,
            "section_title": "Fine-tuning.",
            "char_start_offset": 11021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 210,
                    "matchedPaperCorpusId": "259309049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35791015625
        },
        {
            "corpus_id": "251771599",
            "title": "Application of Deep Learning in Generating Structured Radiology Reports: A Transformer-Based Technique",
            "text": "One of the main challenges of our research was the limitation of training data. To address this problem, we came up with the idea of using transfer learning. A transfer learning model typically consists of two stages: first, unsupervised training on a general-purpose corpus and then second, supervised training for a specific task known as the downstream task. In the first stage, the transformer-based model is optimized with a large amount of unannotated text data Fig. 2 An example of the information schema designed as a hierarchical diagram summarizing the information that can be reported for the gallbladder organ in sonography reports including clinical statements or facts and modifiers such as location or quantity through language modeling techniques that are independent of specific downstream NLP tasks. In the second stage, the pre-trained transformer-based model is fine-tuned for a particular NLP task using a supervised approach.\n\nWe leverage pre-trained language models such as BERT in NLP tasks. BERT is not a unified transfer learning method because BERT-style models can only produce a single prediction for a given input. These models are simply not designed for text generation tasks such as question-answering or summarization. The text-to-text transfer transformer (T5) model overcomes this limitation by outputting a string of text for each input, allowing for question-answering, summarization, and other tasks where a single output is generally insufficient. In our work, we use the T5 model and Scifive, a pre-trained domain-specific adaptation of the T5 model for tasks related to biomedical literature, which is very important for the learning process.\n\n\u2022 T5: The text-to-text transfer transformer (T5) trained on the \"Colossal Clean Crawled Corpus\" (C4) uses a basic encoder-decoder transformer architecture as originally proposed by Vaswani et al. [13]. Each encoder block consists of a self-attention layer and a feedforward neural network. Each decoder block consists of a self-attention layer, an encoder-decoder attention layer, and a feedforward neural network. T5 is pre-trained on masked language modeling with a learning objective called \"span masking,\" where spans of text are replaced with a mask token randomly",
            "score": 0.4006582909838623,
            "section_title": "Transformer Models",
            "char_start_offset": 16130,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012969970703125
        },
        {
            "corpus_id": "219401459",
            "title": "MLE-guided parameter search for task loss minimization in neural sequence modeling",
            "text": "Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empirically performs well as a surrogate objective. Typical approaches to directly optimizing the task loss such as policy gradient and minimum risk training are based around sampling in the sequence space to obtain candidate update directions that are scored based on the loss of a single sequence. In this paper, we develop an alternative method based on random search in the parameter space that leverages access to the maximum likelihood gradient. We propose maximum likelihood guided parameter search (MGS), which samples from a distribution over update directions that is a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its improvement in the task loss. MGS shifts sampling to the parameter space, and scores candidates using losses that are pooled from multiple sequences. Our experiments show that MGS is capable of optimizing sequence-level losses, with substantial reductions in repetition and non-termination in sequence completion, and similar improvements to those of minimum risk training in machine translation.",
            "score": 0.40062500517933664,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0310211181640625
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "Prompt engineering and interactions. Based on the autoregressive property of LLMs, users aim to control the generation results by modifying the input contexts. This largely determines the test-time interactions with LLMs, primarily executed through prompt engineering. Representative methods such as CoT [17] introduce demonstrations in the context to enhance reasoning ability. Other multi-branch designs like ToT and GoT [16,18,19,33,34] have been proposed for rich and reliable context generation and self-checking. Aside from prompt engineering, human-model interactions have not been extensively explored in VLMs. Methods like Kosmos-2 [31], LLaVAInteractive [35], LISA [36], and AlphaCLIP [37] enable grounding perception tasks such as detection, segmentation, caption, and image editing through interaction with LLMs. These task-oriented interactions require additional data collection and task-specific tuning. In contrast, Prompt Highlighter is plug-and-play for general text generation in pretrained models. Classifier-free guidance and controllable generation. Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation. Besides, methods in Diffusion Models [38,39] achieve finegrained control over image generation using text prompts by emphasizing areas within cross-attention maps. Finegrained control over autoregressive generation in LLMs and VLMs is still challenging. Later concurrent works CRG and MARINE [40,41], adopt CFG in VLMs for grounding and mitigating hallucination, but employ a different design for positive-negative pairs compared to our approach.",
            "score": 0.4002901212050553,
            "section_title": "Interactions with Multi-Modal LLMs",
            "char_start_offset": 7961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "258108187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "257913384",
            "title": "FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation",
            "text": "Similar to the language modeling tasks [5] in natural language processing (NLP), it takes the item of maximum probability in global classification as the prediction. Existing list continuation methods [12,30] are also designed based on this strategy, which unfortunately cannot be easily deployed in production with high requirements of efficiency. Chances are it may incur a large increase in model inference time that scales linearly with the length of the generated sequence and quadratically with the item vocabulary size. Since the item vocabulary of recommender systems is much larger than the token vocabulary in NLP due to item indivisibility, the softmax classifier will suffer from excessive computational complexity. \n\nPresent Work. In this paper, we adopt a non-autoregressive approach to accelerate inference efficiency and improve inference quality for item list continuation. As depicted on the right of Figure 1, we address the shortcomings of previous models by using a non-autoregressive generation strategy to decode the next  items simultaneously and a hierarchical search method for fuzzy matching followed by precise localization. \n\nSpecifically, we propose a FAst Non-autoregressive Sequence generation model, namely FANS, with bidirectional Transformer [29] as the backbone network. Concretely, it makes the following improvements on the typical autoregressive BERT4Rec [25] model. 1) Unlike autoregressive models which only append one \u27e8MASK\u27e9 token at one time and iterate the process for  times, FANS splices  \u27e8MASK\u27e9 tokens altogether and goes through the process only once. \n\n2) We leverage item category information (can be obtained by a simple categorizer if not available) and design a two-stage classifier to hierarchically decode latent representations. Simply put, the twostage classifier will first predict item category and then predict item over category-specific item vocabulary. 3) To improve the inference quality of non-autoregressive generation, we propose to train our model with curriculum learning [1] by using easy samples and hard samples in a progressive manner. The sample difficulty is defined based on the length of the item sequence to be predicted, as will be introduced in section 4. \n\nTo summarize, our contributions are listed as follows:",
            "score": 0.4001298744852038,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2235
                },
                {
                    "start": 2238,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "209515672"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "238259589"
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "119181611"
                },
                {
                    "start": 2041,
                    "end": 2044,
                    "matchedPaperCorpusId": "873046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00974273681640625
        },
        {
            "corpus_id": "268531795",
            "title": "Understanding and Improving Training-free Loss-based Diffusion Guidance",
            "text": "A unique advantage of training-free guidance is that it can be applied to universal control formats, such as style, layout, and FaceID [2,46,35] without any additional training efforts.Furthermore, these algorithms have been successfully applied to offline reinforcement learning, enabling agents to achieve novel goals not previously encountered during training [42].In contrast to classifier guidance and classifier-free guidance, it is proved in [25] that training-free guidance does not offer an approximation to the exact conditional energy.Therefore, from a theoretical perspective, it is intriguing to understand how and when these methods succeed or fail.From an empirical standpoint, it is crucial to develop algorithms that can address and overcome these limitations.\n\nThis paper seeks to deepen the understanding of training-free guidance by examining its mechanisms and inherent limitations, as well as overcoming these limitations.Specifically, our major contributions can be summarized as follows:\n\n\u2022 How does training-free guidance work?Although exact conditional energy is difficult to approximate in a training-free manner, from the optimization standpoint, we show that training-free guidance can effectively decrease the guidance loss function.The optimization perspective clarifies the mystery of why the guidance weights should be meticulously designed in relation to the guidance function and diffusion time, as observed in [46].\u2022 When does training-free guidance not work?We theoretically identify the susceptibility of training-free guidance to adversarial gradient issues and slower convergence rates.We attribute these challenges to a decrease in the smoothness of the guidance network in contrast to the classifier guidance.\u2022 Improving training-free guidance: We introduce random augmentation to alleviate the adversarial gradient and Polyak step size scheduling to improve convergence.The efficacy of these methods is empirically confirmed across various diffusion models (i.e., image diffusion and motion diffusion) and under multiple conditions (i.e., segmentation, sketch, text, object avoidance, and targeting)2 .",
            "score": 0.4000511360962318,
            "section_title": "Introduction",
            "char_start_offset": 2070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 777
                },
                {
                    "start": 779,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1011
                },
                {
                    "start": 1013,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1626
                },
                {
                    "start": 1626,
                    "end": 1751
                },
                {
                    "start": 1751,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "266162507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "235455118",
            "title": "Information Extraction From Free-Form CV Documents in Multiple Languages",
            "text": "Automatic extraction of useful information from CVs given in free form is a difficult task in the area of natural language processing (NLP). A system which could convert a free-form CV into a given highly organized structure can be a very valuable tool to recruiters and various job market websites. Useful information in this case includes personal information such as first and last name, residential addresses and spoken language, as well as information about past employments, education and skills or competences of the person. As any of this information can be presented in many different formats, it is not possible to create a simple parser (e.g. using regular expressions) that would accurately extract all the important information. The problem becomes even more complicated when multiple languages have to be supported, which is another obstacle for a potential rule-based parser. \n\nTherefore, in our work, machine learning techniques are used in the context of NLP in order to achieve a high degree of accuracy in extracting the desired information in arbitrary \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Igor Bisio. format in five languages. Our work addresses the CV parsing problem by constructing an NLP system with several interconnected machine learning models, using state-of-the-art NLP models as the basis. Namely, common approaches in NLP involve the use of recurrent neural networks that receive the elements of the input sequence and perform classification for them. A new deep model architecture recently designed for sequential input data, called transformer [1], allows parallel processing of the input sequence. It also allows a certain degree of model interpretability due to the attention layers used as the main element of feature extraction. Our approach uses the transformer architecture and its multilingual implementation of the encoder part in the form of the BERT language model. The model extracts and classifies the relevant sections of a document (personal information, education, past employment, skills) and, at a lower hierarchy level, extract and classifies the corresponding specific information such as names, dates, organizations, positions, university degrees, individual skills and their (self-assessed) competence degrees. \n\nThe models were evaluated using standard metrics: precision, recall and F1 scores on a dataset consisting VOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License.",
            "score": 0.3997513430952634,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2331
                },
                {
                    "start": 2334,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "272831727",
            "title": "Diffusion Models for Intelligent Transportation Systems: A Survey",
            "text": "Following this work, many studies related to traffic trajectory generation and motion planning have designed various classifiers to controllably generate traffic scenarios that comply with traffic rules and ensure trajectory smoothness. For example, the cumulative rewards learned through reinforcement learning [46], motion planning cost function [47], STL formulas based on traffic rules [48], language-based loss function [49], and driving behavior classes [50] have been designed as classifier to generate task-conditioned samples. \n\nClassifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], [50], [53], [54], [55], [34]. This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly. \n\n2) Latent Diffusion Models: \n\nThe latent diffusion models (LDMs) [38] incorporate pretrained perceptual compression models, VQGAN [56], which consist of an encoder E and a decoder D, as illustrated in the Fig. 3. This approach enables diffusion models to leverage a lower-dimensional latent space, thereby reducing the computational burden during training and speeding up inference while maintaining high fidelity in generated outputs. Following this work, Blattmann et al. [57] extended LDM to the video latent diffusion model (VLDM) by introducing temporal layers and finetuning the autoencoder of pre-trained LDM using video data. \n\nLDMs have gained attention in intelligent transportation systems due to their ability to model complex traffic patterns and generate realistic traffic scenarios.",
            "score": 0.39970881183856566,
            "section_title": "1) Conditional Diffusion Models:",
            "char_start_offset": 18929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2170
                },
                {
                    "start": 2173,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "260191316"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "253237771"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "259137395"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "262217065"
                },
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "262217065"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "257921535"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "258298040"
                },
                {
                    "start": 1602,
                    "end": 1606,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 2011,
                    "end": 2015,
                    "matchedPaperCorpusId": "258187553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "273962786",
            "title": "Conditional [MASK] Discrete Diffusion Language Model",
            "text": "Auto-Regressive Models (ARMs) have driven significant advances in NLP (Achiam et al., 2023;Dubey et al., 2024;Team et al., 2023), yet they still face fundamental challenges such as diversity and controllability due to the ARM's innate inductive bias. To address these challenges, a more flexible generative model is required. \n\nSpecifically, ARMs face multiple challenges: they struggle to correct mathematical reasoning errors once made (Wang et al., 2025), and often fail to integrate external knowledge (Hudecek and Dusek, 2023;Sun et al., 2023;Su et al., 2024). These shortcomings arise from ARMs' sequential nature, which prevents them from revising earlier steps. As a result, they cannot effectively foster diversity through temperature-based sampling alone (Lee et al., 2025), nor can they anticipate future requirements at earlier steps, thus undermining controllability when specific keywords must appear later (Lu et al., 2022). One promising alternative is non-autoregressive generation, including Conditional Masked Language Models (CMLMs) (Ghazvininejad et al., 2019a;Kasai et al., 2020) and diffusion models. CMLMs provide strong contextual understanding but lack an effective text generation mechanism. Meanwhile, diffusion models iteratively refine text through denoising, enabling fine-grained control and increased diversity. Recent works explore direct diffusion-based generation (Li et al., 2022;Gat et al., 2024;The et al., 2024;Ye et al., 2025) or hybrid approaches combining diffusion with PLMs and LLMs (Lin et al., 2023;Xiang et al., 2024). However, despite their advantages, Discrete Diffusion Language Models (DDLMs) still suffer from degeneration in conditional generation tasks (Xu et al., 2025), as confirmed by our experiments. \n\nWe therefore propose Diffusion-EAGS, a novel approach that integrates CMLMs into DDLMs to achieve diverse, controllable, and high-quality conditional generation.",
            "score": 0.3993636269115757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 531,
                    "end": 548,
                    "matchedPaperCorpusId": "259370802"
                },
                {
                    "start": 921,
                    "end": 938,
                    "matchedPaperCorpusId": "245218671"
                },
                {
                    "start": 1053,
                    "end": 1082,
                    "matchedPaperCorpusId": "202538740"
                },
                {
                    "start": 1082,
                    "end": 1101,
                    "matchedPaperCorpusId": "220266146"
                },
                {
                    "start": 1451,
                    "end": 1467,
                    "matchedPaperCorpusId": "273482143"
                },
                {
                    "start": 1708,
                    "end": 1725,
                    "matchedPaperCorpusId": "273661614"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0153045654296875
        },
        {
            "corpus_id": "276884868",
            "title": "Energy-Weighted Flow Matching for Offline Reinforcement Learning",
            "text": "Therefore, the following question arises: \n\nQ1. Can we directly obtain an energy-guided diffusion model without auxiliary models? \n\nAnother challenge for energy-guided generative models lies in providing guidance in flow matching models (Chen et al., 2018;Lipman et al., 2022), which is a more general, simulation-free counterpart to diffusion models. Zheng et al. (2023) explored the use of classifier-free guidance for flow matching in offline RL. However, since flow matching models approximate the velocity field u t (x) for the dynamics of the probability density path p t (x), it is highly non-trivial to obtain the guided velocity field u t (x) for the distribution q t (x) under energy guidance. This presents the second key question: \n\nQ2. Can we inject exact energy guidance into general flow matching models? \n\nIn this paper, we answer the aforementioned two questions affirmatively by proposing an energyguided velocity field and an energy-weighted flow matching objective, with extensions to energyweighted diffusion models and applications in offline reinforcement learning. Our contributions are summarized as follows: \n\n\u2022 In response to Q2., for general flow matching, we propose the energy-guided velocity field u t (x), based on the conditional velocity field u t0 (x|x 0 ). The proposed u t (x) is theoretically guaranteed to generate the energy-guided distribution q(x) \u221d p(x) exp(\u2212\u03b2E(x)). \u2022 We introduce the energy-weighted flow matching loss to train a neural network v \u03b8 t that approximates u t (x). The energy-weighted flow matching only requires the conditional vector field u t (x|x 0 ) and the energy E(x 0 ) for x 0 from the dataset. As the answer to Q1., we extend this approach to diffusion models, proposing the energy-weighted diffusion model. Energy-weighted diffusion model learns an energy-guided diffusion model directly without any auxiliary model. \n\n\u2022 We apply these methods to offline reinforcement learning tasks to evaluate the performance of the energy-weighted flow matching and diffusion models. Under this framework, we introduce an iterative policy refinement technique for offline reinforcement learning.",
            "score": 0.399347448316915,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 256,
                    "matchedPaperCorpusId": "49310446"
                },
                {
                    "start": 256,
                    "end": 276,
                    "matchedPaperCorpusId": "252734897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "278394336",
            "title": "Guide your favorite protein sequence generative model",
            "text": "Connections among masked discrete diffusion models, masked language models (MLM) and order-agnostic autoregressive models (OA-AR) were initially pointed out by Austin et al. [17] and Hoogeboom et al. [18]. Recent works [19][20][21] further clarified these connections, including the connections to CTMC-based formulation of diffusion and flow models [32,34,38], and showed that the continuous-time ELBO used to train masked diffusion models is essentially a weighted sum of cross entropy losses predicting the masked states from unmasked positions, where the proportion of masked positions ranges from 0 (complete masking) to 1 (no masking). For particular choices of the noise schedules, these training objectives can be shown to be equivalent to the training objective of order-agnostic autoregressive models [40], (generative) masked language models [17], or discrete flow matching models [38,39] under a masking process. Since ESM3 [4] and ProteinMPNN [1] were trained under the MLM and OA-ARM objectives respectively, we can view them as discrete state-space diffusion and flow models, and sample from them as such, leveraging the generative rate matrices from Equation 3 induced by the trained denoising model, p \u03b8 (x 1 |x t ). This reformulation not only provides a principled way to sample from these models, but also offers additional sampling flexibility, including, but not limited to, the usage of different noise schedules from training and corrector-sampling [38,39], which have been shown to sometimes improve sampling results compared to prior heuristics [41] for sampling from generative masked language models [39]. In addition, this reformulation unlocks the capacity to condition the generative process via guidance, which we introduce in Section 2. \n\nHere, we provide a self-contained derivation of the equivalence between the loss functions of discrete flow matching, masked language models, and order-agnostic autoregressive models.",
            "score": 0.39893173416998845,
            "section_title": "Unifying connections between generative models on discrete state-spaces",
            "char_start_offset": 12282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "270286107"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "270380319"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "270285911"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "249192370"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "267523194"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "13147238"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "267523194"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "271328803"
                },
                {
                    "start": 936,
                    "end": 939,
                    "matchedPaperCorpusId": "270963342"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "249400681"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "267523194"
                },
                {
                    "start": 1477,
                    "end": 1480,
                    "matchedPaperCorpusId": "271328803"
                },
                {
                    "start": 1571,
                    "end": 1575,
                    "matchedPaperCorpusId": "246680316"
                },
                {
                    "start": 1628,
                    "end": 1632,
                    "matchedPaperCorpusId": "271328803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0224151611328125
        },
        {
            "corpus_id": "270214339",
            "title": "Unlocking Guidance for Discrete State-Space Diffusion and Flow Models",
            "text": "Flow matching, on the other hand, is typically implemented with a linear interpolation between a noise and target distribution, which has been shown to behave pathologically for discrete state-spaces, stimulating alternative approaches, such as one which operates on the continuous state-space of the probabilistic simplex (Stark et al., 2024). \n\nA number of discrete time, discrete state-space diffusion approaches have been proposed (Austin et al., 2021;Hoogeboom et al., 2021a;Vignac et al., 2023). On the other hand, Campbell and colleagues have developed frameworks of continuous-time diffusion and flow matching on discrete state-spaces by leveraging continuous-time Markov chains (CTMCs) (Campbell et al., 2022;2024). In such formulations, one effectively learns a denoising neural network that approximates the instantaneous probability of transitioning between states-that is, a rate matrix-instead of a score or flow-generating vector field. However, one major ingredient missing from these approaches that operate on discrete state-spaces is the ability to construct conditional generative models by way of guidance in a principled way (Dhariwal & Nichol, 2021;Sohl-Dickstein et al., 2015;Ho & Salimans, 2021;Song et al., 2021). Indeed, arguably the most important ingredient enabling useful application of diffusion and flow models is that of conditioning the generative process on desired criteria. For example, in protein engineering, we may want to condition sequence generation on a backbone structure, binding affinity, or enzymatic activity (Hsu et al., 2024;Dauparas et al., 2022;Hsu et al., 2022;Li et al., 2023;Vanella et al., 2022). Conditioning of diffusion models is typically achieved by way of introducing guidance, either in a classifier-free way (Ho & Salimans, 2021), or by using a classifier (Dhariwal & Nichol, 2021).",
            "score": 0.39868390015006683,
            "section_title": "Diffusion and Flow-based Generative Models for Science",
            "char_start_offset": 1575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 456,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 480,
                    "end": 500,
                    "matchedPaperCorpusId": "252595881"
                },
                {
                    "start": 695,
                    "end": 718,
                    "matchedPaperCorpusId": "249192370"
                },
                {
                    "start": 1147,
                    "end": 1172,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1172,
                    "end": 1200,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1200,
                    "end": 1220,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1220,
                    "end": 1238,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1559,
                    "end": 1577,
                    "matchedPaperCorpusId": "267682798"
                },
                {
                    "start": 1577,
                    "end": 1599,
                    "matchedPaperCorpusId": "249400681"
                },
                {
                    "start": 1599,
                    "end": 1616,
                    "matchedPaperCorpusId": "248151599"
                },
                {
                    "start": 1632,
                    "end": 1653,
                    "matchedPaperCorpusId": "246310015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15966796875
        },
        {
            "corpus_id": "276928343",
            "title": "Flow Matching for Discrete Systems: Efficient Free Energy Sampling Across Lattice Sizes and Temperatures",
            "text": "where c denotes the desired class, and \u03b3 controls the strength of the classifier guidance. \n\nTo remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6)).",
            "score": 0.39857759446045876,
            "section_title": "Multi-temperature generation",
            "char_start_offset": 14241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 909
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "271956612",
            "title": "Bidirectional Awareness Induction in Autoregressive Seq2Seq Models",
            "text": "Many tasks in Natural Language Processing (NLP) such as Neural Machine Translation (NMT) [34,7], Text Summarization (TS) [3,2,11] and Image Captioning (IC) [30,8] deal with the challenging task of generating meaningful and linguistically correct sentences. This is commonly accomplished by Neural Networks. Typically, models follow the Autoregressive property, meaning that the token distribution predicted on time step t, depends on all the previous tokens from 1 to t \u2212 1. While this approach is intuitive, as the sequential process resembles on a superficial level, how humans communicate, it poorly reflects how we process information, and presents in fact, some limitations. If errors in previous arXiv:2408.13959v1 [cs.CL] 25 Aug 2024 predictions occur, the quality of subsequent predictions is undermined. Additionally, unidirectional decoding fails to capture bidirectional contexts that can be exploited for more effective learning. \n\nSeveral works in Natural Language Processing (NLP) related fields such as Image Captioning (IC) and Neural Machine Translation (NMT) proposed several approaches to combat the limitations of unidirectional decoding. Existing methods can be categorized into two main non-exclusive classes we name for simplicity \"architecture-based\" and \"algorithmic-based\". The first [35,34,37,39] consists of feeding Right-to-Left (R2L) data (or processing the input in a reversed order) to the network, in addition to the standard Left-to-Right (L2R) which often imply also architectural modifications. These methods lead to better performances at the expense of a higher computational cost. The second consists of training and algorithmic modifications [32,6,20,26] which do not focus on the architecture but propose a different framework to predict multiple tokens simultaneously, often at the cost of the final accuracy. In this category, fall the very recent works of Text Diffusion models [18,13]. These methods focus on predicting the result in one single or multiple parallel passages in contrast to the standard autoregressive models.",
            "score": 0.3983531281250545,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 93,
                    "matchedPaperCorpusId": "19169084"
                },
                {
                    "start": 124,
                    "end": 126,
                    "matchedPaperCorpusId": "234681504"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 160,
                    "end": 162,
                    "matchedPaperCorpusId": "251564151"
                },
                {
                    "start": 1310,
                    "end": 1314,
                    "matchedPaperCorpusId": "52091536"
                },
                {
                    "start": 1314,
                    "end": 1317,
                    "matchedPaperCorpusId": "19169084"
                },
                {
                    "start": 1317,
                    "end": 1320,
                    "matchedPaperCorpusId": "116880070"
                },
                {
                    "start": 1688,
                    "end": 1691,
                    "matchedPaperCorpusId": "53105113"
                },
                {
                    "start": 1691,
                    "end": 1694,
                    "matchedPaperCorpusId": "7864745"
                },
                {
                    "start": 1926,
                    "end": 1929,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01094818115234375
        },
        {
            "corpus_id": "258841514",
            "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
            "text": "Pretrained language models (LMs) have achieved impressive results across a range of tasks, such as machine translation, summarization, and dialogue generation (Brown et al., 2020;Touvron et al., 2023). All of these models are pretrained on nexttoken prediction task, encouraging researchers to cast other NLP tasks in the same autoregressive generation framework. By framing tasks as autoregressive generation, pretrained language models * Work done while at EPFL.  (GCD), applied to the task of closed information extraction, where the goal is to extract a list y of subject-relation-object triplets from the input text x. Subjects and objects are constrained to be Wikidata entities, relations to be a Wikidata relation. During decoding, only valid token continuations compliant with the grammar are considered. For simplicity, we omit the special marker symbols [s], [r], and [o] in the schema of the generation process. can be finetuned for specific tasks with minimal modifications to the training process while still benefiting from the advantages of pretraining. More recently, the scaling of language models to larger sizes has introduced notable in-context learning capabilities (Brown et al., 2020;Radford et al., 2019;Schick and Sch\u00fctze, 2021), such that large language models (LLMs) can quickly and effectively adapt to new tasks even without finetuning, when shown only few demonstrations as part of their context. \n\nCertain important tasks, however, such as closed information extraction (cIE), entity disambiguation (ED), or constituency parsing (CP), require the output to follow a predefined format and adhere to a restricted vocabulary (entities, relations, senses, dependency labels, etc.). Whereas LMs excel at generating free-form text, they are not specifically designed for structured prediction tasks where only a small subset of the output space is valid. Consequently, structured prediction tasks present unique challenges because constrained output spaces demand both structural coherence and compliance with a predefined vocabulary.",
            "score": 0.39781364382632933,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022369384765625
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "Such an operation fully takes into account the discrete characteristics of the text, making the generation more flexible.\n\nSeqDiffuSeq (Yuan et al., 2022), an encoder-decoder Transformers architecture, incorporates two key techniques: adaptive noise schedule and self-conditioning, resulting in substantial enhancements in both the quality and speed of text generation.\n\nZero-shot diffusion (Nachmani & Dovrat, 2021), inspired by encoder-decoder architecture, inputs the source language sentence x (i.e., the condition) into the Transformer encoder and the noisy target language sentence y into the decoder.Notably, this work is the first to apply the diffusion model to conditional text generation tasks.\n\nGENIE (Lin et al., 2022) represents a significant advancement in the field of language modeling with its large-scale pre-training approach.Using the masked source sequence s as the input of the encoder and incorporating the continuous paragraph denoise training method, GENIE has demonstrated its ability to generate text that exhibits both high quality and remarkable diversity.This not only showcases the effectiveness of diffusion language models but also opens up new possibilities for various natural language processing tasks.\n\nRDMs (reparameterized diffusion models) (Zheng et al., 2023) introduce reparameterization and a stochastic routing mechanism, leading to two significant advantages: simplified training and flexible sampling.However, currently RDMs can only generate sentences of fixed length.\n\nDiffusion-NAT (Zhou et al., 2023) integrates discrete diffusion models (DDM) and BART into non-autoregressive (NAR) text generation, unifying the inference and denoising processes into a masked token recovery task.Diffusion-NAT focuses on conditional text generation tasks, highlighting the synergistic effect of discrete diffusion models and pre-trained language models in enhancing text generation.CDCD (Dieleman et al., 2022) improves the training process of diffusion models by incorporating score interpolation and time warping techniques, achieving excellent performance in language modeling and machine translation tasks.\n\nDiNoiSer (Ye et al., 2023) argues that simply mapping discrete tokens to continuous space through embedding is not sufficient to fully eliminate the discrete nature of text.",
            "score": 0.3975931805880305,
            "section_title": "Text-driven generation",
            "char_start_offset": 15119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 123,
                    "end": 369
                },
                {
                    "start": 371,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 705
                },
                {
                    "start": 707,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1239
                },
                {
                    "start": 1241,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1516
                },
                {
                    "start": 1518,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 2146
                },
                {
                    "start": 2148,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01337432861328125
        },
        {
            "corpus_id": "276961771",
            "title": "Constrained Discrete Diffusion",
            "text": "There are two key limitations that are currently faced in the development of constrained discrete models. The first regards the size of currently available discrete diffusion models. While current discrete diffusion models provide comparable performance in fluency and perplexity to similarly sized autoregressive models, including state-of-the-art models such as Llama 3.2 1B, their generative capabilities fall short of large-scale language models. Larger scale, closed-source discrete diffusion models have been developed and boasted impressive performance [37], but larger open-source models are yet to be provided to the research community. It is expected that the release of such larger models will further boost the performance of our proposed methods. \n\nThe second key limitation concerns an inherent byproduct of constraint imposition: increased computational overhead. While unconstrained discrete diffusion language models have been shown to provide significant speed-ups over autoregressive language models due to the parallelization of token generation [18,16,49], the inclusion of constraints within the generation process may limit this benefit. Nevertheless, this trade-off is often justified, as imposing constraints is essential in scenarios where correctness outweighs generation speed, such as ensuring safety-critical outputs or supporting rigorous scientific exploration. In such contexts, achieving accurate and reliable outcomes is prioritized over computational efficiency.",
            "score": 0.39732901433796997,
            "section_title": "Limitations",
            "char_start_offset": 26657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1498
                }
            ],
            "ref_mentions": [
                {
                    "start": 1066,
                    "end": 1070,
                    "matchedPaperCorpusId": "264451832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00731658935546875
        },
        {
            "corpus_id": "236459928",
            "title": "Continuous Language Generative Flow",
            "text": "We first employ GloVe word embeddings (Pennington et al., 2014) to map the discrete token sequence to a continuous embedding matrix. Furthermore, we utilize two components: time-dimension permutation and affine coupling with RNN or Transformer non-linearity functions, which allow interaction between words in a se-quence and better contextualizes language semantics. Overall, these proposed components help generate texts in a non-autoregressive manner. \n\nHowever, even though the non-autoregressive model has attracted a lot of research attention because of its fast generation speed, it still hardly surpasses the generation quality of autoregressive models (Ren et al., 2020). Therefore, to make our language flow model learn language generation in a stronger autoregressive manner, we change the flow model's affine coupling and permutation to a uni-directional structure, i.e., each timestep can only attend to previous timesteps. In this way, we enable our model to perform text generation autoregressively. \n\nSome recent works have developed density estimation models targeted on character-level discrete data (DiscreteFlow (Tran et al., 2019)) and explored using the flow architecture as an extra data encoder that provides latent features to support nonautoregressive text generation (FlowSeq (Ma et al., 2019)). While our work shares some similar characteristics, we explore different directions: (1) Dis-creteFlow develops a modulus calculation method to process discrete data. Instead, we use word embedding to transform the discrete input tokens to continuous features, which is simple yet effective. \n\n(2) FlowSeq essentially leverages the flow architecture in a typical encoder-decoder model to support non-autoregressive generation, whereas our models follow the standard generative flow framework and can directly generate texts via their invertible structure in both non-autoregressive or autoregressive manner. (3) Autoregressive flows were previously developed (Papamakarios et al., 2017;Huang et al., 2018) for stronger density estimation ability.",
            "score": 0.3971376712596918,
            "section_title": "Introduction",
            "char_start_offset": 1910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 63,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 661,
                    "end": 679,
                    "matchedPaperCorpusId": "216056470"
                },
                {
                    "start": 1132,
                    "end": 1151,
                    "matchedPaperCorpusId": "157062310"
                },
                {
                    "start": 1303,
                    "end": 1320,
                    "matchedPaperCorpusId": "202539063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00731658935546875
        },
        {
            "corpus_id": "271097733",
            "title": "Autoregressive Speech Synthesis without Vector Quantization",
            "text": "Seed-TTS [Anastassiou et al., 2024] replaces NAR model in VALL-E with a diffusion model, which generates continuous speech representations according generated speech tokens from AR stage.\n\nIn parallel to our work, VALL-E 2 [Chen et al., 2024] shares the same architecture as VALL-E but employs a repetition-aware sampling strategy that promotes more deliberate sampling choices.\n\nOther studies have investigated fully non-autoregressive approaches to increase the speed of model inference.For instance, SoundStorm [Borsos et al., 2023] adapts a parallel, non-autoregressive, confidence-based decoding scheme for the generation of codec codes.StyleTTS 2 [Li et al., 2024b] and NaturalSpeech 3 [Ju et al., 2024] use diffusion model to achieve better TTS synthesis.Voicebox [Le et al., 2024] and Audiobox [Vyas et al., 2023] employ non-autoregressive flow-matching based models for transcript-guided speech generation.Recently, E2 TTS [Eskimez et al., 2024] presents a fully non-autoregressive TTS systems consisting of flow-matching-based mel-spectrogram generator trained with the audio infilling task, and a vocoder.Different previous works, MELLE leverages continuous-valued tokens based autoregressive language modeling approach with variational inference for text-to-speech synthesis.",
            "score": 0.3965629921852295,
            "section_title": "Zero-Shot TTS",
            "char_start_offset": 9715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 189,
                    "end": 378
                },
                {
                    "start": 380,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1287
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 535,
                    "matchedPaperCorpusId": "258715176"
                },
                {
                    "start": 653,
                    "end": 671,
                    "matchedPaperCorpusId": "259145293"
                },
                {
                    "start": 771,
                    "end": 788,
                    "matchedPaperCorpusId": "259275061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01666259765625
        },
        {
            "corpus_id": "254044531",
            "title": "Continuous diffusion for categorical data",
            "text": "The denoising model is a Transformer (without attention masking) which predicts tokens from noisy embeddings ( \u00a73.2) and is trained with the cross-entropy loss ( \u00a73.1). The input noise is time-dependent, with timesteps  sampled non-uniformly during training via time warping ( \u00a73.3). \n\nWith the predicted logits, score function estimates can be obtained through interpolation, which are used for sampling with an ODE solver. See Figure 3 for a more detailed diagram. \n\ninspired approaches to language modelling have recently been proposed (Austin et al., 2021;Hoogeboom et al., 2021a,b;Reid et al., 2022;Savinov et al., 2021), but these depart from the diffusion modelling framework used for perceptual data in several important ways (with a few exceptions, e.g. Li et al. (2022); Strudel et al. (2022)). This usually implies having to give up some of the unique capabilities of this model class, such as the ability to use classifier-free guidance to enhance conditional generation (Ho and Salimans, 2022), which has been instrumental to the success of diffusion-based text-conditional image generators. \n\nIn this paper, we study the suitability of continuous diffusion as a generative modelling paradigm for discrete categorical data, and for textual representations of language in particular. We develop a framework, Continuous diffusion for categorical data (CDCD), based on the diffusion framework proposed by Karras et al. (2022), which enables efficient and straightforward training of diffusion-based language models that are continuous in both time and input space, by embedding discrete tokens in Euclidean space. \n\nOur approach very closely mirrors the training procedure for masked language models such as BERT (Devlin et al., 2018, which are non-autoregressive and non-generative 1 ), and hence should appear familiar to language modelling practitioners. We hope that this will help lower the barrier to entry, and encourage researchers to explore continuous diffusion models for other domains for which categorical representations are best suited. \n\nOur contributions are as follows: \n\n\u2022 We propose score interpolation as an alternative to score matching for diffusion model training.",
            "score": 0.396279018868562,
            "section_title": "Introduction",
            "char_start_offset": 1817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2097
                },
                {
                    "start": 2100,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 560,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "258660886",
            "title": "Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model",
            "text": "Initially, we used the pretrained parameters of the SciFive [30] model and then finetuned it on our specific dataset in relation extraction tasks. The SciFive model was retrained on various text combination, which consisted of the C4 corpus [35], PubMed abstracts, and PMC full-text articles, to optimize the pretrained weights from T5 in the context of biomedical literature. Consistent with the original T5 model [10], SciFive learned to generate a target text sequence for a given text input sequence using a learning objective known as span-based mask language modeling. The output sequence is generated during the decoding phase by applying beam search algorithm. This involves maintaining the top n probable output sequences at each timestep and finally generating the output sequence with the highest probability. \n\nFinally, we employed GPT-3 (Generative Pretrained Transformer) [9] which utilizes constrained self-attention where every token can only attend to its left context. As a decoder-only transformer, it was pretrained on a diverse range of web text to predict the next token in an autoregressive manner given the preceding text. It can generate words only conditioned on the left context, so it cannot learn bidirectional interactions. \n\nPrevious pretrained models have a limitation in that they need additional large, labeled datasets for a task-specific fine-tuning process to achieve desirable performance. Thus, GPT2 was designed as a general language model for various NLP tasks without the need for extensive fine-tuning. It is capable of performing downstream tasks with little or no fine-tuning, including zero-shot and few-shot learning scenarios, where only a few labeled examples are available for fine-tuning. However, the results were not satisfactory in some tasks. They still need fine-tuning on task-specific labeled data to improve the performance. \n\nIn contrast, GPT-3 increased the capacity of transfer language models to 175 billion parameters, thereby allowing the model to utilize its language skills to comprehend tasks with a few examples or natural language instructions.",
            "score": 0.39612936842600177,
            "section_title": "Baseline Methods",
            "char_start_offset": 32312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0272216796875
        },
        {
            "corpus_id": "270199436",
            "title": "Amortizing intractable inference in diffusion models for vision, language, and control",
            "text": "Others estimate the 'guidance' term -the difference in drift functions between the diffusion models sampling the prior and posterior -by training a classifier on noised data [11], but when such data is not available, one must resort to approximations or Monte Carlo estimates [e.g., 68,13,9], which are challenging to scale to high-dimensional problems.Reinforcement learning methods that have recently been proposed for this problem [7,15] are biased and prone to mode collapse (Fig. 1).\n\nContributions.Inspired by recent techniques in training diffusion models to sample distributions defined by unnormalized densities [87,61,76,64], we propose an asymptotically unbiased training objective, called relative trajectory balance (RTB), for training diffusion models that sample from posterior distributions under a diffusion model prior ( \u00a73.1).RTB is derived from the perspective of diffusion models as continuous generative flow networks [37].This perspective also allows us to freely leverage off-policy training, when data with high density under the posterior is available ( \u00a73.2).RTB can be applied to iterative generative processes beyond standard diffusion models: our methods generalize to discrete diffusion models and extend existing methods for autoregressive language models ( \u00a73.3).\n\nOur experiments demonstrate the versatility of our approach in a variety of domains:\n\n\u2022 In vision, we show that RTB achieves competitive classifier-guided image generation for unconditional diffusion vision priors ( \u00a74.1) and can be used to improve caption-conditioned generation under text-to-image foundation model priors ( \u00a74.2). \u2022 In language modeling, we report strong results for infilling tasks with discrete diffusion language models ( \u00a74.3).\u2022 Finally, we show that RTB achieves state-of-the-art results on continuous control benchmarks that leverage score-based behavior priors ( \u00a74.4).",
            "score": 0.3958075089995019,
            "section_title": "Introduction",
            "char_start_offset": 1664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 488
                },
                {
                    "start": 490,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 945
                },
                {
                    "start": 945,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1296
                },
                {
                    "start": 1298,
                    "end": 1382
                },
                {
                    "start": 1384,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 434,
                    "end": 437,
                    "matchedPaperCorpusId": "258833251"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "244729050"
                },
                {
                    "start": 625,
                    "end": 628,
                    "matchedPaperCorpusId": "259316542"
                },
                {
                    "start": 628,
                    "end": 631,
                    "matchedPaperCorpusId": "257220165"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1517333984375
        },
        {
            "corpus_id": "253329098",
            "title": "Generating Semantic Matches Between Maintenance Work Orders for Diagnostic Decision Support",
            "text": "Technical Language Processing (TLP) is a human-in-the-loop and iterative approach to adapting NLP tools to engineering data. TLP applies engineering principles and practices to NLP to exploit the language generated by experts in their technical tasks, systems, and processes. TLP can unlock maintenance knowledge hidden in the text, providing needed insights from the asset health history while making maintenance decisions (Brundage, Sexton, Hodkiewicz, Dima, & Lukens, 2021;Liao et al., 2019). \n\nPre-trained language models based on Transformers, such as BERT (Bidirectional Encoder Representations from Transformers) and its variants, have achieved state-of-the-art results in several downstream NLP tasks on generic benchmark datasets (Whang et al., 2019;Z. Gao, Feng, Song, & Wu, 2019;Yu, Su, & Luo, 2019). However, BERT has been reported to underperform in specialised domains, such as biomedical (Gu et al., 2021;Cho & Lee, 2019), legal (Chalkidis, Fergadiotis, Malakasiotis, Aletras, & Androutsopoulos, 2020) and maintenance (Naqvi, Varnier, Nicod, Zerhouni, & Ghufran, 2022). To overcome this limitation, BERT needs to be fine-tuned on domain-specific corpora. However, for most tasks and domains, labeled data is not available, and data annotation is expensive (Huang, Wei, Cui, Zhang, & Zhou, 2020). Unsupervised training approaches have been proposed to overcome this limitation: learning to embed sentences just using an unlabeled corpus for training (Han & Eisenstein, 2019). This work focuses on developing a decision support solution for the maintenance team by exploiting the knowledge in the MWOs. Our approach is based on retrieving similar cases to help solve new maintenance problems based on past experiences to help diagnostic decision-making. To adapt the BERT model to the maintenance-specific domain, the BERT model is fine-tuned using an unsupervised domain fine-tuning technique, Transformer-based Sequential Denoising Auto-Encoder (TSDAE).",
            "score": 0.3958066469761863,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "211120228"
                },
                {
                    "start": 759,
                    "end": 790,
                    "matchedPaperCorpusId": "207833162"
                },
                {
                    "start": 903,
                    "end": 920,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 920,
                    "end": 936,
                    "matchedPaperCorpusId": "209492113"
                },
                {
                    "start": 1271,
                    "end": 1309,
                    "matchedPaperCorpusId": "227231321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017852783203125
        },
        {
            "corpus_id": "265352082",
            "title": "Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model",
            "text": "Diffusion models. Denoising diffusion probabilistic models, introduced in [54] and further advanced by [26], have emerged as powerful tools for generating diverse data types. They have been successfully applied in various domains such as image synthesis [44,49], video generation [27,36,53], and robotics control systems [1,11,29]. Notably, test-to-image diffusion models have enabled the creation of highly realistic images from textual descriptions [44,49], opening new avenues in digital art and design. \n\nRecent studies have focused on refining the guidance of diffusion models for more precise manipulation over the generative process. Techniques such as adapters [63] and compositional approaches [17,38] have been introduced to incorporate additional input constraints and blend multiple models, respectively, enhancing image quality and generation control. The implementation of classifier-based [13] and classifier-free guidance [25] has also contributed significantly to achieving greater autonomy in the generation process, resulting in outputs that closely align with user intentions. In our work, we utilize Stable Diffusion [45] to generate images based on specific prompts. \n\nRLHF. RLHF stands as a salient strategy in the domain of machine learning when objectives are complex or difficult to define explicitly. This technique has been instrumental across various applications, from gaming, as demonstrated with Atari [4,12], to more intricate tasks in robotics [10,64]. The integration of RLHF into the development of large language models (LLMs) has marked a significant milestone in the field, with notable models like OpenAI's GPT-4 [41], Anthropic's Claude [2], Google's Bard [24], and Meta's Llama 2-Chat [57] leveraging this approach to enhance their performance and relevance. The effectiveness of RLHF in refining the behavior of LLMs to be more aligned with human values, such as helpfulness and harmlessness, has been extensively studied [4,64]. The technique has also proven beneficial in more focused tasks, such as summarization, where models are trained to distill extensive information into concise representations [55].",
            "score": 0.39538631407838765,
            "section_title": "Related Work",
            "char_start_offset": 4557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 258,
                    "end": 261,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "254366300"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "254044710"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "248965046"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation (NLG), a sub-field of natural language processing (NLP), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction. In addition, considering that autoregressive-based pre-training models (PLMs) have recently dominated text generation, we conduct a detailed comparison between diffusion models and PLMs in multiple dimensions, highlighting their respective advantages and limitations. We believe that integrating PLMs into diffusion is a valuable research avenue. We also discuss current challenges faced by diffusion models in text generation and propose potential future research directions, such as improving sampling speed to address scalability issues and exploring multi-modal text generation. By providing a comprehensive analysis and outlook, this survey will serve as a valuable reference for researchers and practitioners interested in utilizing diffusion models for text generation tasks.",
            "score": 0.39537384790136626,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021331787109375
        },
        {
            "corpus_id": "273375140",
            "title": "One Step Diffusion via Shortcut Models",
            "text": "1/128), we instead query the model at d = 0. Joint optimization. Equation 5 consists of an empirical flow-matching objective and a selfconsistency objective, which are jointly optimized during training. The variance of the empirical term is much higher, as it regresses onto random noise pairings with inherent uncertainty, whereas the self-consistency term uses deterministic bootstrap targets. We found it helpful to construct a batch with significantly more empirical targets than self-consistency targets. \n\nThe above behavior also gives us room for computational efficiency. Training requires less selfconsistency targets than empirical targets, and self-consistency targets are also more expensive to generate (requiring two additional forward passes). We can therefore construct a training batch by combining a ratio of 1 \u2212 k empirical targets with k self-consistency targets. We find k = (1/4) to be reasonable. In this way, we can reduce the training cost of a shortcut model to be roughly only \u223c 16% more than that of an equivalent diffusion model2 . \n\nGuidance. Classifier-free guidance (CFG; Ho & Salimans, 2022) has proven to be an essential tool for diffusion models to reach high generation fidelity. CFG provides a linear approximation of a tradeoff between the class-conditional and -unconditional denoising ODE. We find that CFG helps at small step sizes but is error-prone at larger steps when linear approximation is not appropriate. We therefore use CFG when evaluating the shortcut model at d = 0 but forgo it elsewhere. A limitation of CFG in shortcut models is that the CFG scale must be specified before training. \n\nExponential moving average weights. Many recent diffusion models use an exponential moving average (EMA) over weight parameters to improve sample quality. EMA induces a smoothing effect on the generations, which is especially helpful in the in diffusion modelling since the objective has inherent variance. We find that similarly in shortcut models, variance from loss at the d = 0 level can result in large oscillations in the output at d = 1. Utilizing EMA parameters for generating self-consistency targets alleviates this issue. \n\nWeight decay. We find that weight decay is crucial for enabling stability, especially early on in training.",
            "score": 0.3949446732056633,
            "section_title": "TRAINING DETAILS",
            "char_start_offset": 8431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2173
                },
                {
                    "start": 2176,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "276395038",
            "title": "Large Language Diffusion Models",
            "text": "What is now proved was once only imagined. placeholder,placeholder,placeho-William Blake Large language models (LLMs) (Zhao et al., 2023) fall entirely within the framework of generative modeling. Specifically, LLMs aim to capture the true but unknown language Preprint. Generative modeling principles \n\n. \n\nThe predominant approach relies on the autoregressive modeling (ARM)-commonly referred to as the next-token prediction paradigm-to define the model distribution: \n\nwhere x is a sequence of length L, and x i is the i-th token. \n\nThis paradigm has proven remarkably effective (Radford, 2018;Radford et al., 2019;Brown, 2020;OpenAI, 2022) and has become the foundation of current LLMs. Despite its widespread adoption, a fundamental question remains unanswered: Is the autoregressive paradigm the only viable path to achieving the intelligence exhibited by LLMs? \n\nWe argue that the answer is not a simple \"yes\". The key insight overlooked previously is: it is the generative modeling principles (i.e., Eq. ( 1)), rather than the autoregressive formulation (i.e., Eq. ( 2)) itself, that fundamentally underpin the essential properties of LLMs, as detailed below. However, certain inherent limitations of LLMs can be directly traced to their autoregressive nature. \n\nIn particular, we argue that scalability is primarily a consequence of the interplay between Transformers (Vaswani, 2017), model and data size, and Fisher consistency1 (Fisher, 1922) induced by the generative principles in Eq. ( 1), rather than a unique result of ARM. The success of diffusion transformers (Bao et al., 2023;Peebles & Xie, 2023) on visual data (Brooks et al., 2024) supports this claim. \n\nFurthermore, the capabilities of instruction-following and in-context learning (Brown, 2020) appear to be intrinsic properties of all proper conditional generative models on structurally consistent linguistic tasks, rather than exclusive advantages of ARMs.",
            "score": 0.3947805763251957,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 597,
                    "end": 618,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1578,
                    "end": 1596,
                    "matchedPaperCorpusId": "253581703"
                },
                {
                    "start": 1596,
                    "end": 1616,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01450347900390625
        },
        {
            "corpus_id": "258170263",
            "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
            "text": "Large language models (LMs), including masked LMs (e.g., BERT (Devlin et al., 2018)), autoregressive LMs (e.g., GPT (Brown et al., 2020)), and encoder-decoder LMs (e.g., T5 (Raffel et al., 2020), BART (Lewis et al., 2020a)), have obtained state-of-the-art results for various NLP tasks. Among them, the autoregressive LMs like GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) demonstrate noticeable in-context learning ability and excellent long-form text generation results. Due to its importance, the community has spent considerable efforts to scale up such autoregressive generative LMs with more data and parameters and observed significant breakthroughs in a variety of real-world applications (e.g., Brown et al., 2020), including open-ended text generation and various downstream tasks (e.g., question answering). The successful public examples include GPT-3 (w/ 170B parameters) (Brown et al., 2020), Gopher (280B) (Rae et al., 2021), Megatron-Turing (530B) (Smith et al., 2022), and PaLM (540B) (Chowdhery et al., 2022). \n\nAlthough large-scale autoregressive LMs have achieved huge successes, they also suffer from several weaknesses. First, it requires a huge number of model parameters to memorize the world knowledge, which makes it costly for deployment. Second, it is difficult to safeguard factual accuracy, which may provide users with incorrect information (Lee et al., 2022). Third, it is expensive to update the model knowledge learned during pretraining with up-to-date facts (Meng et al., 2022), yielding outdated answers (Lewis et al., 2020b). \n\nTo mitigate the problems above, one line of research proposes to improve language models with retrieval.",
            "score": 0.39464189678423556,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1676
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 83,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 173,
                    "end": 194,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 201,
                    "end": 222,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1500,
                    "end": 1519,
                    "matchedPaperCorpusId": "249282155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0135345458984375
        },
        {
            "corpus_id": "259766123",
            "title": "GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts",
            "text": "Excellent variants including BERT, GPT and T5 constantly break the record of various benchmark tasks, not only bringing the researches on natural language processing into a new era but also making it possible to implement applications that were previously considered challenging tasks. In terms of the infrastructures, pre-trained models can be divided into three types: autoregressive models, autoencoder models, and encoder-decoder models. We mainly introduce the first two models, which are closely related to this research. \n\nThe currently popular GPT series models belong to the typical autoregressive models. These models usually use the decoder part of the Transformer as their basic framework. In the pre-training tasks, autoregressive models calculate the loss by predicting unidirectionally the probability of vocabulary occurrence in the text, which enables the model to possess strong text generation capabilities. The original GPT model was produced in 2018 and utilized a causal language model for pre-training. When performing downstream tasks, the GPT-1 model (Radford & Narasimhan, 2018) could retrieve the corresponding vector of the last token and input it to a fully connected layer to construct a classifier. However, this approach yielded relatively poor classification performance. The GPT-2 model (Radford et al., 2019) , inheriting the infrastructure of the GPT-1 model, uses larger parameters and more data during the training process. The maximum parameter size of the GPT-2 model reaches the scale of 1.5 billion. The GPT-2 model unified all NLP tasks under the framework of text generation-related tasks. Due to the presence of translation and summarization data with indicative nature in the training corpus, it enabled the model to generate correct text in a zero-shot manner with the help of prompts. This process effectively laid the groundwork for prompt learning, which had a profound impact on subsequent research. The GPT-3 model (Brown et al., 2020) represents a significant milestone in the field. By utilizing a massively parallel GPU cluster, GPT-3 achieved a staggering parameter count of 175 billion, and the training data used reached the impressive 45 terabytes. The GPT-3 model introduced the approach of taking task examples as context input to the model.",
            "score": 0.3946033203976118,
            "section_title": "The pre-trained model infrastructures",
            "char_start_offset": 4395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 1967,
                    "end": 1987,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01306915283203125
        },
        {
            "corpus_id": "274597381",
            "title": "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey",
            "text": "Audiobox [201] and P-flow [198] employ nonautoregressive flow-matching models [228] for efficient and stable speech synthesis. VoiceBox [192] also employs flowmatching to generate speech, effectively casting the TTS task into a speech infilling task. SpeechFlow [197] is trained on 60k hours of untranscribed speech with flow matching and mask conditions and can be fine-tuned with task-specific data to match or surpass existing expert models. This highlights the potential of generative models as foundation models for speech applications. HierSpeech++ [200] proposes a hierarchical variational inference method. FlashSpeech [202] is built on a latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher, achieving speech generation in one or two steps. \n\nRecently, E2 TTS [206] converts text input into a character sequence with filler tokens and trains a mel spectrogram generator based on an audio infilling task, achieving humanlevel naturalness. Inspired by E2 TTS, F5-TTS [229] refines the text representation with ConvNext v2 [230], facilitating easier alignment with speech. E1 TTS [213] further distills a diffusion-based TTS model into a one-step generator with distribution matching distillation [231], [232], reducing the number of network evaluations in sampling from diffusion models. SimpleSpeech 2 [212] introduces a flow-based scalar transformer diffusion model. The work also provides a theoretical analysis, showing that the inclusion of a small number of noisy labels in a large-scale dataset is equivalent to introducing classifier-free guidance during model optimization. \n\nOther NAR Approaches. Other works leverage GANbased or masked generative model-based methods for TTS generation. StyleTTS 2 [89] employs large pre-trained speech language models (SLMs) such as Wav2Vec 2.0 [165], Hu-BERT [166], and WavLM [233] as discriminators in combination with a novel differentiable duration modeling approach.",
            "score": 0.39436609449871324,
            "section_title": "A. Non-Autoregressive Architectures",
            "char_start_offset": 45567,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 31,
                    "matchedPaperCorpusId": "268095731"
                },
                {
                    "start": 78,
                    "end": 83,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 136,
                    "end": 141,
                    "matchedPaperCorpusId": "259275061"
                },
                {
                    "start": 262,
                    "end": 267,
                    "matchedPaperCorpusId": "264487385"
                },
                {
                    "start": 627,
                    "end": 632,
                    "matchedPaperCorpusId": "269302492"
                },
                {
                    "start": 897,
                    "end": 902,
                    "matchedPaperCorpusId": "270738197"
                },
                {
                    "start": 1157,
                    "end": 1162,
                    "matchedPaperCorpusId": "255372693"
                },
                {
                    "start": 1214,
                    "end": 1219,
                    "matchedPaperCorpusId": "272689699"
                },
                {
                    "start": 1331,
                    "end": 1336,
                    "matchedPaperCorpusId": "258967550"
                },
                {
                    "start": 1338,
                    "end": 1343,
                    "matchedPaperCorpusId": "269982921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "UE, a standard NLP benchmark suite.\n\nIn this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model's context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\n\nBroadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the",
            "score": 0.39428792553051006,
            "section_title": "body",
            "char_start_offset": 6082,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.020416259765625
        },
        {
            "corpus_id": "268681211",
            "title": "LanguageFlow: Advancing Diffusion Language Generation with Probabilistic Flows",
            "text": "We propose language rectified flow, a surprisingly simple approach to the transport mapping problem, which unifiedly solves both generative modeling and domain transfer.The language rectified flow is an ordinary differential equation (ODE) model that transports distribution source distribution \u03c0 0 to target distribution \u03c0 1 by following straight line paths as much as possible.The straight paths are theoretically preferred because it is the shortest path between two end points, and computationally preferred because it can be precisely arXiv:2403.16995v1[cs.CL] 25 Mar 2024 simulated without time discretization.Hence, flows with straight paths bridge the gap between few-step and continuous-time models.Specifically, we formulate an ODE transport flow as the initial text generator with a simpler trajectory compared with the diffusion model formulated in SDE.Meanwhile, we optimize the transport flow cost for the initial flow model to significantly straighten the learning trajectory while maintaining the model's performance.Further, a VAE is utilized to connect the text sequence space and the flow latent space.This can be used to produce the initial values in the language flow for our controllable text generation.\n\nTo demonstrate control of language rectified flow, we consider three control targets ranging from span-anchored controls (e.g., length control and infilling) to complex structures (e.g., parse trees).Our LF can generate high-quality text, performing favorably relative to the diffusion-based LM with a 27x faster sampling on the controllable text generation task.In addition to these individual control tasks, we conduct experiments on three challenging settings, including sequential editing of text.Results show that composing operators within our method manage to generate or edit high-quality text, substantially improving over respective baselines in terms of quality and efficiency.Furthermore, we provide extensive ablation studies on different design choices for the proposed method, including the evidence with different generation steps, influence of the constrained optimization, flow in the latent space.Our analysis shows that LF contributes the performance improvement, helping the sampling efficiency and generation.With little modification, ours can be easily applied to other NLP tasks for better controllable text generation.Our contributions are summarized as follows:",
            "score": 0.3937825243846387,
            "section_title": "Introduction",
            "char_start_offset": 2033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1226
                },
                {
                    "start": 1228,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1916
                },
                {
                    "start": 1916,
                    "end": 2144
                },
                {
                    "start": 2144,
                    "end": 2259
                },
                {
                    "start": 2259,
                    "end": 2371
                },
                {
                    "start": 2371,
                    "end": 2415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006744384765625
        },
        {
            "corpus_id": "276250574",
            "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",
            "text": "Recently, discrete diffusion models have emerged as a promising approach for discrete data apart from autoregressive models. Additionally, the order-agnostic training of discrete diffusion models opens up the multiple sampling paths during the inference but it also faces some challenges during the training therefore, they seem a promising approach to elicit any order reasoning. Zheng et al. (2023) proposed different ways of implementing an adaptive inference strategy for MDM but a concrete understanding of why such an adaptive inference strategy is needed is still lacking. In this work, we explore various aspects of vanilla MDM training and how adaptive MDM inference can mitigate the issues raised by vanilla MDM training and elicit any order reasoning. \n\nWe also want to mention the concurrent work by Peng et al. (2025) that proposes an alternative adaptive inference strategy by selecting F(\u03b8, x t ) based on the BERT model or the denoiser itself. In particular, Peng et al. (2025) uses the BERT model or the denoiser to obtain the uncertainty of a token and then uses Top-K to decide the positions to unmask it. In contrast to their work, we disentangle the impact of token ordering on MDM training vs. MDM inference and provide a more complete understanding of the motivations for and benefits of adaptive inference. Additionally, our results indicate drawbacks to using Top-K strategy as opposed to Top-K margin in deciding which tokens to unmask when there are multiple values with high probabilities. \n\nBeyond autoregressive models. Efforts to learn the natural language using non-autoregressive modeling began with BERT (Devlin et al., 2019). Non-causal approaches can take advantage of the understanding the text data representation. (Chang et al., 2022) adopted a similar approach for learning image representations. Building on these intuitions, (Shih et al., 2022;Hoogeboom et al., 2021a) proposed any-order modeling, which allows a model to generate in any desired order. Shih et al. (2022) made the same observation that any-order models by default have to solve exponentially more masking problems than autoregressive models.",
            "score": 0.3936480076473762,
            "section_title": "A. Related works",
            "char_start_offset": 41346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 1638,
                    "end": 1659,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0087127685546875
        },
        {
            "corpus_id": "258309719",
            "title": "Exploring Compositional Visual Generation with Latent Classifier Guidance",
            "text": "Conditional generation with diffusion models relies on perturbing unconditional generation with user-specified guidance terms, namely classifier guidance [9,39,42] and classifier-free guidance [15]. Although classifier-free guidance performs competitively in image space and is sometimes more favorable than classifier guidance [15,23], we argue that using classifier guidance in latent diffusion models has its unique advantages. Regarding the underperformance of classifier guidance in the image space, one popular suspicion is that image classifiers tend to learn shortcuts from suspicious correlations. For example, a deep neural network classifier on the attribute \"old\" can be misguided by \"white hair\" and ignore its holistic features. This problem is alleviated in a compact, even disentangled latent space, if the semantic directions of 'old' and 'white hair' are orthogonal. Also, deep image classifiers are typically vulnerable to adversarial attacks, while latent classifiers with much few parameters suffer less from this problem. Another benefit is that classifiers are usually easier to train than diffusion models used in classifier-free guidance. Finally, when the classifiers are linear, classifier guidance resembles linear arithmetic methods, as we will show in Section 3.4.\n\nThe goal of conditional generation is to model the conditional distribution p(z|y) where y is the conditions or attributes. By Bayes rules p(z t |y) = p(zt)p(y|zt) /p(y), the score of the conditional probability \u2207 zt log p(z t |y) can be factorized as the unconditional score \u2207 zt log p(z t ) and the gradient flow \u2207 zt log p(y|z t ). Therefore, one simply needs an unconditional latent diffusion model and a latent classifier to model the conditional score, known as classifier guidance. In practice, the classifier guidance term is usually scaled by a factor \u03b1, such that \u2207 zt log p(z t |y) = \u2207 zt log p(z t ) + \u03b1\u2207 zt log p(y|z t ). The factor \u03b1 serves as a temperature parameter which adds another layer of controllability to the sharpness of the",
            "score": 0.3935591636317776,
            "section_title": "Conditional and Compositional Generation",
            "char_start_offset": 9897,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "14888175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "264451832",
            "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
            "text": "SEDD parameterizes a reverse discrete diffusion process using the ratios of the data distribution. These are learned using score entropy, a novel loss that is analogous to score matching for standard diffusion models (Hyv\u00e4rinen, 2005;Song & Ermon, 2019) and results in several empirical benefits: \n\n1. On core language modeling tasks, SEDD outperforms all existing language diffusion models (Li et al., 2022;Austin et al., 2021;Gulrajani & Hashimoto, 2023;He et al., 2022) by large margins and is competitive with autoregressive models of the same size (beating GPT-2 on its zero-shot perplexity tasks (Radford et al., 2019)). \n\n2. SEDD generates high quality unconditional samples and enables one to naturally trade off compute for quality. When measuring the generative perplexity (given by large models) of unconditional and un-annealed samples from similarly sized models, SEDD beats \n\nWe open source our code at github.com/louaaron/Score-Entropy-Discrete-Diffusion GPT-2 by 6-8\u00d7 and can match performance using 32\u00d7 fewer function evaluations. \n\n3. By directly parameterizing probability ratios, SEDD is highly controllable. In particular, one can prompt SEDD from arbitrary positions without specialized training. For both standard (left to right) and infilling, SEDD outperforms language diffusion models and is comparable with autoregressive models with nucleus sampling (as measured by MAUVE score (Pillutla et al., 2021)).",
            "score": 0.39339020237847633,
            "section_title": "Introduction",
            "char_start_offset": 2007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 234,
                    "matchedPaperCorpusId": "1152227"
                },
                {
                    "start": 391,
                    "end": 408,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 428,
                    "end": 456,
                    "matchedPaperCorpusId": "258967177"
                },
                {
                    "start": 456,
                    "end": 472,
                    "matchedPaperCorpusId": "254044147"
                },
                {
                    "start": 1406,
                    "end": 1429,
                    "matchedPaperCorpusId": "244488758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00894927978515625
        },
        {
            "corpus_id": "270391454",
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "score": 0.3933625054004025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "276574650",
            "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
            "text": "In recent years, large language models (LLMs) have made significant advancements in the field of natural language processing (NLP). These models, built on the Transformer architecture, exhibit powerful language understanding and generation capabilities through large-scale parameterization and extensive data training. The Transformer model, first proposed by Vaswani et al. [25], overcame the limitations of traditional recurrent neural networks (RNNs) [26] and long short-term memory networks (LSTMs) [27] in parallel processing and long-range dependency handling. Its core self-attention mechanism efficiently captures long-distance dependencies within sequences while preserving sequential order through positional encoding, laying the theoretical and technical foundation for subsequent pre-trained language models. \n\nWith the widespread adoption of the Transformer architecture, pre-trained language models have become a focal point in NLP research. BERT [28] introduced a bidirectional encoder and the masked language modeling (MLM) objective, significantly enhancing language understanding and establishing the pre-training and fine-tuning paradigm. OpenAI released the GPT series, which adopted an autoregressive language modeling approach and demonstrated exceptional performance in text generation tasks through left-toright sequential generation. Notably, GPT-3, with 175 billion parameters, emerged as the largest language model at the time, showcasing remarkable zero-shot and few-shot learning capabilities. Subsequently, models such as Google PaLM [29], Alibaba Qwen [30], and Meta LLaMA [31] demonstrated unprecedented performance in language understanding and generation tasks. \n\nThe rapid development of LLMs has revealed emergent abilities [32,33], which refer to the unexpected capabilities models exhibit upon reaching a critical parameter size. These abilities encompass tasks such as language generation [34], complex reasoning [33], multilingual translation [35], and code generation [36]. Contextual learning [37] has become a hallmark of LLMs, enabling them to quickly adapt to new tasks with minimal contextual samples, thereby reducing the need for extensive labeled data. Instruction following allows models to execute complex tasks based on explicit natural language instructions, significantly improving capabilities from text generation to multi-turn dialogue generation.",
            "score": 0.39294469177466534,
            "section_title": "The development of large language models",
            "char_start_offset": 5714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "2763403"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1583,
                    "end": 1587,
                    "matchedPaperCorpusId": "274947736"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "274506286"
                },
                {
                    "start": 1764,
                    "end": 1767,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1928,
                    "end": 1932,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 2035,
                    "end": 2039,
                    "matchedPaperCorpusId": "261531236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0200958251953125
        },
        {
            "corpus_id": "276567002",
            "title": "Generative artificial intelligence: a historical perspective",
            "text": "Autoregressive models [5 ] constitute another type of explicit modeling approach, particularly suited for data consisting of sequential elements. The generation of elements follows a one-by-one approach, where the probability distribution of each element is estimated based on the previously generated values. Given the sequential nature of language and speech, autoregressive models appeared for their generation [30 ] in the 1980s. In the 1990s, autoregressive neural networks capable of processing and generating sequential data were subsequently proposed [31 ]. More recently, autoregressive generation approaches [8 ] have been extended to large-scale neural networks, paving the way for the emergence of foundation models such as GPTs [9 ]. \n\nOther studies have explored approaches to implicit data modeling. For example, normalizing flows [32 ] use a series of invertible transformations, i.e. flows, to convert a prior distribution into a complex data distribution. These methods do not explicitly estimate the data distribution, but instead present a probability density function for data generation. When stochastic transformations are applied, the generation process is considered to be the evolution of a diffusion model. The concept of the diffusion model was introduced by Jarzynski [6 ] in a study of non-equilibrium systems. Stein et al. [33 ] proposed a probabilistic approach to learn the diffusion process using a parameterized model. It has been demonstrated theoretically [34 ,35 ] that flowbased models and diffusion-based models can be amalgamated into a collection of differential equations [36 ]. Although the two types of approaches were not mainstream at that time, their successors, probabilistic diffusion models [37 ,38 ] and flow matching [39 ], gained significant prominence in the deep learning era. \n\nIn addition, another group of approaches leverages artificial neural networks for generative modeling. The term 'artificial neural network' originates from studies of nerve cells [40 ], while its mathematical inception is rooted in hierarchical linear regression [41 ,42 ]. In subsequent developments, artificial neural networks have advanced through the integration of non-linear transformations and the design of complex architectures, now known as deep learning. The basic computational unit in these networks is a neuron [43 ]; neurons connect to each other through nonlinear activation functions.",
            "score": 0.392855330805356,
            "section_title": "Statistical machine learning models",
            "char_start_offset": 11622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2436
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 26,
                    "matchedPaperCorpusId": "122033085"
                },
                {
                    "start": 559,
                    "end": 564,
                    "matchedPaperCorpusId": "13492169"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 846,
                    "end": 851,
                    "matchedPaperCorpusId": "118943322"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "119101580"
                },
                {
                    "start": 1354,
                    "end": 1359,
                    "matchedPaperCorpusId": "53492374"
                },
                {
                    "start": 1615,
                    "end": 1620,
                    "matchedPaperCorpusId": "7341613"
                },
                {
                    "start": 1770,
                    "end": 1775,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 2014,
                    "end": 2019,
                    "matchedPaperCorpusId": "15619658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006313323974609375
        },
        {
            "corpus_id": "254044531",
            "title": "Continuous diffusion for categorical data",
            "text": "We have proposed CDCD, a framework for diffusion of categorical data that is continuous both in time and input space, which enables training of nonautoregressive language models with a procedure reminiscent of BERT (see \u00a74.5). \n\nOur proposed approach has some advantages over autoregressive models, such as the ability to perform arbitrary infilling15 and a flexible sampling procedure which allows trading off sample quality and compute requirements. It also stands to benefit from diffusion model enhancements such as classifier-free guidance and improved sampling algorithms, and from the ability to deterministically map inputs to latents and vice versa (which we have not explored so far). Some limitations compared to autoregressive models were discussed in \u00a72.3. An important research question which we have not yet investigated, is how best to handle variable-length output. With autoregression, this is very naturally handled by introducing an endof-sentence (EOS) token, which the model can predict to indicate that sampling should be halted. Diffusion models on the other hand require a fixed-size 'canvas' which is iteratively refined during sampling, although inserting padding tokens at random during training (and collapsing them after sampling) could allow for some length variation (Strudel et al., 2022), and it may also reduce the incidence of repeated tokens in  Generative models of text are very versatile: they can be used to interpret texts in various formats including newspapers, magazine articles, bibliographies, scientific journals, etc. Thus, text models (e.g. Matlab, text theory) can extract semantic information from the textual data in a versatile way, so that they can be used to infer the structure and logic of the textual processing processes. \n\nTo be or not to be? That is the question. Whether it is a religion, science, art, philosophy, economics, history, sociology, politics or business, it needs to be about the right questions. We say that this question is not the question of absolute answers but also possible answers. is this question the existence of knowledge? \n\nTo be or not to be? That is the question. Whether it is the meaning of art, the meaning of wine, or the meaning of life.",
            "score": 0.39271681199187286,
            "section_title": "Discussion",
            "char_start_offset": 55242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2110
                },
                {
                    "start": 2113,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "276155289",
            "title": "Integrating Natural Language Processing With Vision Transformer for Landscape Character Identification",
            "text": "In parallel, Hochreiter et al. [21], [22] proposed recurrent neural networks (RNNs) and long shortterm memory (LSTM) networks, which can model long-distance dependencies in text sequences. Sutskever et al. [23] developed the Seq2Seq model based on an encoder-decoder architecture, which is especially useful for machine translation tasks. Bahdanau et al. [24] further improved this approach by incorporating the attention mechanism, allowing the model to focus on key information in the input, achieving breakthroughs in tasks such as machine translation. \n\nIn recent years, the introduction of the transformer architecture has fundamentally changed the trajectory of NLP development. The transformer utilizes self-attention (SA) mechanisms to overcome the limitations of RNNs and LSTMs when processing long texts and enables efficient parallel processing of data. Building on the transformer architecture, the BERT model [25] employs bidirectional training to pretrain powerful contextual representations of text. This bidirectional training enables BERT to better understand the context of each word in a sentence, significantly improving performance on a wide range of downstream tasks. Similarly, the GPT model [26], [27], which follows an autoregressive approach, excels in text generation tasks by generating coherent and contextually relevant text from given prompts. Furthermore, the T5 model [28] adopts a unified text format to handle a variety of NLP tasks, achieving excellent performance across multiple tasks. Benefiting from the transformer, NLP has started to integrate with other modalities, such as images and speech, driving the rise of multimodal learning. For examples, multimodal models like CLIP [29] and DALL\u2022 E [30] combine natural language with images for cross-modal understanding and generation. Based on such multimodal learning, NLP has successfully extended beyond traditional text-based tasks, finding applications in diverse fields such as image classification [29], target tracking [31], instance segmentation [32], and object detection [33]. However, how to apply NLP to the field of LCI has yet to be thoroughly investigated.",
            "score": 0.3923730571776032,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 6459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1215,
                    "end": 1219,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1401,
                    "end": 1405,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1994,
                    "end": 1998,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 2016,
                    "end": 2020,
                    "matchedPaperCorpusId": "232427825"
                },
                {
                    "start": 2044,
                    "end": 2048,
                    "matchedPaperCorpusId": "259165099"
                },
                {
                    "start": 2071,
                    "end": 2075,
                    "matchedPaperCorpusId": "218889832"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0225830078125
        },
        {
            "corpus_id": "275920653",
            "title": "Large Language Models to Diffusion Finetuning",
            "text": "We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.",
            "score": 0.39170103449999655,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03460693359375
        },
        {
            "corpus_id": "259237203",
            "title": "Empowering Short Answer Grading: Integrating Transformer-Based Embeddings and BI-LSTM Network",
            "text": "The field of natural language processing (NLP) merges computer science, linguistics, and machine learning to teach computers to interpret human language similar to humans. Progress in machine learning, specifically through deep learning methods, has significantly improved NLP. NLP encompasses the following two primary categories: natural language understanding (NLU), where computers can accurately comprehend human language, and natural language generation (NLG), where computers generate natural language. NLP has a wide range of applications, encompassing various tasks such as short answer grading, essay scoring, machine translation, OCR post-correction, metadata extraction, topic detection and tracking, question answering, and chatbots. NLP is a challenging field due to the various natural languages with unique syntactic rules, which often lead to ambiguous meanings that vary depending on the context. Ambiguity is a prevalent issue in NLP and refers to words and phrases with multiple possible interpretations. In response to the COVID-19 pandemic, the use of technology in the learning process has increased, with NLP being utilized in all academic institutions to facilitate the grading of assignments, quizzes, and exams. In addition, the significance of research in this automatic short answer grading domain is anticipated to grow in the future, as the development of such methodologies will expedite the assessment of student responses through automated means, thereby eliminating the need for human evaluators. Furthermore, this approach has the potential to address the issue of discrepancies in grading due to varying perspectives, thus enhancing the consistency and reliability of the evaluation process. Deeper learningbased methodologies, including transformer models, have been increasingly utilized in the development of ASAG systems. In our model, we utilize transformer models such as T5 and BERT in the embedding phase to convert students' answers and model answers into numerical vectors. Transformers excel at capturing contextual relationships and encoding semantic information. We move beyond older methods such as one-hot encoding and leverage the advanced representation learning capabilities of transformers to generate high-quality embedding vectors. In the training phase, we introduce a BI-LSTM as the neural network architecture for learning and predicting grades. This complements the capabilities of transformers by capturing long-term dependencies and understanding the coherence and flow of answers.",
            "score": 0.39123683670194065,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2289
                },
                {
                    "start": 2290,
                    "end": 2406
                },
                {
                    "start": 2407,
                    "end": 2545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0216217041015625
        },
        {
            "corpus_id": "237194902",
            "title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters",
            "text": "low performance on hold-out data per se, it limits the possibilities for model evaluation and demands a certain level of trust from the end user. In many cases, adapting the model to the target domain is not possible or requires some technical proficiency. In addition, these models are often trained once-and-for-all while our framework allows for an interactive approach to evaluate model performance and offers the rich variety of pretrained adapters being available from the community-driven AdapterHub.\n\nFurther, we argue that advancements in NLP research should be made available to the researchers most profiting from them as soon as possible -not only for the sake of accelerating research outside of NLP but also to enable a feedback loop informing NLP researchers about the shortcomings of such models. While the generalization capabilities of state-of-the-art language models are subject to increased scrutiny within NLP (Sanchez et al., 2018;Gururangan et al., 2020;Tu et al., 2020), the datasets and tasks to test them often originate from within the same community, thereby introducing a selection bias (Ramponi and Plank, 2020). By enabling interdisciplinary researchers to evaluate NLP models without the technical barriers involved, we are able to gain more insights about the robustness and out-of-domain performance of these models. Our application is a first step into this direction.",
            "score": 0.3906223188140045,
            "section_title": "Broader Impact Statement",
            "char_start_offset": 24568,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 954,
                    "matchedPaperCorpusId": "21677829"
                },
                {
                    "start": 1117,
                    "end": 1142,
                    "matchedPaperCorpusId": "219176513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0141143798828125
        },
        {
            "corpus_id": "272969309",
            "title": "Using Deep Autoregressive Models as Causal Inference Engines",
            "text": "Previous works have explored using autoregressive models for inferring causal effects. Here, we give a brief overview of the studies. \n\nSequencification for statistical engines. Various machine learning disciplines have employed linearized observations for their tasks. In natural language processing (NLP), linearization (which we call sequencification) has been used to encode a syntactic tree into a sequence for building a language model-based parser [Vinyals et al., 2015, Liu et al., 2022, Sheng et al., 2023]. In reinforcement learning (RL), an episode can be encoded into a sequence of states, actions, and reward variables. An autoregressive model is then trained on these sequences to capture relationships among the variables [Chen et al., 2021, Janner et al., 2021]. These instances suggest that AR models trained on sequencified data adeptly learn statistical dependencies among multiple variables. \n\nLanguage models as causal engines. One challenging aspect of CI in high-dimensional space is satisfying positivity constraints [D'Amour et al., 2021, Tu and Li, 2022, Egami et al., 2022, Gui and Veitch, 2022]. Despite this limitation, previous studies use techniques from NLP, such as topic models [Sridhar andGetoor, 2019, Mozer et al., 2020], latent variable models [Keith et al., 2020], and contextual embeddings [Veitch et al., 2019[Veitch et al., , 2020]], to produce low-dimensional embeddings that may satisfy positivity. \n\nBesides applying NLP techniques to reduce the dimensionality of observations, we can use natural language as a proxy for observed confounders. For instance, Roberts et al. [2020] applies a textmatching algorithm using contextual embeddings and topic models to estimate causal effects given a proxy text. Furthermore, with sufficiently large models and text corpuses, LLMs can generalize to unseen knowledge [Chowdhery et al., 2023]. Hence, we rely on pre-trained LLMs to take advantage of the prior model knowledge and train better statistical inference engines.",
            "score": 0.3904365262339755,
            "section_title": "Related work",
            "char_start_offset": 6174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 476,
                    "matchedPaperCorpusId": "14223"
                },
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "253116993"
                },
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "266176017"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "235313679"
                },
                {
                    "start": 1041,
                    "end": 1062,
                    "matchedPaperCorpusId": "88517092"
                },
                {
                    "start": 1079,
                    "end": 1099,
                    "matchedPaperCorpusId": "3613569"
                },
                {
                    "start": 1350,
                    "end": 1374,
                    "matchedPaperCorpusId": "220793347"
                },
                {
                    "start": 1602,
                    "end": 1623,
                    "matchedPaperCorpusId": "3538372"
                },
                {
                    "start": 1852,
                    "end": 1876,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00803375244140625
        },
        {
            "corpus_id": "270737586",
            "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
            "text": "Filtering and annotating textual data are routine tasks in many areas, like social media or news analytics. Automating these tasks allows to scale the analyses wrt. speed and breadth of content covered and decreases the manual effort required. Due to technical advancements in Natural Language Processing, specifically the success of large foundation models, a new tool for automating such annotation processes by using a text-to-text interface given written guidelines without providing training samples has become available. In this work, we assess these advancements in-the-wild by empirically testing them in an annotation task on German Twitter data about social and political European crises. We compare the prompt-based results with our human annotation and preceding classification approaches, including Naive Bayes and a BERT-based fine-tuning/domain adaptation pipeline. Our results show that the prompt-based approach - despite being limited by local computation resources during the model selection - is comparable with the fine-tuned BERT but without any annotated training data. Our findings emphasize the ongoing paradigm shift in the NLP landscape, i.e., the unification of downstream tasks and elimination of the need for pre-labeled training data.",
            "score": 0.39026516289379376,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03662109375
        },
        {
            "corpus_id": "277043181",
            "title": "A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis",
            "text": "ARLMs is a statistical approach to NLP for text generation. As the name \"auto-regressive\" suggests it is based on predicting one word at a time, with previously generated words serving as context. Hence the model relies on its own past outputs to inform future predictions. For example, given the output of the model \"I like,\" the next word in the output should be either \"cats\" or \"chocolate\" since probabilities have been learned on the training data. Several NLP tasks were proven to outperform traditional approaches with auto-regressive language models. \n\nIn question-answering and text-completion tasks, GPT-3 has already set benchmarks showing that ALM can indeed serve as a strong pretext for learning robust language representations. Autoregressive models, like GPT-3, are widely recognized for achieving state-of-the-art performance across numerous NLP benchmarks; therefore, many discussions surrounding architecture and training methodologies have led to such a position. \n\nWhile MLM used in BERT focuses on masked word prediction for a sentence with both left and right contexts, ALM uses a strict left-to-right approach. This distinction makes ALM better suited to generative tasks where sequential coherence is crucial (Liao et al., 2020). It can predict words based on prior context, which helps make a subtle language structure and semantics and can be used for several tasks other than text generation like summarization and sentiment analysis.",
            "score": 0.3900696768713991,
            "section_title": "Auto-regressive Language Modeling (ARLMs)",
            "char_start_offset": 29549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1462
                }
            ],
            "ref_mentions": [
                {
                    "start": 1234,
                    "end": 1253,
                    "matchedPaperCorpusId": "216144416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121917724609375
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "Nonetheless, there is still merit in exploring architectures that incorporate inductive biases tailored to visual signals. VAR (Tian et al., 2024) utilizes hierarchical multi-scale tokenization to capture visual structures better, while other approaches (Chang et al., 2022;Xie et al., 2024b) leverage masked image modeling strategies to enhance performance. Additionally, recent works (Zhou et al., 2024;Xie et al., 2024b) highlight the advantages of employing bidirectional attention over unidirectional ones for processing visual signals. Exploring how inductive biases can be integrated into autoregressive models, and their potential impacts on scalability and multimodal fusion, presents a challenging yet promising area for future research. \n\nDownstream task. Current research on downstream tasks for visual autoregressive models largely focuses on zero-shot capabilities in a limited range of tasks or on designing models specifically tailored for individual downstream applications. In this respect, visual autoregressive models lag behind diffusion and language models. A critical open question remains whether visual autoregressive models can follow the trajectory of language models, which quickly adapt to new tasks via techniques such as prompt tuning and instruction tuning, or attain the versatility of diffusion models, which excel across various visual tasks including structure control, style transfer, and image editing. As research on autoregressive models advances, developing a unified autoregressive model that could adapt to downstream tasks is a primary focus in future work.",
            "score": 0.3895861841020124,
            "section_title": "Challenges and Future Work",
            "char_start_offset": 100420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0119171142578125
        },
        {
            "corpus_id": "273661614",
            "title": "Energy-Based Diffusion Language Models for Text Generation",
            "text": "We conclude that this phenomenon is due to the fact that during the early stage of sampling the diffusion model is prone to make more errors in independent x 0 prediction, since there is little information on the full sequence. This encourages us to explore sampling with a short time window w for higher efficiency, which we discuss in detail in section 5.3. Detailed pseudo code for the sampling procedure is provided in Algorithm 1. In practice, we observe that a relatively small importance sampling size can already yield significant performance gain, though theoretically we would only recover exact samples from the joint model distribution asymptotically when the number of importance samples goes to infinity. Diffusion Language Model (MDLM) (Sahoo et al., 2024) and MD4 (Shi et al., 2024). We notice other recent works such as discrete flow matching (Campbell et al., 2024;Gat et al., 2024), but they are either not open-sourced or focus on non-language settings. Therefore, we take their concurrent work with similar formulation MDLM (Sahoo et al., 2024) and MD4 (Shi et al., 2024) as the representative for comparison. On the small-scale Text8 benchmark, we additionally evaluate other discrete generative models including Plaid (Gulrajani & Hashimoto, 2024), Bayesian Flow Network (Graves et al., 2023), Any-order Autoregressive Models ARDM (Hoogeboom et al., 2022) and MAC (Shih et al., 2022), and flow-based methods IAF/SCF (Ziegler & Rush, 2019), AR Argmax Flow (Hoogeboom et al., 2021), Discrete Flow (Tran et al., 2019), and Multinomial Diffusion (Hoogeboom et al., 2021). Note that, these methods are not scaled up yet to the large dictionary and long sequence length, and thus we only involve them in the small-scale benchmark. Metrics.",
            "score": 0.3894600590766953,
            "section_title": "EFFICIENT GENERATION VIA IMPORTANCE SAMPLING",
            "char_start_offset": 23210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 860,
                    "end": 883,
                    "matchedPaperCorpusId": "267523194"
                },
                {
                    "start": 1241,
                    "end": 1270,
                    "matchedPaperCorpusId": "258967177"
                },
                {
                    "start": 1387,
                    "end": 1406,
                    "matchedPaperCorpusId": "249152160"
                },
                {
                    "start": 1439,
                    "end": 1461,
                    "matchedPaperCorpusId": "59413778"
                },
                {
                    "start": 1478,
                    "end": 1502,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 1518,
                    "end": 1537,
                    "matchedPaperCorpusId": "157062310"
                },
                {
                    "start": 1565,
                    "end": 1589,
                    "matchedPaperCorpusId": "235262511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007289886474609375
        },
        {
            "corpus_id": "274281285",
            "title": "KL-geodesics flow matching with a novel sampling scheme",
            "text": "Non-autoregressive language models generate all tokens simultaneously, offering potential speed advantages over traditional autoregressive models, but they face challenges in modeling the complex dependencies inherent in text data. In this work, we investigate a conditional flow matching approach for text generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional simplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which correspond to linear interpolation in logit space. We provide a theoretical justification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid x_t, t)\\) yields the exact flow matching velocity under logit interpolation. To address the suboptimal performance of basic inference, we propose a novel empirical sampling scheme that iteratively samples from the conditional distribution and introduces additional noise, significantly improving results despite lacking full theoretical underpinnings. Furthermore, we propose a hybrid inference method that combines the basic approach with the sampling scheme. This method demonstrates superior performance on both conditional and unconditional text generation experiments compared to previous SOTA method for discrete flow matching.",
            "score": 0.38944901727853704,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007965087890625
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "Large-scale pre-trained language models (PLMs) based on transformers represented by GPT (decoder-only model), BERT (encoder-only model), and T5 (encoder-decoder model) provide a strong foundation for natural language processing tasks.Among the articles published in recent years, publications based on pre-training have occupied the mainstream position, hence this survey examines the similarities and differences between PLMs and diffuison models.\n\nThrough deep learning training on a large-scale corpus, a pre-trained model can not only learn richer and more targeted semantic information, but also understand the grammar and context of natural language, and generate coherent and logical text.PLMs have shown impressive results in many NLP domains and applications.Their training process can be divided into: (1) Pre-training: PLMs first train a general and large-scale language model on large-scale text, which contains rich contextual semantic information;\n\n(2) Fine-tuning: according to different downstream tasks, the pre-training model performs discriminative learning on labeled data.\n\nWhen comparing autoregressive and diffusion models, it is imperative to balance their merits and drawbacks in terms of generation speed, diversity, and other relevant factors.This consideration facilitates a judicious selection based on specific application scenarios and task requirements.In this survey, we compare PLMs and diffusion-based text generation models across the following four dimensions, as shown in Table 3 below.",
            "score": 0.38905645338654005,
            "section_title": "COMPARISON BETWEEN TEXT DIFFUSION MODELS AND PLMS",
            "char_start_offset": 23421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 448
                },
                {
                    "start": 450,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 961
                },
                {
                    "start": 963,
                    "end": 1093
                },
                {
                    "start": 1095,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01132965087890625
        },
        {
            "corpus_id": "272827362",
            "title": "Will Large Language Models be a Panacea to Autonomous Driving?",
            "text": "Modern language processing models originated from two paradigms: rule-based and statistical. Rule-based language models rely on manually defined grammar, semantics, and pragmatics rules to generate natural language using a set of handcrafted rules. The limitation of this approach lies in the need for a significant number of manually set rules, making it challenging to cover all scenarios in real language corpora. Statistical language models, on the other hand, depend on vocabulary statistical distributions in large corpora, such as n-gram models that predict text using n words in context. These models marked the beginning of rationalist approaches in language processing. \n\nWith the advancement of deep learning, researchers began using neural network models to learn the complex structures and semantic information of natural language. RNNs became a classical model framework for handling natural language and other time-series problems. By introducing a recurrent structure and weight parameters shared across time steps, RNNs can handle natural language text sequences of different lengths and capture hidden features. LSTM networks [36,37], an improvement over traditional RNNs, introduced memory cells, selectively retaining or forgetting information to effectively address issues like gradient vanishing or exploding. Some works building upon this foundation include the Gated Recurrent Unit (GRU) [38]. In 2017, the Transformer [39], relying on its powerful contextual processing ability, gradually started being applied to NLP tasks. Compared to previous statistical models, these neural network-based models share better approaches to handle representations and features within language, thereby showing better performances in specific NLP tasks. \n\nWith the advancement of network structures, a new training method for language models, pre-training, also contribute to further evolvement of language models. In 2018, Google introduced BERT (Bidirectional Encoder Representations from \n\nTransformers) [40], a pre-trained language model based on attention mechanisms. It adopts a two-stage strategy, initially undergoing unsupervised pre-training on a large-scale corpus and then fine-tuning with supervised learning on specific tasks to adapt to downstream tasks. The first generative pre-trained model, GPT-1 [41], was also released by OpenAI in 2018.",
            "score": 0.38899186860664736,
            "section_title": "Development of Large Language Models",
            "char_start_offset": 20157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2000
                },
                {
                    "start": 2003,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2279
                },
                {
                    "start": 2280,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 1148,
                    "end": 1151,
                    "matchedPaperCorpusId": "1915014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0141143798828125
        },
        {
            "corpus_id": "269032939",
            "title": "Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution",
            "text": "If alternative models were considered for training our legal language model, the outcome could vary based on the specific characteristics and capabilities of each model.Here's a hypothetical overview of potential outcomes if alternative models were chosen:\n\nBERT (Bidirectional Encoder Representations from Transformers): BERT is another popular pre-trained language model [14] known for its bidirectional contextual representation learning.While BERT offers strong performance on a wide range of NLP tasks, its unidirectional nature may limit its ability to capture long-range dependencies and syntactic structures present in legal texts.As a result, the model may struggle to effectively process complex legal language, potentially leading to suboptimal performance on tasks requiring deep semantic understanding.\n\nGPT (Generative Pre-trained Transformer): GPT models, including GPT-3, are renowned for their generative capabilities [13] and contextual understanding of text.However, their autoregressive nature and lack of bidirectional context may pose challenges for tasks requiring comprehensive comprehension and analysis of legal documents.While GPT models excel in text generation and completion tasks, they may not be as well-suited for tasks such as legal text summarization or sentiment analysis, where a nuanced understanding of legal concepts is essential.\n\nXLNet: XLNet is a transformer-based language model that leverages permutation-based training to capture bidirectional context while maintaining the advantages of autoregressive modeling.XLNet's innovative training scheme enables it to achieve strong performance on various NLP benchmarks, including tasks requiring deep contextual understanding.If XLNet were chosen for training our legal language model, it could potentially offer comparable performance to the Google/Flan-T5-XXL model, particularly in tasks involving complex syntactic and semantic analysis of legal texts.\n\nwhile alternative models such as BERT, GPT, and XLNet offer unique strengths and capabilities, the choice of the Google/Flan-T5-XXL model for our research was informed by its advanced architecture, fine-tuning capabilities, community support, and proven performance in processing and analyzing natural language text, including legal documents like the Indian Constitution.\n\nTraining the Google Flan-T5 XXL model involves several mathematical concepts and computations.",
            "score": 0.38877889155121725,
            "section_title": "Alternative Models",
            "char_start_offset": 14582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 256
                },
                {
                    "start": 258,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 815
                },
                {
                    "start": 817,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1148
                },
                {
                    "start": 1148,
                    "end": 1370
                },
                {
                    "start": 1372,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1947
                },
                {
                    "start": 1949,
                    "end": 2321
                },
                {
                    "start": 2323,
                    "end": 2417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0136871337890625
        },
        {
            "corpus_id": "269050904",
            "title": "TEXT GENERATION MODELS FOR PARAPHRASE ON KAZAKH LANGUAGE",
            "text": "This study delves\u00a0into the relatively unexplored domain of natural language processing for the Kazakh language\u2014a language with limited computational resources. The paper dissects the effectiveness of diffusion models and transformers in generating text, specifically paraphrases, which is a critical aspect of machine learning applications such as chatbots, virtual assistants, and automated translation services. \nThe researchers methodically adapt these advanced models to understand and generate Kazakh text, tackling the unique challenges posed by the language's complex morphology. The paper is comprehensive in its approach, covering everything from the initial adaptation of the models to the Kazakh language context, to the creation of specialized tokenizer tools, to the translation and preparation of datasets for effective training. \nThrough rigorous testing and performance analysis, the study identifies the strengths and weaknesses of each model type. This is critical as it informs the direction of future research and model development, with the goal of enhancing the fluency and accuracy of automated Kazakh text generation. The paper also discusses the broader impact of its findings, suggesting that the methodologies and insights gained could inform similar efforts in other low-resource languages, thereby contributing to the global field of NLP. \nThe research concludes with reflections on the implications of their findings for the ongoing development of machine learning technologies, asserting the potential of these technologies to accommodate the intricacies of any language, given the right approach and resources. This work not only advances the technical capabilities for Kazakh text generation but also serves as a testament to the potential of machine learning to bridge language gaps and foster greater digital inclusivity.",
            "score": 0.38871903349187303,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.031494140625
        },
        {
            "corpus_id": "270123534",
            "title": "Preference Alignment with Flow Matching",
            "text": "In conclusion, this research introduces Preference Flow Matching (PFM), a novel add-on approach that offers a practical, efficient, and scalable solution for integrating human preferences. This research highlights the potential of flow matching as a powerful tool for preference alignment and opens new avenues for further exploration and development in the field of RLHF. The ability to align black-box models with human preferences without extensive model modifications marks a critical step forward, with broad implications for the deployment and usability of AI systems in real-world applications. \n\nOur theoretical and empirical analyses demonstrate that PFM achieves alignment performance comparable to standard RLHF methods while being more resilient to preference overfitting. The iterative flow matching technique further enhances alignment quality, by continually refining the preference alignment without modifying the underlying pre-trained model parameters. \n\nDespite these promising results, the current design of the PFM framework entails several challenges in the natural language processing (NLP) domain. The PFM framework, as currently designed, relies on the autoencoder to work with fixed-sized embeddings to handle variable-length texts. To scale our method to more complex NLP tasks, future research should explore ways to adapt the PFM framework to long-form texts. \n\nmodel using p 0 as source. It is also worth noting that the flow matching model (with true p 0 ) achieves better performance even compared to the planning method, that explicitly uses a trajectory with higher reward. This suggests that using a flow matching for preference alignment can provide better alignment than simply conducting an exhaustive search.",
            "score": 0.3887059150180646,
            "section_title": "Conclusion and Limitations",
            "char_start_offset": 28817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0257568359375
        },
        {
            "corpus_id": "257102873",
            "title": "Language Model Crossover: Variation through Few-Shot Prompting",
            "text": "A recent paradigm in ML is to train increasingly large models on internet-scale data, e.g. BERT and GPT-3 on text [13,32], or DALL-E and stable diffusion on captioned images [93,95]. Such models are sometimes called foundation models [9], as they provide a broad foundation from which they can be specialized to many specific domains (e.g. \n\nwith supervised fine-tuning (i.e., further training on a domain-specific dataset) or prompt-engineering). Foundation models have enabled a vibrant ecosystem of specialized models [120] that can be combined in a plug-and-play way (e.g. models that measure sentiment of text [14], summarize text [111], write code [83], rank the aesthetics of images [30,56,103], and create high-dimensional embeddings of text or images [94,132]. One contribution of this paper is to demonstrate how evolutionary methods can easily leverage this growing ecosystem to evolve high-quality artifacts in diverse applications. \n\nOne particularly exciting class of foundation models are pre-trained language models (LMs) that model the distribution of text. While early LMs used markov chains [107] or recurrent neural networks [40], more recently the transformer architecture [118] has enabled significant progress in NLP. Let  be a vocabulary of text tokens, e.g., words or other atomic pieces of text. Then,  * is the set of strings made up of tokens from  . Given an input string  1  2 . . .   in \u2208  * , a large autoregressive transformer-based LM (LLM) probabilistically generates an output string: \n\nwhere   in + are all sampled autoregressively: \n\nwhere LLM  is the softmax distribution over  induced by a single forward pass through the transformer model. \n\nThe method in this paper focuses on one emergent capability of LLMs: the potential to learn from text examples provided as input to the model when generating an output, which is called in-context learning or few-shot prompting [13,119]. For example, including input-output examples of a text classification task in a prompt will improve an LLM's performance at that task.",
            "score": 0.38848167892419766,
            "section_title": "Foundation Models",
            "char_start_offset": 6044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 636,
                    "end": 641,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "206487032"
                },
                {
                    "start": 694,
                    "end": 697,
                    "matchedPaperCorpusId": "1577831"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 764,
                    "end": 768,
                    "matchedPaperCorpusId": "248512473"
                },
                {
                    "start": 1110,
                    "end": 1115,
                    "matchedPaperCorpusId": "5747983"
                },
                {
                    "start": 1910,
                    "end": 1914,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0243377685546875
        },
        {
            "corpus_id": "274769123",
            "title": "Future of Natural Language Processing (Human Like Conversation)",
            "text": "Natural language processing (NLP) has revolutionized artificial intelligence by solving the challenge of enabling machines to understand, interpret, and reproduce human language. The importance of NLP lies in its applications in many areas, including medicine, education, business, and media. This study explores the basic concepts, methods, and developments in NLP and aims to provide a general introduction to the field. The study first outlines the history of NLP and traces its evolution from a policy-based approach to modern deep learning. Fundamental techniques such as tokenization, parsing, sentiment analysis, and language modeling are summarized to illustrate the diversity of available tools. Recent innovations, particularly Transformer-based architectures such as BERT and GPT, are important for demonstrating their performance in tasks such as translation, writing, and conversational AI. The paper concludes by highlighting the strengths and versatility of NLP and discussing current issues such as biases in language structure and the development of multilingualism. This summary provides valuable guidance for researchers and practitioners who wish to understand and contribute to the rapidly evolving nature of natural language processing.",
            "score": 0.3880313882270611,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01473236083984375
        },
        {
            "corpus_id": "266551068",
            "title": "Semantic Guidance Tuning for Text-To-Image Diffusion Models",
            "text": "Meanwhile, when the model fails to accurately render a specific concept, the similarity score for that concept fluctuates and generally remains lower, highlighting the direct impact of concept representation on the quality of the generated image. \n\nConcept Guidance Based on the observation that the cosine similarity k is associated with semantic alignment, we propose a concept guidance term \u03d5 that modulates the noise estimate of the diffusion model based on the similarity. Formally, extended to Eq. (2), we compute, \u03f5 \u03b8 (z t ) + w g s p + \u03d5(z t , S c , s p ), (8) in which S c = {s a , s 1 s , s 2 s , ..., s n s }. The concept guidance \u03d5 is defined as, \n\nwhere w c is the concept guidance scale and s c \u2208 S c . The \u03b3 is a delta function based on the cosine similarity between s p and s c , \n\nThe threshold \u03b7 is a hyperparameter that corresponds to the minimum inclusion of a concept in prompt score. Naturally, larger \u03b7 increases the effect of concept guidance. We empirically found that \u03b7 = 1/(n + 1) generally works well. The guidance direction \u03c8(s p , s c ) is determined as, \n\nFor subject concepts, a lower cosine similarity suggests a divergence of it from the prompt score. In such cases, our approach involves steering the model's guidance towards these concepts to ensure their inclusion. Conversely, the abstract concept is understood as an independent component of the main score that is distinct from the subject concepts. Therefore, a low cosine similarity here indicates that the main score closely resembles the aggregate of subject concepts, potentially leading to a fusion of concepts in the generated results. To counteract this, when the cosine similarity for the abstract concept decreases, we adjust the guidance so that k(s p , s s ) > \u03b7. \n\nNote that our method doesn't need extra training or optimization and can be used with any diffusion model that uses classifier-free guidance [11].",
            "score": 0.3876258272559843,
            "section_title": "Concept Extraction",
            "char_start_offset": 13472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "247628171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "265051037",
            "title": "Holistic Evaluation of Text-To-Image Models",
            "text": "Models using prompt engineering techniques produce images that are more visually appealing. Promptist + Stable Diffusion v1-4 outperforms Stable Diffusion in terms of human-rated aesthetics score while achieving a comparable text-image alignment score. 14  These findings emphasize the importance of using human ratings for evaluating image generation models in future research. 17. Diffusion vs autoregressive models. Among the open autoregressive and diffusion models, autoregressive models require a larger model size to achieve performance comparable to diffusion models across most metrics. Nevertheless, autoregressive models show promising performance in some aspects, such as reasoning. Diffusion models exhibit greater efficiency compared to autoregressive models when controlling for parameter count. [20,56,57,58]. In particular, in natural language processing (NLP), the adoption of meta-benchmarks [59,60,61,62] and holistic evaluation [1] across multiple scenarios or tasks has allowed for comprehensive assessments of models and accelerated model improvements. However, despite the growing popularity of image generation and the increasing number of models being developed, a holistic evaluation of these models has been lacking. Furthermore, image generation encompasses various technological and societal impacts, including alignment, quality, originality, toxicity, bias, and fairness, which necessitate comprehensive evaluation. Our work fills this gap by conducting a holistic evaluation of image generation models across 12 important aspects. \n\nBenchmarks for image generation. Existing benchmarks primarily focus on assessing image quality and alignment, using automated metrics [23,36,24,63,64,65]. Widely used benchmarks such as MS-COCO [21] and ImageNet [20] have been employed to evaluate the quality and alignment of generated images. Metrics like Fr\u00e9chet Inception Distance (FID) [23,66], Inception Score [36], and CLIPScore [24] are commonly used for quantitative assessment of image quality and alignment. \n\nTo better capture human perception in image evaluation, crowdsourced human evaluation has been explored in recent years [25,6,35,67]. However, these evaluations have been limited to assessing aspects such as alignment and quality. Building upon these crowdsourcing techniques, we extend the evaluation to include additional aspects such as aesthetics, originality, reasoning, and fairness.",
            "score": 0.3872102711136516,
            "section_title": "7.",
            "char_start_offset": 23237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 815,
                    "end": 818,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "229156320"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "1687220"
                },
                {
                    "start": 1711,
                    "end": 1714,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 1717,
                    "end": 1720,
                    "matchedPaperCorpusId": "118648975"
                },
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1779,
                    "end": 1783,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "249879327"
                },
                {
                    "start": 1933,
                    "end": 1937,
                    "matchedPaperCorpusId": "1687220"
                },
                {
                    "start": 2158,
                    "end": 2162,
                    "matchedPaperCorpusId": "165163969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01238250732421875
        },
        {
            "corpus_id": "253370609",
            "title": "A Comparison of SVM against Pre-trained Language Models (PLMs) for Text Classification Tasks",
            "text": "Text classification is the task of classifying text (e.g., tweets, news, and customer reviews) into different categories (i.e., tags). It is a challenging task especially when the text is 'technical'. We define 'technical' text in terms of the vocabulary used to describe a given document, e.g., classifying health records, human genomics, IT discussion forums, etc. These kinds of documents require special pre-processing since the basic NLP pre-processing steps may remove critical words necessary for correct classification, resulting in a performance drop of the deployed system [1]. \n\nRecently, pre-trained language models (PLMs) such as BERT [2] and ELMO [3] have shown promising results in several NLP tasks, including spam filtering, sentiment analysis, and question answering. In comparison to traditional models, PLMs require less feature engineering and minimal effort in data cleaning. Thus becoming the consensus for many NLP tasks [4]. \n\nWith an enormous number of trainable parameters, these PLMs can encode a substantial amount of linguistic knowledge that is beneficial to contextual representations [4]. For example, word polysemy (i.e., the coexistence of multiple meanings for a word or a phrase -e.g., 'bank' could mean 'river bank' or 'financial bank') in a domain-free text. \n\nIn contrast, in a domain-specific text that contains technical jargon, a word has a more precise meaning (i.e., monosemy) [5]. For example, the word 'run' in an IT text would generally only mean 'execute' and not 'rush'. Thus, it appears that domain-specific text classification will likely not benefit from the rich linguistic knowledge encoded in PLMs. \n\nDespite the widespread use of PLMs in a broad range of downstream tasks, their performance is still being evaluated by researchers for their drawbacks [6].",
            "score": 0.38716678888434175,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "230536436"
                },
                {
                    "start": 648,
                    "end": 651,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00997161865234375
        },
        {
            "corpus_id": "273482143",
            "title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning",
            "text": "We further designed this task to be symmetric, ensuring that simply training with reversed output, as suggested by Bachmann & Nagarajan (2024), cannot solve subgoals with all PDs. For comparison, we include Auto-regressive (AR), reverse AR (Bachmann & Nagarajan, 2024), and teacherless training (Monea et al., 2023;Bachmann & Nagarajan, 2024), which can be seen as a lookahead method that produce all target tokens from the source input, and our proposed diffusion model (detailed in \u00a73.2). For all the models, we keep the model architecture fixed as the same 3-layer Transformer with approximately 6M parameters. More details can be found in Appendix \u00a7C. \n\nDiscussion. We examine the performance of all the models in two scenarios. In the first scenario, we generate a fixed number of 50k instances with mixed planning distance. We plot the accuracy on the held-out evaluation set for each model in the left figure of Figure 2. Our findings indicate that autoregressive models (AR and Reverse AR) are only effective in solving cases where the PD equals 0 or 1 (or equivalently, 5 and 4 in the reverse setting). Due to the aforementioned subgoal imbalance phenomenon, when PD is less than 2, the task barely involves any planning, allowing models to simply copy from the input with ease. However, for larger PDs, AR models barely outperform random guessing. Teacherless training fails to adequately fit the training data, resulting in the production of illegal paths. In contrast, our diffusion model achieves perfect accuracy across all PD values. \n\nIn the second scenario, we investigate whether the challenging subgoals can be naturally resolved through data or model scaling, akin to the success observed in large language models (Kaplan et al., 2020;Wei et al., 2022a).",
            "score": 0.38704924365381255,
            "section_title": "SUBGOAL IMBALANCE IN AUTOREGRESSIVE AND DIFFUSION MODELING",
            "char_start_offset": 9453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1774
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0161590576171875
        },
        {
            "corpus_id": "268536735",
            "title": "Diffusion Model for Data-Driven Black-Box Optimization",
            "text": "Diffusion models successfully incorporate diverse guidance in practical applications.For image generation, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be externally trained, i.e., classifier guidance [Dhariwal and Nichol, 2021] or implicitly specified by a conditioned diffusion model, i.e., classifier-free guidance [Ho and Salimans, 2022].Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022].Other types of guidance are also explored in Nichol et al. [2021], Graikos et al. [2022], Bansal et al. [2023].Similar ideas have been explored in sequence modeling problems.In offline reinforcement learning, Decision Diffuser [Ajay et al., 2023] is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills.For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step.The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.These appealing empirical performance raises condensed curiosity of their theoretical underpinnings.",
            "score": 0.38704866882156796,
            "section_title": "Guided Diffusion Models",
            "char_start_offset": 10336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1499
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 637,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 771,
                    "end": 791,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 908,
                    "end": 927,
                    "matchedPaperCorpusId": "254044710"
                },
                {
                    "start": 1150,
                    "end": 1167,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "277510202",
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "text": "Diffusion and flow-based models are the state-of-the-art (SOTA) for generating high-quality images and videos, with recent advancements broadly categorized into Score Matching [10,27,30,[34][35][36] and Flow Matching [5,6,20,24,45] approaches. Flow matching directly predicts a velocity, enabling a more interpretable transport process and faster convergence compared with score-based diffusion methods. Hence, recent SOTA in text-to-image/video models increasingly adopt flow matching. In this paper, we follow the unifying perspective on diffusion and flow models presented by Lipman et al. [22]. We broadly use the term flow matching models to refer to any model trained using flow matching, where samples are generated by solving an ordinary differential equation (ODE). \n\nNext, classifier-free guidance (CFG) [9,44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results. \n\nWe empirically analyze the effect of CFG when the learned velocity is underfitted, i.e., inaccurate, using a mixture of Gaussians as the data distribution.",
            "score": 0.3869335149548918,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "252111177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "245117852",
            "title": "MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning",
            "text": "Self-supervised representation learning with transformer models (Vaswani et al., 2017) has become the dominant technique in Natural Language Processing in recent years, with encoder transformer models trained using a Masked Language Modeling (MLM) objective (Devlin et al., 2019) excelling at Natural Language Understanding tasks, and autoregressive decoder models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020) displaying impressive Natural Language Generation at increasingly large scales. Vision Language (VL) modeling -the modeling of joint image-text representations for tasks such as image captioning or visual question answering (VQA) -has followed suit, with the transformer encoder becoming the prevalent architecture in recent research. A popular approach among the latest state of the art VL models is to use a BERT-style encoder language model (LM) in combination with an object detection backbone such as Faster-RCNN (Ren et al., 2015). This approach, while displaying impressive performance on challenging benchmarks, has a number of drawbacks (see Section 2), in particular not being able solve VL tasks in an open-ended, generative fashion. \n\nA recent line of work (Tsimpoukelli et al., 2021;Wang et al., 2021;Sollami and Jain, 2021) explores VL modeling using autoregressive decoder models trained with a language modeling objective. SimVLM (Wang et al., 2021) shows impressive performance, but requires prohibitively large amounts of pretraining data and the training of language and vision components in tandem. Frozen (Tsimpoukelli et al., 2021) shows that a pretrained autoregressive language model can, without any finetuning to the LM weights themselves, be harnessed to train a visual prefix which enables images to be used as its input.",
            "score": 0.3867045515810293,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1780
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 86,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 386,
                    "end": 411,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018798828125
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "Diffusion probabilistic model (DPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020b] is a powerful generative model that employs a forward diffusion process to gradually add noise to data and generate new data from noise through a reversed process. DPM's exceptional sample quality and scalability have significantly contributed to the success of Artificial Intelligence Generated Content (AIGC) in various domains, including images [Saharia et al., 2022, Ramesh et al., 2022, 2021, Rombach et al., 2022], videos [Ho et al., 2022b, Singer et al., 2022, Ho et al., 2022a, Molad et al., 2023], and 3D objects [Poole et al., 2022, Lin et al., 2023, Wang et al., 2023]. \n\nConditional generation is one of the core tasks of AIGC. With the diffusion formulation, condition injection, especially the classical class condition, becomes more transparent as it can be modeled as an extra term during the reverse process. To align with the diffusion process, Dhariwal and Nichol [2021] proposed classifier guidance (CG) to train a time/noise-dependent classifier and demonstrated significant quality improvement over the unguided baseline. Ho and Salimans [2022] later proposed classifier-free guidance (CFG) to implicitly implement the classifier gradient with the score function difference and achieved superior performance in the classical class-conditional image generation. However, both CG and CFG require extra training with labeled data, which is not only time-consuming but also practically cumbersome, especially when adapting to new conditions. To reduce computational costs, training-free guidance methods have been proposed [Bansal et al., 2023] that take advantage of pretrained discriminative models. \n\nDespite the improved flexibility, training-free guidance has not demonstrated convincing performance compared to CG & CFG in formal quantitative evaluation of guiding diffusion generation. There seems to be an irreconcilable trade-off between performance and flexibility and the current guidance schemes are still to be desired.",
            "score": 0.3866509228738399,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 81,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 548,
                    "end": 569,
                    "matchedPaperCorpusId": "252595919"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 966,
                    "end": 992,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1644,
                    "end": 1665,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "264405937",
            "title": "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
            "text": "Recently, there has been a significant emphasis on pre-trained language models (PLMs) that utilize self-supervised learning Discard the response r. \n\nfiltered ). 5: end for 6: Select an instruction y \u2208 Y . 7: Let function A : (x, y) \u2192 x which augments each entry \n\nx \u2208 X with an instruction y. 8: for i \u2190 (1 . . . len(X cleaned )) do 9: \n\ncleaned , y). 10: end for 11: Return Augmented dataset X augmented on extensive raw text data [38]. Notable examples of such models include GPT-3 [22], PaLM [39], Chinchilla [40], LLaMA [23], and Falcon 40B [41]. By training on largescale texts using self-learning tasks like masked word prediction, sentence sequence recognition, text completion, and text generation [19], [42] PLMs acquire a comprehensive understanding of language. In addition, these models enhance the semantic representation of words by considering contextual dynamics and provide a unified framework for various NLP tasks. Currently, there are three standard models [43] structures in PLMs: autoregressive language models, autoencoding language models, and hybrid language models. Representative models for each design are GPT [22], BERT [20], and T5 [21], respectively. Autoregressive language models follow a standard approach where language modeling is done decoder-only, predicting words one by one through one-way Algorithm 3 Fine-tuning of a pre-trained language model. Constant random_seed, input sequence size, number of layers, number of hidden layer nodes, number of classifier outputs. \n\nInput Training set (X \n\nend for 7: \n\nloss \u2190 E(Y \n\nCalculate \u2207\u0398 for backpropagation. 9: \n\nAdjust parameters using an optimizer to minimize the loss. \n\n10: \n\nend for 11: end for 12: Return Task-specific Fine-tuned model. language encoding-decoding and token-by-token prediction of words. Autoencoding language models randomly mask words in a sentence, use bidirectional encoding to capture context, and then predict the masked words based on the encoded information. Finally, hybrid language models combine the approaches of the previous two models.",
            "score": 0.38638294135907447,
            "section_title": "C. PRE-TRAINED LANGUAGE MODELS (PLMS)",
            "char_start_offset": 18789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "258447166"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01554107666015625
        },
        {
            "corpus_id": "253510941",
            "title": "Rethinking data-driven networking with foundation models: challenges and opportunities",
            "text": "Then, after having learnt general linguistic patterns, the models can be fine-tuned on specific downstream tasks, with considerably fewer labeled instances. GPT-3 [5], a more recent language model, even eliminates the need for fine-tuning; and reduces the amount of labeled examples further by one order or magnitude, and even eliminates the need for any labeled data for some tasks. In addition to NLP, foundational models are being applied in vision [9,16,59], tabular data [97], and programming languages [17,27]. \n\nWe observe that network traffic analysis, and management share a number of properties with NLP and other domains where foundational models have been very successful: \n\n\u2022 Similarities of downstream tasks: First, similar to the status in NLP prior to the emergence of foundational models, specific solutions with their own preprocessing, features extraction, architectures, and datasets, are currently being developed for different tasks (e.g., congestion control [1,24,90], adaptive bitrate streaming [50], datacenter-scale automatic traffic optimization [7], job scheduling [31,51,51], resource management [65,91,99], network planning [104], packet classification [39], performance prediction [49], congestion prediction [56], performance estimation [100], malware detection [29,93], mapping from a low-quality video to a high-quality version [96], or semi-automated generation of protocol implementations from specification text [95].) Next, we observe that most of the underlying adopted machine learning approach behind those solutions (e.g., classification, anomaly detection, generator, and reinforcement learning) are areas where foundational models have been successfully applied, or being explored (Section 3.1). \n\n\u2022 Abundance of unlabeled data: Second, there is abundant unlabeled network data: For example, universities' networks have reported 1 to 2 TB of traffic on average per day [21,30]; and data-center networks and content providers generate more than 10 TB on average daily [28,38,85] (Section 3.2).",
            "score": 0.3862052975337792,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "220425306"
                },
                {
                    "start": 981,
                    "end": 984,
                    "matchedPaperCorpusId": "220872289"
                },
                {
                    "start": 984,
                    "end": 987,
                    "matchedPaperCorpusId": "174800601"
                },
                {
                    "start": 987,
                    "end": 990,
                    "matchedPaperCorpusId": "2052171"
                },
                {
                    "start": 1019,
                    "end": 1023,
                    "matchedPaperCorpusId": "27435112"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "51803617"
                },
                {
                    "start": 1097,
                    "end": 1100,
                    "matchedPaperCorpusId": "52918846"
                },
                {
                    "start": 1100,
                    "end": 1103,
                    "matchedPaperCorpusId": "52918846"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "59222742"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "matchedPaperCorpusId": "189926808"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "207758306"
                },
                {
                    "start": 1154,
                    "end": 1159,
                    "matchedPaperCorpusId": "236961702"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "889967"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "220872135"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "245541903"
                },
                {
                    "start": 1269,
                    "end": 1274,
                    "matchedPaperCorpusId": "236961623"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "50783335"
                },
                {
                    "start": 1298,
                    "end": 1301,
                    "matchedPaperCorpusId": "203902799"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "52987676"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "222290932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0154876708984375
        },
        {
            "corpus_id": "271051269",
            "title": "AgriLLM:Harnessing Transformers for Framer Queries",
            "text": "Neural sequence-to-sequence (seq2seq) models have proven to be highly effective in handling various human language tasks and modeling sequential data. These models can process and generate hu-man language, starting with the basics of natural language processing (NLP). Transformers, a significant advancement in NLP, enable more efficient handling of long-range dependencies in sequences. \n\nSeq2seq models, which build on transformers, encode an input sequence into a fixed representation and then decode it into an output sequence. Large language models (LLMs) and autoregressive models, such as GPT-3 and ChatGPT, further enhance the capabilities of seq2seq models by generating coherent and contextually appropriate text based on given prompts (Floridi and Chiriatti, 2020). These sophisticated models offer a powerful approach for improving agricultural knowledge distribution and expert guidance accessibility by enabling advanced data processing and interpretation (Neubig, 2017). \n\nThe evolution of Natural Language Processing (NLP) progressed from seq2seq models like recurrent neural networks to transformer models. Transformers revolutionized the field by enabling efficient parallel processing and capturing long-range dependencies in text with greater accuracy (Malte and Ratadiya, 2019). Historically, automated query answering relied on keyword matching and predefined rules, leading to robotic and inaccurate responses. However, the recent emergence of Language Models and transformers signals a transformative phase. \n\nTrained on extensive datasets of text and code, LLMs showcase exceptional proficiency in understanding and generating human-like language. This positions them as pioneers in automated query answering, providing unmatched fluency, dynamic comprehension, continuous learning, and expertise in multiple languages. LLMs handle intricate questions with nuance, decipher ambiguous queries, improve iteratively through user interactions, and surmount language barriers. Despite ongoing advancements, challenges like factual inconsistencies and biases necessitate attention. Nevertheless, the rapid evolution of LLM technology suggests a revolution in our interaction with information, making automated query answering more intuitive and advantageous.",
            "score": 0.3859874770105918,
            "section_title": "Seq2Seq Transformers and LLMs",
            "char_start_offset": 5581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2278
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0215301513671875
        },
        {
            "corpus_id": "274150206",
            "title": "Combining Autoregressive and Autoencoder Language Models for Text Classification",
            "text": "In this paper, I propose CAALM, an approach combining autoregressive and autoencoder models for text classification. Overall, results show that CAALM increases evaluation metrics for baseline BERT models and on top of other approaches such as BERT-NLI, particularly with smaller datasets and abstract tasks. \n\nThe performance gains shown in this paper should be enough for scholars implementing BERT text classification approaches to consider experimenting with autoregressive generated context. There is a sweet spot for CAALM efficiency that lies on training samples between 500 and 5000 for most datasets, and while the average gains are around 2-3 percentage points for Macro F1 and balanced accuracy, gains above 10% were observed for some sample size and model combinations. While CAALM might be computationally expensive to implement with large inference samples, assessing its gains over a relatively small datasets is not, and therefore is a viable alternative when seeking performance gains without labeling additional data. \n\nIn theory, any instruction tuned model can be used for CAALM, including popular commercial models such as ChatGPT or Claude, requiring only the addition of a new column to the text dataset that includes the LLM generated intermediate text or labels. It is possible that, by using models with a higher number of parameters, researchers observe gains even greater to the ones shown in this paper. In addition, the the highest performance metrics from the CAALM experiments do not account for potential hyperparameter search and tuning, or prompt adjusting, which could yield even higher gains. In line with previous research (Bucher & Martini, 2024), results show that for most sample sizes and tasks, baseline BERT and CAALM had superior performance when compared with zero-shot autoregressive predictions. However, these zero-shot models are still useful for very small samples or very infrequent classes. Additionally, few-shot learning or autoregressive model fine-tuning approaches were not tested, which might yield slightly superior performance when compared to zero-shot classification. \n\nThere are two downsides to the CAALM approach as implemented in this paper. The main one is that it is a computationally intensive approach, given that it involves generating contextual text not only for the training and validation data, but also for any data being inferred.",
            "score": 0.38586730914698686,
            "section_title": "Discussion",
            "char_start_offset": 31203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0234222412109375
        },
        {
            "corpus_id": "246411325",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
            "text": "The recently released foundation models [8], such as BERT [12], GPT-2 [52], and RoBERTa [37], represent a paradigm shift in which AI systems can be built by pretraining a general class of models at scale and then adapting them for a wide range of downstream tasks through transfer learning. Such models became ubiquitous in state-of-the-art natural language processing (NLP) systems by embracing the effectiveness of a combination of factors: the transformer architecture [67], self-supervised learning, few-shot conditioning [9], and fine-tuning. Importantly, many recent works have established that scaling up models greatly improves their performance, with especially substantial performance improvements in the zero-shot and few-shot settings. For example, GPT-3 [9], an autoregressive language model with 175 billion parameters, performs competitively on language tasks using in-context learning without fine-tuning or gradient updates. Such in-context learning allows models to perform new language tasks with only simple instructions and a few optional examples. The effectiveness of this method was further enhanced by recent model adaptation work such as prompt tuning [33], which efficiently adapts large language models to individual tasks with robust task performance. Other intriguing capabilities exhibited by large language models include, but are not limited to, free-form generation of coherent, long-form text like news stories, generating responses with real-world knowledge, as well as performing rudimentary mathematical operations. \n\nThe rapid development of large language models in recent years has also been fueled by growth in computational resources, availability of large datasets and evolving software stacks. State-of-the-art supercomputing clusters address the computation, memory and networking need of model training at this scale. Careful processing of high-quality, high-volume and diverse datasets directly contributes to model performance in downstream tasks as well as model convergence. New approaches to numerical manipulation and training recipes were developed aiming at improved optimization efficiency and stability. However, to sustain the seemingly exponential growth of model parameter size (see Figure 1), substantial progress in developing new methods, infrastructure and training capabilities is needed. \n\nTraining such large models is challenging for two reasons. First, it is no longer possible to fit the parameters of these models in the memory of even the largest GPU.",
            "score": 0.38560507213266904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2353
                },
                {
                    "start": 2356,
                    "end": 2414
                },
                {
                    "start": 2415,
                    "end": 2523
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0212860107421875
        },
        {
            "corpus_id": "273163054",
            "title": "Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting",
            "text": "Diffusion Models and Conditional Flow Matching. Diffusion models have been applied to several fields and achieved state-of-the-art performance (Ho et al., 2020;Hoogeboom et al., 2022;Lienen et al., 2024). Various works have demonstrated the effectiveness of unconditional models in conditional tasks through diffusion guidance (Epstein et al., 2023;Dhariwal & Nichol, 2021;Bansal et al., 2023;Nichol et al., 2021;Avrahami et al., 2022), solving inverse problems (Kawar et al., 2022), or iterative resampling (Lugmayr et al., 2022). Conditional Flow Matching (Lipman et al., 2022) is a recent approach proposed as an alternative to and generalization of diffusion models. This framework has been extended to incorporate couplings between data and prior samples, and trained to solve the dynamic optimal transport problem (Tong et al., 2023). Albergo et al. ( 2023) demonstrated the applicability of data-dependent couplings for stochastic interpolants to improve in-painting and super-resolution image generation. Lastly, recent work shows the potential for conditional generation by solving an optimization problem and differentiating through the flow (Ben-Hamu et al., 2024). \n\nGenerative Models for Time Series. Various generative models have been adapted to time series modeling, including Generative Adversarial Networks (GANs) (Yoon et al., 2019), normalizing flows (Rasul et al., 2020;Alaa et al., 2020), and Variational Autoencoders (VAEs) (Desai et al., 2021). Recent works have successfully applied diffusion models to time series. The first approach, TimeGrad (Rasul et al., 2021), applies a diffusion model on top of an LSTM to perform autoregressive multivariate time series forecasting, which was later extended by Bilo\u0161 et al. (2023) to settings with continuous time and non-isotropic noise distributions. Note that unlike TSFlow, Bilo\u0161 et al. (2023) neither use optimal transport paths nor couplings.",
            "score": 0.3854932879530238,
            "section_title": "RELATED WORK",
            "char_start_offset": 26345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1915
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 183,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "259075134"
                },
                {
                    "start": 327,
                    "end": 349,
                    "matchedPaperCorpusId": "258999106"
                },
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 413,
                    "end": 435,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 462,
                    "end": 482,
                    "matchedPaperCorpusId": "246411364"
                },
                {
                    "start": 508,
                    "end": 530,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 820,
                    "end": 839,
                    "matchedPaperCorpusId": "259847293"
                },
                {
                    "start": 1332,
                    "end": 1351,
                    "matchedPaperCorpusId": "202774781"
                },
                {
                    "start": 1391,
                    "end": 1409,
                    "matchedPaperCorpusId": "235613667"
                },
                {
                    "start": 1570,
                    "end": 1590,
                    "matchedPaperCorpusId": "231719657"
                },
                {
                    "start": 1728,
                    "end": 1747,
                    "matchedPaperCorpusId": "258823491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015777587890625
        },
        {
            "corpus_id": "277321603",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "text": "Guidance in Diffusion Models. Classifier-Free Guidance (CFG) [28] has become the de facto guidance technique for conditional generation with diffusion models, leading to notable improvements in both condition alignment and image quality. However, recent research has highlighted some of its limitations. Kynk\u00e4\u00e4nniemi et al. [41] have shown that the specific timesteps at which CFG is applied significantly impact image diversity, and proposed to restrict CFG to certain intervals. \n\nAnother line of work [1,31] addresses the limited applicability of CFG for text-based conditions when using off-the-shelf diffusion models like Stable Diffusion [52]. These approaches introduce a guidance technique that extends to a broader range of generation tasks, including unconditional generation, inverse problems, and conditional generation with non-text conditions (e.g., depth maps [67]). Recently, Karras et al. [38] propose Autoguidance which uses the noise estimate from an under-trained version of itself, instead of unconditional noise, to resolve inherent issues of the entangled guidance for condition alignment and image quality. A more detailed discussion on autoguidance can be found in the Appendix. However, previous works have not explored how the dynamics of CFG shift when a diffusion model is fine-tuned for a specific task [7,46,64]. In this work, we address the critical issue of unconditional noise degradation that occurs during fine-tuning and pro-pose a novel solution by combining noise predictions from multiple diffusion models. \n\nMerging Diffusion Models. Aligned with the mixtureof-experts [8] and model merging [65] literature on foundation models, there is growing research on methods for merging diffusion models to enable effective composition of multiple conditions. Diffusion Soup [3] directly merges weights of different diffusion models, Mix-of-Show [25] combines the weights of LoRA adapters [32], and Max-Fusion [48] merges intermediate model features. Notably, leveraging the iterative denoising process of diffusion models, merging their noise estimates has emerged as a simple yet powerful technique for composing conditions.",
            "score": 0.3852037787878375,
            "section_title": "Related Works",
            "char_start_offset": 5639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "268692048"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1807,
                    "end": 1810,
                    "matchedPaperCorpusId": "270391477"
                },
                {
                    "start": 1942,
                    "end": 1946,
                    "matchedPaperCorpusId": "269148434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "260917710",
            "title": "Real-World Implementation and Integration of an Automatic Scoring System for Workplace Safety Courses in Italian",
            "text": "Natural Language Processing (NLP) techniques can be applied in different ways and to different real-world and industrial cases. Many NLP techniques are also implemented in machine learning or deep learning applications to solve specific tasks in specific domains. In the healthcare system, NLP is used to automatically classify clinical reports [7] or to infer people's mental states from what they write on social media [8]; in the marketing and customer care sectors, several companies implement an NLP application to predict customer needs [9] or analyse customer reaction or satisfaction [10]. These models typically use reviews or social media posts and tweets to predict how users feel about a specific topic, and companies can use this information to improve their marketing strategies. \n\nA very popular and implemented NLP solution consists of conversational systems. In recent years, many companies have created their virtual assistant solutions to improve customer communication, provide better 24/7 support, and reduce the workload of employees [11,12]. A conversational system is a complex structure that leverages on several NLP subsystems, such as a Natural Language Understanding model and a multi-label classifier that predicts the intent of the user. In addition to NLP components, the realisation of a complete chatbot requires other structures, such as a front-end interface and a back-end data layer [12]. Today, there are many different frameworks that provide this kind of service, such as Watson IBM, Google Dialogflow, Microsoft Luis, and Facebook Wit.ai [13]. \n\nIn recent years, several approaches and frameworks have been developed to solve scoring and graduation tasks. The two main branches of this problem are: Automatic Essay Scoring (AES) [14] and Automatic Short Answer Grading (ASAG) [15]. This second task can be tackled in two ways: with a regression approach, which tries to predict a score in a continuous range (usually between 0 and 5), and a classification approach, in which the system has to assert the correctness of the user's answer. \n\nFollowing the evolution of machine learning and deep learning, over the years, many different techniques have been implemented to solve ASAG tasks. One of the first methods combines the Bag-of-Words representation and the Support Vector Machine to provide a score [16].",
            "score": 0.3850570052089507,
            "section_title": "Related Work",
            "char_start_offset": 5068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2350
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 348,
                    "matchedPaperCorpusId": "226255191"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "17828909"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "69726907"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "238819582"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "247044369"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "219311714"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "219311714"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "76660838"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "199465779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01751708984375
        },
        {
            "corpus_id": "277244406",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation",
            "text": "We introduce Gumbel-Softmax FM, a generative framework that leverages temperature-controlled Gumbel-softmax interpolants for smooth transport from noisy to clean distributions on the simplex. We define a new velocity field that follows a mixture of learned interpolations between categorical distributions that converge to high-quality sequences (Section 3). \n\n2. Gumbel-Softmax Score Matching. As an alternative generative framework using the same Gumbel-softmax interpolant, we propose Gumbel-Softmax SM that estimates the gradient of probability density at varying temperatures to enable sampling from high-density regions on the simplex (Section 4). \n\n3. Straight-Through Guided Flow Matching (STGFlow). Given the lack of post-training guidance methods for discrete flow matching, we introduce Straight-Through Guided Flow Matching, a novel training-free classifier-based guidance algorithm that leverages straightthrough gradients to guide the flow trajectory towards high-scoring sequences (Section 5). We apply this method to generate high-affinity peptide binders to target proteins (Section 6.4). \n\n4. Biological Sequence Generation. We apply our framework to conditional DNA promoter design, de novo protein sequence generation, and target-binding peptide design, demonstrating competitive performance compared to autoregressive and discrete diffusion-based baselines (Section 6). \n\nOur framework offers several theoretical and empirical advantages over autoregressive and discrete diffusion models, and we believe it will serve as a foundation for controllable flow matching for discrete sequence generation.",
            "score": 0.38485294305296736,
            "section_title": "arXiv:2503.17361v1 [cs.LG] 21 Mar 2025",
            "char_start_offset": 2910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "264451832",
            "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
            "text": "SEDD is a discrete diffusion model that focuses on score matching, the crucial ingredient for continuous diffusions (Song & Ermon, 2019;Ho et al., 2020). Many such works also focus on reversing a discrete diffusion process (Campbell et al., 2022;Benton et al., 2022;Sun et al., 2023), so score entropy is naturally related with prior training objectives. However, SEDD focuses on a principled, scalable, and performant objective (namely denoising score entropy), filling in shortcomings found in previous works. In particular, prior methods train either with the equivalent of implicit score entropy (which is intractable and high variance) or propose alternate losses that suffer from other issues. These critical differences enable large improvements for language tasks, where prior discrete diffusion models have conspicuously struggled on. \n\nFurthermore, SEDD achieves better results (for both perplexity and generation) than even continuous diffusion models (without resorting to empirically driven heuristics). This is desirable since discrete data should necessitate a novel approach. Future work could adapt empirical designs from continuous diffusion, further improving performance. \n\nFinally, SEDD challenges autoregressive models, achieving competitive perplexities (beating GPT-2) and generation quality (beating nucleus sampling). While there is still a large gap with modern large language models, we believe that future work can bridge this using SEDD as a backbone. \n\nSEDD vs Autoregressive Sampling Iterations. SEDD and autoregressive models have significantly different sampling procedures due to the introduction of the KV-cache for standard decoder-only transformer models. In particular, this complicates the inference code (as each network pass changes from being a standard full batch forward) and trades off speed with memory. For example, for our (known) unoptimized codebase and the existing huggingface transformers library (Wolf et al., 2020), we observed that SEDD matches autoregressive inference time when using around 100 steps but can increase the batch size by roughly 4 \u2212 6 times by removing the KV-cache memory.",
            "score": 0.38445983734336464,
            "section_title": "Related Work",
            "char_start_offset": 28409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 152,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 223,
                    "end": 246,
                    "matchedPaperCorpusId": "249192370"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "254096040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "207756529",
            "title": "Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection",
            "text": "It is thus not surprising that transfer learning has also attracted tremendous research interest among the NLP community in recent years. Transfer learning usually involves fine-tuning a pre-trained model, i.e., a model that was trained in an unsupervised setting on a massive unlabeled dataset. Instead of learning from scratch, the learning process for a downstream task can then benefit from the weights of the pre-trained models. In other words, the process only needs to adjust the weights to adapt to the specific task, usually with supervised learning on a relatively smaller set of labeled data. Compared to using pre-trained word embeddings, employing a pre-trained language modeling approach allows all model weights-including weights in the pre-trained layers and the task-specific layers-to be jointly updated for a downstream NLP task. Recent advancements in NLP have acquired convincing empirical proof that supports the shift of paradigm from shallow to deep pre-training [12,13,14]. Highly promising results have been achieved using pre-trained language models like Universal Language Model Fine-tuning (ULMFiT) [13], OpenAI's Generative Pre-trained Transformer [14], and Google's Bidirectional Encoder Representations from Transformers (BERT) [12] on a wide range of NLP tasks. \n\nBERT was introduced by [12] to overcome some limitations in the implementation of language models. They argued that the unidirectional architecture of language models like OpenAI's GTP [14] could be suboptimal. BERT is a deep bidirectional model trained using two novel pre-training tasks that allowed the language model to capture (1) the contextual information of tokens from both left-to-right and right-to-left directions and (2) the relations between sentences. Although the concept underlying the development of BERT is simple, it has been empirically proven as more potent than other pre-trained language models, rewriting the records of state-of-the-art results on benchmark datasets in 11 NLP tasks.",
            "score": 0.38409295967382573,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 16016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 2005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02337646484375
        },
        {
            "corpus_id": "253510695",
            "title": "Replacing Language Model for Style Transfer",
            "text": "This paper introduces a new sequence-to-sequence framework for text style transfer called replacing language model (RLM). In virtue of pretrained language models such as BERT, our model autoregressively predicts a target text span based on the generated prefix and the remaining source suffix in the masked language modeling (MLM) paradigm, and further scores the newly generated span based on the probability of reconstructing the corresponding word in the original sentence. Moreover, unlike prior sentence-level disentangling methods, we eliminate the style information from the word-level content embeddings with information-theoretic guidance, providing fine-grained control to generate transferred tokens. The empirical results on Yelp and Amazon review dataset demonstrate the effectiveness of the proposed RLM. As a novel generation scheme, RLM combines autoregressive generators' flexibility and non-autoregressive models' accuracy. However, a potential limitation is about the transfer diversity, that RLM does not rewrite sentences with different word orders (e.g., active voice to passive voice), which might be an advantage in contrast to other sequence-sequence tasks requiring order preservation, such as voice conversion. From this perspective, We believe the replacing language model will have a further impact in broader sequence-to-sequence modeling scenarios such as machine translation, text rewriting, and speech processing. \n\nA TRANSFERRED SAMPLES",
            "score": 0.38399544347359704,
            "section_title": "CONCLUSION",
            "char_start_offset": 27228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035675048828125
        },
        {
            "corpus_id": "258685532",
            "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
            "text": "One of the primary limitations of NLP modeling in materials science, including this work, is the low quantity of available data as discussed in Section 2. This analysis is affected by this limitation as well given that our evaluations were performed in a low-data setting within a dataset that was already limited in size. We believe that future work can improve upon this study by applying larger datasets, both in the number of samples and in the scope of tasks, to similar problem settings. The small nature of the datasets applied in this study also presents the danger that some of the models may have memorized certain answers instead of achieving a broader understanding, which could be mitigated by enlarging the datasets and making the tasks more complex.\n\nMoreover, we did not study the generalization of NLP models beyond the materials science domain, including adjacent domains such as chemistry and physics. This targeted focus was intentional but imposes limitations on whether the proposed techniques and insights we gained from our analysis are transferable to other domains, including applying NLP models for scientific tasks outside of materials science.\n\nAnother limitation of our study is the fact that we focused on BERT-based models exclusively and did not study autoregressive models, including large language models with billions of parameters highlighted in the introduction. The primary reason for focusing on BERT-based models was the diversity of available models trained on different scientific text corpora. Large autoregressive models, on the other hand, are mostly trained on general text corpora with some notable exceptions, such as Galactica (Taylor et al., 2022). We believe that future work analyzing a greater diversity of language models, including large autoregressive models pretrained on different kinds of text, would significantly strengthen the understanding surrounding the ability of NLP models to perform text-based tasks in materials science.\n\nWhile the results presented in this study indicate that domain-specific pretraining can lead to noticeable advantages in downstream performance on text-based materials science tasks, we would like to highlight the associated risks and costs of pretraining a larger set of customized language models for different domains. The heavy financial and environmental costs associated with these pretrain-ing procedures merit careful consideration of what conditions may warrant expensive pretraining and which ones may not. When possible, we encourage future researchers to build upon existing",
            "score": 0.3839732954365934,
            "section_title": "Limitations",
            "char_start_offset": 30124,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01132965087890625
        },
        {
            "corpus_id": "276106972",
            "title": "Latent Thought Models with Variational Bayes Inference-Time Computation",
            "text": "In comparison with traditional autoregressive models (Radford et al., 2019) and more recent diffusion-based approaches (Lou et al., 2024;Shi et al., 2024;Sahoo et al., 2024), LTMs demonstrate superior efficiency in data and parameters, and excel in several key NLP tasks: \n\n\u2022 Pretraining perplexity: Given fixed training compute, LTM-Medium achieves perplexity comparable to GPT-2-Large (10.95 vs. 11.5) with equivalent trFLOPs/tok but only 6.7% of GPT-2-Large parameters. LTM-Small achieves 11.85 perplexity with 26% less trFLOPs/tok and 5.0% of GPT-2-Large parameters. Our most sampleefficient model, LTM-Large, reaches a validation perplexity of 5.58 using only 76M parameters trained on 3B tokens. \n\n\u2022 Language modeling: LTMs' superior pretraining perplexity translates to zero-shot language modeling performance, with LTM-Medium and LTM-Large achieving perplexity reductions of 52.2% and 79.9% compared to state-of-the-art results at GPT-2 scale. \u2022 Arithmetic reasoning: LTMs demonstrate emergent fewshot in-context learning at scales that are significantly smaller than GPTs. This is significant even in our smallest model, LTM-Small. This capability scales further with increased model and latent size. \u2022 Text generation: LTM-Large outperform both autoregressive and diffusion counterparts in conditional sentence completion when measured with MAUVE score (Pillutla et al., 2021). In unconditional generation, LTM-Large achieves generative perplexity (Dieleman et al., 2022) and token-level entropy (Zheng et al., 2024) comparable to GPT-2-Large, while being significantly faster. \n\nContributions. Language models with explicit latent thought vectors that follow a prior model in latent space are much under-explored in recent years. Compared to ground tokens, the latent thought vectors provide a highly compact, abstract and structured representation in a lifted latent space.",
            "score": 0.3833559697588336,
            "section_title": "Introduction",
            "char_start_offset": 4140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 75,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 119,
                    "end": 137,
                    "matchedPaperCorpusId": "264451832"
                },
                {
                    "start": 1363,
                    "end": 1386,
                    "matchedPaperCorpusId": "244488758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01056671142578125
        },
        {
            "corpus_id": "271328459",
            "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
            "text": "Classifier-free guidance [22] is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits.",
            "score": 0.3832814172581209,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 6690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 424
                },
                {
                    "start": 426,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 618
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "270562252",
            "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
            "text": "Classfier guidance. Dhariwal and Nichol (2021); Song et al. (2020) introduced classifier guidance, a method that entails training a classifier and incorporating its gradients to guide inference (while freezing pre-trained models). However, a notable drawback of this technique lies in the classifier's accuracy in predicting y from intermediate x t , resulting in cumulative errors during the diffusion process. To mitigate this issue, several studies propose methods to circumvent it through reconstruction, referred to as reconstruction guidance in this work. Specifically, they employ certain approximations that map intermediate states x t back to the original input space x 0 , allowing the classifier to be learned solely from x 0 to y (Ho et al., 2022;Han et al., 2022;Chung et al., 2022;Finzi et al., 2023;Bansal et al., 2023). In contrast to these works, our approach focuses on fine-tuning the diffusion model itself rather than relying on an inference-time technique. While the strict comparison between model fine-tuning and inference-time techniques is not feasible, we theoretically elucidate the distinctions and connections of our approach with classifier guidance in Section 5.1. \n\nClassfier-free guidance. Classifier-free guidance (Ho and Salimans, 2022) is a method that directly conditions the generative process on both data and context, bypassing the need for explicit classifiers. This methodology has been widely and effectively applied, for example, in text-to-image models (Nichol et al., 2021;Saharia et al., 2022;Rombach et al., 2022). While the original research does not explore classifier-free guidance within the scope of fine-tuning pre-trained diffusion models, several subsequent studies address fine-tuning scenarios Zhang et al. (2023); Xie et al. (2023). As elucidated in Section 5.2, compared to classifier-free guidance, our approach can improve sample efficiency and leverage conditional independence to facilitate offline dataset construction. \n\nFine-tuning via RL. Several previous studies have addressed the fine-tuning of diffusion models by optimizing relevant reward functions.",
            "score": 0.3832814172581209,
            "section_title": "RELATED WORKS",
            "char_start_offset": 2298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 46,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 759,
                    "end": 776,
                    "matchedPaperCorpusId": "249674651"
                },
                {
                    "start": 795,
                    "end": 814,
                    "matchedPaperCorpusId": "259145376"
                },
                {
                    "start": 814,
                    "end": 834,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1520,
                    "end": 1541,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1541,
                    "end": 1562,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1774,
                    "end": 1791,
                    "matchedPaperCorpusId": "258108246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "273233481",
            "title": "Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction",
            "text": "For instance, in Language Modeling (LM) it is desirable to bias the model's generations to be sanitized from harmful responses (Zou et al., 2023;Perez et al., 2022), or aiming to generate protein sequences that are highly likely to be successfully synthesized and expressed in real wet lab settings (Verkuil et al., 2022;Dauparas et al., 2022). Put succinctly, highly performant discrete generative models are required to be aligned in a manner that fine-tuning against downstream reward models has the intended effect of controllable generation, wherein the model post fine-tuning selects high-scoring samples from the universe of possible high-fidelity generations. \n\nThe standard approach for incorporating steerability into discrete generative models, which are autoregressive, using pre-defined reward models is often framed as a fine-tuning task using reinforcement learning from human feedback (RLHF) (Christiano et al., 2017;Rafailov et al., 2024). However, applying RLHF frameworks to diffusion models is far more challenging. Unlike autoregressive models, diffusion models do not allow for straightforward computation of a sample's exact likelihood without costly simulations. Although fine-tuning diffusion models that bypass exact likelihood computation can yield simulation-free algorithms that resemble RLHF (Wallace et al., 2024;Uehara et al., 2024a), these methods effectively optimize a loose lower bound to the true RLHF objective, leading to unstable training and suboptimal fine-tuning performance. Consequently, steering diffusion models in continuous spaces is primarily done through inference techniques that leverage the gradient of a conditional model in the form of guidance (Dhariwal and Nichol, 2021;Ho and Salimans, 2022). Unfortunately, discrete settings do not allow for principled definitions of guidance due to the lack of a conventional gradient operator. As a result, at present, there exists no scalable and rigorous method to steer and align Masked Diffusion Models to optimize desired reward models. \n\nMain contributions. In this paper, we cast the problem of steering a Masked Diffusion Model as a task of probabilistic inference in sampling from a target Bayesian posterior.",
            "score": 0.38302524290627415,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2037
                },
                {
                    "start": 2040,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 321,
                    "matchedPaperCorpusId": "255097177"
                },
                {
                    "start": 321,
                    "end": 343,
                    "matchedPaperCorpusId": "249400681"
                },
                {
                    "start": 933,
                    "end": 955,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1322,
                    "end": 1344,
                    "matchedPaperCorpusId": "265352136"
                },
                {
                    "start": 1701,
                    "end": 1728,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03662109375
        },
        {
            "corpus_id": "268419938",
            "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts",
            "text": "While QLoRA fine-tuning performed comparably for some cases, ICL triumphed overall, particularly when including proprietary models GPT-3. We consider trade-offs of different model types: autoregressive and sequence-to-sequence (seq2seq). Seq2seq models (FLAN-T5, FLAN-UL2) performed very well on syntactical metrics such as BLEU but worse on others (Figure 3d), suggesting that these models excel more at matching word choice than matching semantic or conceptual meaning. Note seq2seq models are often constrained to much shorter context length than autoregressive models such as GPT-4, because seq2seq models require the memory-intensive step of encoding the input sequence into a fixed-size context vector. Among open-source models, seq2seq (FLAN-T5, FLAN-UL2) performs better than autoregressive (Llama-2, Vicuna) models on radiology reports but worse on patient questions and progress notes (Figure 3c). Given that these latter datasets have higher lexical variance (Table 3) and more heterogeneous formatting compared to radiology reports, we posit that autoregressive models may perform better with increasing data heterogeneity and complexity. \n\nThe overwhelming evidence from our reader study suggests that adapting LLMs can outperform human experts in terms of completeness, correctness, and conciseness. When qualitatively analyzing summaries, we notice a few general trends. As implied by the completeness scores, GPT-4 excelled at identifying and understanding the most relevant information from the source text. However, both GPT-4 and human experts faced challenges interpreting ambiguity, such as user queries in patient health questions. Consider Example 1 of Figure A4, in which the input question mentioned \"diabetes and neuropathy.\" GPT-4 mirrored this phrasing verbatim, while the human expert interpreted it as \"diabetic neuropathy.\" This highlights GPT-4's tendency toward a literal approach without interpretation, which may either be advantageous or limiting.",
            "score": 0.3830199261342971,
            "section_title": "Attribute",
            "char_start_offset": 16920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01322174072265625
        },
        {
            "corpus_id": "252873612",
            "title": "Subword Segmental Language Modelling for Nguni Languages",
            "text": "In this paper we proposed subword segmental language modelling (SSLM), an approach that unifies language modelling and subword segmentation. We showed that SSLM improves intrinsic LM performance for low-resource agglutinative languages, while yielding subwords that approximate morphemes better than previous approaches. As opposed to most neural model architectures in NLP research, which are either language-agnostic or overfit to high-resource languages, our model is designed to suit agglutinative languages like the Nguni languages of South Africa. Our results show that learning subword segmentation in training overcomes some of the limitations of existing subword segmenters. For future work, the SSLM could be applied to downstream NLP tasks suited to its autoregressive architecture, such as text generation. More generally, the idea of learning subword segmentation during training could be adapted to other NLP models and tasks.",
            "score": 0.3826450785775506,
            "section_title": "Conclusion",
            "char_start_offset": 25375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01282501220703125
        },
        {
            "corpus_id": "265466183",
            "title": "Graph Prompt Learning: A Comprehensive Survey and Beyond",
            "text": "Due to the growing number of model parameters, the conventional pre-training and fine-tuning process is evolving into a new approach termed pre-training, prompting, and predicting [50]. In this paradigm, instead of manually adapting the pre-trained model for specific downstream tasks, these tasks are reformulated to resemble those addressed during the pre-training phase, aided by prompts. Prompts in NLP take various shapes, including cloze prompts , which complete textual strings like those used in masked language models, and prefix prompts [40,43], where the input text precedes the answer slot, as employed by autoregressive language models. Some studies involve manually designed templates based on human insights [2,70,71], while others explore automated template learning. This includes searching for templates in a discrete space [35,25,73,16] or conducting prompting directly in the embedding space [43,40,86,65]. Such a paradigm enables a single pre-trained model to address a multitude of downstream tasks in an unsupervised manner, which has been widely demonstrated by large language models. In light of this, the application of prompting techniques in the context of graph-based tasks is currently an area of active exploration.",
            "score": 0.38254913808679514,
            "section_title": "A Brief History of Prompt Learning",
            "char_start_offset": 27138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1246
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "238260199"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "221703107"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 846,
                    "end": 849,
                    "matchedPaperCorpusId": "232168628"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 912,
                    "end": 916,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "233231453"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0215301513671875
        },
        {
            "corpus_id": "273695177",
            "title": "Efficient Learned Query Execution over Text and Tables [Technical Report]",
            "text": "While GIO uses template-based code generation, Evaporate uses LLMs such as GPT-4. However, we found that code generation is hard with free-form text, as in our setting. Text-to-Table [69] and STable [46] are sequence-to-sequence models trained to transform tables into text. Both introduce several model adjustments to ensure that the model outputs a correctly structured table. Different from Text-To-Table, STable can output table cells in arbitrary order. Unlike our work, both are trained in a supervised manner from scratch for every new data set. LLMs for Data Management. By now, many research groups have integrated language models into data systems to tackle various data management tasks. Language models have been used to tune databases [60], solve data engineering tasks like entity matching, entity resolution, or missing value imputation [1,18,35,40,65], or augment databases with knowledge stored inside of LLMs [49,63]. Moreover, Foundation Models for data management promise to be a solution for many different data management problems [33,64]. Pre-training Models. Large pre-trained language models [4,15,37,44] are by now dominating NLP and are quickly adapted for multimodal [32,39,53,54] and tabular data [13,25,66]. To reduce the overhead of adaption to downstream tasks, pre-training objectives began to be more aligned with the downstream task for many core-NLP [19,27,30,48] and also structure-aware tasks [36,42,51,72]. Most similar to our pre-training objectives are those pretraining procedures that rely on weak or distant supervision to align pre-training more to the downstream task. ReasonBERT [12] uses a pre-training objective inspired by distant supervision for the downstream task of multi-hop hybrid question answering. StruG's [11] pre-training data set designed for the text-to-SQL task is based on the table-to-text data set ToTTo [43], which was extracted from Wikipedia using heuristics and is much smaller than our data set.",
            "score": 0.3824529298653995,
            "section_title": "RELATED WORK",
            "char_start_offset": 76354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1967
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "237433655"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "249579165"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "252355353"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "248965029"
                },
                {
                    "start": 864,
                    "end": 867,
                    "matchedPaperCorpusId": "249605481"
                },
                {
                    "start": 931,
                    "end": 934,
                    "matchedPaperCorpusId": "259205622"
                },
                {
                    "start": 1126,
                    "end": 1129,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1199,
                    "end": 1202,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "233864627"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "235447018"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "220128068"
                },
                {
                    "start": 1396,
                    "end": 1399,
                    "matchedPaperCorpusId": "230433978"
                },
                {
                    "start": 1431,
                    "end": 1435,
                    "matchedPaperCorpusId": "236034533"
                },
                {
                    "start": 1435,
                    "end": 1438,
                    "matchedPaperCorpusId": "225066758"
                },
                {
                    "start": 1438,
                    "end": 1441,
                    "matchedPaperCorpusId": "229331741"
                },
                {
                    "start": 1441,
                    "end": 1444,
                    "matchedPaperCorpusId": "221995589"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "216641852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017578125
        },
        {
            "corpus_id": "267408029",
            "title": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning",
            "text": "In this section, we present full experimental results encompassing text classification tasks for the Masked Language Model (MLM) and question answering, summarization, and dialogue generation tasks for the autoregressive LLM. In all tables, the best performance is indicated with boldfaced underline, and the second-best value is represented with underline for methods that use the same best hyperparameters by \u00a7 4.2. The final column 'Avg.' provides a summary of overall results for each method across various datasets or metrics. The terms 'Full' and 'Freeze' in Tables 14 and 15 specify the exploration of optimal hyperparameters using either the entire model or a model with half of its weights frozen, as discussed in \u00a7 4.  Text Classification. Table 14 demonstrates the consistently better performance of BOMF over other baselines that employ the same best hyperparameters. These findings affirm the effectiveness of BOMF in the context of single-metric NLP tasks. \n\nQuestion Answering. Table 15 presents the complete experimental results for the questionanswering task. BOMF consistently surpasses other baselines utilizing the same best hyperparameters. These outcomes prove the effectiveness of BOMF in the realm of multi-metric NLP tasks, improving both F1 and EM metrics concurrently. \n\nSummarization. Table 16 provides empirical evidence that BOMF achieves the highest average performance across evaluated metrics. While Learned SWA and Greedy SWA exhibit the best performance results in R1 and R2 metrics, respectively, they experience declines across other metrics. However, our approach demonstrates a consistent improvement across all metrics. These results prove the efficacy of BOMF in multi-metric NLP tasks. \n\nDialogue Generation. achieve such comprehensive enhancements. This proves the efficacy of our multi-objective method in effectively considering multiple metrics with conflicting correlations.",
            "score": 0.38234654586599415,
            "section_title": "C.4 Full Experimental Results",
            "char_start_offset": 48389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1921
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029144287109375
        },
        {
            "corpus_id": "272694637",
            "title": "Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models",
            "text": "Importantly, these KL-based methods do not require access to real data, as the KL divergence is defined with respect to the model distribution. While these approaches have successfully approximated the data distribution in a data-free manner, they often suffer from performance degradation when compared to the original, pretrained teacher diffusion model. Consequently, additional loss terms that require access to the original training data or data synthesized with the pretrained diffusion models are often necessary to mitigate this performance degradation. However, employing these terms voids the data-free feature of the process. In response to these challenges, Score identity Distillation (SiD) has emerged as an effective data-free solution for matching distributions by minimizing a model-based Fisher divergence. Although directly computing this divergence is intractable, its minimization is effectively converted into a model-based score distillation loss. This data-free method facilitates the distillation of the pretrained score function from the teacher diffusion model into a potentially superior one-step student generator. Inspired by the success of this data-free score distillation, we are motivated to integrate its loss into our algorithm, SFD, to enhance its effectiveness and efficiency in generative modeling with data-free unlearning. \n\nEvaluation of Machine Unlearning When applying MU to classification tasks, effectivenessoriented metrics include unlearning accuracy, which evaluates how accurately the model performs on the forget set after unlearning [22]. Utility-oriented metrics include remaining accuracy, which measures the updated model's performance on the retain set post-unlearning [70], and testing accuracy, which assesses the model's generalization capability after unlearning. For generation tasks, accuracy-based metrics use a post-generation classifier to evaluate the generated content [89], while quality metrics assess the overall utility of the generated outputs [20]. A significant limitation of these metrics, particularly in measuring unlearning effectiveness, is their heavy dependence on the specific unlearning tasks [19]. To address this, we train an external classifier to evaluate unlearning accuracy (UA), ensuring that the generated images do not belong to the forgetting class or concept.",
            "score": 0.38232376176081295,
            "section_title": "CONCLUSION",
            "char_start_offset": 31364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2353
                }
            ],
            "ref_mentions": [
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 2016,
                    "end": 2020,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 2176,
                    "end": 2180,
                    "matchedPaperCorpusId": "264305818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0726318359375
        },
        {
            "corpus_id": "259165510",
            "title": "A Survey of Diffusion Models in Natural Language Processing",
            "text": "SUNDAE (Savinov et al., 2021) proposes stepunrolled text generation and is the first non-AR method to show strong results in both machine translation and unconditional text generation. \n\nDiffusER (Reid et al., 2023) employs a 2dimensional beam search and edit-based text generation. Instead of a pure end-to-end approach, the system divides the task into edit tagging and generation. It generates a sequence of edits to transform a random noise distribution into high-quality output. \n\nDiffusionBERT (He et al., 2022) combines diffusion models with Pre-trained Language Models (PLMs) (Devlin et al., 2018;Lewis et al., 2019;Raffel et al., 2019;Brown et al., 2020;Qiu et al., 2020) by training BERT in reverse of a discrete diffusion process. It introduces a new noise schedule for the forward diffusion process and incorporates the time step into BERT (Devlin et al., 2018). By including the time step, DiffusionBERT captures lost temporal information during diffusion, enhancing the accuracy of the reverse process. \n\nSSD-LM (Han et al., 2022) stands out due to two key features. Firstly, it is semi-autoregressive, enabling iterative generation of text blocks and dynamic length adjustment during decoding. Secondly, it is simplex-based, directly applying diffusion on the natural vocabulary space instead of a learned latent space. This approach facilitates the incorporation of classifier guidance and mod-ular control without the need for modifications to existing classifiers. \n\nMasked-Diffuse LM (Chen et al., 2023a) employs strategic soft-masking, informed by linguistic features, to corrupt both discrete and continuous textual data. It iteratively denoises the data by predicting the categorical distribution. The gradual introduction of perturbations via soft-masking, following an easy-first-generation approach, enhances structural coherence, overall quality, and flexibility in text generation. This pioneering work utilizes linguistic features to effectively corrupt and recover input textual data, improving the generation process. \n\nRDMs (Zheng et al., 2023) introduces a novel reparameterization technique for discrete diffusion models.",
            "score": 0.38221509378393304,
            "section_title": "Discrete Diffusion Models",
            "char_start_offset": 9409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2154
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 215,
                    "matchedPaperCorpusId": "259298665"
                },
                {
                    "start": 663,
                    "end": 680,
                    "matchedPaperCorpusId": "212747830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048858642578125
        },
        {
            "corpus_id": "268424453",
            "title": "Generative AI in healthcare: an implementation science informed translational path on application, integration and governance",
            "text": "Large language models (LLMs) are powerful AI models that have shown promise in various natural language processing (NLP) tasks [25]. In particular, the availability of OpenAI's GPT-4 [26], Anthropic's Claude [27] and Google's PaLM2 [28] has significantly galvanised the progress of not just NLP but the field of AI in general, whereby commentators are discussing achievement of human-level performance by AI [10,29]. LLMs like Ope-nAI's GPT-4 are based on the autoregressive model. An autoregressive model is used to generate sequences, such as sentences in natural language, by predicting a next item based on previous ones [30]. The difference between LLMs and traditional language models lies in their capabilities and training methods [25]. LLMs, like GPT-4, utilise the Transformers architecture, which has proven to be effective for understanding the context of words in a sentence. A transformer uses a mechanism called 'attention' to weigh the importance of words when making predictions [31]. This mechanism allows the model to consider the entire history of a sentence, making it a powerful tool for sequence prediction tasks. LLMs are trained on a large corpus of text data. During training, the model learns to predict the next word in a sentence given the previous words. It does this by adjusting its internal parameters to minimise the difference between its predictions and the actual words that follow in the training data. \n\nOne of the key advantages of LLMs is their ability to perform many language processing tasks without the need for additional training data [32]. This is because they have already been trained on a vast corpus of text, allowing them to generate coherent and contextually relevant responses based on the input they receive. This makes them particularly useful as references or oracles for text summarization models. Text summarization is a complex task that involves understanding the main points of a piece of text and then condensing these points into a shorter form. LLMs can be used to generate summaries of text, which can then be used as a reference or 'gold standard' for other summarization models [25].",
            "score": 0.3820704212284347,
            "section_title": "Large language models",
            "char_start_offset": 6270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "258687456"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "259162481"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.011871337890625
        },
        {
            "corpus_id": "268264822",
            "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
            "text": "In this section, we spotlight several pivotal and widely utilized text-to-image foundational models.Detailed information regarding these models is systematically compiled and presented in   \u2022 GLIDE [21].To generate images aligned with free-form text prompts, GLIDE intuitively replace the class label in class-conditioned diffusion models (i.e.ADM [18]) with text, formalizing the first text-to-image diffusion model.The authors explore two different guidance for text-conditioning.\n\nFor classifier guidance, GLIDE trains a CLIP model in noisy image space to provide CLIP guidance.Following [20], GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation.For text condition, GLIDE first transforms the input text c into a token sequence via a trainable transformer [36].Subsequently, they replace the class embedding with the pooled text features and further concatenate the projected sequence text features to the attention context at each attention layer in diffusion model.GLIDE trains the diffusion model and text transformer on the same dataset as DALL\u2022E [22].The diffusion model is trained to predict p(x t\u22121 |x t , c) and generate images with CFG.\n\n\u2022 Imagen [24].Following GLIDE, Imagen adopts classifierfree guidance (CFG) for text-to-image generation.Instead of training a task-specified text encoder from scratch in GLIDE, Imagen leverages a pre-trained and frozen large language model (LLM) as its text encoder, aiming to reduce computational demands.The authors conduct a comparative analysis of various LLMs, including those trained on imagetext datasets (e.g., CLIP [39]) and solely on text corpora (e.g., BERT [40], T5 [37]).Their findings suggest that increasing the scale of language models more effectively enhances the fidelity of samples and the congruence between image and text, compared to the enlargement of image diffusion models.Furthermore, Imagen's exploration into different text conditioning methods reveals cross-attention as the most effective technique.",
            "score": 0.3815300320438647,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 7043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 482
                },
                {
                    "start": 484,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1354
                },
                {
                    "start": 1356,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1662
                },
                {
                    "start": 1662,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 2055
                },
                {
                    "start": 2055,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 965,
                    "end": 969,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1260,
                    "end": 1264,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "273969525",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "text": "Visual Generation with Flow-based Generative Models. Recent years have witnessed remarkable progress in visual generation through diffusion models [32,83], leading to impressive models like [67,73,[76][77][78][79]. Building on these advances, flow-based generative models [3,55,61] emerged as a simplified alternative framework. These approaches have recently enabled advanced visual generation models [23,36] that achieve superior empirical performance with faster sampling. Our work demonstrates that rectified flow [60][61][62] can be effectively integrated into LLMs, creating unified models that excel in both understanding and generation tasks. \n\nUnified Models For Understanding and Generation. The development of multimodal large language models (MLLMs) has enabled effective integration of text and visual information. Building upon powerful LLMs [7,91,92], recent MLLMs [2,15,49,56,58,64] have demonstrated exceptional multimodal understanding capabilities. Current research increasingly focuses on architectures that can simultaneously handle visual understanding and generation tasks. One approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88,97]. While straightforward to implement, these approaches are inherently limited by their image tokenization quality. \n\nOur work focuses on developing unified models that combine autoregressive capabilities with flow/diffusion models, leveraging their proven effectiveness in visual generation.",
            "score": 0.3815057359631878,
            "section_title": "Related Work",
            "char_start_offset": 2700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "254854449"
                },
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 272,
                    "end": 275,
                    "matchedPaperCorpusId": "252668615"
                },
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "261697392"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 883,
                    "end": 886,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1467,
                    "end": 1470,
                    "matchedPaperCorpusId": "259765944"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0302734375
        },
        {
            "corpus_id": "272827578",
            "title": "NLP4PBM: a systematic review on process extraction using natural language processing with rule-based, machine and deep learning methods",
            "text": "We have observed a paradigm shift over the last 5 years that coincides with the emergence of DL in NLP; authors are increasingly using DL models such as Transformers (e.g., BERT [9]) and LSTM [10] in process extraction. Until now, 4 works have applied Large Language Models (LLM), as discussed in Section 8.4. \n\nQuishpi et al. [46] note that classical, rule-based AI approaches can still have advantages over DL systems: (1) training DL systems from scratch requires huge amounts of annotated training data, and the cost of producing such datasets may be higher than the cost of encoding expert knowledge into rules; and (2) DL systems, in contrast to rule-based systems, operate as black boxes, and it is difficult to tailor their behaviour to improve in case of wrong answers. \n\nIndeed, regarding training data (1), Neuberger et al. [29] hypothesised that the reason the end-to-end DL model Jerex, which was trained from scratch, was outperformed, was that the training data (PET dataset) was not yet extensive enough. Regarding the tailoring of behavior ( 2), the same authors point out that rule-based systems are also hard to adapt to even minor changes in the data. \n\nGoossens et al. [9] found that a pre-trained BERT model, fine-tuned on process extraction (i.e., not trained from scratch), outperforms ML methods and even a DL model trained from scratch. Hence, regarding training data (1), pre-trained models may obviate the need for large-scale training datasets. However, fine-tuning still requires training on a labeled dataset for adjusting parameters. Moreover, pretrained models may have difficulty with highly domain-specific terminology such as found in scientific texts [25]. \n\nFinally, authors have pointed out that certain DL models may be intrinsically more suited towards automated process extraction: \n\n\u2022 LSTMs are adept at handling sequences, making them more suitable for modeling processes. Goossens et al. [9] evaluated a Bi-LSTM-CRF model, pointing out that their particular structure affords them better insights into sentences.",
            "score": 0.3805965496984789,
            "section_title": "Use of Deep Learning for NLP",
            "char_start_offset": 43626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "251873522"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "229376425"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "237393623"
                },
                {
                    "start": 1190,
                    "end": 1193,
                    "matchedPaperCorpusId": "251873522"
                },
                {
                    "start": 1688,
                    "end": 1692,
                    "matchedPaperCorpusId": "51612223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01422882080078125
        },
        {
            "corpus_id": "263829606",
            "title": "Higher-Order DeepTrails: Unified Approach to *Trails",
            "text": "This work introduces a novel methodology to analyze and describe sequential user behavior. For this, we follow established settings as introduced in HypTrails and its follow-up extensions. Given a set of user observations modeled as sequences, the goal is to either find the best matching hypothesis that explains the observed user behavior (HypTrails [2] & MixedTrails [3]) or to find \"interesting\" subgroups of users that behave differently from other groups (SubTrails [4]). These existing approaches address this topic by limiting themselves to analyzing single-step transition behavior, hence breaking the observed sequences into first-order Markov chains and analyzing these using Bayesian inference. We argue that this inherently fails to take into account the sequential nature of the data and therefore propose using sequential machine learning models to address this problem. Specifically, we use autoregressive language models, traditionally applied to Natural Language Modeling and sequential data, based on the intuition that the models will discover and utilize higher-order dependencies. \n\nThe following sections explain how we model user behavior represented as sequences with autoregressive language models, as well as measure how well a (higher-order) hypothesis matches these user sequences. In addition to these HypTrails [2] and MixedTrails [3] settings, we also explore a setting without available hypotheses to show how to take advantage of transition features to analyze the sequences in a self-supervised manner (cf. SubTrails [4]). We can address all these settings using our language model-based approach with only minor modifications needed between the settings, as depicted in Figure 1 and described in the following. We begin by introducing our common underlying methodology in Sections 3.1 and 3.2.",
            "score": 0.38052552642276294,
            "section_title": "Methodology",
            "char_start_offset": 5769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1102
                },
                {
                    "start": 1105,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "950912"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "207238953"
                },
                {
                    "start": 1362,
                    "end": 1365,
                    "matchedPaperCorpusId": "950912"
                },
                {
                    "start": 1552,
                    "end": 1555,
                    "matchedPaperCorpusId": "207238953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01490020751953125
        },
        {
            "corpus_id": "269982345",
            "title": "A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models",
            "text": "In [60], the authors also present FlowSeq, another model for non-autoregressive sequence generation that utilizes latent variables.Non-autoregressive models try to decouple the dependencies of decoding history during generation, which can result in poor performance.FlowSeq, however, use generative flow [102], a method capable of transforming simple distributions like Gaussians into complex ones through invertible transformations.By encoding the prior distribution p \u03b8 (z|x) using generative flow, FlowSeq introduces meaningful latent variables z into the non-autoregressive generation process, thereby modeling dependencies between output tokens while enabling efficient parallel decoding.This approach addresses the limitations of naively assuming token independence in basic nonautoregressive models and represents a significant advancement in the field of sequence-to-sequence modeling.FlowSeq performs similarly to the best non-autoregressive models, with decoding time remaining nearly constant regardless of sequence length.\n\nBuilding upon the idea of integrating non-autoregressive decoding with latent variables, [61] propose a novel non-autoregressive neural sequence model employing iterative refinement.This approach applies to various sequence generation tasks beyond machine translation.The model operates as both a latent variable model and a conditional denoising autoencoder, integrating a learning algorithm that combines lower-bound maximization and reconstruction error minimization.To address the trade-off between generation latency and quality, the authors devise an iterative inference strategy with an adaptive number of steps.They further specialize the learning algorithm to interpret the entire model as a latent variable model, with each refinement step viewed as denoising.This methodology, implemented using the Transformer architecture, is evaluated on both machine translation and image caption generation tasks.Results demonstrate that the proposed non-autoregressive model performs similarly to the autoregressive counterpart while significantly accelerating decoding.Moreover, the qualitative analysis highlights the efficacy of iterative refinement in gradually enhancing target sequences across multiple steps.\n\nIn [62], authors introduce the Syntactically Supervised Transformer (SynST).",
            "score": 0.38042091146006096,
            "section_title": "Non-autoregressive Models",
            "char_start_offset": 69053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1034
                },
                {
                    "start": 1036,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1948
                },
                {
                    "start": 1948,
                    "end": 2106
                },
                {
                    "start": 2106,
                    "end": 2251
                },
                {
                    "start": 2253,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "202539063"
                },
                {
                    "start": 304,
                    "end": 309,
                    "matchedPaperCorpusId": "12554042"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "3438497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121917724609375
        },
        {
            "corpus_id": "267949411",
            "title": "Diffusion models in text generation: a survey",
            "text": "In addition to handling the three aforementioned text generation tasks individually, current research on diffusion models in text generation often focuses on addressing multiple tasks simultaneously.\n\nSelf-conditioned embedding diffusion (SED) (Strudel et al., 2022) proposes a continuous diffusion mechanism called self-conditioned embedding, which learns a flexible and scalable diffusion model suitable for both conditional and unconditional text generation.Notably, this study can support text padding, laying the foundation for exploring embedding space design and padding capabilities.\n\nStep-unrolled Denoising Autoencoder (SUNDAE) (Savinov et al., 2021) introduces the training mechanism of unrolled denoising based on Autoencoders.Compared to the usual denoising approach, it requires fewer iterations to converge and demonstrates good performance in machine translation and unconditional text generation tasks.Additionally, it breaks the autoregressive limitation and can fill arbitrary blank patterns in templates, paving the way for new approaches to text editing and text repair.\n\nLatent Diffusion for Language Generation (LD4LG) (Lovelace et al., 2022), unlike other works that transfer discrete text to continuous space by embedding, learns the process of diffusion over the latent space of pre-trained language models and extends this framework from unconditional text generation to conditional text generation.\n\nSemi-autoregressive Simplex-based Diffusion Language Model (SSD-LM) (Han, Kumar & Tsvetkov, 2023), a semi-autoregressive diffusion language model that performs diffusion over the natural vocabulary space, enables flexible output length and modularity control through these two key designs features.On unconstrained and controlled text generation tasks, SSD-LM outperforms the autoregressive baseline model in terms of quality and diversity.",
            "score": 0.3803954688771788,
            "section_title": "Multi-mode text generation",
            "char_start_offset": 21500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 201,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 591
                },
                {
                    "start": 593,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1091
                },
                {
                    "start": 1093,
                    "end": 1426
                },
                {
                    "start": 1428,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 1496,
                    "end": 1525,
                    "matchedPaperCorpusId": "253237701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017578125
        },
        {
            "corpus_id": "258187022",
            "title": "Revisiting k-NN for Fine-tuning Pre-trained Language Models",
            "text": "have shown superior performance across a wide range of language-related downstream tasks (Kowsari et al., 2019;Nan et al., 2020). Afterward, the conventional paradigm fine-tuning, which extends extra task-specific classifiers on the top of PLMs, has been proposed to apply PLMs for downstream tasks. Recently, a new paradigm called prompt-tuning, which originated from GPT-3 (Brown et al., 2020), has been introduced and has shown better results for PLMs on few-shot and zero-shot tasks. Finetuning has proved to be effective on supervised tasks and is widely used as the standard method for natural language processing (NLP). Despite the effectiveness of adapting PLMs, parametric-based eager learners (Friedman, 2017), like PLMs with neural networks, require estimating the model parameters with an intensive learning stage. Besides, Training a large PLM model can require significant computing resources and energy, which have negative environmental consequences. As a result, there has been a growing interest in developing more efficient and sustainable methods for training and deploying PLMs. \n\nA stark contrast to PLMs is the k-NN classifier: a simplest machine learning algorithm that does not have a training phase but simply predicts labels based on the nearest training examples instead. NLP researchers (Khandelwal et al., 2020;He et al., 2021) have found that k-NN enable excellent unconditional language modeling (Khandelwal et al., 2020;He et al., 2021) during test phrase. According the definition in (Friedman, 2017), k-NN is actually a lazy learner that can avoid over-fitting of parameters (Boiman et al., 2008) and effectively smooths out the impact of isolated noisy training data (Orhan, 2018). Though k-NN has the above advantages, previous works only leverage k-NN for testing, and there is no systematic examination of the full utilization of k-NN for PLMs. \n\nTo this end, we have conducted a comprehensive and in-depth empirical study of the k-NN classifier for natural language understanding (NLU).",
            "score": 0.38036655230425703,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 111,
                    "matchedPaperCorpusId": "119297355"
                },
                {
                    "start": 111,
                    "end": 128,
                    "matchedPaperCorpusId": "218613850"
                },
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1316,
                    "end": 1341,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1341,
                    "end": 1357,
                    "matchedPaperCorpusId": "237452184"
                },
                {
                    "start": 1428,
                    "end": 1453,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1453,
                    "end": 1469,
                    "matchedPaperCorpusId": "237452184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0174407958984375
        },
        {
            "corpus_id": "278165590",
            "title": "Bi-directional Model Cascading with Proxy Confidence",
            "text": "In the last decade large, transformer-based, pretrained language models (LLMs) have surged in both popularity and capability, radically improving the state of the art in a wide range of tasks, from traditional Natural Language Processing (NLP) tasks to more general problems that were previously impossible, such as complex logical reasoning and the application of expert knowledge. However, these abilities have often required increasingly large model sizes where inference is expensive, raising concerns ranging from user budgets [6] to environmental impacts [35,37], including among the NLP community [1]. In response to this, a diverse range of techniques have been developed to accelerate or improve the efficiency of LLMs. Some approaches aim to reduce the size of individual models like quantization [49] and model pruning [38]. Others have drawn from the field of adaptive computation, developing systems of models that collaborate to improve some aspect of performance. Conceptually relying on the fact that not all tasks are equally difficult, these approaches make the inference process dependent on the individual sample, leveraging smaller models or subsystems to handle easy instances. This concept has been used to provide large improvements in model throughput using draft-and-verify paradigms [5,25] as well as to optimize the accuracy-efficiency trade-off [28]. Early-exit model cascading is a technique that has been used in other machine learning domains for some time [31,46] that takes advantage of this principle by engaging a sequence of models of increasing size and capability. Inference is performed on each sample starting with the smallest model and exiting the cascade if the output of any model appears sufficient by some measure of confidence. This approach has recently been applied to NLP and LLMs and has delivered significant efficiency improvements [50]. \n\nHowever, there are some notable limitations that we address in this paper. \n\nIn cascading models, there is typically a sequence of models of increasing complexity,  1 . . .   , and Jitkrittum et al. [21] observe that standardly, the decision to defer from a smaller model   to larger model  +1 is based only on some measure of confidence from the smaller model   . This can result in various failure modes, for example in confidence-based deferral erroneously forwarding samples where  +1 performs worse than   .",
            "score": 0.38036495550700244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "273789709"
                },
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "254096365"
                },
                {
                    "start": 1493,
                    "end": 1496,
                    "matchedPaperCorpusId": "2715202"
                },
                {
                    "start": 1886,
                    "end": 1890,
                    "matchedPaperCorpusId": "263671564"
                },
                {
                    "start": 2093,
                    "end": 2097,
                    "matchedPaperCorpusId": "259360997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.016845703125
        },
        {
            "corpus_id": "259075458",
            "title": "PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model",
            "text": "We present a two-stage latent text diffusion model that uses an autoencoder to condense lengthy texts into a limited number of paragraph embeddings, and a continous time diffusion model that learns the distribution of these embeddings. Our proposed model alleviates the issue of repetition and advances generation diversity across different tasks. Compared to text diffusion models that perform diffusion solely on token or token embedding space, our method generates fluent text with improved diversity and reduced repetition. There may be toxicity or fairness issues in the dateset we used that we have not been able to identify. There are several limitations that warrant further investigation. \n\nOur work relies on an autoregressive decoder for converting latent representation into coherent text. It is possible to explore the feasibility of non-autoregressive decoders to bolster efficiency while minimizing conversion errors and hallucination in the generation. Furthermore, the classifier-free guidance approach results in a discrepancy between training and inference data distribution when feeding to the diffusion backbone. It would be interesting to investigate a \"calibration\" strategy for the latent code to better fit the data distribution during training. We observed when the value of p is within (0, 0.7), there exists a correlation between the S overall and the PPL of the generation obtained from training PLANNER on the corresponding z (Figure 4). Performing a grid search on each task using diffusion models is an expensive process. Thus, we opted to use the surrogate S overall to choose the optimal p during the training of the paragraph embedder. However, it has been observed that an increase in the value of p leads to a deviation between the two. This could be attributed to a higher conversion error that occurs when p is excessively large.",
            "score": 0.37999377388989825,
            "section_title": "Conclusion",
            "char_start_offset": 30747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "267897676",
            "title": "Text Diffusion with Reinforced Conditioning",
            "text": "In this paper, we start by taking a thorough analysis of the vanilla self-conditioning approach and observe it suffers from degradation -marginalizing the diffusion latent. Hampered by such degradation, sampling with self-conditioning heavily depends on the quality of the first step (from pure Gaussian) and fails to fully utilize the diffusion process. Besides, by analyzing current sampling methods in text diffusion models, we discover and study the misalignment issue, bringing out insights in designing a better variance schedule. \n\nMotivated by our findings, we propose TREC, a novel approach that empower Text Diffusion models with Reinforced Conditioning. Specifically, we develop a novel reinforced self-conditioning that mitigates the degradation by directly motivating quality improvements from self-conditions with reward signals. Furthermore, we propose time-aware variance scaling that facilitates training of diffusion. We conduct a series of experiments on various tasks of NLG, including machine translation, paraphrasing, and question generation. Results show that composing operators within our method manages to generate high-quality sequences, outperforming a series of autoregressive, non-autoregressive, and diffusion baselines. Detailed analysis demonstrates the effectiveness of TREC in mitigating degradation of self-conditioning with reward signals, as well as leveraging the diffusion process to iteratively refine its output.",
            "score": 0.37988574726957564,
            "section_title": "Introduction",
            "char_start_offset": 1887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1455
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0181884765625
        },
        {
            "corpus_id": "259095911",
            "title": "On the Design Fundamentals of Diffusion Models: A Survey",
            "text": "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]. \n\nInstead of removing classifiers, self-guidance [166] reduces or removes the requirement of annotation by using internal values like activations, attention maps, and intermediate representations [198], [199] to compute guidance. Such design helps finer-grained control and is compatible with aforementioned guidance.",
            "score": 0.37955109953353117,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 44026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "This approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023).",
            "score": 0.37955109953353117,
            "section_title": "Transformer Next Token Prediction: A Classification Perspective",
            "char_start_offset": 12637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "259316437",
            "title": "Transformers and large language models in healthcare: A review",
            "text": "Foundation models are large-scale AI systems trained on vast amounts of data to be adapted for a wide range of downstream tasks [23]. LLMs colloquially refer to a class of foundation models with parameters on the order of billions trained on language corpora with billions of words to generate human-like language and solve different NLP tasks. Most LLMs use the Transformer architecture, the current default architecture for processing sequential data as of 2023. The success of LLMs comes from the self-supervised pre-training paradigm, which takes advantage of large free text data without annotation. This pre-training technique enabled LLMs to generate coherent and realistic language, making them useful for various applications such as text completion, dialogue generation, and content generation. Large generative AI models trained to generate text and question answering are autoregressive decoder-only language models. Examples of autoregressive decoder-only language models include PaLM [24], GPT-3 [25], Chinchilla, LLaMA [26], PaLM2 [27] used in BARD chatbot, and GPT-4 [28]. These models are trained on billions of tokens obtained from datasets such as Common Crawl, WebText2, Books1, Books2, Wikipedia, Stack Exchange, PubMed, ArXiv, Github, Gutenberg, and many more. Some of the domain-specific LLMs include Galactica [29], trained on curated human scientific knowledge corpora, BloombergGPT [30], trained on proprietary financial data, and CodeX [31] for code generation. A timeline of popular LLMs is displayed in Fig. 8. The number of parameters in LLMs and the size of their training data has increased rapidly, reaching up to trillions of tokens [26]. The capabilities of LLMs appear to be a function of the amount of data, parameters, and computation resources rather than architectural design advancements [32]. The scaled-up language models develop abilities beyond the trained outcomes called 'emergent abilities,' which are not designed but discovered after deployment [33]. For example, GPT-3 showed few-shot prompting ability; when provided few input-outputs for a natural language task, the model can perform the task on unseen samples without further training or gradient updates to",
            "score": 0.37933108941041876,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 16710,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0164031982421875
        },
        {
            "corpus_id": "276249479",
            "title": "History-Guided Video Diffusion",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance (Ho et al., 2022b;Wang et al., 2023). For frame conditioning, \"first frame\" guidance is commonplace in image-to-video models (Blattmann et al., 2023a;  Yang et al., 2024), or \"fixed set of few frames\" (Blattmann et al., 2023b;Gupta et al., 2023;Watson et al., 2024), likewise in multi-view diffusion models (Gao et al., 2024). \n\nOur work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score -similar to prior works in compositional generative models (Du & Kaelbling, 2024;Liu et al., 2022;Du et al., 2023), we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance. Diffusion Forcing. Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF) (Chen et al., 2024) proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid (Yin et al., 2024) builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model.",
            "score": 0.3792469027442187,
            "section_title": "Preliminaries and Related Work",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 762,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 1127,
                    "end": 1144,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1144,
                    "end": 1160,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 1558,
                    "end": 1577,
                    "matchedPaperCorpusId": "270869622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "278481053",
            "title": "GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions",
            "text": "LLMs are a subset of GenAI and have emerged as highly capable systems, demonstrating remarkable proficiency in various Natural Language Processing (NLP) tasks (Corvello, 2024). The introduction of advanced models such as OpenAI's GPT-4, Gemini, DeepSeek, Anthropic's Claude, and Google's PaLM2 have significantly accelerated advancements in both NLP and the broader field of artificial intelligence (Maarouf et al., 2025;Lang et al., 2024). There is a speculation that these models are approaching human-level performance. LLMs like GPT-4 operate on an autoregressive framework, which enables them to generate sequences such as sentences by predicting each subsequent word based on prior context. Unlike traditional language models, LLMs benefit from more sophisticated training techniques and enhanced capabilities (Chang et al., 2024;Zhao et al. 2025). They rely on Transformer-based architectures, which use an 'attention' mechanism to assess the significance of words within a sentence. This allows them to evaluate the entire context of a given text, making them exceptionally effective for sequential predictions. \n\nThe LLMs are trained on vast amounts of textual data, learning to predict words by continuously refining internal parameters to minimize the gap between their predictions and actual language patterns (Chang et al., 2024;Zhao et al. 2025). A significant advantage of LLMs is their ability to perform various language tasks without requiring additional specialized training. Since they have been pre-trained on extensive datasets, they can generate meaningful and contextually appropriate responses based solely on user input. \n\nOne of their practical applications is in text summarization, where LLMs can analyse and condense large bodies of text into concise summaries (Chang et al., 2024;Zhao et al. 2025). These summaries can serve as high-quality benchmarks for evaluating other summarization models. Beyond summarization, LLMs are employed in tasks such as text classification, which automatically sorts text into predefined categories. This capability is useful in areas such as spam detection and sentiment analysis, where the model can efficiently process and categorize vast amounts of data. Another application involves evaluating authorship and attribution.",
            "score": 0.37913905045293267,
            "section_title": "Large Language Models: A brief overview",
            "char_start_offset": 2896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 439,
                    "matchedPaperCorpusId": "272531577"
                },
                {
                    "start": 816,
                    "end": 836,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 1322,
                    "end": 1342,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 1791,
                    "end": 1811,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015777587890625
        },
        {
            "corpus_id": "258557950",
            "title": "The Current State of Summarization",
            "text": "Figure 1: Current summarization systems can be broadly divided into pre-trained encoder-decoder models and large autoregressive language models. In general, instruction-tuned models are most capable when it comes to zero-shot summarization. Other encoder-decoder models usually require fine-tuning, while autoregressive LLMs are less effective without instruction tuning. Some pre-trained encoder-decoder models are specifically designed for the summarization task. \n\nal., 2020) can be observed, which is trained by removing important sentences from the input document and tasking the model with regenerating them. In a comprehensive evaluation of 23 models for the summarization task, Fabbri et al. (2021) conclude that PEGASUS, BART, and T5 \"consistently performed the best on most dimensions\", which involves human evaluations as well as automatic metrics. \n\nRecently, a task-specific fine-tuning mechanism called BRIO (Y. Liu, Liu, et al., 2022) was proposed for summarization. This method introduces a contrastive learning component to prevent assigning the entire distribution mass to the reference summary and instead account for candidate summaries as well. BRIO has been applied to several models, including BART and PEGASUS. Another noteworthy model is Z-Code++ (P. He et al., 2023), as it incorporates an intermediate task-adaptive fine-tuning step using a broad collection of summarization datasets before fine-tuning on a specific summarization task. This method has been shown to be especially effective in low-resource settings.",
            "score": 0.3790751358187445,
            "section_title": "Domain-Specific Models",
            "char_start_offset": 2868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1543
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 706,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 1276,
                    "end": 1292,
                    "matchedPaperCorpusId": "251719571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0182647705078125
        },
        {
            "corpus_id": "269982020",
            "title": "Lessons from the Trenches on Reproducible Evaluation of Language Models",
            "text": "Due to the time-consuming nature of developing good benchmarks and the rapid pace of change in NLP research in the past decade, many widely used language model evaluation benchmarks do not represent the current paradigm of how language models are trained.This has two major impacts:\n\n1. Benchmarks are being used for purposes they were not originally designed for or designed for validity under: for example, a large number of benchmarks have been built around fine-tuning on a known training set and closed space of labels (Wang et al., 2019b;a).\n\n2. There is no \"ground-truth\" implementation from the original benchmark authors for many of these popular benchmarks \"retrofitted\" to be used with autoregressive LMs.\n\nIn the absence of a clear standard, the community's methodology for evaluating on these benchmarks may be fragmented or undocumented (Clark et al., 2018;Paperno et al., 2016).\n\nTo illustrate the effects of this development timeline, Figure 1 shows how many prominent LM benchmarks were designed prior to shifts such as in-context learning and chat interaction, and therefore were not designed to take these formats and approaches into account.This can affect validity or difficulty in unforeseen ways.\n\nFigure 1: A timeline showing the relative release dates of a selection of notable benchmarks used to evaluate LMs, as compared to the release dates of BERT (Devlin et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and ChatGPT, used as approximate standins for shifts in how the community uses and therefore evaluates LMs.Common practice for evaluating autoregressive language models today diverges from the method described in the paper for all listed tasks except MMLU and MATH.",
            "score": 0.37880020005475745,
            "section_title": "Fast-changing Progress and Conventions",
            "char_start_offset": 12553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 282
                },
                {
                    "start": 284,
                    "end": 547
                },
                {
                    "start": 549,
                    "end": 716
                },
                {
                    "start": 718,
                    "end": 893
                },
                {
                    "start": 895,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1219
                },
                {
                    "start": 1221,
                    "end": 1565
                },
                {
                    "start": 1565,
                    "end": 1723
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014556884765625
        },
        {
            "corpus_id": "259376776",
            "title": "KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models for Clinical Trial Entailment Identification",
            "text": "The full results are shown in Table 2. We can observe that PTLMs with a Masked Language Modeling (MLM) objective generally can achieve satisfactory performance. The majority of them achieve an F1 score of above 0.6 on the test set. The GPT2 family, on the other hand, struggles with the task and can only achieve comparable performances when the parameter is over 700 million, which is nearly seven times more than other discriminative PTLMs. One possible reason is that autoregressive language modeling is not that competitive at classification tasks, and the GPT2 model cannot learn the negative samples well. Our best model is finetuned based on the DeBERTa-v3-large, which reaches a 0.764 F1 score and is significantly outperforming other models. This may be due to the fact that DeBERTa-v3-large possesses the largest number of parameters, and its disentangled parameter-sharing technique is effective for evidence-understanding tasks (He et al., 2023b). Such a result enables our model to secure a fifth rank on the official leaderboard and implies that advanced PTLM can be proficient in solving the textual entailment identification task. We further study the ablation of our proposed prompt. DeBERTa-v3-large finetuned on two different input prompts are compared as baselines. Special Token stands for replacing the separator tokens [SEP] with natural language guidance such as \"the evidence from CTR1 is\". Statement stands for using the statement only for prediction. The results, shown in Table 3, support our claim that leveraging predefined special tokens as separators and concatenating evidence after the statement is effective. \n\nMeanwhile, our best model's precision score is ranked fourth, and the recall score is ranked thirteenth. This indicates that the model is wrongly classifying many entailments as contradictions, which causes a relatively low recall and is considered a major limitation of our system. By further observing the errors made by our system, we find that challenging CTR-statement pairs involving complex or mathematical reasoning (Ferreira and Freitas, 2020) are often wrongly classified.",
            "score": 0.37865062268947197,
            "section_title": "Results",
            "char_start_offset": 13924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 940,
                    "end": 958,
                    "matchedPaperCorpusId": "244346093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0102081298828125
        },
        {
            "corpus_id": "276317202",
            "title": "Theoretical Benefit and Limitation of Diffusion Language Model",
            "text": "Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off is not yet well understood. In this paper, we present a rigorous theoretical analysis of a widely used type of diffusion language model, the Masked Diffusion Model (MDM), and find that its effectiveness heavily depends on the target evaluation metric. Under mild conditions, we prove that when using perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling steps regardless of sequence length, demonstrating that efficiency can be achieved without sacrificing performance. However, when using the sequence error rate--which is important for understanding the\"correctness\"of a sequence, such as a reasoning chain--we show that the required sampling steps must scale linearly with sequence length to obtain\"correct\"sequences, thereby eliminating MDM's efficiency advantage over autoregressive models. Our analysis establishes the first theoretical foundation for understanding the benefits and limitations of MDMs. All theoretical findings are supported by empirical studies.",
            "score": 0.3786337456459354,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014007568359375
        },
        {
            "corpus_id": "27764139",
            "title": "Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling",
            "text": "Applying reinforcement learning to natural language processing (NLP) tasks recently attracts enormous interest for two main purposes, i.e., optimizing the model according to nondifferentiable objectives and accelerating the model speed. Lei et al. [2016] propose a method to select a subset of a review passage for sentiment analysis from a specific aspect. He et al. [2016a] use reinforcement learning method to finetune a bilingual machine translation model by well-trained monolingual language models. Yogatama et al. [2016] use built-in transition-based parsing module to generate semantic constituency parsing tree for downstream NLP tasks by using reinforcement learning. Yu et al. [2017] propose a reinforcement learning based skim reading method, which is implemented on recurrent models, to skim the insignificant time slots to achieve higher time efficiency. Choi et al. [2017a] separately implement a hard attention or a soft attention on a question answering task to generate the document summary. Shen et al. [2017b] use reinforcement learning method to at-tend the memory with dynamic episode number rather than fixed one for adequate and efficient machine comprehension. Hu et al. [2017] employ policy gradient method to optimize the model with non-differentiable objective of machine comprehension, i.e., F1 score of matching the prediction with the ground truth. Li et al. [2017] propose a service dialogue system to sell movie tickets online, in which the agent in reinforcement learning is used to select which user's information should be obtained in next dialogue round for minimum number of dialogue rounds to sell the ticket successfully. Zhang and Lapata [2017] simplify a sentence with objectives of maximum simplicity, relevance and fluency, where all three objectives are all non-differentiable w.r.t the parameters of model. \n\nHard attention mechanisms are proposed to alleviate the weakness of soft attention mechanisms, which are separately applied to computer vision (CV) or NLP tasks in existing works. The hard attention mechanism is first proposed for visual question answer task [Xu et al., 2015] and empirically outperform the soft attention, where the hard attention is used to focus on a particular, small and local image region for a time slot of a recurrent model, whose output is used to compose the answer sentence.",
            "score": 0.37841797187386594,
            "section_title": "Related Work",
            "char_start_offset": 26852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 254,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "4029193"
                },
                {
                    "start": 869,
                    "end": 888,
                    "matchedPaperCorpusId": "19172224"
                },
                {
                    "start": 1010,
                    "end": 1029,
                    "matchedPaperCorpusId": "2317858"
                },
                {
                    "start": 1380,
                    "end": 1396,
                    "matchedPaperCorpusId": "9672033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00974273681640625
        },
        {
            "corpus_id": "250303382",
            "title": "Processing of Condition Monitoring Annotations with BERT and Technical Language Substitution: A Case Study",
            "text": "The improved performance of the LSTM-system with embedding inputs is also important for evaluation as it indicates a better distribution of the embedding space with regard to fault classes. It also reinforces the concept of assisting language model evaluation through a keyword-based system, as the substitution removes the keywords from the language model input space. Consequently, the improvements seen in the classification step must be from a superior distributional representation of the underlying semantics rather than a \"short-circuit\" mapping between keywords. \n\nThe lack of intrinsic or extrinsic evaluation tools or datasets for language models in the technical domain obfuscates potential improvements in technical language understanding. Cadavid et al. (2020) evaluated technical language representations using internal properties of the dataset, with equipment descriptions, symptoms and equipment importance as input, and type of disturbance (dominant or recessive) and maintenance workload (hours) as outputs. The existence of these two possible outputs facilitates an extrinsic evaluation, though one not necessarily in complete correlation with annotation semantics, as their results indicate that the TF-IDF-based methods by far outperform pre-trained CamemBERT and often perform just shy of fine-tuned CamemBERT. Thus, the increased natural language understanding of CamemBERT is either impeded by OOV technical terms, or the evaluation method has poor correlation with language understanding. Without access to resources such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) from NLP, it is difficult to properly evaluate whether the language model is overfitted to the specific language distribution. Nandyala et al. (2021) used an English dataset without obvious extrinsic evaluation tasks, and thus rely on quantitative evaluation in word similarity, sentence similarity and word cluster projections to compare their different distributional word vector models. These methods offer some initial insight into the workings of language models on technical language, but without quantitative values it is difficult to compare models and evaluation relies on subjective human judge-ment. Ideally, work towards creating a technical language version of GLUE and SuperGLUE can be initiated to further the research into adaption, pre-training and fine-tuning of language models on technical language.",
            "score": 0.3782712426174152,
            "section_title": "DISCUSSION",
            "char_start_offset": 28487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2429
                }
            ],
            "ref_mentions": [
                {
                    "start": 1556,
                    "end": 1575,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1590,
                    "end": 1609,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00847625732421875
        },
        {
            "corpus_id": "252762155",
            "title": "On Distillation of Guided Diffusion Models",
            "text": "Denoising diffusion probabilistic models (DDPMs) [4,37,39,40] have achieved state-of-the-art performance on image generation [22,[26][27][28]31], audio synthesis [11], molecular generation [44], and likelihood estimation [10]. Classifierfree guidance [6] further improves the sample quality of diffusion models and has been widely used in large-scale diffusion model frameworks including GLIDE [23], Stable Diffusion [28], DALL\u00b7E 2 [26], and Imagen [31]. However, one key limitation of classifier-free guidance is its low sampling efficiency-it requires evaluating two diffusion models tens to hundreds of times to generate one sample. This limitation has hindered the application of classifier-free guidance models in real-world settings. Although distillation approaches have been proposed for diffusion models [33,38], these approaches are not directly applicable to classifier-free guided diffusion models. To deal with this issue, we propose a two-stage distillation approach to improving the sampling efficiency of classifier-free guided models. In the first stage, we introduce a single student model to match the combined output of the two diffusion models of the teacher. In the second stage, we progressively distill the model learned from the first stage to a fewer-step model using the approach introduced in [33]. Using our approach, a single distilled model is able to handle a wide range of different guidance strengths, allowing for the trade-off between sample quality and diversity efficiently. To sample from our model, we consider existing deterministic samplers in the literature [33,38] and further propose a stochastic sampling process.\n\nOur distillation framework can not only be applied to standard diffusion models trained on the pixel-space [4,36,39], but also diffusion models trained on the latent-space of an autoencoder [28,35] (e.g., Stable Diffusion [28]). For diffusion models directly trained on the pixel-space, our experiments on ImageNet 64x64 and CIFAR-10 show that the proposed distilled model can generate samples visually comparable to that of the teacher using only 4 steps and is able to achieve",
            "score": 0.3781926412736706,
            "section_title": "Introduction",
            "char_start_offset": 1206,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 137,
                    "end": 141,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "221818900"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1768,
                    "end": 1771,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1771,
                    "end": 1774,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1774,
                    "end": 1777,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1855,
                    "end": 1858,
                    "matchedPaperCorpusId": "235421901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "267523116",
            "title": "LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text",
            "text": "The contributions of this paper are three-fold: \n\n\u2022 We introduce two dedicated datasets for legal violation identification, based on previous class action cases and legal news. These datasets, which include new legal entities, were generated using LLMs and validated by domain experts. \n\n\u2022 We evaluate various language models, including BERT-based models and LLMs, across two different NLP tasks, offering valuable insights into their applicability and limitations in the context of legal NLP. \n\n\u2022 We implement a two-setup approach employing both NER and NLI tasks, providing a methodology for legal violation detection and resolution.",
            "score": 0.37795908480217155,
            "section_title": "Contributions",
            "char_start_offset": 2656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 50,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01467132568359375
        },
        {
            "corpus_id": "3261603",
            "title": "Using single layer networks for discrete, sequential data: An example from Natural Language Processing",
            "text": "The automatic parsing of natural language poses a significant problem, and neural computing techniques can contribute to its solution. For an overview of the scope for work in NLP see [5, pages 4-11]. Our prototype gives results of over 90% correct on declarative sentences from technical manuals (see Section 8). \n\nAutomated syntactic analysis of natural language has, in the last decade, been characterised by two paradigms. Traditional AI, rule based methods contrast with probabilistic approaches, in which stochastic models are developed from large corpora of real texts. Neural techniques fall into the broad category of the latter, data driven methods, with trainable models developed from examples of known parses. The parser we have implemented uses a hybrid approach: rule based techniques are integrated with neural processors. \n\nParsing can be taken as a pattern matching task, in which a number of parses are postulated for some text. A classifier distinguishes between the desired parse and incorrect structures. The pattern matching capabilities of neural networks have a particular contribution to make to the process, since they can conveniently model negative as well as positive relationships. The occurrence of some words or groups of words inhibit others from following, and these constraints can be exploited. Arguments on the need for negative information in processing formal languages [6] can be extended to natural language. This is an important source of information which has been difficult for traditional probabilistic methods to access [7,8]. Neural methods also have the advantage that training is done in advance, so the run time computational load is low. representation of sequential data (Section 4). A description of the hybrid system used in our work is given, (Section 5). Then we examine some of the design issues for the neural components of this system. First, the data itself is examined closely. Then we consider how the data can be transformed for processing with a single layer net (Section 6). We also comment on the use of a Bayesian classifier, which performs slightly less well than the neural networks. Section 7 describes the three different networks: (a) the Perceptron, (b) Hodyne and (c) an LMS model. \n\nIn Section 8 we compare the performance of the three networks. Generalization is good, providing that enough training data is used.",
            "score": 0.3776692586930299,
            "section_title": "Natural language processing (NLP)",
            "char_start_offset": 2865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2256
                },
                {
                    "start": 2259,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2390
                }
            ],
            "ref_mentions": [
                {
                    "start": 1410,
                    "end": 1413,
                    "matchedPaperCorpusId": "12438987"
                },
                {
                    "start": 1570,
                    "end": 1572,
                    "matchedPaperCorpusId": "2769726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121002197265625
        },
        {
            "corpus_id": "266693667",
            "title": "Exploring the effectiveness of instruction tuning in biomedical language processing",
            "text": "Transformers have become the cornerstone of modern NLP, providing the backbone for a wide * Both authors contributed equally to this work. array of applications including machine translation, question-answering, and text summarisation (Vaswani et al., 2017). Their self-attention mechanisms and parallelised architecture have proven to be highly effective in capturing the nuances of human language (Devlin et al., 2019). \n\nAutoregressive language models, exemplified by the Generative Pre-trained Transformer series like GPT (Radford et al., 2018) and GPT-3 (Brown et al., 2020), have revolutionised the way NLP is approached. These models, operating as decoder-only transformers, excel at generating text in a sequential, token-by-token manner, leveraging their attention mechanisms to focus on relevant segments of input text. Models based on this architecture, such as GPT-4 have demonstrated a remarkable ability to perform a variety of language tasks without the need for task-specific fine-tuning, showcasing strong zero-shot and few-shot learning capabilities. This feature allows these models to effectively respond to text-based prompts, including those with a limited number of examples or instructions, thereby enabling a more interactive and dynamic text generation process. \n\nMedical language models, particularly encoderonly models like BioBERT and ClinicalBERT, have been instrumental in advancing tasks such as medical diagnosis, biomedical literature mining, and clinical information extraction (Clusmann et al., 2023;Kormilitzin et al., 2021). Excelling in areas like classification and Named Entity Recognition (NER), these models have significantly contributed to biomedical NLP. However, they often lack inherent capabilities in interpreting and executing natural language instructions or generating reports from arXiv:2401.00579v1 [cs.CL] 31 Dec 2023 medical Electronic Health Records (EHRs). This limitation has spurred research into developing generative Large Language Models (LLMs) capable of handling more dynamic tasks, aiming to parallel the performance of specialised encoder-only models in the biomedical domain.",
            "score": 0.37737268431951554,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 257,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 399,
                    "end": 420,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 559,
                    "end": 579,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1536,
                    "end": 1561,
                    "matchedPaperCorpusId": "211817856"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028106689453125
        },
        {
            "corpus_id": "273234048",
            "title": "DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation",
            "text": "Our proposed framework, built on an autoregressive model, naturally extends to discrete token modeling tasks. This includes discrete latent modeling for image generation (Gu et al., 2024) and multi-modal generation (Team, 2024). By leveraging a shared architecture, we jointly optimize for both continuous denoising and next-token prediction loss using cross-entropy for discrete latents which we call Kaleido-DART considering its architectural similarity as Gu et al. (2024) (see Figure 5). \n\nTo balance inference across modalities, we reweight the discrete loss (Eq (3)) according to the relative lengths between the discrete and image tokens: \n\nwhere \u03bb = # text tokens # image tokens . It is important to note that our approach is markedly distinct from several concurrent works aimed at unifying autoregressive and diffusion models within a single parameter space (Zhou et al., 2024;Xie et al., 2024;Zhao et al., 2024;Xiao et al., 2024). In these efforts, the primary goal is to adapt language model architectures to perform diffusion tasks, without modifying the underlying diffusion process itself to account for the shift in model design. As we have discussed, the Markovian nature of diffusion models inherently limits their ability to leverage generation history, a feature that lies at the core of autoregressive models. In contrast, DART is designed to merge the advantages of both autoregressive and diffusion frameworks, fully exploiting the autoregressive capabilities. This formulation also allows for seamless integration into LLM pipelines.",
            "score": 0.377157740675947,
            "section_title": "MULTI-MODAL GENERATION",
            "char_start_offset": 18053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01302337646484375
        },
        {
            "corpus_id": "253540326",
            "title": "Stance Classification of Social Media Texts for Under-Resourced Scenarios in Social Sciences",
            "text": "Advances in research and industrial applications of computational linguistics over the past decades have led to a variety of approaches and algorithms developed for the tasks of retrieval, processing, and generation of text data. One of the important developments in this regard is the involvement of statistical machine learning (ML) approaches [20,21] in contrast (or in addition) to traditional natural language processing (NLP) approaches, which often rely on custom manually engineered processing steps, rules, and features. The ML paradigm, instead, typically focuses on reducing the given problem to one of the typical ML tasks, such as classification or clustering, which can be achieved by transforming the input text data into a suitable representation, for instance, numerical vectors. Research on this topic has led to the invention of distributional and eventually distributed representations [22] such as word embeddings [23,24] and even sentences and document embeddings [25]. Artificial neural networks and deep networks, in particular, have been actively used for learning such representations and/or eventually for the respective \"downstream\" tasks such as text classification [26][27][28][29]. \n\nHowever, a typical supervised ML approach has a critical requirement of labeled training data of sufficient volume and quality. Deep learning approaches, in particular, can benefit from large training datasets to generalize well and adequately process unseen data when deployed. Collection, curation, annotation, and quality control of such labeled data are all time-and resource-intensive tasks. The proposed strategies for these issues include among others, crowdsourcing [30], active learning [31], distant/weak supervision [32,33], and transfer learning [34] involving existing pre-trained models, e.g., the state-of-the-art BERT models [35]. All these strategies have their advantages and disadvantages, and we discuss some of them in the following in the context of our more specific task.",
            "score": 0.3767293600595612,
            "section_title": "Related Work 2.1. Data-Related Concerns for Text Mining Tasks",
            "char_start_offset": 3957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 986,
                    "end": 990,
                    "matchedPaperCorpusId": "2407601"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "1009397"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "51872504"
                },
                {
                    "start": 1689,
                    "end": 1693,
                    "matchedPaperCorpusId": "7008675"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "30334806"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "14350387"
                },
                {
                    "start": 1746,
                    "end": 1749,
                    "matchedPaperCorpusId": "12245213"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0129241943359375
        },
        {
            "corpus_id": "270380319",
            "title": "Simple and Effective Masked Diffusion Language Models",
            "text": "Diffusion models excel at producing realistic, high-quality images and have received significant attention as potential tools for generating discrete data such as text [1,23,25], biological sequences [2], and graphs [45,48].Unlike autoregressive (AR) approaches, diffusion-based methods are not constrained to generate data sequentially, and therefore have the potential to improve long-term planning, controllable generation, and sampling speed.However, discrete diffusion methods exhibit a performance gap relative to AR models [1,17,19,25], especially in language modeling.The standard measure of language modeling performance is log-likelihood: when controlling for parameter count, prior work reports a sizable log-likelihood gap between AR and diffusion models.\n\nIn this work, we show that simple masked diffusion language modeling (MDLM) combined with effective training recipes is more performant than previously thought [1,19].We develop a well-engineered MDLM implementation that significantly improves discrete diffusion log-likelihood; we further improve likelihood using a simple substitution-based parameterization of the reverse diffusion process that enables deriving a Rao-Blackwellized continuous-time variational lower bound (ELBO) with improved tightness.Interestingly, our objective has a simple form: it is a weighted average of masked language modeling (MLM) losses [10], and can be used to endow BERT-style, encoder-only models with principled generation capabilities.We complement this framework with efficient samplers-including ones that can generate semi-autoregressively like a typical language model.Our masked diffusion models achieve a new state-of-the-art among diffusion models on language modeling benchmarks and approach the perplexity of AR models within 15-25%.Surprisingly, simple engineering choices significantly improve performance in both our models and simple baselines that were previously thought to perform poorly.Our framework also extends to non-language domains, including biological sequence modeling.We pre-train DNA sequence models and observe similar or higher downstream performance compared to classical BERT-style training, while also introducing generative capabilities that classical masked DNA language models lack.",
            "score": 0.37671230701985353,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 1961
                },
                {
                    "start": 1961,
                    "end": 2052
                },
                {
                    "start": 2052,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "258762371"
                },
                {
                    "start": 216,
                    "end": 220,
                    "matchedPaperCorpusId": "256900800"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "258967177"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01061248779296875
        },
        {
            "corpus_id": "233289747",
            "title": "Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research",
            "text": "The interaction between task paradigms, data characteristics, and available resources helps to determine what types of implementations are appropriate to the task. Implementations can be further broken down into representation technologies, for mathematically representing the language units to be analyzed; modeling architectures, for capturing regularities within that language; and optimization strategies (when using machine learning), for efficiently estimating model parameters from data. In low-resource settings, highly parameterized models such as BERT may not be appropriate, while large-scale GPU server farms enable highly complex model architectures. When the overall goal is factorized into multiple NLP tasks, optimization often involves joint or multi-task learning (Caruana, 1997). Ex. 1: large language models, dictionary matching, OCR, multi-task learning. Ex. 2: dictionary matching, small neural models.\n\nEvaluation Once a solution has been designed, it must be evaluated in terms of both the specific NLP problem(s) and the overall goal of the application. Standardized NLP task formulations typically define benchmark metrics which can be used for evaluating the NLP components: F-1 and AUC for information extraction, MRR and NDCG for information retrieval, etc. The design of these metrics is its own extensive area of research (Jones and Galliers, 1996;Hirschman and Thompson, 1997;Graham, 2015), and even established evaluation methods may be constantly revised (Grishman and Sundheim, 1995). Critically for the translational researcher, some metrics may be preferred over others (e.g., precision over recall), and standardized evaluation metrics may not reflect the goals and needs of applications (Friedman and Hripcsak, 1998). Improvements on standardized evaluation metrics (such as increased AUC) may even obscure degradations in application-relevant performance measures (such as decreased process efficiency). Translational researchers thus have the opportunity to work with NLP experts and SMEs to identify or develop metrics that capture both the effectiveness of the NLP system and its contribution to the application's overall goal. Ex. 1: F-1, patient outcomes. Ex. 2: F-1, billing rates.\n\nInterpretation Interpretability and analysis of NLP and other machine learning systems has been the focus of extensive research in recent years (Gilpin et al",
            "score": 0.37659205846469357,
            "section_title": "NLP Technologies",
            "char_start_offset": 24859,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 797,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 1379,
                    "end": 1408,
                    "matchedPaperCorpusId": "60658512"
                },
                {
                    "start": 1408,
                    "end": 1421,
                    "matchedPaperCorpusId": "8651371"
                },
                {
                    "start": 1489,
                    "end": 1518,
                    "matchedPaperCorpusId": "5188467"
                },
                {
                    "start": 1726,
                    "end": 1755,
                    "matchedPaperCorpusId": "16394105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0169219970703125
        },
        {
            "corpus_id": "247516845",
            "title": "Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",
            "text": "The emergence of foundation models (Bommasani et al., 2021) such as large-scale autoencoding models (e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019)) and autoregressive language models (e.g., GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), Megatron-Turing (Smith et al., 2022) and Gopher (Rae et al., 2021)) as well as multimodal vision and language models, such as FLAVA (Singh et al., 2021) and Perceiver (Jaegle et al., 2021), established a paradigm shift in Artificial Intelligence (AI). These foundation models, also called neural platforms, are built using self-supervised pretraining at scale. They are then able to be easily adapted to a wide range of downstream tasks via transfer learning (Bommasani et al., 2021) and finetuning (Lee et al., 2019). \n\nThe wide community adoption of foundation models can be explained by their key properties, two of which are emergent behavior and homogenization -which also make foundation models appealing for adaption across science and security domains. Emergence, or emergent behavior, reflect new behaviors that a model introduces or is capable of that it was not explicitly trained to perform. Homogenization is the consolidation of methods for building machine learning systems across a wide range of tasks. Another key advantage of scaling language models is that they perform competitively on language tasks using in-context learning without fine-tuning or gradient updates. Thus, in-context learning allows foundation models to be effectively used across new downstream tasks with only simple instructions and a few optional examples. \n\nIn this work we focus on a science domain (chemistry) and demonstrate the value and limitations of large-scale language models evaluated across a wide range of in-domain (science-focused) and out-of-domain tasks. Unlike the majority of work on foundation models that focuses on pretraining these models on book corpora, web pages, Wikipedia and mixed sources, e.g., the Pile (Gao et al., 2020), we pretrain our models on scientific literature.",
            "score": 0.3764504406268623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 430,
                    "end": 451,
                    "matchedPaperCorpusId": "232110866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01024627685546875
        },
        {
            "corpus_id": "233324191",
            "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures",
            "text": "The GPT family (I, II, and III) are autoregressive language models, based on transformer decoder blocks, unlike denoising autoencoder-based BERT. GPT-3 is trained on 175 billion parameters from a dataset of 300 billion tokens of text used for generating training examples for the model. Since GPT-3 is 10 times the size of any previous language model and for all tasks and purposes it employs few-shot learning via a text interface, without gradient updates or finetuning it achieves task agonism. It employs unsupervised pre-training, where the language model acquires a wide range of skills and pattern recognition capabilities. These are implemented on the fly to swiftly adapt to or identify the desired task. GPT-3 achieves SOTA in several NLP tasks although its few-shot learning falls short in reproducing similar results for other tasks.",
            "score": 0.37644959824794927,
            "section_title": "V-F LANGUAGE MODELS ARE FEW-SHOT LEARNERS: GPT-III",
            "char_start_offset": 41609,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0154876708984375
        },
        {
            "corpus_id": "257632030",
            "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4",
            "text": "Text-to-Text Transfer Transformer (T5) [51] and BART [52] are two examples of such models. T5 is one of the primary encoder-decoder methods that utilizes a \"text-to-text\" approach. This means that both input and output data are transformed into a standardized text format, enabling T5 to be trained on various NLP tasks, including machine translation, question-answering, and summarization, using the same model architecture. Another frequently used encoder-decoder method is BART, which combines the bidirectional encoder from BERT and the autoregressive decoder from GPT. BART takes advantage of the bidirectional modeling abilities of the encoder while retaining the autoregressive properties for generative tasks. \n\nRecently, a number of very large language models have emerged, including Megatron-Turing Natural Language Generation (MT-NLG), a monolithic transformer English language model with 530 billion parameters that outperforms prior state-of-the-art models in zero-, one-, and few-shot settings [53]. Pathways Language Model (PaLM), a dense decoder-only Transformer model, has 540 billion parameters and is trained with the pathways system. It achieved state-of-the-art few-shot performance across most tasks, with significant margins in many cases [54]. Additionally, BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is another transformer-based large language model, which is trained on approximately 176 billion parameters [55]. Furthermore, Jurassic-1 models come in two sizes, with the Jumbo version (178 billion parameters) being the largest and most sophisticated language model ever released for general use by developers [56].",
            "score": 0.3764282361764644,
            "section_title": "Prompt",
            "char_start_offset": 10574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1677
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 43,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "270199515",
            "title": "Leveraging Large Language Models for Entity Matching",
            "text": "Entity matching (EM) is a foundational task in data integration, essential for applications in various domains including data warehousing, e-commerce, healthcare, and social media [1,15,18,26,30].The objective of EM is to identify and link records that refer to the same real-world entities across disparate data sources.Traditional EM techniques, which often rely on rule-based systems or machine learning models with manually engineered features, face significant challenges when dealing with diverse, noisy, and unstructured data.\n\nRecent advancements in natural language processing (NLP) have led to the development of Large Language Models (LLMs) like GPT-4, which are trained on vast corpora of text and possess advanced capabilities in understanding and generating human-like text [10].These models have shown remarkable performance in various NLP tasks, suggesting their potential to revolutionize EM by addressing the limitations of traditional methods [19].\n\nThis paper envisions a future where LLMs are central to EM, offering improved accuracy, scalability, and adaptability.We will explore the advantages of LLMs in EM, discuss related work on weak supervision and unsupervised EM, outline the challenges, and propose future research directions.",
            "score": 0.3763844360401283,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 967
                },
                {
                    "start": 969,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1258
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "233486427"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "44063437"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "214802540"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "218982116"
                },
                {
                    "start": 788,
                    "end": 792,
                    "matchedPaperCorpusId": "257445349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014617919921875
        },
        {
            "corpus_id": "256231177",
            "title": "Imitating Human Behaviour with Diffusion Models",
            "text": "This resulted in a discrete approximation of the policy. Although having limitations, it was able to outperform other approximations in a number of environment, using both human and synthetic data. \n\nHere, we focus on generative modelling with diffusion models, evaluating the impact of different neural network architectures and different sampling schemes, and showing they are expressive enough to model the policy without implicit approximations, outperforming previous approaches, including discretisation and K-Means+Residual. \n\nDiffusion models. One main difference with previous works on diffusion models is that in order to obtain high quality samples, they relied on guiding the sampling process, either by using an expensive classifier (Dhariwal and Nichol, 2021), or by training a conditional and an unconditional model together (Ho and Salimans, 2021). However, as explained in Sec. 3.2, guidance can bias sampling towards low likelihood trajectories, which is problematic for BC applications, so in order to obtain high quality and high likelihood samples, we introduce two novel guidance-free methods. \n\nRL and Diffusion Models. Up to our knowledge, the only previous work that used diffusion models for RL is by Janner et al. (2022) who proposed Diffuser: a model that predicts all states and actions of a complete trajectory; and uses classifier-like guidance for trajectory optimization and similar U-Net architectures to those from text-to-image diffusion models (Ramesh et al., 2022;Saharia et al., 2022) 2022) use a conditional diffusion model with an MLP to predict a single action at a time conditioned on the current state. However, their focus is on trajectory optimization, relying on bootstrapping to learn a value function that is used for (classifier-like) guidance, and test their approach in environments with low dimensional state sets. Here, we focus on BC, for which we observe guidance can bias the sampling towards low-likelihood trajectories, so we introduce two novel guidance-free conditional sampling methods. In addition, we explore the impact of multiple architectures, including transformers, and demonstrate their performance even under high-dimensional state sets. \n\nAjay et al. ( 2023) provide a further concurrent work.",
            "score": 0.3761453928616171,
            "section_title": "A EXTENDED RELATED WORK",
            "char_start_offset": 32224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2208
                },
                {
                    "start": 2211,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 746,
                    "end": 773,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 840,
                    "end": 863,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "248965046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "264289212",
            "title": "Automatic News Summerization",
            "text": "It has been demonstrated that the Transformer is effective for a variety of sequence transduction tasks, including machine translation, text summarization, and speech recognition. This paper introduced the attention mechanism which then in the later future is used to create state-of-the-art NLP Models including Generative Pre-trained Transformer (GPT). Though the paper focuses on Machine Translation it made the path for future models to do Text Summerization too . \n\nLewis [9] introduces BART (Bidirectional and AutoRegressive Transformer), a denoise sequence-to-sequence pretraining method aimed at enhancing language generation, translation, and comprehension tasks. The core concept of BART revolves around using denoising autoencoders, a form of unsupervised learning, to train the model before its application. By deliberately perturbing input sequences and training the model to reconstruct the original texts, BART gains precise language representations and discerns crucial language patterns. This denoising pre-training approach equips BART to effectively handle noise and real-world data complexities. \n\nThe model also employs bidirectional training, combining masked language modeling and left-to-right autoencoding. This versatility enables BART to excel in both auto-regressive and bidirectional tasks. The authors also acknowledge some limitations of the model, such as challenges in dealing with uncommon words and optimization during training. Additionally, the rogue score on CNN/Daily Mail was 44.16, 21.28, and 40.9 on Rogue-1, Rogue-2, and Rogue-L respectively. \n\nBrown's [10] paper \"Language Models are Zero Shot Learners\" examines the capabilities of large-scale language models, primarily focusing on the GPT-3 (Generative Pretrained Transformer 3) model. The authors demonstrate that these language models can perform a variety of tasks without task-specific fine-tuning, thereby rendering them zero-shot learners. GPT-3's ability to generate responses to queries, even for tasks it was not explicitly trained on, is surprisingly robust given its large number of parameters and pre-training on diverse datasets.",
            "score": 0.3759501038075557,
            "section_title": "II. LITRATURE REVIEW",
            "char_start_offset": 6583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0189361572265625
        },
        {
            "corpus_id": "248496292",
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "text": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters. Our goal was to replicate the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data curation and training efficiency. We described training details, evaluated performance in a number of NLP and dialogue settings, and characterized behaviors with respect to bias, toxicity and hate speech. We also described many other limitations the models have, and discussed a wide set of considerations for responsibly releasing the models. We believe the entire AI community would benefit from working together to develop guidelines for responsible LLMs, and we hope that broad access to these types of models will increase the diversity of voices defining the ethical considerations of such technologies. Full evaluations on all 16 NLP tasks, with comparisons where available. We find that across most tasks, GPT-3 models and OPT models perform similarly, but some tasks display highly erratic behavior. Full evaluations on all 16 NLP tasks, with comparisons to the GPT-3 reported performance. As with zero-shot, performance is roughly similar for most tasks, with some tasks demonstrating erratic behavior.",
            "score": 0.3757690334148384,
            "section_title": "Conclusion",
            "char_start_offset": 33919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0156707763671875
        },
        {
            "corpus_id": "270869635",
            "title": "LLM-Generated Natural Language Meets Scaling Laws: New Explorations and Data Augmentation Methods",
            "text": "In the vanguard of contemporary AI advancements lies the deployment of LLM (e.g., GPT-4) marking a paradigm shift in NLP tasks.Moreover, the incorporation of reinforcement learning from human feedback has further refined LLM, endowing them with the capability to produce more natural and contextually appropriate responses [25][26].\n\nDespite its monumental success, LLM is not without concerns.One critical issue is that their autoregressive nature is inherently tailored more towards generation than understanding.This distinction becomes particularly significant in specialized verticals, such as industrial safety [27], where the paucity of domain-specific training data can render LLM less effective, and there are often limitations when directly used for tasks such as hazard classification [19].Furthermore, the substantial costs associated with the training and deployment of LLM cannot be overlooked.Given these challenges, LLM encounters difficulties when applied directly to certain NLP tasks, but rather acts as an advanced assistive tool, especially impressive for few-shot text classification [1,3,9,[11][12][13].\n\nIn scenarios where labeled data is scarce, a crucial challenge lies in training an accurate text classification model [33].\n\nThe scarcity of labeled data often stems from difficulties in acquisition due to privacy concerns or the high costs associated with manual annotation.LLM data augmentation emerges as a direct and effective strategy to mitigate this dilemma: using LLM-generated labeled texts as an augmentation of the original training dataset size [9].This strategy addresses a critical limitation of current text data augmentation methods [33,34]: the trade-off between fidelity and diversity.On the one hand, ensuring the correctness of the generated data's labels (fidelity) is paramount, as inaccuracies can lead to poor model performance.On the other hand, generating data that is sufficiently varied (diversity) is essential for building robust models for classifier training [13].Where, the choice of classifier often falls on the efficient models from the Bert family Nevertheless, there are two primary concerns that merit careful consideration.The first concern revolves around the authenticity and consistency of Daug with human language.There is an uncertainty regarding whether Daug truly mirrors the nature of human language.",
            "score": 0.3757589218886275,
            "section_title": "LLM-based data augmentation",
            "char_start_offset": 2989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 332
                },
                {
                    "start": 334,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1126
                },
                {
                    "start": 1128,
                    "end": 1251
                },
                {
                    "start": 1253,
                    "end": 1403
                },
                {
                    "start": 1403,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2286
                },
                {
                    "start": 2286,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "244715234"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "256827127"
                },
                {
                    "start": 1106,
                    "end": 1109,
                    "matchedPaperCorpusId": "248987297"
                },
                {
                    "start": 1117,
                    "end": 1121,
                    "matchedPaperCorpusId": "253384628"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "261494014"
                },
                {
                    "start": 1677,
                    "end": 1681,
                    "matchedPaperCorpusId": "261494014"
                },
                {
                    "start": 1681,
                    "end": 1684,
                    "matchedPaperCorpusId": "59523656"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.012969970703125
        },
        {
            "corpus_id": "272987400",
            "title": "On the Power of Decision Trees in Auto-Regressive Language Modeling",
            "text": "In recent years, Large Language Models (LLMs) have achieved outstanding results in tasks such as natural language understanding, coding, and mathematical reasoning. LLMs predominantly utilize the Transformer architecture Vaswani et al. (2023), establishing it as the standard in this field. However, recent initiatives (Gu & Dao, 2023;Sun et al., 2023;Ma et al., 2023;De et al., 2024) have begun to challenge the dominance of Transformers. These alternatives, while not yet matching Transformer performance, offer advantages in terms of inference time efficiency. Moreover, some works are revisiting traditional non-neural network models for language modeling, such as classical symbolic models (Wong et al., 2023). These developments indicate a shift towards diverse, efficient, and interpretable language modeling methodologies. \n\nTree-based models, particularly favored for handling tabular data (Grinsztajn et al., 2022), continue to hold significant importance. While tree-based methods are mostly used for classification and regression tasks, Auto-regressive Decision Trees (ARDTs) (Meek et al., 2002) have been studied for time-series prediction, offering a simpler and more interpretable alternative to complex nonlinear approaches. Although the ARDT approach was not originally designed for language tasks, it has demonstrated considerable promise in various time-series datasets, outperforming traditional autoregressive models while maintaining ease of interpretation. Motivated by these results, our study seeks to explore the potential of ARDTs for language prediction tasks, assessing whether they could serve as a viable, interpretable alternative to complex, resource-intensive language models. The decision process of the decision trees. We visualize part of the tree ensemble, and can observe which word is most relevant for the splitting rule at each node. \n\nTo understand the power of ARDTs, we first conduct theoretical studies demonstrating that ARDTs, using decision trees as next-token predictors, can compute more complex functions than traditional decision trees. We explore the classes of functions ARDTs can compute, showing their ability to simulate functions computed by automata, Turing machines, or sparse circuits through intermediate \"chain-of-thought\" computations.",
            "score": 0.3755455153684872,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 899,
                    "end": 924,
                    "matchedPaperCorpusId": "252697222"
                },
                {
                    "start": 1088,
                    "end": 1107,
                    "matchedPaperCorpusId": "40368000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01348114013671875
        }
    ],
    "quotes": {
        "cost": 0.2446379999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[249926846 | Yu et al. | 2022 | Citations: 1133]",
                "snippets": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[252780170 | Xu | 2022 | Citations: 12]",
                "snippets": "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "classification free guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 695,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 84
                            },
                            {
                                "start": 85,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 260
                            },
                            {
                                "start": 261,
                                "end": 421
                            },
                            {
                                "start": 422,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 695
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[256416107 | Nava et al. | 2022 | Citations: 6]",
                "snippets": "We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifier-free guidance (Ho & Salimans, 2021) improves the performance of generated base networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1313,
                        "end": 1677,
                        "sentence_offsets": [
                            {
                                "start": 1299,
                                "end": 1341
                            },
                            {
                                "start": 1344,
                                "end": 1549
                            },
                            {
                                "start": 1550,
                                "end": 1688
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifier-free guidance (Ho & Salimans, 2021) improves the performance of generated base networks."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[257102889 | Huang et al. | 2023 | Citations: 26]",
                "snippets": "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance (Ho, 2022) to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Manuscript submitted to ACM",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 661,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 30
                            },
                            {
                                "start": 31,
                                "end": 202
                            },
                            {
                                "start": 203,
                                "end": 315
                            },
                            {
                                "start": 316,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 548
                            },
                            {
                                "start": 549,
                                "end": 661
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance (Ho, 2022) to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[259095911 | Chang et al. | 2023 | Citations: 56]",
                "snippets": "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1264,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 137
                            },
                            {
                                "start": 138,
                                "end": 262
                            },
                            {
                                "start": 263,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 588
                            },
                            {
                                "start": 591,
                                "end": 627
                            },
                            {
                                "start": 628,
                                "end": 705
                            },
                            {
                                "start": 706,
                                "end": 801
                            },
                            {
                                "start": 802,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1094
                            },
                            {
                                "start": 1095,
                                "end": 1264
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[259308807 | Sanchez et al. | 2023 | Citations: 55]",
                "snippets": "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[260334572 | Kornblith et al. | 2023 | Citations: 15]",
                "snippets": "Classifier-free guidance (CFG) (Ho, 2022) aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans (Ho, 2022) originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12]47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . .",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-free guidance for image captioning",
                        "pdf_hash": "",
                        "start": 521,
                        "end": 1823,
                        "sentence_offsets": [
                            {
                                "start": 521,
                                "end": 631
                            },
                            {
                                "start": 634,
                                "end": 714
                            },
                            {
                                "start": 715,
                                "end": 829
                            },
                            {
                                "start": 830,
                                "end": 891
                            },
                            {
                                "start": 892,
                                "end": 1031
                            },
                            {
                                "start": 1032,
                                "end": 1176
                            },
                            {
                                "start": 1177,
                                "end": 1376
                            },
                            {
                                "start": 1377,
                                "end": 1413
                            },
                            {
                                "start": 1416,
                                "end": 1440
                            },
                            {
                                "start": 1441,
                                "end": 1574
                            },
                            {
                                "start": 1575,
                                "end": 1661
                            },
                            {
                                "start": 1662,
                                "end": 1823
                            }
                        ],
                        "ref_mentions": [
                            "249145348",
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (CFG) (Ho, 2022) aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans (Ho, 2022) originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12]47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . ."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[266053531 | Zhang et al. | 2023 | Citations: 24]",
                "snippets": "Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation.\n\nIn conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nLLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation.\n\nIn conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nLLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[267069249 | Zhou et al. | 2024 | Citations: 6]",
                "snippets": "Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n(\n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 90,
                        "end": 449,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 107
                            },
                            {
                                "start": 108,
                                "end": 393
                            },
                            {
                                "start": 396,
                                "end": 397
                            },
                            {
                                "start": 400,
                                "end": 468
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n(\n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[267547708 | Stark et al. | 2024 | Citations: 58]",
                "snippets": "In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models...we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 435,
                        "end": 782,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 988,
                        "end": 1140,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[267750410 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Diffusion models, notably divided into classifierguided (Dhariwal et al., 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022)Yang et al., 2023;Ho et al., 2022;(Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2022) for controllable molecule and motion generation, respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[247839510 | Hoogeboom et al. | 2022 | Citations: 622]": "This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.",
                    "[248965384 | Voleti et al. | 2022 | Citations: 307]": "Video prediction is a challenging task. The quality of video frames from current state-of-the-art (SOTA) generative models tends to be poor and generalization beyond the training data is difficult. Furthermore, existing prediction frameworks are typically not capable of simultaneously handling other video-related tasks such as unconditional generation or interpolation. In this work, we devise a general-purpose framework called Masked Conditional Video Diffusion (MCVD) for all of these video synthesis tasks using a probabilistic conditional score-based denoising diffusion model, conditioned on past and/or future frames. We train the model in a manner where we randomly and independently mask all the past frames or all the future frames. This novel but straightforward setup allows us to train a single model that is capable of executing a broad range of video tasks, specifically: future/past prediction -- when only future/past frames are masked; unconditional generation -- when both past and future frames are masked; and interpolation -- when neither past nor future frames are masked. Our experiments show that this approach can generate high-quality frames for diverse types of videos. Our MCVD models are built from simple non-recurrent 2D-convolutional architectures, conditioning on blocks of frames and generating blocks of frames. We generate videos of arbitrary lengths autoregressively in a block-wise manner. Our approach yields SOTA results across standard video prediction and interpolation benchmarks, with computation times for training models measured in 1-12 days using $\\le$ 4 GPUs. Project page: https://mask-cond-video-diffusion.github.io ; Code : https://github.com/voletiv/mcvd-pytorch",
                    "[252595883 | Tevet et al. | 2022 | Citations: 767]": "Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ ."
                },
                "metadata": [
                    {
                        "section_title": "Related works",
                        "pdf_hash": "",
                        "start": 45,
                        "end": 1009,
                        "sentence_offsets": [
                            {
                                "start": 5,
                                "end": 248
                            },
                            {
                                "start": 249,
                                "end": 328
                            },
                            {
                                "start": 329,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 1009
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "247839510",
                            "248965384",
                            "247839510",
                            "252595883"
                        ],
                        "quote": "Diffusion models, notably divided into classifierguided (Dhariwal et al., 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022)Yang et al., 2023;Ho et al., 2022;(Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2022) for controllable molecule and motion generation, respectively."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268230892 | Kim et al. | 2024 | Citations: 23]",
                "snippets": "Classifier-free guidance (Ho, 2022)) also aims at conditional generation but relies on both unconditional and conditional scores.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "B.3 SCORE CORRECTION IN DIFFUSION MODEL",
                        "pdf_hash": "",
                        "start": 714,
                        "end": 853,
                        "sentence_offsets": [
                            {
                                "start": 714,
                                "end": 853
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022)) also aims at conditional generation but relies on both unconditional and conditional scores."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268264822 | Cao et al. | 2024 | Citations: 38]",
                "snippets": "GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Text-to-Image Diffusion Models",
                        "pdf_hash": "",
                        "start": 597,
                        "end": 855,
                        "sentence_offsets": [
                            {
                                "start": 581,
                                "end": 855
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[268536735 | Li et al. | 2024 | Citations: 13]",
                "snippets": "Classifier-free guidance [Ho and Salimans, 2022] has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022]...For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step. The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254044710 | Ajay et al. | 2022 | Citations: 408]": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making."
                },
                "metadata": [
                    {
                        "quote": "Classifier-free guidance [Ho and Salimans, 2022] has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022]",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Guided Diffusion Models",
                        "pdf_hash": "",
                        "start": 864,
                        "end": 1154,
                        "sentence_offsets": [
                            {
                                "start": 855,
                                "end": 1111
                            },
                            {
                                "start": 1111,
                                "end": 1280
                            }
                        ],
                        "ref_mentions": [
                            "254044710"
                        ],
                        "quote": "For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step. The authors further show that gradients of any classifier can be incorporated to control and guide the text generation."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[268681777 | Souri et al. | 2024 | Citations: 1]",
                "snippets": "Initially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Guidance in Diffusion Models",
                        "pdf_hash": "",
                        "start": 913,
                        "end": 1815,
                        "sentence_offsets": [
                            {
                                "start": 913,
                                "end": 1106
                            },
                            {
                                "start": 1108,
                                "end": 1234
                            },
                            {
                                "start": 1234,
                                "end": 1362
                            },
                            {
                                "start": 1362,
                                "end": 1470
                            },
                            {
                                "start": 1470,
                                "end": 1600
                            },
                            {
                                "start": 1600,
                                "end": 1815
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Initially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[269983348 | Tan et al. | 2024 | Citations: 0]",
                "snippets": "In this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "F Ablation on Classifier-free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1133,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 115
                            },
                            {
                                "start": 116,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 361
                            },
                            {
                                "start": 362,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 636
                            },
                            {
                                "start": 637,
                                "end": 716
                            },
                            {
                                "start": 719,
                                "end": 934
                            },
                            {
                                "start": 935,
                                "end": 1133
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270562252 | Zhao et al. | 2024 | Citations: 7]",
                "snippets": "Classfier-free guidance. Classifier-free guidance (Ho and Salimans, 2022) is a method that directly conditions the generative process on both data and context, bypassing the need for explicit classifiers. This methodology has been widely and effectively applied, for example, in text-to-image models (Nichol et al., 2021;(Saharia et al., 2022)(Rombach et al., 2021). While the original research does not explore classifier-free guidance within the scope of fine-tuning pre-trained diffusion models, several subsequent studies address fine-tuning scenarios Zhang et al. (2023); (Xie et al., 2023). As elucidated in Section 5.2, compared to classifier-free guidance, our approach can improve sample efficiency and leverage conditional independence to facilitate offline dataset construction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[248986576 | Saharia et al. | 2022 | Citations: 6075]": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.",
                    "[258108246 | Xie et al. | 2023 | Citations: 81]": "Diffusion models have proven to be highly effective in generating high-quality images. However, adapting large pre-trained diffusion models to new domains remains an open challenge, which is critical for real-world applications. This paper proposes DiffFit, a parameter-efficient strategy to fine-tune large pre-trained diffusion models that enable fast adaptation to new domains. DiffFit is embarrassingly simple that only fine-tunes the bias term and newly-added scaling factors in specific layers, yet resulting in significant training speed-up and reduced model storage costs. Compared with full fine-tuning, DiffFit achieves 2\u00d7 training speed-up and only needs to store approximately 0.12% of the total model parameters. Intuitive theoretical analysis has been provided to justify the efficacy of scaling factors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior or competitive performances compared to the full fine-tuning while being more efficient. Remarkably, we show that DiffFit can adapt a pre-trained low-resolution generative model to a high-resolution one by adding minimal cost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of 3.02 on ImageNet 512\u00d7512 benchmark by fine-tuning only 25 epochs from a public pre-trained ImageNet 256\u00d7256 checkpoint while being 30\u00d7 more training efficient than the closest competitor."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS",
                        "pdf_hash": "",
                        "start": 1199,
                        "end": 1985,
                        "sentence_offsets": [
                            {
                                "start": 1199,
                                "end": 1223
                            },
                            {
                                "start": 1224,
                                "end": 1403
                            },
                            {
                                "start": 1404,
                                "end": 1563
                            },
                            {
                                "start": 1564,
                                "end": 1792
                            },
                            {
                                "start": 1793,
                                "end": 1985
                            }
                        ],
                        "ref_mentions": [
                            "248986576",
                            "245335280",
                            "258108246"
                        ],
                        "quote": "Classfier-free guidance. Classifier-free guidance (Ho and Salimans, 2022) is a method that directly conditions the generative process on both data and context, bypassing the need for explicit classifiers. This methodology has been widely and effectively applied, for example, in text-to-image models (Nichol et al., 2021;(Saharia et al., 2022)(Rombach et al., 2021). While the original research does not explore classifier-free guidance within the scope of fine-tuning pre-trained diffusion models, several subsequent studies address fine-tuning scenarios Zhang et al. (2023); (Xie et al., 2023). As elucidated in Section 5.2, compared to classifier-free guidance, our approach can improve sample efficiency and leverage conditional independence to facilitate offline dataset construction."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271161847 | Jolicoeur-Martineau et al. | 2024 | Citations: 1]",
                "snippets": "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "CLASSIFIER-FREE GUIDANCE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 713,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 234
                            },
                            {
                                "start": 235,
                                "end": 407
                            },
                            {
                                "start": 408,
                                "end": 442
                            },
                            {
                                "start": 443,
                                "end": 507
                            },
                            {
                                "start": 508,
                                "end": 647
                            },
                            {
                                "start": 648,
                                "end": 713
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[271328459 | Lan et al. | 2024 | Citations: 9]",
                "snippets": "Classifier-free guidance (Ho, 2022) is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 618,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 205
                            },
                            {
                                "start": 205,
                                "end": 391
                            },
                            {
                                "start": 391,
                                "end": 424
                            },
                            {
                                "start": 426,
                                "end": 525
                            },
                            {
                                "start": 525,
                                "end": 618
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022) is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271915699 | Dinh et al. | 2024 | Citations: 0]",
                "snippets": "We found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[272593086 | Kerby et al. | 2024 | Citations: 3]",
                "snippets": "Classifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation (Ho, 2022). During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 814,
                        "end": 1399,
                        "sentence_offsets": [
                            {
                                "start": 814,
                                "end": 996
                            },
                            {
                                "start": 997,
                                "end": 1279
                            },
                            {
                                "start": 1280,
                                "end": 1399
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation (Ho, 2022). During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[272831727 | Peng et al. | 2024 | Citations: 4]",
                "snippets": "Classifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], (Niedoba et al., 2023), (Rempe et al., 2023), [54], [55], (Zhu et al., 2023). This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257921535 | Rempe et al. | 2023 | Citations: 117]": "We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.",
                    "[258298040 | Zhu et al. | 2023 | Citations: 45]": "Pervasive integration of GPS-enabled devices and data acquisition technologies has led to an exponential increase in GPS trajectory data, fostering advancements in spatial-temporal data mining research. Nonetheless, GPS trajectories contain personal geolocation information, rendering serious privacy concerns when working with raw data. A promising approach to address this issue is trajectory generation, which involves replacing original data with generated, privacy-free alternatives. Despite the potential of trajectory generation, the complex nature of human behavior and its inherent stochastic characteristics pose challenges in generating high-quality trajectories. In this work, we propose a spatial-temporal diffusion probabilistic model for trajectory generation (DiffTraj). This model effectively combines the generative abilities of diffusion models with the spatial-temporal features derived from real trajectories. The core idea is to reconstruct and synthesize geographic trajectories from white noise through a reverse trajectory denoising process. Furthermore, we propose a Trajectory UNet (Traj-UNet) deep neural network to embed conditional information and accurately estimate noise levels during the reverse process. Experiments on two real-world datasets show that DiffTraj can be intuitively applied to generate high-fidelity trajectories while retaining the original distributions. Moreover, the generated results can support downstream trajectory analysis tasks and significantly outperform other methods in terms of geo-distribution evaluations.",
                    "[262217065 | Niedoba et al. | 2023 | Citations: 14]": "Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing."
                },
                "metadata": [
                    {
                        "section_title": "1) Conditional Diffusion Models:",
                        "pdf_hash": "",
                        "start": 538,
                        "end": 1534,
                        "sentence_offsets": [
                            {
                                "start": 538,
                                "end": 560
                            },
                            {
                                "start": 561,
                                "end": 747
                            },
                            {
                                "start": 748,
                                "end": 893
                            },
                            {
                                "start": 894,
                                "end": 1073
                            },
                            {
                                "start": 1074,
                                "end": 1182
                            },
                            {
                                "start": 1185,
                                "end": 1218
                            },
                            {
                                "start": 1219,
                                "end": 1413
                            },
                            {
                                "start": 1414,
                                "end": 1534
                            }
                        ],
                        "ref_mentions": [
                            "262217065",
                            "257921535",
                            "258298040"
                        ],
                        "quote": "Classifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], (Niedoba et al., 2023), (Rempe et al., 2023), [54], [55], (Zhu et al., 2023). This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273098800 | Gandikota et al. | 2024 | Citations: 11]",
                "snippets": "This approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Transformer Next Token Prediction: A Classification Perspective",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 342,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            },
                            {
                                "start": 156,
                                "end": 342
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273142392 | Chen et al. | 2024 | Citations: 13]",
                "snippets": "Practical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method (Dhariwal et al., 2021) which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Classifier and classifier-free guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2028,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 198
                            },
                            {
                                "start": 199,
                                "end": 381
                            },
                            {
                                "start": 382,
                                "end": 443
                            },
                            {
                                "start": 444,
                                "end": 536
                            },
                            {
                                "start": 539,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 725
                            },
                            {
                                "start": 726,
                                "end": 815
                            },
                            {
                                "start": 816,
                                "end": 980
                            },
                            {
                                "start": 981,
                                "end": 1116
                            },
                            {
                                "start": 1117,
                                "end": 1267
                            },
                            {
                                "start": 1268,
                                "end": 1404
                            },
                            {
                                "start": 1405,
                                "end": 1424
                            },
                            {
                                "start": 1427,
                                "end": 1452
                            },
                            {
                                "start": 1455,
                                "end": 1541
                            },
                            {
                                "start": 1542,
                                "end": 1634
                            },
                            {
                                "start": 1637,
                                "end": 1796
                            },
                            {
                                "start": 1797,
                                "end": 1866
                            },
                            {
                                "start": 1867,
                                "end": 2028
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "Practical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method (Dhariwal et al., 2021) which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ]."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276107584 | Guo et al. | 2025 | Citations: 3]",
                "snippets": "Classifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks (Peng et al., 2024)Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E CLASSIFIER FREE GUIDANCE FOR EACH TASK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 327,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 179
                            },
                            {
                                "start": 180,
                                "end": 327
                            }
                        ],
                        "ref_mentions": [
                            "274023942"
                        ],
                        "quote": "Classifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks (Peng et al., 2024)Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276249479 | Song et al. | 2025 | Citations: 18]",
                "snippets": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 482,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 125
                            },
                            {
                                "start": 126,
                                "end": 298
                            },
                            {
                                "start": 299,
                                "end": 397
                            },
                            {
                                "start": 400,
                                "end": 482
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276409208 | Wang et al. | 2025 | Citations: 2]",
                "snippets": "Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol and(Nichol et al., 2021). In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231979499 | Nichol et al. | 2021 | Citations: 3724]": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Coarse-to-Fine Flow Matching",
                        "pdf_hash": "",
                        "start": 1424,
                        "end": 2199,
                        "sentence_offsets": [
                            {
                                "start": 1399,
                                "end": 1630
                            },
                            {
                                "start": 1631,
                                "end": 1773
                            },
                            {
                                "start": 1774,
                                "end": 1931
                            },
                            {
                                "start": 1932,
                                "end": 2004
                            },
                            {
                                "start": 2007,
                                "end": 2199
                            }
                        ],
                        "ref_mentions": [
                            "231979499"
                        ],
                        "quote": "Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol and(Nichol et al., 2021). In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[276647345 | Zhu et al. | 2025 | Citations: 1]",
                "snippets": "Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "1) Forward Diffusion Process:",
                        "pdf_hash": "",
                        "start": 22,
                        "end": 799,
                        "sentence_offsets": [
                            {
                                "start": 22,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 242
                            },
                            {
                                "start": 243,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 514
                            },
                            {
                                "start": 515,
                                "end": 644
                            },
                            {
                                "start": 645,
                                "end": 799
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276928343 | Tuo et al. | 2025 | Citations: 0]",
                "snippets": "To remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6)).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multi-temperature generation",
                        "pdf_hash": "",
                        "start": 93,
                        "end": 909,
                        "sentence_offsets": [
                            {
                                "start": 93,
                                "end": 270
                            },
                            {
                                "start": 273,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 435
                            },
                            {
                                "start": 436,
                                "end": 450
                            },
                            {
                                "start": 453,
                                "end": 485
                            },
                            {
                                "start": 486,
                                "end": 715
                            },
                            {
                                "start": 716,
                                "end": 909
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6))."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276961040 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "Instead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance (Dhariwal et al., 2021). It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1599,
                        "end": 2080,
                        "sentence_offsets": [
                            {
                                "start": 1599,
                                "end": 1778
                            },
                            {
                                "start": 1779,
                                "end": 1926
                            },
                            {
                                "start": 1927,
                                "end": 2080
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "249145348"
                        ],
                        "quote": "Instead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance (Dhariwal et al., 2021). It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277510202 | Fan et al. | 2025 | Citations: 2]",
                "snippets": "Next, classifier-free guidance (CFG) [9]44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 777,
                        "end": 2054,
                        "sentence_offsets": [
                            {
                                "start": 777,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1035
                            },
                            {
                                "start": 1036,
                                "end": 1312
                            },
                            {
                                "start": 1313,
                                "end": 1471
                            },
                            {
                                "start": 1472,
                                "end": 1606
                            },
                            {
                                "start": 1607,
                                "end": 1650
                            },
                            {
                                "start": 1651,
                                "end": 1874
                            },
                            {
                                "start": 1875,
                                "end": 1980
                            },
                            {
                                "start": 1981,
                                "end": 2054
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Next, classifier-free guidance (CFG) [9]44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[278171703 | Liang et al. | 2025 | Citations: 0]",
                "snippets": "However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios...another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time...When w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1 for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 9,
                        "end": 448,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 448
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time"
                    },
                    {
                        "section_title": "D. Evaluation Results",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 591,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 63
                            },
                            {
                                "start": 64,
                                "end": 233
                            },
                            {
                                "start": 234,
                                "end": 591
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1 for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.036579,
        "cot": "For this query about classifier-free guidance (CFG) in the context of language models and Flow Matching approaches, I need to organize the quotes into cohesive dimensions that will help create a comprehensive response.\n\nFirst, I'll identify the key dimensions needed to answer this query:\n\n1. \"Introduction/Background\" - This section will provide essential context about what classifier-free guidance is and its origins in diffusion models. This will help establish the foundation for understanding the rest of the analysis.\n\n2. \"Technical Implementation in Autoregressive Language Models\" - This dimension will focus on how CFG has been technically adapted and implemented specifically for autoregressive language models, which is a key part of the query.\n\n3. \"Technical Implementation in Flow Matching Approaches\" - The query specifically mentions Flow Matching approaches in NLP, so we need a dimension covering how CFG has been implemented in this context.\n\n4. \"Empirical Evaluation and Performance\" - This dimension will cover the empirical evaluation of CFG in language models and Flow Matching, including metrics, comparison studies, and performance impacts.\n\n5. \"Advantages of Classifier-Free Guidance in NLP\" - This dimension will highlight the benefits and advantages of using CFG in NLP applications.\n\n6. \"Limitations and Challenges\" - This will address the observed limitations of CFG when applied to language models and Flow Matching, as requested in the query.\n\nFor each dimension, I need to decide whether a synthesis or list format is more appropriate:\n\n1. \"Introduction/Background\" - This should be a synthesis paragraph as it needs to provide a coherent explanation of the fundamental concepts.\n\n2. \"Technical Implementation in Autoregressive Language Models\" - This should be a synthesis format since it requires explaining the technical approaches in a coherent narrative.\n\n3. \"Technical Implementation in Flow Matching Approaches\" - This should also be a synthesis format to explain the technical aspects clearly.\n\n4. \"Empirical Evaluation and Performance\" - This should be a synthesis format to coherently present the findings from various studies.\n\n5. \"Advantages of Classifier-Free Guidance in NLP\" - This would work well as a list format to clearly enumerate the distinct advantages.\n\n6. \"Limitations and Challenges\" - This would also benefit from a list format to clearly identify the separate issues and challenges.\n\nNow I'll assign each quote to the most relevant dimension(s):",
        "plan": {
            "Introduction/Background (synthesis)": [
                0,
                2,
                3,
                4,
                11,
                12,
                13,
                14,
                18,
                20,
                21,
                29
            ],
            "Technical Implementation in Autoregressive Language Models (synthesis)": [
                0,
                5,
                6,
                7,
                8,
                17,
                18,
                22,
                23
            ],
            "Technical Implementation in Flow Matching Approaches (synthesis)": [
                9,
                10,
                25,
                26,
                28,
                30
            ],
            "Empirical Evaluation and Performance (synthesis)": [
                1,
                5,
                12,
                15,
                17,
                19,
                24
            ],
            "Advantages of Classifier-Free Guidance in NLP (list)": [
                3,
                4,
                5,
                16,
                17
            ],
            "Limitations and Challenges (list)": [
                1,
                5,
                19,
                20,
                27,
                30,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Classifier-free guidance (CFG) is a technique developed for diffusion models that enhances conditional generation without requiring separate classifiers. It has evolved from its origins in image generation to become a fundamental mechanism in various generative tasks, including text generation and NLP applications. (16 sources)",
                "text": "\nClassifier-free guidance (CFG) emerged as a transformative technique in generative modeling, first introduced by Ho (2022) to address limitations of classifier guidance in diffusion models <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. While classifier guidance requires training a separate classifier model alongside the diffusion model <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>, classifier-free guidance eliminates this requirement by jointly training conditional and unconditional variants of the same model <Paper corpusId=\"276961040\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This approach allows for a trade-off between sample quality and diversity similar to classifier guidance, but with greater computational efficiency <Paper corpusId=\"272593086\" paperTitle=\"(Kerby et al., 2024)\" isShortName></Paper>.\n\nAt its core, CFG involves training a model that can perform both conditional generation G(z, c), where c represents conditioning information, and unconditional generation G(z) <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This dual capability is achieved by randomly dropping out the conditional information during training with a certain probability <Paper corpusId=\"271328459\" paperTitle=\"(Lan et al., 2024)\" isShortName></Paper>. During inference, the model combines predictions from both conditional and unconditional paths using a weighted average:\n\nf(x) = (1 - \u03b3)f_\u03b8(x) + \u03b3f_\u03b8(x, c)\n\nwhere \u03b3 is the guidance scale that controls the influence of the conditioning information <Paper corpusId=\"271328459\" paperTitle=\"(Lan et al., 2024)\" isShortName></Paper>. By adjusting this scale, users can prioritize either diversity (lower values) or fidelity to the conditioning (higher values) <Paper corpusId=\"259095911\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>.\n\nCFG has proven particularly effective in image generation tasks, with GLIDE demonstrating that it yields superior results in both image photo-realism and textual alignment compared to other guidance mechanisms <Paper corpusId=\"268264822\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>. It has become standard in state-of-the-art text-to-image diffusion models <Paper corpusId=\"268536735\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Beyond image generation, CFG has been successfully applied to autoregressive models for text-to-image generation, where it significantly improves output image-text alignment, especially for challenging text prompts <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nThe technique has further expanded into other domains, including trajectory generation for autonomous vehicles <Paper corpusId=\"272831727\" paperTitle=\"(Peng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257921535\" paperTitle=\"(Rempe et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258298040\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"262217065\" paperTitle=\"(Niedoba et al., 2023)\" isShortName></Paper>, hypernetworks <Paper corpusId=\"256416107\" paperTitle=\"(Nava et al., 2022)\" isShortName></Paper>, and image editing <Paper corpusId=\"257102889\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. In the context of natural language processing, Diffusion LM has demonstrated that diffusion models can be trained on discrete text spaces with gradient-based guidance for controlled text generation <Paper corpusId=\"268536735\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nDespite its advantages, CFG has limitations. The most significant drawback is that the attributes used for guidance must be fixed during training, limiting flexibility for new guidance criteria post-training <Paper corpusId=\"268681777\" paperTitle=\"(Souri et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272593086\" paperTitle=\"(Kerby et al., 2024)\" isShortName></Paper>. This constraint has motivated research into more adaptable guidance mechanisms that retain CFG's efficiency while addressing its limitations.",
                "citations": [
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Instead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance (Dhariwal et al., 2021). It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022)."
                        ],
                        "paper": {
                            "corpus_id": 276961040,
                            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
                            "authors": [
                                {
                                    "authorId": "2144306665",
                                    "name": "Xiaoming Zhao"
                                },
                                {
                                    "authorId": "2281750850",
                                    "name": "Alexander G. Schwing"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Kerby et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation (Ho, 2022). During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training."
                        ],
                        "paper": {
                            "corpus_id": 272593086,
                            "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
                            "authors": [
                                {
                                    "authorId": "2243157079",
                                    "name": "Thomas Kerby"
                                },
                                {
                                    "authorId": "2243003711",
                                    "name": "Kevin R. Moon"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Lan et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits."
                        ],
                        "paper": {
                            "corpus_id": 271328459,
                            "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
                            "authors": [
                                {
                                    "authorId": "2312327383",
                                    "name": "Yun-Han Lan"
                                },
                                {
                                    "authorId": "37188394",
                                    "name": "Wen-Yi Hsiao"
                                },
                                {
                                    "authorId": "2312341186",
                                    "name": "Hao-Chung Cheng"
                                },
                                {
                                    "authorId": "2312339261",
                                    "name": "Yi-Hsuan Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Society for Music Information Retrieval Conference",
                            "n_citations": 9
                        },
                        "score": 0.947265625
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]."
                        ],
                        "paper": {
                            "corpus_id": 259095911,
                            "title": "On the Design Fundamentals of Diffusion Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2186402069",
                                    "name": "Ziyi Chang"
                                },
                                {
                                    "authorId": "2872629",
                                    "name": "G. Koulieris"
                                },
                                {
                                    "authorId": "1993718816",
                                    "name": "Hubert P. H. Shum"
                                }
                            ],
                            "year": 2023,
                            "venue": "Pattern Recognition",
                            "n_citations": 56
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation."
                        ],
                        "paper": {
                            "corpus_id": 268264822,
                            "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2203366502",
                                    "name": "Pu Cao"
                                },
                                {
                                    "authorId": "2273950538",
                                    "name": "Feng Zhou"
                                },
                                {
                                    "authorId": "2273931256",
                                    "name": "Qing Song"
                                },
                                {
                                    "authorId": "39702333",
                                    "name": "Lu Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 38
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance [Ho and Salimans, 2022] has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022]",
                            "For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step. The authors further show that gradients of any classifier can be incorporated to control and guide the text generation."
                        ],
                        "paper": {
                            "corpus_id": 268536735,
                            "title": "Diffusion Model for Data-Driven Black-Box Optimization",
                            "authors": [
                                {
                                    "authorId": "2258812689",
                                    "name": "Zihao Li"
                                },
                                {
                                    "authorId": "2279340788",
                                    "name": "Hui Yuan"
                                },
                                {
                                    "authorId": "2242535459",
                                    "name": "Kaixuan Huang"
                                },
                                {
                                    "authorId": "103688818",
                                    "name": "Chengzhuo Ni"
                                },
                                {
                                    "authorId": "2292682601",
                                    "name": "Yinyu Ye"
                                },
                                {
                                    "authorId": "2258803874",
                                    "name": "Minshuo Chen"
                                },
                                {
                                    "authorId": "2258997595",
                                    "name": "Mengdi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.9169921875
                    },
                    {
                        "id": "(Peng et al., 2024)",
                        "snippets": [
                            "Classifier-free-based. The classifier-free mechanism combines unconditional and conditional diffusion models, achieving a balance between fidelity and diversity without the need to train a separate classifier. Additionally, it should be noted that the conditional diffusion model can employ either a concatenation mechanism or a cross-attention mechanism. In classifier-free diffusion guidance [51], the authors jointly train a conditional and an unconditional diffusion model, setting the condition c to \u2205 for the unconditional model. Then, a weighted average of the conditional and unconditional scores is used to estimate the score function: \n\nwhere w is also a guidance scale. For many traffic-related generation tasks, researchers have employed the classifier-free guidance mechanism to regulate the diversity of the generated outputs [52], (Niedoba et al., 2023), (Rempe et al., 2023), [54], [55], (Zhu et al., 2023). This approach prevents the outputs from following the conditional guidance too closely or being constrained too tightly."
                        ],
                        "paper": {
                            "corpus_id": 272831727,
                            "title": "Diffusion Models for Intelligent Transportation Systems: A Survey",
                            "authors": [
                                {
                                    "authorId": "2293615241",
                                    "name": "Mingxing Peng"
                                },
                                {
                                    "authorId": "2267078966",
                                    "name": "Kehua Chen"
                                },
                                {
                                    "authorId": "2293665950",
                                    "name": "Xusen Guo"
                                },
                                {
                                    "authorId": "2294891908",
                                    "name": "Qiming Zhang"
                                },
                                {
                                    "authorId": "2255592398",
                                    "name": "Hongliang Lu"
                                },
                                {
                                    "authorId": "2255501442",
                                    "name": "Hui Zhong"
                                },
                                {
                                    "authorId": "2321889100",
                                    "name": "Di Chen"
                                },
                                {
                                    "authorId": "2257227452",
                                    "name": "Meixin Zhu"
                                },
                                {
                                    "authorId": "2298579285",
                                    "name": "Hai Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.94140625
                    },
                    {
                        "id": "(Rempe et al., 2023)",
                        "snippets": [
                            "We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page."
                        ],
                        "paper": {
                            "corpus_id": 257921535,
                            "title": "Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion",
                            "authors": [
                                {
                                    "authorId": "40084473",
                                    "name": "Davis Rempe"
                                },
                                {
                                    "authorId": "2566332",
                                    "name": "Zhengyi Luo"
                                },
                                {
                                    "authorId": "32200465",
                                    "name": "X. B. Peng"
                                },
                                {
                                    "authorId": "145412874",
                                    "name": "Ye Yuan"
                                },
                                {
                                    "authorId": "144040368",
                                    "name": "Kris Kitani"
                                },
                                {
                                    "authorId": "32113848",
                                    "name": "Karsten Kreis"
                                },
                                {
                                    "authorId": "37895334",
                                    "name": "S. Fidler"
                                },
                                {
                                    "authorId": "2528439",
                                    "name": "O. Litany"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 117
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "Pervasive integration of GPS-enabled devices and data acquisition technologies has led to an exponential increase in GPS trajectory data, fostering advancements in spatial-temporal data mining research. Nonetheless, GPS trajectories contain personal geolocation information, rendering serious privacy concerns when working with raw data. A promising approach to address this issue is trajectory generation, which involves replacing original data with generated, privacy-free alternatives. Despite the potential of trajectory generation, the complex nature of human behavior and its inherent stochastic characteristics pose challenges in generating high-quality trajectories. In this work, we propose a spatial-temporal diffusion probabilistic model for trajectory generation (DiffTraj). This model effectively combines the generative abilities of diffusion models with the spatial-temporal features derived from real trajectories. The core idea is to reconstruct and synthesize geographic trajectories from white noise through a reverse trajectory denoising process. Furthermore, we propose a Trajectory UNet (Traj-UNet) deep neural network to embed conditional information and accurately estimate noise levels during the reverse process. Experiments on two real-world datasets show that DiffTraj can be intuitively applied to generate high-fidelity trajectories while retaining the original distributions. Moreover, the generated results can support downstream trajectory analysis tasks and significantly outperform other methods in terms of geo-distribution evaluations."
                        ],
                        "paper": {
                            "corpus_id": 258298040,
                            "title": "DiffTraj: Generating GPS Trajectory with Diffusion Probabilistic Model",
                            "authors": [
                                {
                                    "authorId": "2051691467",
                                    "name": "Yuanshao Zhu"
                                },
                                {
                                    "authorId": "89987905",
                                    "name": "Yongchao Ye"
                                },
                                {
                                    "authorId": "2145409015",
                                    "name": "Shiyao Zhang"
                                },
                                {
                                    "authorId": "2197532318",
                                    "name": "Xiangyu Zhao"
                                },
                                {
                                    "authorId": "2116508870",
                                    "name": "James J. Q. Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 45
                        },
                        "score": 0
                    },
                    {
                        "id": "(Niedoba et al., 2023)",
                        "snippets": [
                            "Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing."
                        ],
                        "paper": {
                            "corpus_id": 262217065,
                            "title": "A Diffusion-Model of Joint Interactive Navigation",
                            "authors": [
                                {
                                    "authorId": "41032454",
                                    "name": "Matthew Niedoba"
                                },
                                {
                                    "authorId": "104018453",
                                    "name": "J. Lavington"
                                },
                                {
                                    "authorId": "2242387232",
                                    "name": "Yunpeng Liu"
                                },
                                {
                                    "authorId": "3814221",
                                    "name": "Vasileios Lioutas"
                                },
                                {
                                    "authorId": "2167148238",
                                    "name": "Justice D. Sefas"
                                },
                                {
                                    "authorId": "2217970852",
                                    "name": "Xiaoxuan Liang"
                                },
                                {
                                    "authorId": "2242251179",
                                    "name": "Dylan Green"
                                },
                                {
                                    "authorId": "2502496",
                                    "name": "Setareh Dabiri"
                                },
                                {
                                    "authorId": "2242252496",
                                    "name": "Berend Zwartsenberg"
                                },
                                {
                                    "authorId": "2072346802",
                                    "name": "Adam Scibior"
                                },
                                {
                                    "authorId": "144109143",
                                    "name": "Frank Wood"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nava et al., 2022)",
                        "snippets": [
                            "We introduce Hypernetwork Latent Diffusion Models (HyperLDM) as a costlier but more powerful alternative to pure HyperCLIP guidance to find task-adapted networks within the latent space of a hypernetwork model (Figure 2.C). We show how combining this approach with classifier-free guidance (Ho & Salimans, 2021) improves the performance of generated base networks."
                        ],
                        "paper": {
                            "corpus_id": 256416107,
                            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2129786387",
                                    "name": "Elvis Nava"
                                },
                                {
                                    "authorId": "51194506",
                                    "name": "Seijin Kobayashi"
                                },
                                {
                                    "authorId": "2023392128",
                                    "name": "Yifei Yin"
                                },
                                {
                                    "authorId": "50191333",
                                    "name": "Robert K. Katzschmann"
                                },
                                {
                                    "authorId": "48117063",
                                    "name": "B. Grewe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 6
                        },
                        "score": 0.9423828125
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance (Ho, 2022) to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier."
                        ],
                        "paper": {
                            "corpus_id": 257102889,
                            "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
                            "authors": [
                                {
                                    "authorId": "2186281333",
                                    "name": "Nisha Huang"
                                },
                                {
                                    "authorId": "1443761295",
                                    "name": "Fan Tang"
                                },
                                {
                                    "authorId": "40441149",
                                    "name": "Weiming Dong"
                                },
                                {
                                    "authorId": "39945992",
                                    "name": "Tong-Yee Lee"
                                },
                                {
                                    "authorId": "2155590336",
                                    "name": "Changsheng Xu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Souri et al., 2024)",
                        "snippets": [
                            "Initially, guidance within diffusion models was predominantly achieved through two methods: classifier guidance [Dhariwal and Nichol, 2021] and classifier-free guidance [Ho and Salimans, 2022].\n\nClassifier guidance [Dhariwal and Nichol, 2021] involves training a separate classifier, adept at handling noisy image inputs.This classifier generates a guidance signal during the diffusion process, steering the generative model toward desired outcomes.However, this method necessitates the training of a specialized classifier, often a resource-intensive task.In contrast, classifier-free guidance [Ho and Salimans, 2022] internalizes the guidance mechanism within the model's architecture.This method, while eliminating the need for an external classifier, comes with its limitation: once trained, its adaptability is restricted, unable to accommodate different classifiers or evolving guidance criteria."
                        ],
                        "paper": {
                            "corpus_id": 268681777,
                            "title": "Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion",
                            "authors": [
                                {
                                    "authorId": "78859465",
                                    "name": "Hossein Souri"
                                },
                                {
                                    "authorId": "7364835",
                                    "name": "Arpit Bansal"
                                },
                                {
                                    "authorId": "2075283338",
                                    "name": "Hamid Kazemi"
                                },
                                {
                                    "authorId": "120165773",
                                    "name": "Liam H. Fowl"
                                },
                                {
                                    "authorId": "2056290221",
                                    "name": "Aniruddha Saha"
                                },
                                {
                                    "authorId": "2284863781",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "2264628126",
                                    "name": "Andrew Gordon Wilson"
                                },
                                {
                                    "authorId": "2254122195",
                                    "name": "Rama Chellappa"
                                },
                                {
                                    "authorId": "2237426712",
                                    "name": "Tom Goldstein"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.85205078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Implementation in Autoregressive Language Models",
                "tldr": "Classifier-free guidance in autoregressive language models involves combining conditional and unconditional predictions during inference without requiring model retraining. It is implemented by manipulating the logit distributions, allowing for controlled text generation with adjustable guidance strength. (11 sources)",
                "text": "\nAdapting classifier-free guidance (CFG) from diffusion models to autoregressive language models (LLMs) has proven remarkably effective, despite the architectural differences between these model types. Unlike diffusion models which predict noise, autoregressive models predict the next token in a sequence based on previous tokens. In autoregressive language models, CFG implementation works by combining conditional and unconditional predictions during the inference process <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nThe mathematical formulation for CFG in autoregressive language models builds on the same principles as in diffusion models. Given a sequence of tokens, the model combines the logits from conditional and unconditional predictions using a guidance scale parameter \u03b3 <Paper corpusId=\"260334572\" paperTitle=\"(Kornblith et al., 2023)\" isShortName></Paper>. This can be expressed as:\n\nf(x) = (1 - \u03b3)f_\u03b8(x) + \u03b3f_\u03b8(x, c)\n\nwhere f_\u03b8(x) represents the logits from the unconditional model (typically achieved by passing zeros or null tokens in place of conditioning information) and f_\u03b8(x, c) represents the logits from the conditional model <Paper corpusId=\"271328459\" paperTitle=\"(Lan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>.\n\nA notable advantage of CFG in autoregressive language models is that it can often be implemented \"out-of-the-box\" without the need for further model training <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. This contrasts with text-to-image models, where the model typically needs to be explicitly trained with condition dropout to support CFG. For pre-trained LLMs, the implementation often involves:\n\n1. Running the model twice in parallel: once with the conditioning prompt and once with a null or empty prompt\n2. Computing a weighted average of the resulting logit distributions\n3. Sampling the next token from this modified distribution <Paper corpusId=\"260334572\" paperTitle=\"(Kornblith et al., 2023)\" isShortName></Paper>\n\nZhang et al. expressed the CFG sampling process for language models mathematically as a modification of next-token prediction probabilities, where the likelihood of each token is adjusted based on both conditional and unconditional predictions <Paper corpusId=\"266053531\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This approach enables controllable text generation by manipulating the probability distribution from which tokens are sampled.\n\nThe guidance scale parameter \u03b3 (or \u03c9 in some formulations) is critical in controlling the degree to which generation adheres to the conditioning information <Paper corpusId=\"267069249\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. Higher values of \u03b3 push the model more strongly toward the conditional direction while moving away from the unconditional prediction <Paper corpusId=\"271161847\" paperTitle=\"(Jolicoeur-Martineau et al., 2024)\" isShortName></Paper>. In practice, \u03b3 values typically need tuning based on the specific task and desired output characteristics <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\nSome implementations have further refined this approach by introducing negative prompts within the unconditional branch, effectively guiding the generation away from undesired content <Paper corpusId=\"266053531\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This capability is particularly valuable for controlling harmful or undesirable outputs in language models.\n\nThe theoretical foundation of CFG in language models draws directly from the classifier-free guidance work in diffusion models <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. In both cases, the approach leverages the model's own understanding of the relationships between inputs and outputs, without requiring an external classifier <Paper corpusId=\"273142392\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Kornblith et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance (CFG) (Ho, 2022) aims to generate outputs that maximize or otherwise achieve high values of \n\nwhere proportionality holds because p(x|y)/p(x) = p(y|x)/p(y) and p(y) is fixed. The parameter \u03b3 is called the guidance scale and controls the trade-off between maximization of p(x|y) and p(y|x). When \u03b3 = 1, l \u03b8,\u03b3 (x, y) = p(x|y) and guidance has no effect. Setting \u03b3 > 1 inflates the probability of the image given the caption p(y|x) relative to the unconditional probability of the caption p(x). Ho and Salimans (Ho, 2022) originally proposed CFG in the context of diffusion models, which estimate the score functions \u2207 log p(x|y) and \u2207 log p(x). Although l \u03b8,\u03b3 (x, y) factorizes autoregressively, it is not a normalized probability distribution, so it is not entirely clear how one should sample tokens when performing autoregressive generation. Crowson [8] suggested to sample from \n\nwhere f \u03b8 (x n\u22121 , . . . , x 1 , 0) are logits generated by the model without conditioning, usually by passing zeros in place of the conditioning information. This formulation has been successfully applied in autoregressive image models [12]47]. In our experiments, we adopt this formulation as well, but since we decode greedily, i.e., at each step we take the token that maximizes q\u03b8,\u03b3 (x n |x n\u22121 , . . ."
                        ],
                        "paper": {
                            "corpus_id": 260334572,
                            "title": "Guiding image captioning models toward more specific captions",
                            "authors": [
                                {
                                    "authorId": "2135550613",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "2111917831",
                                    "name": "Lala Li"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2167669221",
                                    "name": "Thao Nguyen"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 15
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Lan et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) is a technique initially developed for diffusion models in generative modeling to enhance the quality and relevance of the outputs without the need for an external classifier.This approach involves training the generative model in both a conditional and an unconditional manner, combining the output score estimates from both methods during the inference stage.The mathematical expression is as\n\nHere, \u03b3 represents the guidance scale, which adjusts the influence of the conditioning information.We perform a weighted average of f \u03b8 (x, c) and f \u03b8 (x) when sampling from the output logits."
                        ],
                        "paper": {
                            "corpus_id": 271328459,
                            "title": "MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation",
                            "authors": [
                                {
                                    "authorId": "2312327383",
                                    "name": "Yun-Han Lan"
                                },
                                {
                                    "authorId": "37188394",
                                    "name": "Wen-Yi Hsiao"
                                },
                                {
                                    "authorId": "2312341186",
                                    "name": "Hao-Chung Cheng"
                                },
                                {
                                    "authorId": "2312339261",
                                    "name": "Yi-Hsuan Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Society for Music Information Retrieval Conference",
                            "n_citations": 9
                        },
                        "score": 0.947265625
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [20] enables a control on Diffusion Models' generation process without a conven-  tional classifier. Specifically, CFG's step-wise sampling allows users to employ a negative prompt within the unconditional branch, effectively guiding the generation away from harmful distributions. This approach has been extended to language models by LLM-CFG [21], allowing a controllable text generation and improved performance. However, LLM-CFG still requires a pair-wise prompt design and does not support partial token-level reweighting within the context, which is vital for controlling VLM's generation.\n\nIn conditioned Diffusion Models [42], given a noisy image x and a class condition c, the model predicts probability likelihood P, for the conditioned step-wise sample, P\u0398 (x|c) \u221d P \u0398 (x) \u2022 P \u03a6 (c|x) \u03b3 . Here, P \u03a6 is a classifier, and \u03b3 is the guidance strength controlling the weight of likelihood on c. Ho et al. [20] observed that guidance can be offered without a classifier. Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nLLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning."
                        ],
                        "paper": {
                            "corpus_id": 266053531,
                            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
                            "authors": [
                                {
                                    "authorId": "2145915052",
                                    "name": "Yuechen Zhang"
                                },
                                {
                                    "authorId": "152230789",
                                    "name": "Shengju Qian"
                                },
                                {
                                    "authorId": "2272493196",
                                    "name": "Bohao Peng"
                                },
                                {
                                    "authorId": "25059098",
                                    "name": "Shu Liu"
                                },
                                {
                                    "authorId": "2273012826",
                                    "name": "Jiaya Jia"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 24
                        },
                        "score": 0.96142578125
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Ho et al. [33] introduce the classifier-free guidance technique, blending both conditioned noise prediction \u03f5 \u03b8 (z t , t, \u03c4 (y)) and unconditioned noise prediction \u03f5 \u03b8 (z t , t, \u2205), to extrapolate one from another, \u03b5\u03b8 (z t , t, \u03c4 (y)) = \u03c9\u03f5 \u03b8 (z t , t, \u03c4 (y)) + (1 \u2212 \u03c9)\u03f5 \u03b8 (z t , t, \u2205). \n\n(\n\nwhere \u2205 is the embedding of a null text and \u03c9 is the guidance scale."
                        ],
                        "paper": {
                            "corpus_id": 267069249,
                            "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
                            "authors": [
                                {
                                    "authorId": "2152175923",
                                    "name": "Mingyuan Zhou"
                                },
                                {
                                    "authorId": "12212948",
                                    "name": "Rakib Hyder"
                                },
                                {
                                    "authorId": "2119019",
                                    "name": "Ziwei Xuan"
                                },
                                {
                                    "authorId": "2268757799",
                                    "name": "Guo-Jun Qi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 6
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Jolicoeur-Martineau et al., 2024)",
                        "snippets": [
                            "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain."
                        ],
                        "paper": {
                            "corpus_id": 271161847,
                            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
                            "authors": [
                                {
                                    "authorId": "2303409806",
                                    "name": "Alexia Jolicoeur-Martineau"
                                },
                                {
                                    "authorId": "14398916",
                                    "name": "A. Baratin"
                                },
                                {
                                    "authorId": "2311115257",
                                    "name": "Kisoo Kwon"
                                },
                                {
                                    "authorId": "49934559",
                                    "name": "Boris Knyazev"
                                },
                                {
                                    "authorId": "49889702",
                                    "name": "Yan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Gandikota et al., 2024)",
                        "snippets": [
                            "This approach requires no external classifier -we leverage the model's own understanding of concept relationships as encoded in its next-token predictions. Our perspective is inspired by the classifier-free guidance work (Ho & Salimans, 2022) in diffusion model research which has later been adopted to language models (Sanchez et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273098800,
                            "title": "Erasing Conceptual Knowledge from Language Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "2140009998",
                                    "name": "Sheridan Feucht"
                                },
                                {
                                    "authorId": "2225941937",
                                    "name": "Samuel Marks"
                                },
                                {
                                    "authorId": "2284996653",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.921875
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Practical implementations of conditional score estimation, such as classifier and classifier-free guidance methods, build upon ( 19 ) for reduced computational cost or better performance [47 ,104 ]. We begin with the classifier guidance method (Dhariwal et al., 2021) which is arguably the first method to allow conditional generation in diffusion models similar to GANs or flow models [105 ,106 ]. Specifically when conditional information y is discrete, e.g. image categories, the conditional score \u2207 log p t (x t | y ) is rewritten via Bayes' rule as \n\nwhere c t is the likelihood function of an external classifier. In other words, classifier guidance combines the unconditional score function with the gradient of an external classifier. The external classifier is trained using the diffused data points in the forward process. As a result, the performance of classifier guidance methods is sometimes limited, since it is difficult to train the external classifier with highly corrupted data. Later, classifier-free guidance proposes to remove the external classifier, circumventing the limitation caused by classifier training. The idea of classifierfree guidance is to introduce a mask signal to randomly ignore y and unify the learning of conditional and unconditional scores. Specifically, let \u03c4 \u2208 {\u2205 , id } be a mask signal, where \u2205 means to ignore the conditional information y and id to keep y with id y = y . For \u03c4 = \u2205 , we have \n\nand, for \u03c4 = id , we have \n\nNote that ( 21 ) coincides with ( 19 ), and recall that t 0 is an early stopping time. Combining the two cases, the classifier-free guidance method minimizes the loss function min \n\nwhere s (x t , \u03c4 y, t ) denotes a unified score network, i.e. s (x t , \u03c4 y, t ) = s (x t , t ) when \u03c4 = \u2205 and s (x t , \u03c4 y, t ) = s (x t , y, t ) when \u03c4 = id . Here \u03c4 is randomly chosen among \u2205 and id following distribution P \u03c4 . The simplistic choice on P \u03c4 is a uniform distribution on {\u2205 , id } , while it is preferred to bias towards setting \u03c4 = id more often in some applications [47 ]."
                        ],
                        "paper": {
                            "corpus_id": 273142392,
                            "title": "Opportunities and challenges of diffusion models for generative AI",
                            "authors": [
                                {
                                    "authorId": "2258803874",
                                    "name": "Minshuo Chen"
                                },
                                {
                                    "authorId": "2295990628",
                                    "name": "Song Mei"
                                },
                                {
                                    "authorId": "2296663941",
                                    "name": "Jianqing Fan"
                                },
                                {
                                    "authorId": "2258997595",
                                    "name": "Mengdi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "National Science Review",
                            "n_citations": 13
                        },
                        "score": 0.96533203125
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Implementation in Flow Matching Approaches",
                "tldr": "Classifier-free guidance has been adapted for flow matching models by jointly training conditional and unconditional vector fields through condition dropout during training. During inference, these vector fields are linearly combined using a guidance scale parameter to enhance sample quality and controllability in various sequence generation tasks. (7 sources)",
                "text": "\nFlow matching approaches represent an alternative framework to diffusion models and autoregressive models for generative tasks in NLP. The implementation of classifier-free guidance (CFG) in flow matching models follows similar principles to those used in diffusion models but operates on vector fields rather than score functions <Paper corpusId=\"276928343\" paperTitle=\"(Tuo et al., 2025)\" isShortName></Paper>. \n\nIn flow matching models, CFG is implemented by training models that can generate both conditional vector fields (when provided with conditioning information) and unconditional vector fields (when conditioning is masked or removed). This is typically achieved by randomly dropping out the conditioning information during training with a probability p_drop <Paper corpusId=\"276409208\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This dual training enables the model to learn two distinct vector fields for the same data distribution.\n\nDuring inference, the guided vector field is computed through linear blending of the conditional and unconditional vector fields:\n\nv_*(x, t, c; w) = v_*(x, t, \u2205) + w \u00b7 (v_*(x, t, c) - v_*(x, t, \u2205))\n\nwhere v_* represents the vector field, c indicates the full conditioning information, \u2205 represents the masked or reduced conditioning state, and w is the guidance scale parameter <Paper corpusId=\"276409208\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This formulation is analogous to the weighted score approach used in diffusion models, where the guidance scale w (or \u03c9 in some formulations) controls the degree of conditioning influence <Paper corpusId=\"276249479\" paperTitle=\"(Song et al., 2025)\" isShortName></Paper>.\n\nThe theoretical foundation for CFG in flow matching builds on an important relationship between flow fields and score functions. Tuo et al. demonstrated that a linear relationship exists between the score function and the Dirichlet flow (the marginal velocity field), allowing CFG techniques from diffusion models to be directly applied to flow matching approaches <Paper corpusId=\"276928343\" paperTitle=\"(Tuo et al., 2025)\" isShortName></Paper>. Similarly, Stark et al. derived a connection between mixture scores and flow vector fields that enables both classifier and classifier-free guidance in flow matching models <Paper corpusId=\"267547708\" paperTitle=\"(Stark et al., 2024)\" isShortName></Paper>.\n\nFlow matching with CFG has been particularly valuable for sequence generation tasks. Stark et al. introduced \"distilled Dirichlet flow matching,\" which enables one-step sequence generation with minimal performance degradation, resulting in significant speedups compared to autoregressive models while maintaining the benefits of CFG for improving generation quality <Paper corpusId=\"267547708\" paperTitle=\"(Stark et al., 2024)\" isShortName></Paper>. This approach has been successfully applied to DNA sequence generation with specific design targets.\n\nIn the context of speech synthesis, Wang et al. implemented CFG in their FELLE system through joint training of coarse and fine flow matching models using both conditional and unconditional objectives <Paper corpusId=\"276409208\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. Their implementation randomly masks the speech prompt during training to enable dual vector field learning.\n\nHowever, CFG in flow matching models can face challenges when the learned velocity fails to accurately capture the dataset's distribution. Fan et al. observed that when a model is underfitted, the conditional and unconditional predictions during sampling can create a mismatch, causing CFG to guide samples in directions that deviate from optimal trajectories <Paper corpusId=\"277510202\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>. This suggests that in some cases, modified CFG implementations might be needed to achieve optimal results.\n\nThe implementation of CFG in flow matching approaches builds upon the established techniques from diffusion models <Paper corpusId=\"267750410\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>, adapting them to the continuous flow framework while maintaining the core principle of linearly combining conditional and unconditional predictions to enhance generation quality and controllability.",
                "citations": [
                    {
                        "id": "(Tuo et al., 2025)",
                        "snippets": [
                            "To remove the need for a separate classifier model, Ho and Salimans [38] introduced classifier-free guidance, which linearly combines unconditional and conditional score models: \n\nIt can be shown that this formulation implicitly corresponds to a classifier P (c | x) = P (x, c; t) P (x; t). Substituting this implicit classifier back into Eq. ( 11) leads to \n\nwhich closely matches Eq. ( 12). Thus, classifier-free guidance implicitly steers the generative process by shifting the balance between conditional and unconditional scores, thereby increasing the likelihood of the desired class c without a separate classifier. In the context of flow matching, one can further show (see Appendix B) that a linear relationship exists between the score and the Dirichlet flow (i.e. the marginal velocity field in Eq. ( 6))."
                        ],
                        "paper": {
                            "corpus_id": 276928343,
                            "title": "Flow Matching for Discrete Systems: Efficient Free Energy Sampling Across Lattice Sizes and Temperatures",
                            "authors": [
                                {
                                    "authorId": "2349541183",
                                    "name": "Ping Tuo"
                                },
                                {
                                    "authorId": "2351208777",
                                    "name": "ZeZhu Zeng"
                                },
                                {
                                    "authorId": "2349541600",
                                    "name": "Jiale Chen"
                                },
                                {
                                    "authorId": "2313482115",
                                    "name": "Bingqing Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance (CFG) is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models [Ho andSalimans, 2022, Nichol and(Nichol et al., 2021). In FELLE, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives. During training, we randomly mask the speech prompt with probability p drop for unconditional learning, which enables each model to learn dual vector fields. At inference, guided vector fields are computed through linear blending: \n\nwhere * \u2208 {c, f } denotes the model stage, c represents the full conditions, c indicates the reduced conditioning state where the speaker prompt is masked, and w represents the guidance scale."
                        ],
                        "paper": {
                            "corpus_id": 276409208,
                            "title": "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching",
                            "authors": [
                                {
                                    "authorId": "2337542688",
                                    "name": "Hui Wang"
                                },
                                {
                                    "authorId": "2334437685",
                                    "name": "Shujie Liu"
                                },
                                {
                                    "authorId": "2308630482",
                                    "name": "Lingwei Meng"
                                },
                                {
                                    "authorId": "2280887661",
                                    "name": "Jinyu Li"
                                },
                                {
                                    "authorId": "2135622500",
                                    "name": "Yifan Yang"
                                },
                                {
                                    "authorId": "2275765829",
                                    "name": "Shiwan Zhao"
                                },
                                {
                                    "authorId": "2338021437",
                                    "name": "Haiyang Sun"
                                },
                                {
                                    "authorId": "2290439718",
                                    "name": "Yanqing Liu"
                                },
                                {
                                    "authorId": "2275584632",
                                    "name": "Haoqin Sun"
                                },
                                {
                                    "authorId": "2275780366",
                                    "name": "Jiaming Zhou"
                                },
                                {
                                    "authorId": "2337241715",
                                    "name": "Yan Lu"
                                },
                                {
                                    "authorId": "2275736194",
                                    "name": "Yong Qin"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.96484375
                    },
                    {
                        "id": "(Song et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning."
                        ],
                        "paper": {
                            "corpus_id": 276249479,
                            "title": "History-Guided Video Diffusion",
                            "authors": [
                                {
                                    "authorId": "2345339612",
                                    "name": "Kiwhan Song"
                                },
                                {
                                    "authorId": "8786274",
                                    "name": "Boyuan Chen"
                                },
                                {
                                    "authorId": "2243186471",
                                    "name": "Max Simchowitz"
                                },
                                {
                                    "authorId": "2344972384",
                                    "name": "Yilun Du"
                                },
                                {
                                    "authorId": "2263905014",
                                    "name": "Russ Tedrake"
                                },
                                {
                                    "authorId": "2280906248",
                                    "name": "Vincent Sitzmann"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Stark et al., 2024)",
                        "snippets": [
                            "In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models",
                            "we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets."
                        ],
                        "paper": {
                            "corpus_id": 267547708,
                            "title": "Dirichlet Flow Matching with Applications to DNA Sequence Design",
                            "authors": [
                                {
                                    "authorId": "2124211700",
                                    "name": "Hannes St\u00e4rk"
                                },
                                {
                                    "authorId": "2064253810",
                                    "name": "Bowen Jing"
                                },
                                {
                                    "authorId": "2283429637",
                                    "name": "Chenyu Wang"
                                },
                                {
                                    "authorId": "1630330616",
                                    "name": "Gabriele Corso"
                                },
                                {
                                    "authorId": "2271178326",
                                    "name": "Bonnie Berger"
                                },
                                {
                                    "authorId": "2254308896",
                                    "name": "R. Barzilay"
                                },
                                {
                                    "authorId": "35132120",
                                    "name": "T. Jaakkola"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 58
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "Next, classifier-free guidance (CFG) [9]44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results."
                        ],
                        "paper": {
                            "corpus_id": 277510202,
                            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
                            "authors": [
                                {
                                    "authorId": "2340406432",
                                    "name": "Weichen Fan"
                                },
                                {
                                    "authorId": "2323508574",
                                    "name": "Amber Yijia Zheng"
                                },
                                {
                                    "authorId": "2240073241",
                                    "name": "Raymond A. Yeh"
                                },
                                {
                                    "authorId": "2324070583",
                                    "name": "Ziwei Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Diffusion models, notably divided into classifierguided (Dhariwal et al., 2021) and classifier-free (Ho & Salimans, 2022) categories, have significantly impacted data generation across various domains. (Hoogeboom et al., 2022)Yang et al., 2023;Ho et al., 2022;(Voleti et al., 2022). The classifier-guided diffusion guides the model during inference phase by independently training a classifier and supervising the model with its gradient, which is inefficient when computing gradient at each time step and sometimes the generation quality is deficient as the guidance is not involved in the training. By contrast, classifier-free diffusion guidance blends score estimates from both a conditional diffusion model and an unconditional one with time step as a parameter, exemplified by E(3) Equivariant Diffusion Model (EDM) (Hoogeboom et al., 2022) and Motion Diffusion Model (MDM) (Tevet et al., 2022) for controllable molecule and motion generation, respectively."
                        ],
                        "paper": {
                            "corpus_id": 267750410,
                            "title": "Text2Data: Low-Resource Data Generation with Textual Control",
                            "authors": [
                                {
                                    "authorId": "2284725554",
                                    "name": "Shiyu Wang"
                                },
                                {
                                    "authorId": "2284730919",
                                    "name": "Yihao Feng"
                                },
                                {
                                    "authorId": "2284685690",
                                    "name": "Tian Lan"
                                },
                                {
                                    "authorId": "2211974297",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "2284676271",
                                    "name": "Yu Bai"
                                },
                                {
                                    "authorId": "2279097262",
                                    "name": "Ran Xu"
                                },
                                {
                                    "authorId": "2258793468",
                                    "name": "Huan Wang"
                                },
                                {
                                    "authorId": "2256976968",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "2238207181",
                                    "name": "Silvio Savarese"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Empirical Evaluation and Performance",
                "tldr": "Empirical evaluations of classifier-free guidance in NLP tasks have shown mixed but generally positive results, with significant improvements in question answering, reasoning, and translation tasks. Performance gains are task-dependent, with guidance strength requiring careful tuning and sometimes equivalent to using models with twice the parameter count. (7 sources)",
                "text": "\nThe empirical evaluation of classifier-free guidance (CFG) in language models has revealed varying degrees of effectiveness across different NLP tasks and model architectures. Sanchez et al. demonstrated that CFG substantially improves the performance of Pythia, GPT-2, and LLaMA-family models across a wide range of tasks including question answering, reasoning, code generation, and machine translation <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. Their work showed that CFG enabled LLaMA-7B to achieve state-of-the-art performance on the LAMBADA benchmark, outperforming even PaLM-540B, which has significantly more parameters <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\nOne of the most striking findings is that CFG can provide performance improvements equivalent to using a model with twice the parameter count, effectively doubling inference efficiency while maintaining comparable quality <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. This allows for training smaller models that can run on more modest hardware while achieving results comparable to larger models. Additionally, CFG has been shown to stack effectively with other inference-time techniques like Chain-of-Thought and Self-Consistency, yielding further improvements on difficult tasks <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\nIn speech translation tasks, CFG has demonstrated consistent benefits. Tan et al. found that non-autoregressive transformer models trained with CFG improved translation quality regardless of whether they used original or normalized speech units <Paper corpusId=\"269983348\" paperTitle=\"(Tan et al., 2024)\" isShortName></Paper>. Interestingly, they observed that CFG provided more substantial improvements for original units than normalized ones, suggesting that the regularization effect from CFG is more pronounced when the input data has greater variability <Paper corpusId=\"269983348\" paperTitle=\"(Tan et al., 2024)\" isShortName></Paper>.\n\nHuman evaluations have also confirmed CFG's effectiveness. In a user study conducted by Sanchez et al., there was a 75% preference for GPT4All using CFG over the baseline model when handling challenging form-driven and content-driven prompts <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. Similarly, GLIDE's implementation of CFG yielded more preferable results in both image photo-realism and textual alignment according to human evaluators <Paper corpusId=\"268264822\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>.\n\nHowever, not all implementations of CFG have shown significant improvements. Xu reported that their text generation model failed to improve substantially over a non-guided baseline, with only marginal BLEU score improvements on the validation set <Paper corpusId=\"252780170\" paperTitle=\"(Xu, 2022)\" isShortName></Paper>. This suggests that the effectiveness of CFG may depend on the specific model architecture, training procedure, and application domain.\n\nThe guidance strength parameter (w or \u03b3) has emerged as a critical factor affecting performance. Tan et al. found that small values like w = 0.5 or w = 1 empirically brought the most improvement in translation tasks, with these improvements becoming more noticeable as the number of decoding iterations increased <Paper corpusId=\"269983348\" paperTitle=\"(Tan et al., 2024)\" isShortName></Paper>. Jolicoeur-Martineau et al. used w = 1.5 as a general default value in their analyses, though they acknowledged that practitioners could tune this parameter for further performance gains <Paper corpusId=\"271161847\" paperTitle=\"(Jolicoeur-Martineau et al., 2024)\" isShortName></Paper>.\n\nAn important consideration in evaluating CFG is the timing and intensity of guidance during the sampling process. Dinh et al. discovered that applying strong guidance throughout the entire sampling process could be counterproductive due to \"model-fitting issues,\" where samples become overly tuned to match the classifier's parameters rather than generalizing the expected condition <Paper corpusId=\"271915699\" paperTitle=\"(Dinh et al., 2024)\" isShortName></Paper>. Their work demonstrated that reducing or excluding guidance at numerous timesteps could mitigate this issue <Paper corpusId=\"271915699\" paperTitle=\"(Dinh et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, CFG has been successfully incorporated into various tasks based on diffusion models, with Guo et al. highlighting the contribution of different guidance strengths to improving result quality during sampling <Paper corpusId=\"276107584\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper>. A limitation consistently noted across studies is that CFG requires careful tuning and exploration, as guidance values that work well in one context (such as long-form generation) might be poorly suited for another <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Tan et al., 2024)",
                        "snippets": [
                            "In this section, we compare non-autoregressive transformers trained with and without classifier-free guidance (CG). We train CMLM models with classifier-free guidance and evaluate them with 5, 10, 15 iterations of decoding. From Table 8, we observe that CG improves the quality of translation whether the model is trained on original or normalized speech units. We find CG brings more improvement on the original units than the normalized units. This happens because normalized speech units are more conformed and already result in a large improvement in their translation quality, making the regularization effect from CG less obvious. Nevertheless, the best-performing system is achieved with both DIFFNORM and CG. \n\nComparing different hyperparameters w, we find a small value like w = 0.5 or w = 1 brings the most improvement empirically, and such improvements are more noticeable when the number of decoding iterations is larger. For example, comparing the results under 5 and 15 iterations, we find w = 0 gives better results when the number of iterations is small while w = 0.5 obtains the best performance with 15 iterations."
                        ],
                        "paper": {
                            "corpus_id": 269983348,
                            "title": "DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation",
                            "authors": [
                                {
                                    "authorId": "28000727",
                                    "name": "Weiting Tan"
                                },
                                {
                                    "authorId": "2167508843",
                                    "name": "Jingyu (Jack) Zhang"
                                },
                                {
                                    "authorId": "2248152247",
                                    "name": "Lingfeng Shen"
                                },
                                {
                                    "authorId": "1783281",
                                    "name": "Daniel Khashabi"
                                },
                                {
                                    "authorId": "2264196844",
                                    "name": "Philipp Koehn"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 0
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "GLIDE additionally investigates classifier-free guidance (CFG) for comparison, which yields more preferable results in both image photo-realism and textual alignment by human evaluators and is chosen as the fundamental mechanism for text-to-image generation."
                        ],
                        "paper": {
                            "corpus_id": 268264822,
                            "title": "Controllable Generation with Text-to-Image Diffusion Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2203366502",
                                    "name": "Pu Cao"
                                },
                                {
                                    "authorId": "2273950538",
                                    "name": "Feng Zhou"
                                },
                                {
                                    "authorId": "2273931256",
                                    "name": "Qing Song"
                                },
                                {
                                    "authorId": "39702333",
                                    "name": "Lu Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 38
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Xu, 2022)",
                        "snippets": [
                            "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0."
                        ],
                        "paper": {
                            "corpus_id": 252780170,
                            "title": "CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning",
                            "authors": [
                                {
                                    "authorId": "2174547685",
                                    "name": "Shi-You Xu"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.8193359375
                    },
                    {
                        "id": "(Jolicoeur-Martineau et al., 2024)",
                        "snippets": [
                            "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain."
                        ],
                        "paper": {
                            "corpus_id": 271161847,
                            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
                            "authors": [
                                {
                                    "authorId": "2303409806",
                                    "name": "Alexia Jolicoeur-Martineau"
                                },
                                {
                                    "authorId": "14398916",
                                    "name": "A. Baratin"
                                },
                                {
                                    "authorId": "2311115257",
                                    "name": "Kisoo Kwon"
                                },
                                {
                                    "authorId": "49934559",
                                    "name": "Boris Knyazev"
                                },
                                {
                                    "authorId": "49889702",
                                    "name": "Yan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Dinh et al., 2024)",
                        "snippets": [
                            "We found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue."
                        ],
                        "paper": {
                            "corpus_id": 271915699,
                            "title": "Compress Guidance in Conditional Diffusion Sampling",
                            "authors": [
                                {
                                    "authorId": "2153473846",
                                    "name": "Anh-Dung Dinh"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Guo et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) [Ho and Salimans 2022] has been incorporated for various tasks (Peng et al., 2024)Zhang et al. 2024Zhang et al. , 2025b] ] based on diffusion models. As shown in Table12, strengths of classifier free guidance for each task are presented, which contributing to the results' quality during sampling."
                        ],
                        "paper": {
                            "corpus_id": 276107584,
                            "title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
                            "authors": [
                                {
                                    "authorId": "2112225563",
                                    "name": "Ziyan Guo"
                                },
                                {
                                    "authorId": "2343790499",
                                    "name": "Zeyu Hu"
                                },
                                {
                                    "authorId": "2343735032",
                                    "name": "Na Zhao"
                                },
                                {
                                    "authorId": "2258965291",
                                    "name": "De Wen Soh"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.91162109375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Advantages of Classifier-Free Guidance in NLP",
                "tldr": "Classifier-free guidance offers significant advantages in NLP applications, including improved performance across diverse tasks without model retraining, computational efficiency equivalent to using models with twice the parameter count, and enhanced controllability of generated outputs. (5 sources)",
                "text": "\n- **No Need for Separate Classifier Models**: CFG eliminates the requirement to train additional classifier models, simplifying the guidance process while leveraging the generative model's own knowledge for better directional control <Paper corpusId=\"257102889\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>.\n\n- **Works \"Out-of-the-Box\" for Language Models**: Unlike text-to-image applications, CFG can be implemented in autoregressive language models without additional training, making it immediately applicable to existing pre-trained models <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Significant Performance Improvements**: CFG has been shown to enhance performance across a wide range of NLP tasks including question answering, reasoning, code generation, and machine translation, achieving state-of-the-art results in some benchmarks <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Computational Efficiency**: Implementing CFG provides performance improvements equivalent to using a model with twice the parameter count, effectively doubling inference efficiency while maintaining comparable quality. This allows for training smaller models that can run on more modest hardware with lower training costs <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Compatible with Other Inference Techniques**: CFG can be effectively combined with other inference-time methods such as Chain-of-Thought and Self-Consistency, yielding further improvements particularly on difficult tasks <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Flexible Conditioning Control**: The guidance weight parameter (w) allows practitioners to control the degree of conditioning influence, offering a tunable mechanism to balance between unconditional generation and strict adherence to conditioning information <Paper corpusId=\"259095911\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271161847\" paperTitle=\"(Jolicoeur-Martineau et al., 2024)\" isShortName></Paper>.\n\n- **Enhanced Cross-Modal Guidance**: For multimodal applications, CFG strengthens directional guidance between different modalities (such as text and images), improving alignment between generated content and conditioning information <Paper corpusId=\"257102889\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n- **Improved Handling of Complex Conditioning**: CFG is particularly effective when dealing with conditioning information that would be difficult to predict using a traditional classifier, making it suitable for complex NLP tasks with nuanced conditioning requirements <Paper corpusId=\"257102889\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n- **Increased Output Faithfulness and Coherence**: Human evaluations have demonstrated a strong preference for outputs generated using CFG, with a 75% preference rate for assistant responses on challenging form-driven and content-driven prompts <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance (Ho, 2022) to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier."
                        ],
                        "paper": {
                            "corpus_id": 257102889,
                            "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
                            "authors": [
                                {
                                    "authorId": "2186281333",
                                    "name": "Nisha Huang"
                                },
                                {
                                    "authorId": "1443761295",
                                    "name": "Fan Tang"
                                },
                                {
                                    "authorId": "40441149",
                                    "name": "Weiming Dong"
                                },
                                {
                                    "authorId": "39945992",
                                    "name": "Tong-Yee Lee"
                                },
                                {
                                    "authorId": "2155590336",
                                    "name": "Changsheng Xu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 26
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "To avoid the extra classifier, classifier-free guidance replaces the classifier by a mixture of unconditional model and vanilla guidance. It encourages the model in the direction of guidance and simultaneously discourages away from unconditional direction [196]. As shown in Figure 13, instead of training two models, a conditional model and an unconditional one are formulated uniformly by dropping out conditions c with a probability p [172]. The two models are learned jointly as if they were a singular conditional model [24]. In particular, classifier-free guidance is formulated as: \n\nwhere w is the weight of conditions. The weight is slightly different from its counterpart in classifier guidance. When w = 0, the classifier-free guidance becomes unconditional models without vanilla guidance. The vanilla guidance is a special case when w = 1. In this case, the unconditional model is suppressed and conditions are incorporated through vanilla guidance [38]. If w > 1, the classifier-free guidance restrains the unconditional model and prioritizes conditions further by larger weights. The score from classifier-free guidance deviates quickly away from the unconditional score, and thus, samples that better satisfy the conditions will be generated [197]."
                        ],
                        "paper": {
                            "corpus_id": 259095911,
                            "title": "On the Design Fundamentals of Diffusion Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2186402069",
                                    "name": "Ziyi Chang"
                                },
                                {
                                    "authorId": "2872629",
                                    "name": "G. Koulieris"
                                },
                                {
                                    "authorId": "1993718816",
                                    "name": "Hubert P. H. Shum"
                                }
                            ],
                            "year": 2023,
                            "venue": "Pattern Recognition",
                            "n_citations": 56
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Jolicoeur-Martineau et al., 2024)",
                        "snippets": [
                            "To enforce better conditioning of the properties, we use classifier-free guidance, originally designed for diffusion models (Ho & Salimans, 2022), and found beneficial for autoregressive language models as well (Sanchez et al., 2023). This technique involves directing the model more toward the conditional model's direction while pushing it away from the unconditional model's direction by an equal amount. Figure 2 illustrates this concept. The amount of guidance typically requires hyperparameter-tuning. However, for simplicity and generality, in all analyses, we arbitrarily set the guidance parameter w to 1.5, where w = 1 means no guidance. Practitioners can tune the guidance for further performance gain."
                        ],
                        "paper": {
                            "corpus_id": 271161847,
                            "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
                            "authors": [
                                {
                                    "authorId": "2303409806",
                                    "name": "Alexia Jolicoeur-Martineau"
                                },
                                {
                                    "authorId": "14398916",
                                    "name": "A. Baratin"
                                },
                                {
                                    "authorId": "2311115257",
                                    "name": "Kisoo Kwon"
                                },
                                {
                                    "authorId": "49934559",
                                    "name": "Boris Knyazev"
                                },
                                {
                                    "authorId": "49889702",
                                    "name": "Yan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92578125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Challenges",
                "tldr": "Despite its benefits, classifier-free guidance in NLP faces significant limitations including computational overhead from requiring dual forward passes, challenges in tuning guidance parameters, and issues with model underfitting that can lead to suboptimal results. (8 sources)",
                "text": "\n- **Computational Overhead**: CFG requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational requirements during inference, which limits its applicability in real-time scenarios <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n- **Parameter Tuning Challenges**: The guidance strength parameter requires careful tuning and exploration, as values that work well in one context (such as long-form generation) might be poorly suited for another context <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Fixed Attributes During Training**: A fundamental limitation of CFG is that the attributes used for guidance must be fixed during the training process, limiting flexibility when new guidance criteria are needed post-training <Paper corpusId=\"272593086\" paperTitle=\"(Kerby et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>.\n\n- **Inconsistent Performance Across Applications**: While CFG has shown significant improvements in some NLP tasks, it has failed to deliver substantial benefits in others. Some text generation models showed only marginal BLEU score improvements over non-guided baselines <Paper corpusId=\"252780170\" paperTitle=\"(Xu, 2022)\" isShortName></Paper>.\n\n- **Model-Fitting Issues**: Applying strong guidance throughout the entire sampling process can be counterproductive due to \"model-fitting issues,\" where samples become overly tuned to match the classifier's parameters rather than generalizing the expected condition <Paper corpusId=\"271915699\" paperTitle=\"(Dinh et al., 2024)\" isShortName></Paper>.\n\n- **Sensitivity to Underfitting**: When a model is underfitted, CFG can guide samples in directions that significantly deviate from optimal trajectories. The mismatch between conditional and unconditional predictions during sampling can cause the estimated velocity to contradict the optimal velocity, particularly in the first step of generation <Paper corpusId=\"277510202\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\n- **Training Instability with Higher Guidance Weights**: Implementing higher guidance weights during training (w \u2265 1) can lead to model collapse or prevent normal inference, resulting in noise output instead of meaningful content <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n- **Integration Complexity**: Unifying conditional and unconditional models requires careful implementation of condition dropout during training, necessitating additional model components like learned \"null\" embeddings <Paper corpusId=\"276647345\" paperTitle=\"(Zhu et al., 2025)\" isShortName></Paper>.\n\n- **Balance Between Fidelity and Diversity**: Achieving the optimal balance between output fidelity and diversity through the guidance coefficient requires experimentation and may vary depending on the specific task and domain <Paper corpusId=\"276647345\" paperTitle=\"(Zhu et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liang et al., 2025)",
                        "snippets": [
                            "However, CFG incurs substantial computational cost as it requires two forward passes, which hinders its applicability in real-time scenarios",
                            "another core factor affecting inference efficiency is the widespread use of classifier-free guidance (CFG), which requires performing both conditional and unconditional inference for each sampling step, effectively doubling the computational overhead during inference. In the field of image generation, Tang et al. [20] proposed a model-guidance training approach that enables diffusion models to remove the need for CFG at inference time",
                            "When w is slightly increased, model training tends to collapse. Based on empirical values from Classifier-Free Guidance (CFG) reasoning, we aimed to set w close to the target value of 2, which is used in the F5-TTS inference process. However, during actual training, we found that setting w \u2265 1 for training, the model training loss will not collapse, but it will lead to the model can't inference normally, and the inference results are all noise, the analysis is that if we don't use stop gradient, the vt(x0) gradient back-propagation will cause the model to learn the value of a trival."
                        ],
                        "paper": {
                            "corpus_id": 278171703,
                            "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2278618341",
                                    "name": "Yuzhe Liang"
                                },
                                {
                                    "authorId": "2358111037",
                                    "name": "Wenzhe Liu"
                                },
                                {
                                    "authorId": "2358041541",
                                    "name": "Chunyu Qiang"
                                },
                                {
                                    "authorId": "2229877177",
                                    "name": "Zhikang Niu"
                                },
                                {
                                    "authorId": "2324996330",
                                    "name": "Yushen Chen"
                                },
                                {
                                    "authorId": "2116609277",
                                    "name": "Ziyang Ma"
                                },
                                {
                                    "authorId": "2278584538",
                                    "name": "Wenxi Chen"
                                },
                                {
                                    "authorId": "2358116915",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2358098456",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "2321881822",
                                    "name": "Xie Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.94580078125
                    },
                    {
                        "id": "(Sanchez et al., 2023)",
                        "snippets": [
                            "In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.\n\nIn contrast to text-to-vision, CFG in autoregressive language modeling works out-of-the-box, without the need to further train the model.\n\nOur work also integrates into a growing body of inference techniques aimed at perturbing the logit distributions of an LM [45]73]. We demonstrate that by doubling the inference FLOP using CFG brings performances of a model about twice the size. This allows training smaller models, which can be ran on smaller hardware, and are cheaper to train.\n\nOur work faces the following limitations: CFG requires tweaking and exploration: \u03b3 values that might work in one context (i.e. long-form generation) might be poorly suited for another context."
                        ],
                        "paper": {
                            "corpus_id": 259308807,
                            "title": "Stay on topic with Classifier-Free Guidance",
                            "authors": [
                                {
                                    "authorId": "2056723344",
                                    "name": "Guillaume Sanchez"
                                },
                                {
                                    "authorId": "2072838294",
                                    "name": "Honglu Fan"
                                },
                                {
                                    "authorId": "51444076",
                                    "name": "Alexander Spangher"
                                },
                                {
                                    "authorId": "34490455",
                                    "name": "Elad Levi"
                                },
                                {
                                    "authorId": "1451644426",
                                    "name": "Pawan Sasanka Ammanamanchi"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 55
                        },
                        "score": 0.95947265625
                    },
                    {
                        "id": "(Kerby et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation (Ho, 2022). During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training."
                        ],
                        "paper": {
                            "corpus_id": 272593086,
                            "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
                            "authors": [
                                {
                                    "authorId": "2243157079",
                                    "name": "Thomas Kerby"
                                },
                                {
                                    "authorId": "2243003711",
                                    "name": "Kevin R. Moon"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu, 2022)",
                        "snippets": [
                            "The effectiveness of applying classification-free guidance is examined in our model. The guidance provided is the CLIP text feature of ground truth label caption. Table 1 shows the comparison between the baseline and classification-free guidance-trained model. In contrast to the previous success of applying classification-free guidance, our model fails to improve significantly over none classification guided baseline. We further test the 2 classification-free-guided models' performance by evaluating BLEU score on the validation set. The model trained using classifier-free guidance hardly outperform the simpler baseline model, with the parameter suggested by Jonathan and Tim [32] (w = 0."
                        ],
                        "paper": {
                            "corpus_id": 252780170,
                            "title": "CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning",
                            "authors": [
                                {
                                    "authorId": "2174547685",
                                    "name": "Shi-You Xu"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.8193359375
                    },
                    {
                        "id": "(Dinh et al., 2024)",
                        "snippets": [
                            "We found that enforcing guidance throughout the sampling process is often counterproductive due to the model-fitting issue, where samples are 'tuned' to match the classifier's parameters rather than generalizing the expected condition. This work identifies and quantifies the problem, demonstrating that reducing or excluding guidance at numerous timesteps can mitigate this issue."
                        ],
                        "paper": {
                            "corpus_id": 271915699,
                            "title": "Compress Guidance in Conditional Diffusion Sampling",
                            "authors": [
                                {
                                    "authorId": "2153473846",
                                    "name": "Anh-Dung Dinh"
                                },
                                {
                                    "authorId": "51023221",
                                    "name": "Daochang Liu"
                                },
                                {
                                    "authorId": "2288626806",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Fan et al., 2025)",
                        "snippets": [
                            "Next, classifier-free guidance (CFG) [9]44] is a widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the user's \"intended\" conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the user's interpretation of a text prompt and its corresponding image differs from the dataset distribution. Second, it could result from a learning limitation, where the learned velocity fails to accurately capture the dataset's distribution. In this work, we focus on the latter issue. When the model is underfitted, a mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in a direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x 0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results."
                        ],
                        "paper": {
                            "corpus_id": 277510202,
                            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
                            "authors": [
                                {
                                    "authorId": "2340406432",
                                    "name": "Weichen Fan"
                                },
                                {
                                    "authorId": "2323508574",
                                    "name": "Amber Yijia Zheng"
                                },
                                {
                                    "authorId": "2240073241",
                                    "name": "Raymond A. Yeh"
                                },
                                {
                                    "authorId": "2324070583",
                                    "name": "Ziwei Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Zhu et al., 2025)",
                        "snippets": [
                            "Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9."
                        ],
                        "paper": {
                            "corpus_id": 276647345,
                            "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model",
                            "authors": [
                                {
                                    "authorId": "2347677379",
                                    "name": "Yimin Zhu"
                                },
                                {
                                    "authorId": "2347797817",
                                    "name": "Linlin Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92041015625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.185487
    }
}